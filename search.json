[{"title":"golang项目引用GitHub私有仓库module","url":"/golang项目引用GitHub私有仓库module.html","content":"## 1.创建go module项目\n\n<img src=\"/images/517519-20241019214247291-2081870372.png\" width=\"600\" height=\"581\" loading=\"lazy\" />\n\nmodule的名字假设为go-test\n\n<img src=\"/images/517519-20241019214747049-1769269194.png\" width=\"500\" height=\"434\" loading=\"lazy\" />\n\nmodule项目创建成功后，将go.mod文件中的 module \"go-test\" 修改成\n\n```\nmodule \"github.com/tonglin0325/go-test\"\n\n```\n\n避免引用的时候go get报错，如下\n\n```\ngo get github.com/tonglin0325/go-test@latest\ngo: github.com/tonglin0325/go-test@latest (v1.0.0) requires github.com/tonglin0325/go-test@v1.0.0: parsing go.mod:\n\tmodule declares its path as: go-test\n\t        but was required as: github.com/tonglin0325/go-test\n\n```\n\n在module中写好代码后上传到github上，并在github上给项目打上tag\n\n<img src=\"/images/517519-20241019221052393-373242894.png\" width=\"700\" height=\"597\" loading=\"lazy\" />\n\n如果GitHub repository设置成public的话，此时就可以使用如下命令引用这个module了\n\n```\n➜  /Users/lintong/coding/go/gin-template git:(main) ✗ $ go get github.com/tonglin0325/go-test@latest\ngo: downloading github.com/tonglin0325/go-test v1.0.1\ngo: added github.com/tonglin0325/go-test v1.0.1\n\n```\n\n## 2.引用私有仓库的go module\n\n如果电脑配置了github的私钥的话，可以直接使用go get命令来引用私有仓库的依赖\n\n如果没有设置的话，则可以使用github的token来下载依赖\n\n在github用户settings的Developer settings中创建一个token，token name假设为goland\n\n<img src=\"/images/517519-20241019223454556-1656569630.png\" width=\"400\" height=\"202\" loading=\"lazy\" />\n\n指定go module的项目，并设置contents设置为read only\n\n<img src=\"/images/517519-20241019224009494-1573477091.png\" width=\"500\" height=\"199\" loading=\"lazy\" />\n\n创建后获得token\n\n重新设置github使用账号+token来拉取依赖\n\n```\ngit config --global url.\"https://tonglin0325:your_token@github.com\".insteadOf \"https://github.com\"\n\n```\n\n查看git配置是否添加成功\n\n```\ngit config --global -l\n\n```\n\n如果添加错误可以unset之后再重新set\n\n```\ngit config --global --unset xxx\n\n```\n\n指定拉取go-test依赖的时候使用私有仓库\n\n```\ngo env -w GOPRIVATE=github.com/tonglin0325/go-test\n\n```\n\n查看go环境变量\n\n```\ngo env\n\n```\n\n如果设置失败，可以删除后再添加\n\n```\ngo env -u GOPRIVATE\n\n```\n\n最后使用go get命令引用依赖\n\n```\ngo get github.com/tonglin0325/go-test@latest\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["golang"]},{"title":"k8s学习笔记——Istio和Kong","url":"/k8s学习笔记——Istio和Kong.html","content":"**<strong>服务网格（Service Mesh）**是一种用于处理微服务架构中服务间通信的基础设施层。它的主要作用是提供可靠的服务发现、负载均衡、故障恢复、指标监控和安全性，通常无需对服务代码进行大量修改。服务网格通过在每个服务实例旁边部署一个轻量级代理（sidecar）来实现这些功能。</strong>\n\n## 1.Istio\n\n**Istio** 是一个开源的服务网格（Service Mesh），透明地层叠到现有的分布式应用程序之上。Istio 强大的功能提供了一种统一且更高效的方式来保护、连接和监控服务。Istio 是实现负载均衡、服务间身份验证和监控的途径&mdash;&mdash;几乎无需修改服务代码。它为你提供：\n\n- 集群中服务间的安全通信，采用双向 TLS 加密，基于强身份的身份验证和授权\n- HTTP、gRPC、WebSocket 和 TCP 流量的自动负载均衡\n- 通过丰富的路由规则、重试、故障切换和故障注入对流量行为进行细粒度控制\n- 支持访问控制、速率限制和配额的可插拔策略层和配置 API\n- 集群内所有流量（包括集群入口和出口）的自动化指标、日志和跟踪\n\n参考：[https://istio.io/latest/docs/overview/what-is-istio/](https://istio.io/latest/docs/overview/what-is-istio/)\n\nIstio 的流量管理模型源于和服务一起部署的 Envoy 代理。 网格内服务发送和接收的所有 data plane 流量都经由 Envoy 代理， 这让控制网格内的流量变得异常简单，而且不需要对服务做任何的更改。\n\n**Envoy** 是在 Istio 里使用的高性能代理，用于为所有服务网格里的服务调度进出的流量。\n\n参考：[https://istio.io/latest/zh/docs/concepts/traffic-management/](https://istio.io/latest/zh/docs/concepts/traffic-management/)\n\n## 2.Kong\n\nKong 提供了两个主要产品：Kong Gateway 和 Kuma。\n\n**Kong Gateway** 是一个云原生 API 网关，专注于管理、配置和路由 API 请求。它提供了强大的插件系统，可以扩展其功能以实现身份验证、速率限制、日志记录等功能。Kong Gateway 主要用于处理进出 API 的流量，但它不具备完整的服务网格功能。\n\n**Kuma** 是 Kong 开发的一个服务网格，旨在简化服务间通信的管理。Kuma 提供了服务网格的所有核心功能，包括服务发现、负载均衡、故障恢复、可观察性和安全性。Kuma 可以部署在多个环境中，包括 Kubernetes 和虚拟机。\n","tags":["k8s"]},{"title":"k8s学习笔记——ingress","url":"/k8s学习笔记——ingress.html","content":"**Ingress** 提供从集群外部到集群内**服务（service）**的 HTTP 和 HTTPS 路由。 流量路由由 Ingress 资源所定义的规则来控制。\n\n下面是 Ingress 的一个简单示例，可将所有流量都发送到同一 Service：\n\n<img src=\"/images/ingress.svg\" alt=\"ingress-diagram\" />\n\n通过配置，Ingress 可为 Service 提供外部可访问的 URL、对其流量作负载均衡、 终止 SSL/TLS，以及基于名称的虚拟托管等能力。 **Ingress 控制器** 负责完成 Ingress 的工作，具体实现上通常会使用某个负载均衡器， 不过也可以配置边缘路由器或其他前端来帮助处理流量。\n\n参考：[https://kubernetes.io/zh-cn/docs/concepts/services-networking/ingress/](https://kubernetes.io/zh-cn/docs/concepts/services-networking/ingress/)\n\n典型的Ingress 控制器有 [ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/) ，[Istio Ingress](https://istio.io/latest/zh/docs/tasks/traffic-management/ingress/kubernetes-ingress/)（一个基于 [Istio](https://istio.io/zh/) 的 Ingress 控制器），[用于 Kubernetes 的 Kong Ingress 控制器](https://github.com/Kong/kubernetes-ingress-controller#readme)（一个用来驱动 [Kong Gateway](https://konghq.com/kong/) 的 Ingress 控制器）\n\n参考：[https://kubernetes.io/zh-cn/docs/concepts/services-networking/ingress-controllers/](https://kubernetes.io/zh-cn/docs/concepts/services-networking/ingress-controllers/)\n","tags":["k8s"]},{"title":"Presto学习笔记——Go客户端连接Presto","url":"/Presto学习笔记——Go客户端连接Presto.html","content":"## 1.查询PrestoDB（facebook版本）\n\n### 1.创建PrestoDB环境\n\n使用docker创建presto测试环境\n\n```\nhttps://hub.docker.com/r/prestodb/presto/tags\n\n```\n\n拉取镜像\n\n```\ndocker pull prestodb/presto:0.284\n\n```\n\n启动\n\n```\ndocker run -p 8080:8080 -ti -v /Users/lintong/Downloads/config.properties:/opt/presto-server/etc/config.properties -v /Users/lintong/Downloads/jvm.config:/opt/presto-server/etc/jvm.config prestodb/presto:0.284\n\n```\n\n其中config.properties配置文件\n\n```\ncoordinator=true\nnode-scheduler.include-coordinator=true\nhttp-server.http.port=8080\nquery.max-memory=5GB\nquery.max-memory-per-node=1GB\ndiscovery-server.enabled=true\ndiscovery.uri=http://localhost:8080\n\n```\n\njvm.config\n\n```\n-server\n-Xmx4G\n-XX:+UseG1GC\n-XX:G1HeapRegionSize=32M\n-XX:+UseGCOverheadLimit\n-XX:+ExplicitGCInvokesConcurrent\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:+ExitOnOutOfMemoryError\n-Djdk.attach.allowAttachSelf=true\n\n```\n\n配置参考：[https://github.com/prestodb/presto/tree/master/docker/etc](https://github.com/prestodb/presto/tree/master/docker/etc)\n\n[https://prestodb.github.io/docs/current/installation/deployment.html](https://prestodb.github.io/docs/current/installation/deployment.html)\n\n访问localhost:8080\n\n<img src=\"/images/517519-20240107224957284-852123923.png\" width=\"700\" height=\"336\" />\n\n### 2.使用client连接PrestoDB\n\n可以使用presto-go-client，官方文档\n\n```\nhttps://github.com/prestodb/presto-go-client\n\n```\n\n引入依赖\n\n```\ngo get github.com/prestodb/presto-go-client/presto\n\n```\n\ngo连接数据库使用的是database/sql，还需要额外引入具体数据库的driver，比如\n\npresto的github.com/prestodb/presto-go-client/presto\n\nmysql的github.com/go-sql-driver/mysql\n\nimpala的github.com/bippio/go-impala\n\n查询presto代码\n\n```\nimport (\n\t\"database/sql\"\n\t\"fmt\"\n\t_ \"github.com/prestodb/presto-go-client/presto\"\n)\n\nfunc main() {\n\tdsn := \"http://user@localhost:8080?catalog=default&amp;schema=test\"\n\tdb, err := sql.Open(\"presto\", dsn)\n\tif err != nil {\n\t\tfmt.Println(err)\n\t}\n\n\trows, err := db.Query(\"SELECT name, age FROM foobar WHERE id=?\")\n\tif err != nil {\n\t\tfmt.Println(err)\n \t}\n\tdefer func() {\n\t\t_ = db.Close()\n\t}()\n\t// 迭代结果行\n\tfor rows.Next() {\n\t\tvar col1 string\n\t\tvar col2 int\n\t\tif err := rows.Scan(&amp;col1, &amp;col2); err != nil {\n\t\t\tfmt.Println(\"扫描行失败：\", err)\n\t\t\treturn\n\t\t}\n\t\tfmt.Println(col1, col2)\n\t}\n}\n\n```\n\n参考：[golang+presto查询在数据平台中ad hoc查询](https://www.jianshu.com/p/025fed13bfbc)\n\n如果想同时查询多行的话，也可以先定义一个struct\n\n```\ntype YourTable struct {\n　　Col1 string\n　　Col2 int\n　　Col3 float64\n　　// 其他列...\n}\n\n```\n\n然后在迭代查询的时候使用这个struct\n\n```\n// 迭代结果行\nfor rows.Next() {\n　　var row YourTable\n　　if err := rows.Scan(&amp;row.Col1, &amp;row.Col2, &amp;row.Col3); err != nil {\n　　　　fmt.Println(\"扫描行失败：\", err)\n　　　　return\n　　}\n　　fmt.Println(row)\n}\n\n```\n\n### 3.使用REST API连接PrestoDB\n\n参考官方文档\n\n```\nhttps://prestodb.io/docs/current/develop/client-protocol.html\n\n```\n\n其实presto-go-client底层也是使用presto的REST API的POST请求来提交任务，参考源码\n\n```\nhttps://github.com/prestodb/presto-go-client/tree/master/presto#L635\n\n```\n\n## 2.查询Trino（社区版本）\n\n### 1.创建Trino环境\n\n创建Trino环境用于测试可以使用docker\n\n```\nhttps://hub.docker.com/r/trinodb/trino/tags\n\n```\n\n拉取镜像\n\n```\ndocker pull trinodb/trino:435\n\n```\n\n启动\n\n```\ndocker run -p 8080:8080 -ti trinodb/trino:435\n\n```\n\n访问localhost:8080，默认密码不启用，随便输入一个用户名\n\n<img src=\"/images/517519-20240107225450906-279955707.png\" width=\"300\" height=\"211\" />\n\n界面\n\n<img src=\"/images/517519-20240107225553783-1783863032.png\" width=\"700\" height=\"364\" />\n\n### 2.使用client连接trino\n\n可以使用trino-go-client，官方文档\n\n```\nhttps://github.com/trinodb/trino-go-client\n\n```\n\n引入依赖\n\n```\ngo get github.com/trinodb/trino-go-client/trino\n\n```\n\n查询trino代码\n\n```\nimport (\n\t\"database/sql\"\n\t\"fmt\"\n)\nimport _ \"github.com/trinodb/trino-go-client/trino\"\n\nfunc main() {\n\tdsn := \"http://user@localhost:8080?catalog=default&amp;schema=test\"\n\tdb, err := sql.Open(\"trino\", dsn)\n\tif err != nil {\n\t\tfmt.Println(err)\n\t}\n\tdefer func() {\n\t\t_ = db.Close()\n\t}()\n\tdb.Query(\"SELECT * FROM foobar WHERE id=?\", 1, sql.Named(\"X-Trino-User\", string(\"Alice\")))\n}\n\n```\n\n### 3.使用REST API连接TrinoDB\n\n参考官方文档\n\n```\nhttps://trino.io/docs/current/develop/client-protocol.html\n\n```\n\n其实trino-go-client底层也是使用trino的REST API的POST请求来提交任务，参考源码\n\n```\nhttps://github.com/trinodb/trino-go-client/blob/master/trino/trino.go#L820\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["presto"]},{"title":"go学习笔记——wire依赖注入","url":"/go学习笔记——wire依赖注入.html","content":"wire是google开源的使用依赖注入来自动连接组件的代码生成工具\n\n安装\n\n```\ngo install github.com/google/wire/cmd/wire@latest\n\n```\n\n官方使用文档：\n\n[https://github.com/google/wire/blob/main/docs/guide.md](https://github.com/google/wire/blob/main/docs/guide.md)\n\n文档参考：\n\n[手把手，带你从零封装Gin框架（十二）：使用 Wire 依赖注入重构](https://juejin.cn/post/7101966893522616356)\n\n[golang中的依赖注入之wire](https://juejin.cn/post/7124694553667305485)\n\n参考项目：\n\n[https://github.com/jassue/gin-wire](https://github.com/jassue/gin-wire)\n\n如果遇到下面报错\n\n```\nwire: /xxx/cmd/server/wire.go:17:1: inject wireApp: unused provider set \"ProviderSet\"\nwire: xx/cmd/server: generate failed\nwire: at least one generate failure\n\n```\n\n这是因为设置了依赖注入的方法没能找到调用者，在gin项目中，调用的顺序一般是data层->service层->handler层->router层->httpserver->app\n\n所以需要把调用的ProviderSet在wire.go文件中完整的写出来，不能出现中间中断的情况，wire_gen才能正常的生成\n\n比如写了data，service，router，httpserver和app，但是漏了handler，这样就会报上面的错误\n\n如果只写了router，httpserver和app，这样是可以正常生成的\n\n可以参考：[https://github.com/jassue/gin-wire/blob/main/cmd/app/wire_gen.go](https://github.com/jassue/gin-wire/blob/main/cmd/app/wire_gen.go) 中的调用层次\n\n<img src=\"/images/517519-20240303155002954-1768762066.png\" width=\"800\" height=\"550\" />\n\n<!--more-->\n&nbsp;\n","tags":["golang"]},{"title":"screen使用教程","url":"/screen使用教程.html","content":"在terminal上使用跳板机远程登录其他机器的时候，经常会因为和跳板机的连接断开而丢失会话，如下\n\n<img src=\"/images/517519-20230322234309773-711501839.png\" width=\"400\" height=\"61\" />\n\n这时候可以使用screen命令来创建和恢复会话\n\n1.创建会话\n\n```\nscreen \n或者\nscreen -S session_name\n\n```\n\n这时我们就进到了一个screen会话中，比如我们进到/tmp目录下\n\n<img src=\"/images/517519-20230322233429641-420003311.png\" width=\"600\" height=\"100\" />\n\n2.查看现有的会话\n\n这时候我们把这个terminal窗口直接关闭掉，另外开启一个terminal，输入如下命令来查看现有的会话\n\n```\nscreen -ls\n\n```\n\n会发现当前的会话仍然存在，处于分离状态\n\n<img src=\"/images/517519-20230322233647113-2032972234.png\" width=\"500\" height=\"78\" />\n\n3.恢复会话\n\n```\nscreen -r session_name\n```\n\n<img src=\"/images/517519-20230322233906904-1771483351.png\" width=\"400\" height=\"41\" />\n\n会发现我们刚才的/tmp目录的会话就被恢复了\n\n<img src=\"/images/517519-20230322234007471-1708114574.png\" width=\"500\" height=\"80\" />\n\n参考：[Linux--screen远程必备 #screen常用命令-会话的创建、恢复、删除(&amp;重命名删除)用法](https://blog.csdn.net/qq_34243930/article/details/106771285)\n","tags":["Linux"]},{"title":"go学习笔记——常用命令","url":"/go学习笔记——常用命令.html","content":"## 1.查找go依赖\n\ngo依赖可以去下面网站查找package\n\n```\nhttps://pkg.go.dev/\n\n```\n\n比如\n\n```\nhttps://pkg.go.dev/github.com/confluentinc/confluent-kafka-go#section-readme\n\n```\n\n## 2.go切换源\n\n```\n# 启用 Go Modules 功能\ngo env -w GO111MODULE=on\n# 切换源\ngo env -w  GOPROXY=https://goproxy.io,direct\n# 确认是否生效\ngo env\n\n```\n\n## 3.安装指定依赖\n\n```\ngo get -u github.com/confluentinc/confluent-kafka-go/kafka\ngo get: added github.com/confluentinc/confluent-kafka-go v1.9.2\n\n```\n\n如果想安装特定版本的依赖\n\n```\ngo get github.com/confluentinc/confluent-kafka-go/kafka@v1.9.2\n\n```\n\n## 4.下载依赖\n\n```\ngo mod download\n\n```\n\n## 5.清理无用的依赖\n\n下载缺失依赖，并清理无用的依赖（包括清理 go.mod 中的记录）\n\n```\ngo mod tidy\n\n```\n\ngo.mod文件中就会出现所安装的依赖\n\n<img src=\"/images/517519-20221213215012776-2071918062.png\" width=\"800\" height=\"124\" />\n\n参考：[真官方依赖管理 Go Modules 怎么玩（从入门到放弃）](https://farer.org/2018/11/11/go-modules-notes/)\n\n## 6.在项目根目录下会生成go.mod文件\n\n```\ngo mod init [模块名]\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["golang"]},{"title":"go学习笔记——gin框架","url":"/go学习笔记——gin框架.html","content":"gin是一款轻量级的go web开发框架，官方文档\n\n```\nhttps://gin-gonic.com/docs/examples/\n\n```\n\n## 1.gin web项目结构\n\n参考\n\n```\nhttps://github.com/voyagegroup/gin-boilerplate\n\n```\n\ngin+protobuf wire参考\n\n```\nhttps://github.com/mohuishou/blog-code/tree/main/01-go-training/04-project/10-layout\n\n```\n\n## 2.gin web quick start\n\n```\nhttps://gin-gonic.com/docs/quickstart/\n\n```\n\n在官方文档中提供了2个quick start的demo，一个稍微复杂，一个比较简单\n\n简单的例子如下，创建一个/ping接口，返回pong\n\n```\npackage main\n\nimport \"github.com/gin-gonic/gin\"\n\nfunc main() {\n\tr := gin.Default()\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(200, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\tr.Run() // listen and serve on 0.0.0.0:8080\n}\n\n```\n\n## 3.路由分组\n\n如果gin项目的接口比较多的话，可以使用路由分组\n\n```\nhttps://gin-gonic.com/docs/examples/grouping-routes/\n\n```\n\n参考：[【Go】基于 Gin 从0到1搭建 Web 管理后台系统后端服务（三）路由、自定义校验器和 Redis](https://juejin.cn/post/7220329768108146743)\n\n## 4.数据绑定\n\n将请求的参数传递给接口中的变量，需要使用gin的数据绑定\n\n如果是path parameter\n\n使用c.Param(\"name\")\n\n```\nhttps://gin-gonic.com/docs/examples/param-in-path/\n\n```\n\n使用c.ShouldBindUri(&amp;person)\n\n```\nhttps://gin-gonic.com/docs/examples/bind-uri/\n\n```\n\n如果是query parameter或者post请求的form-data\n\n使用c.ShouldBind(&amp;user)\n\n```\nhttps://gin-gonic.com/docs/examples/bind-query-or-post/\n\n```\n\n## 5.统一的response\n\n可以如下定义统一的接口response，其中的interface{}类似java中的泛型T\n\n```\ntype ControllerResponse struct {\n\tCode int\n\tMsg  string\n\tData interface{}\n}\n\nfunc Response(ctx *gin.Context, code int, msg string, data interface{}) {\n\tresp := ControllerResponse{Code: code, Msg: msg, Data: data}\n\tctx.JSON(code, resp)\n\tctx.Abort()\n}\n\nfunc Success(ctx *gin.Context, msg string, data interface{}) {\n\tResponse(ctx, 200, msg, data)\n}\n\nfunc Fail(ctx *gin.Context, msg string, data interface{}) {\n\tResponse(ctx, 500, msg, data)\n}\n\n```\n\n参考：[go语言web开发系列之十五:gin框架统一定义API错误码](https://blog.csdn.net/weixin_43881017/article/details/112471847)\n\n## 6.Gin的middleware中间件\n\n### 1.使用middleware\n\ngin的middleware需要实现**gin.HandlerFunc**，可以用其来实现诸如jwt，auth等功能，参考\n\n[https://github.com/gin-gonic/contrib/blob/master/jwt/jwt.go](https://github.com/gin-gonic/contrib/blob/master/jwt/jwt.go)\n\n[https://gin-gonic.com/zh-cn/docs/examples/using-middleware/](https://gin-gonic.com/zh-cn/docs/examples/using-middleware/)\n\n下面定义了3个middleware\n\n```\n// MiddleWare1 定义中间件1\nfunc MiddleWare1() gin.HandlerFunc {\n\treturn func(c *gin.Context) {\n\t\tfmt.Println(\"前 m1\")\n\t\tc.Next()\n\t\tfmt.Println(\"后 m1\")\n\t}\n}\n\n// MiddleWare2 定义中间件2\nfunc MiddleWare2() gin.HandlerFunc {\n\treturn func(c *gin.Context) {\n\t\tfmt.Println(\"前 m2\")\n\t\tc.Next()\n\t\tfmt.Println(\"后 m2\")\n\t}\n}\n\n// MiddleWare3 定义中间件3\nfunc MiddleWare3() gin.HandlerFunc {\n\treturn func(c *gin.Context) {\n\t\tfmt.Println(\"前 m3\")\n\t\tc.Next()\n\t\tfmt.Println(\"后 m3\")\n\t}\n}\n\n```\n\n在gin中添加middleware\n\n```\nfunc main() {\n   // 创建路由\n   engine := gin.Default()\n\n   // 注册中间件\n   engine.Use(MiddleWare1(), MiddleWare2(), MiddleWare3())\n\n   // 路由规则\n   engine.GET(\"/ping\", func(c *gin.Context) {\n      fmt.Println(\"hello world\")\n      c.JSON(200, gin.H{\"msg\": \"pong\"})\n   })\n   err:=engine.Run(\":3000\")\n   if err != nil {\n      fmt.Println(err)\n   }\n}\n\n```\n\n输出如下\n\n```\n前 m1\n前 m2\n前 m3\nhello world\n后 m3\n后 m2\n后 m1\n[GIN] 2023/12/25 - 00:06:11 | 200 |      35.726&micro;s |       127.0.0.1 | GET      \"/ping\"\n\n```\n\n接口返回pong\n\n### 2.Next()和Abort()\n\nNext()之前的代码会在执行HandlerFunc之前执行，之后的代码会在执行HandlerFunc之后执行\n\n如果将middleware2中的c.Next()修改成c.Abort()\n\n```\n// MiddleWare2 定义中间件2\nfunc MiddleWare2() gin.HandlerFunc {\n\treturn func(c *gin.Context) {\n\t\tfmt.Println(\"前 m2\")\n\t\tc.Abort()\n\t\tfmt.Println(\"后 m2\")\n\t}\n}\n\n```\n\n则输出将会如下\n\n```\n前 m1\n前 m2\n后 m2\n后 m1\n[GIN] 2023/12/25 - 00:11:05 | 200 |      16.262&micro;s |       127.0.0.1 | GET      \"/ping\"\n\n```\n\n接口无返回\n\n可以Abort()不会阻止当前middleware的执行，但是会中止下游middleware以及HandlerFunc的执行\n\n参考：[[Go Package] gin 中间件流程控制:c.Next() / c.Abort（）](https://juejin.cn/post/7134929302822322207)\n\n### 3.全局中间件和局部中间件\n\n#### 1.白名单\n\n上面给出的例子中，middleware将对全部的请求生效，这里可以通过指定白名单的方法来使其只对部分请求生效\n\n#### 2.局部中间件\n\n参考：[Golang Gin 局部、全局 中间件使用](https://blog.csdn.net/qq_34556414/article/details/129922392)\n\n## 7.在Gin中使用http handler中间件\n\n除了使用gin的middleware来实现中间件之外，也可以使用Go的标准库的net/http包来初始http请求\n\n参考：[GO中间件(Middleware)](https://blog.csdn.net/guyan0319/article/details/89183816)\n\n在gin中使用的话，需要将http.Handler转换成gin.HanlderFunc\n\n```\n// MiddleWare4 定义中间件4\nfunc MiddleWare4(next http.Handler) http.Handler {\n\treturn http.HandlerFunc(func(writer http.ResponseWriter, request *http.Request) {\n\t\tfmt.Println(\"前 m4\")\n\t\tnext.ServeHTTP(writer, request)\n\t\tfmt.Println(\"后 m4\")\n\t})\n}\n\n// 将 http.Handler 包装成 Gin 中间件的函数\nfunc WrapHandler(handler http.Handler) gin.HandlerFunc {\n\treturn func(c *gin.Context) {\n\t\thandler.ServeHTTP(c.Writer, c.Request)\n\t\tc.Next()\n\t}\n}\n\n```\n\n在router中添加middleware，注意需要将middleware写在局部路由的前面才能生效\n\n```\nwrappedMiddleware := middleware.WrapHandler(middleware.MiddleWare4(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t// 空的 http.HandlerFunc，只为了包装中间件\n})))\nr.Use(wrappedMiddleware, middleware.MiddleWare1(), middleware.MiddleWare2())\n\n// Ping test\nr.GET(\"/ping\", func(c *gin.Context) {\n\tc.String(http.StatusOK, \"pong\")\n})\n\n```\n\n参考：[https://github.com/go-kratos/examples/blob/main/http/middlewares/middlewares.go](https://github.com/go-kratos/examples/blob/main/http/middlewares/middlewares.go) \n\n<!--more-->\n&nbsp;\n","tags":["golang"]},{"title":"go学习笔记——Kratos框架","url":"/go学习笔记——Kratos框架.html","content":"官方文档\n\n```\nhttps://go-kratos.dev/en/docs/getting-started/start/\n\n```\n\n## 1.安装Go\n\n参考：[mac安装go1.20](https://www.cnblogs.com/tonglin0325/p/5524764.html)\n\n## 2.安装Kratos框架\n\nkratos依赖protobuf grpc等框架，需要先进行安装\n\n```\nbrew install grpc\nbrew install protobuf\nbrew install protoc-gen-go\nbrew install protoc-gen-go-grpc\n\n```\n\n验证\n\n```\nprotoc --version\nprotoc-gen-go --version\nprotoc-gen-go-grpc --version\n\n```\n\n安装kratos框架\n\n```\ngo install github.com/go-kratos/kratos/cmd/kratos/v2@latest\n\n```\n\n## 3.创建项目\n\n```\n# 创建项目\nkratos new helloworld\n# 也可以指定仓库\nkratos new helloworld -r https://gitee.com/go-kratos/kratos-layout.git\n\n```\n\n编译项目\n\n```\ncd helloworld\n\n# 安装依赖\nmake init\n\n# 生成wire依赖注入\ngo generate ./...\n# 或者使用make\nmake generate\n\n# 生成bin执行文件\ngo build -o ./bin/ ./...\n# 或者使用make\nmake build\n\n# 运行\n./bin/helloworld -conf ./configs\n或者\nkratos run\n\n# 测试\ncurl 'http://127.0.0.1:8000/helloworld/kratos'\n{\"message\":\"Hello kratos\"}%\n\n```\n\n项目结构\n\n<img src=\"/images/517519-20240114110648722-2006585277.png\" width=\"300\" height=\"569\" />\n\nKratos项目如果想要使用**debug模式**来运行\n\n需要将run kind从File修改成Directory，并指定Directory为main.go和wire_gen.go所在的目录，否则会报\n\n```\n# command-line-arguments\n./main.go:77:23: undefined: wireApp\n\n```\n\n同时还需要额外指定conf文件的路径，否则会报\n\n```\npanic: stat ../../configs: no such file or directory\n\n```\n\n配置如下\n\n```\n-conf ./configs/config.yaml\n\n```\n\n其中config.yaml的路径是相对于Working directory的，如果有多个config文件，也可以写文件夹，比如\n\n```\n-conf ./configs\n\n```\n\n<img src=\"/images/517519-20240114150451846-1493406162.png\" width=\"700\" height=\"469\" />\n\n或者修改成package，指定成package path\n\n<img src=\"/images/517519-20240124213410086-1169458254.png\" width=\"700\" height=\"481\" />\n\ndebug成功\n\n<img src=\"/images/517519-20240114151508054-414842422.png\" width=\"700\" height=\"392\" />\n\n如果想要添加额外的API，需要先添加proto文件\n\n```\nkratos proto add api/helloworld/demo.proto\n\n```\n\n<img src=\"/images/517519-20240114113025510-266443633.png\" width=\"700\" height=\"430\" />\n\n编译proto文件生成model和grpc代码\n\n```\nkratos proto client api/helloworld/demo/demo.proto\n\n```\n\n或者使用make命令\n\n```\nmake api\n\n```\n\n注意make api命令无法生成validate验证代码，需要额外使用make validate命令，但是kratos proto client命令是可以的\n\n参考：[https://go-kratos.dev/docs/component/middleware/validate/](https://go-kratos.dev/docs/component/middleware/validate/)\n\n<img src=\"/images/517519-20240114113514912-1396594130.png\" width=\"300\" height=\"162\" />\n\n生成service模板代码\n\n```\nkratos proto server api/helloworld/demo/demo.proto -t internal/service\n\n```\n\n<img src=\"/images/517519-20240114114153099-1701711080.png\" width=\"800\" height=\"260\" />\n\n参考文档：[从0到1安装启动Kratos框架](https://segmentfault.com/a/1190000043858476) 和 [https://go-kratos.dev/en/docs/getting-started/usage](https://go-kratos.dev/en/docs/getting-started/usage)\n\n## 4.Example项目\n\n```\nhttps://github.com/go-kratos/examples\n\n```\n\n## 5.Middleware\n\n区别于gin的middleware实现的是**gin.HandlerFunc**以及在gin中基于**http.Handler**的实现，参考：<!--more-->\n&nbsp;[go学习笔记&mdash;&mdash;gin框架](https://www.cnblogs.com/tonglin0325/p/18183202)\n\n### 1.Kratos的middleware.Middleware\n\nkratos的middleware需要实现的是**middleware.Middleware**，可以参考官方例子：[https://github.com/go-kratos/examples/blob/main/i18n/internal/pkg/middleware/localize/localize.go](https://github.com/go-kratos/examples/blob/main/i18n/internal/pkg/middleware/localize/localize.go)\n\n可以通过context的transport中获得header等信息\n\n在middleware之后执行某些逻辑使用\n\n&nbsp;\n\n```\ndefer func() {\n                // Do something on exiting\n                 }()\n\n```\n\n&nbsp;\n\n&nbsp;参考：[https://go-kratos.dev/docs/component/middleware/overview/](https://go-kratos.dev/docs/component/middleware/overview/)\n\nmiddleware中设置header，参考：[go kratos框架跨域中间件实现(v2)](https://blog.csdn.net/xmcy001122/article/details/126618680)\n\n其他：[go微服务框架Kratos笔记（七）使用jwt认证中间件](https://www.cnblogs.com/zly-go/p/15532783.html)\n\n&nbsp;\n\n### 2.在middle.Middleware中使用白名单\n\n参考：[https://github.com/go-kratos/beer-shop/blob/main/app/shop/interface/internal/server/http.go#L21](https://github.com/go-kratos/beer-shop/blob/main/app/shop/interface/internal/server/http.go#L21)\n\n### 3.Kratos中使用net/http包的http.Handler\n\n如果想在kratos中使用**http.Handler**，可以参考官方例子：[https://github.com/go-kratos/examples/blob/main/http/middlewares/middlewares.go](https://github.com/go-kratos/examples/blob/main/http/middlewares/middlewares.go)\n\n以及 [https://github.com/tx7do/kratos-cms/blob/main/backend/pkg/middleware/auth/auth.go](https://github.com/tx7do/kratos-cms/blob/main/backend/pkg/middleware/auth/auth.go) 或者 [04-参数校验中间件](https://cit965.netlify.app/docs/go-kratos/%E5%8F%82%E6%95%B0%E6%A0%A1%E9%AA%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/)\n\n### 4.Kratos中使用kratos的http包的http.FilterFunc\n\n需要引入\n\n```\nkratos_http \"github.com/go-kratos/kratos/v2/transport/http\"\n\n```\n\n参考：[kratos v2 基于http Filter 编写鉴权拦截器](https://blog.csdn.net/qq_39093583/article/details/127251013) 和 [Go Kratos2.0 Http请求如何获取IP地址(Filter,Middleware)](https://blog.csdn.net/u014647332/article/details/124705582)\n\n&nbsp;\n\n&nbsp;\n","tags":["golang"]},{"title":"特征预处理——编码","url":"/特征预处理——编码.html","content":"对于标称型数据，在特征处理的时候，需要对其进行编码\n\n在编码之前，如果训练集和测试集是分开的，则需要对其进行合并，避免标称数据丢失\n\n```\n# 合并\ndf = train.append(test).reset_index()\n# 列名\noriginal_columns = list(df.columns)\n\n```\n\n常用的编码方式如下\n\n## 1.Label编码\n\n对于一个有m个category的特征，经过label encoding以后，每个category会映射到0到m-1之间的一个数。label encoding适用于ordinal feature （特征存在内在顺序）。\n\n```\nfrom sklearn import preprocessing\n\nfor col in original_columns:\n    enc = preprocessing.LabelEncoder()\n    enc.fit(np.concatenate([train[col], test[col]]))\n    train[col] = enc.transform(train[col])\n    test[col] = enc.transform(test[col])\n\n```\n\n或者\n\n```\ntrain_data = train_data.replace({'BsmtQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, np.NaN: 0}})\n\n```\n\n## 2.顺序编码\n\n类似于Label编码\n\n参考：[数据转化](https://wutaoblog.com.cn/2021/08/20/data_preprocess/)\n\n```\nfrom sklearn import preprocessing\n\nenc = preprocessing.OrdinalEncoder()\nX = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\nenc.fit(X)\n>> OrdinalEncoder()\n\nenc.transform([['female', 'from US', 'uses Safari']])\n>> array([[0., 1., 1.]])\n\n```\n\n## 3.one-hot编码\n\n对于一个有m个category的特征，经过独热编码（OHE）处理后，会变为m个二元特征，每个特征对应于一个category。这m个二元特征互斥，每次只有一个激活。\n\n独热编码解决了原始特征缺少内在顺序的问题，但是缺点是对于high-cardinality categorical feature (category数量很多)，编码之后特征空间过大（此处可以考虑PCA降维），而且由于one-hot feature 比较unbalanced，树模型里每次的切分增益较小，树模型通常需要grow very deep才能得到不错的精度。因此OHE一般用于category数量 <4的情况。\n\n参考：[机器学习 | 数据缩放与转换方法（1）](https://xie.infoq.cn/article/1a72c74d073a807c4a738284f) 和 [kaggle编码categorical feature总结](https://zhuanlan.zhihu.com/p/40231966)\n\n使用sklearn来one-hot编码\n\n```\nfrom sklearn import preprocessing\n\nenc = preprocessing.OneHotEncoder()\n# enc = preprocessing.OneHotEncoder(handle_unknown='ignore') # 当 handle_unknown='ignore' 被指定而在转换的过程中碰到了未知的枚举特征值，不会产生任何错误，但是该特征的 one-hot 编码列将会被全部置 0\n\nX = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\nenc.fit(X) \n>> OneHotEncoder() \n\nenc.transform([['female', 'from US', 'uses Safari'], ['male', 'from Europe', 'uses Safari']]).toarray() \n>> array([[1., 0., 0., 1., 0., 1.], [0., 1., 1., 0., 0., 1.]]) \n\n# 查看每个特征的特征值 \nenc.categories_ \n>> [array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object), array(['uses Firefox', 'uses Safari'], dtype=object)]\n\n```\n\n使用pandas来one-hot编码\n\n```\nimport pandas as pd\n\nX = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\ndf = pd.DataFrame(X, columns=['sex','country','browser'])\n# columns是需要编码的列名\ndf = pd.get_dummies(df, columns=['sex','country'], dummy_na = True)\nprint(df)\n\n        browser  sex_female  sex_male  sex_nan  country_from Europe  country_from US  country_nan\n0   uses Safari           0         1        0                    0                1            0\n1  uses Firefox           1         0        0                    1                0            0\n\n```\n\n## 4.target编码\n\n参考：[三种Target Encoding方式总结](https://zhuanlan.zhihu.com/p/466389890)<!--more-->\n&nbsp;和 [kaggle编码categorical feature总结](https://zhuanlan.zhihu.com/p/40231966)\n\n**Target Encoding**是任何一种可以从目标中派生出数字替换特征类别的编码方式。这种目标编码有时被称为平均编码。应用于二进制目标时，也被称为bin counting。\n\ntarget encoding的缺点主要有：\n\n- 未知类别，会产生过拟合风险；\n- 空值，采用填充的方法不能很好的进行评估；\n- 长尾类别，对长尾类别这种少量数据的编码会导致过拟合；\n\ntarget encoding的优点主要有：\n\n- **高维数据特征：**具有大量类别的可能很难编码：One-hot会生成太多维度，而替代方案（如标签编码）可能不适合该类型。Target encoding在此处就很好的解决了这个问题；\n- **领域经验特征：**根据之前的经验，即使某项数据它在特征度量方面得分很低，你也可能会觉得一个分类特征应该很重要。Target encoding有助于揭示特征的真实信息。\n\n## 5.模型自动编码\n\n- XGBoost和Random Forest，不能直接处理categorical feature，必须先编码成为numerical feature。\n- lightGBM和CatBoost，可以直接处理categorical feature。\n<ul>\n- lightgbm： 需要先做label encoding。用特定算法（[On Grouping for Maximum Homogeneity](https://link.zhihu.com/?target=https%3A//www.researchgate.net/publication/242580910_On_Grouping_for_Maximum_Homogeneity)）找到optimal split，效果优于ONE。也可以选择采用one-hot encoding，。[Features - LightGBM documentation](https://link.zhihu.com/?target=https%3A//lightgbm.readthedocs.io/en/latest/Features.html%3Fhighlight%3Dcategorical%23optimal-split-for-categorical-features)\n- CatBoost： 不需要先做label encoding。可以选择采用one-hot encoding，target encoding (with regularization)。[CatBoost &mdash; Transforming categorical features to numerical features &mdash; Yandex Technologies](https://link.zhihu.com/?target=https%3A//tech.yandex.com/catboost/doc/dg/concepts/algorithm-main-stages_cat-to-numberic-docpage/)\n","tags":["Python"]},{"title":"kafka学习笔记——topic配置","url":"/kafka学习笔记——topic配置.html","content":"在创建kafka topic的时候可以添加很多配置，如下表格\n\n参考：[Kafka Topic配置](https://www.orchome.com/669)\n\n<!--more-->\n&nbsp;\n\n\n\n|参数名|含义|值\n| ---- | ---- | ---- \n|cleanup.policy|日志清除的策略，默认为 delete。如果要使用日志压缩，就需要让策略包含 compact。需要注意的是，如果开启了 compact 策略，则客户端提交的消息的 key 不允许为 null，否则提交报错|compact\n|compression.type|指定给定主题的最终压缩类型。此配置接受标准压缩编解码器(&ldquo;gzip&rdquo;、&ldquo;snappy&rdquo;、&ldquo;lz4&rdquo;、&ldquo;zstd&rdquo;)。设置为'producer'这意味着保留生产者设置的原始压缩编解码。&ldquo;gzip&rdquo;：压缩效率高，适合高内存、CPU；&ldquo;snappy&rdquo;：适合带宽敏感性，压缩力度大|producer\n|delete.retention.ms|保留删除消息压缩topic的删除标记的时间。此设置还给出消费者如果从offset 0开始读取并确保获得最终阶段的有效快照的时间范围（否则，在完成扫描之前可能已经回收了）。|86400000&nbsp;(24 hours)\n|file.delete.delay.ms|从文件系统中删除文件之前等待的时间|60000\n|flush.messages|此设置允许指定我们强制fsync写入日志的数据的间隔。例如，如果这被设置为1，我们将在每个消息之后fsync; 如果是5，我们将在每五个消息之后fsync。一般，我们建议不要设置它，使用复制特性来保持持久性，并允许操作系统的后台刷新功能更高效。可以在每个topic的基础上覆盖此设置（请参阅每个主题的配置部分）。|10000\n|flush.ms|此设置允许我们强制fsync写入日志的数据的时间间隔。例如，如果这设置为1000，那么在1000ms过去之后，我们将fsync。 一般，我们建议不要设置它，并使用复制来保持持久性，并允许操作系统的后台刷新功能，因为它更有效率|1000\n|follower.replication.throttled.replicas|follower复制限流列表。该列表应以[PartitionId]的形式描述一组副本：[BrokerId]，[PartitionId]：[BrokerId]：...或者通配符'*'可用于限制此topic的所有副本。|&nbsp;\n|index.interval.bytes|此设置控制Kafka向其offset索引添加索引条目的频率。默认设置确保我们大致每4096个字节索引消息。 更多的索引允许读取更接近日志中的确切位置，但使索引更大。你不需要改变这个值。|4096\n|leader.replication.throttled.replicas|在leader方面进行限制的副本列表。该列表设置以[PartitionId]的形式描述限制副本：[PartitionId]:[BrokerId],[PartitionId]:[BrokerId]:...或使用通配符&lsquo;*&rsquo;限制该topic的所有副本。|&nbsp;\n|max.message.bytes|kafka允许的最大的消息批次大小。如果增加此值，并且消费者的版本比0.10.2老，那么消费者的提取的大小也必须增加，以便他们可以获取大的消息批次。<br />在最新的消息格式版本中，消息总是分组批量来提高效率。在之前的消息格式版本中，未压缩的记录不会分组批量，并且此限制仅适用于该情况下的单个消息。|10485760\n|message.format.version|指定消息附加到日志的消息格式版本。该值应该是一个有效的ApiVersion。例如：0.8.2, 0.9.0.0, 0.10.0，更多细节检查ApiVersion。通过设置特定的消息格式版本，并且磁盘上的所有现有消息都小于或等于指定版本。不正确地设置此值将导致消费者使用旧版本，因为他们将接收到&ldquo;不认识&rdquo;的格式的消息。|0.11.0-IV2\n|message.timestamp.difference.max.ms|&nbsp;|9223372036854775807\n|message.timestamp.type|&nbsp;|CreateTime\n|min.cleanable.dirty.ratio|此配置控制日志压缩程序将尝试清除日志的频率(假设启用了日志压缩)。默认情况下，我们将避免清理超过50％日志被压缩的日志。 该比率限制日志中浪费的最大空间重复(在最多50％的日志中可以是重复的50％)。更高的比率意味着更少，更有效的清洁，但意味着日志中的浪费更多。|0.5\n|min.compaction.lag.ms|消息在日志中保持不压缩的最短时间。仅适用于正在压缩的日志。|0\n|min.insync.replicas|当生产者设置应答为\"all\"(或&ldquo;-1&rdquo;)时，此配置指定了成功写入的副本应答的最小数。如果没满足此最小数，则生产者将引发异常(NotEnoughReplicas或NotEnoughReplicasAfterAppend)。当min.insync.replicas和acks强制更大的耐用性时。典型的情况是创建一个副本为3的topic，将min.insync.replicas设置为2，并设置acks为&ldquo;all&rdquo;。如果多数副本没有收到写入，这将确保生产者引发异常。&nbsp;|1\n|preallocate|如果我们在创建新的日志段时在磁盘上预分配该文件，那么设为True。|false\n|retention.bytes|如果我们使用&ldquo;删除&rdquo;保留策略，则此配置将控制日志可以增长的最大大小，之后我们将丢弃旧的日志段以释放空间。默认情况下，没有设置大小限制则仅限于时间限制。|-1\n|retention.ms|如果我们使用&ldquo;删除&rdquo;保留策略，则此配置控制我们将保留日志的最长时间，然后我们将丢弃旧的日志段以释放空间。这代表SLA消费者必须读取数据的时间长度。&nbsp;|604800000\n|segment.bytes|此配置控制日志的段文件大小。一次保留和清理一个文件，因此较大的段大小意味着较少的文件，但对保留率的粒度控制较少。|104857600\n|segment.index.bytes|此配置控制offset映射到文件位置的索引的大小。我们预先分配此索引文件，并在日志滚动后收缩它。通常不需要更改此设置。|10485760\n|segment.jitter.ms|从计划的段滚动时间减去最大随机抖动，以避免异常的段滚动|0\n|segment.ms|此配置控制Kafka强制日志滚动的时间段，以确保保留可以删除或压缩旧数据，即使段文件未满|604800000\n|unclean.leader.election.enable|是否将不在ISR中的副本作为最后的手段选举为leader，即使这样做可能会导致数据丢失|false\n\n&nbsp;问题集合：[apache kafka技术分享系列(目录索引)](https://blog.csdn.net/lizhitao/article/details/39499283)　　\n\n&nbsp;\n","tags":["kafka"]},{"title":"HBase学习笔记——客户端API","url":"/HBase学习笔记——客户端API.html","content":"**介绍HBase的Java API**，参考：[HBase读写的几种方式（一）java篇](https://www.cnblogs.com/swordfall/p/10301707.html)<!--more-->\n&nbsp;和&nbsp;[Hbase--put、BufferedMutator、get](https://blog.csdn.net/XiaodunLP/article/details/100168615)\n\n## **1.写HBase**\n\n### **1.单行put**\n\nHTable非线程安全，切较为低效\n\n<img src=\"/images/517519-20201229164640569-1533529902.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n### **2.客户端的写缓冲区和List<Put>**\n\n一个put操作都是一个RPC操作，只适合**小数据量**的操作，HBase的API配置了客户端的**写缓冲区**，缓冲区负责收集put宝座，然后调用RPC操作一次性将put送往服务器。\n\n默认情况下，**客户端缓冲区是禁用**的，可以通过将**自动刷写（****autoflush=false）**来激活缓冲区，如果**（****autoflush=true）**，用户每次调用put方法是都会触发刷写。当激活缓冲区后，单行put不会产生RPC调用，因为存储的Put实例保存在客户端进程的内存中。只有**flushCommits方法**调用后才会将所有的修改传送到远程服务器。\n\n默认的写缓冲区大小为2MB（2097152字节），可以在客户端中配置，也可以在habse-site.xml中的**hbase.client.write.buffer** 配置一个较大的预设值，比如20MB\n\n**显式刷写**，即用户调用&nbsp;**flushCommits<strong>方法**</strong>\n\n**<strong>隐式刷写**</strong>，即会在调用put方法和setWriteBufferSize方法后比较**超出缓冲区大小**后调用&nbsp;**flushCommits<strong>方法**</strong>\n\n**<strong><img src=\"/images/517519-20201229164602009-1001146075.png\" alt=\"\" loading=\"lazy\" />**</strong>\n\n&nbsp;\n\n### **3.使用BufferedMutator**\n\norg.apache.hadoop.hbase.client.BufferedMutator主要用来对HBase的单个表进行操作。它和Put类的作用差不多，但是主要用来实现批量的异步写操作。\n\n使用BufferedMutator进行批量异步插入的方式，效率更高，参考：[HBase新版本Java API](https://www.jianshu.com/p/00f0a2b7a283) 和 [Hbase实战--数据读写解析](https://blog.csdn.net/aA518189/article/details/89190693)\n\n```\nimport org.apache.hadoop.hbase.client.BufferedMutator;\n/** Helper function to create a table and return the rows that it created. */\n\nprivate static void writeData(String tableId, int numRows) throws Exception {\n  Connection connection = admin.getConnection();\n  TableName tableName = TableName.valueOf(tableId);\n  BufferedMutator mutator = connection.getBufferedMutator(tableName);\n  List<Mutation> mutations = makeTableData(numRows);\n  mutator.mutate(mutations);\n  mutator.flush();\n  mutator.close();\n}\n\n```\n\n&nbsp;\n","tags":["HBase"]},{"title":"Flink学习笔记——读写HBase","url":"/Flink学习笔记——读写HBase.html","content":"## 1.如果是**csa(Cloudera Streaming Analytics)版本**的高版本HBase\n\n可以参考Cloudera官方例子，通过引入官方提供的flink-hbase来实现\n\n```\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-hbase_2.11</artifactId>\n    <version>1.9.0-csa1.0.0.0</version>\n</dependency>\n\n```\n\n要求flink最低版本1.9.0，hbase最低版本2.1.0-cdh6.3.0，然后就可以使用HBaseSinkFunction来写Hbase\n\n```\nhttps://docs.cloudera.com/csa/1.2.0/datastream-connectors/topics/csa-hbase-configuration.html\n\n```\n\n<!--more-->\n&nbsp;\n\n## 2.如果是**低版本的HBase**\n\n则需要自行使用**Java API**来写HBase，比如我使用hbase版本为1.2.0-cdh5.1.6.2，可以参考：[HBase读写的几种方式（三）flink篇](https://www.cnblogs.com/swordfall/p/10527423.html) 和 [Flink 消费kafka数据写入hbase](https://blog.csdn.net/weixin_42003671/article/details/103196825)\n\n写HBase的时候注意开启Hbase regionserver的hbase.regionserver.port端口防火墙，默认为60020\n\nJava API写Hbase的方式主要有2种：\n\n### 1.一种是使用Table的**put API**\n\n可以参考\n\n```\nhttps://github.com/phillip2019/flink-parent/blob/master/flink-connectors/flink-connector-hbase/src/main/java/com/aikosolar/bigdata/flink/connectors/hbase/SimpleHBaseTableSink.java\n\n```\n\n使用Flink来写Hbase的时候，需要继承RichSinkFunction，然后重写invoke方法，在invoke方法中调用HBase的put API\n\n需要注意的是调用HBase Table的put API较为低效，即使是使用了List<Put>，也会消耗较多的资源，建议使用第二种的**BufferedMutator**\n\n```\nimport org.apache.commons.io.IOUtils;\nimport org.apache.commons.lang3.StringUtils;\nimport org.apache.flink.configuration.Configuration;\nimport org.apache.flink.streaming.api.functions.sink.RichSinkFunction;\nimport org.apache.flink.util.Preconditions;\nimport org.apache.hadoop.hbase.HBaseConfiguration;\nimport org.apache.hadoop.hbase.TableName;\nimport org.apache.hadoop.hbase.client.*;\nimport org.apache.hadoop.hbase.util.Bytes;\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\n\npublic class SimpleHBaseTableSink extends RichSinkFunction<ConsumerRecord<String, String>> {\n\n    private final String tableName;\n    private final HBaseWriterConfig writerConfig;\n\n    private transient TableName tName;\n    private transient Connection connection;\n\n    public SimpleHBaseTableSink(HBaseWriterConfig writerConfig, String tableName) throws Exception {\n        Preconditions.checkNotNull(writerConfig);\n        Preconditions.checkNotNull(tableName);\n\n        this.writerConfig = writerConfig;\n        this.tableName = tableName;\n    }\n\n    @Override\n    public void open(Configuration parameters) throws Exception {\n        this.tName = TableName.valueOf(tableName);\n\n        org.apache.hadoop.conf.Configuration conf = HBaseConfiguration.create();\n        this.writerConfig.getHbaseConfig().forEach(conf::set);\n        this.connection = ConnectionFactory.createConnection(conf);\n    }\n\n    @Override\n    public void invoke(ConsumerRecord<String, String> record, Context context) throws Exception {\n        Table table = null;\n        try {\n            table = connection.getTable(this.tName);\n            String rowKey = String.valueOf(record.offset());\n            String value = record.value();\n            Put put = new Put(Bytes.toBytes(rowKey));\n            put.addColumn(Bytes.toBytes(\"test_family\"), Bytes.toBytes(\"test_col\"), Bytes.toBytes(value));\n\n            if (StringUtils.isNotBlank(writerConfig.getDurability())) {\n                try {\n                    Durability d = Durability.valueOf(writerConfig.getDurability());\n                    put.setDurability(d);\n                } catch (Exception e) {\n                    e.printStackTrace();\n                }\n            }\n            table.put(put);\n        } finally {\n            IOUtils.closeQuietly(table);\n        }\n    }\n\n    @Override\n    public void close() throws Exception {\n        if (this.connection != null) {\n            this.connection.close();\n            this.connection = null;\n        }\n    }\n}\n\n```\n\nHBaseWriterConfig\n\n```\nimport lombok.AccessLevel;\nimport lombok.AllArgsConstructor;\nimport lombok.Getter;\nimport lombok.NoArgsConstructor;\nimport org.apache.commons.collections.MapUtils;\n\nimport java.io.Serializable;\nimport java.util.HashMap;\nimport java.util.Map;\n\n@Getter\n@AllArgsConstructor(access = AccessLevel.PRIVATE)\n@NoArgsConstructor(access = AccessLevel.PRIVATE)\npublic class HBaseWriterConfig implements Serializable {\n    private int writeBufferSize = -1;\n    private int asyncFlushSize = -1;\n    private long asyncFlushInterval = -1;\n    private boolean async;\n    private String durability;\n    private Map<String, String> hbaseConfig = new HashMap<>();\n\n    public static class Builder {\n        private int writeBufferSize = 5 * 1024 * 1024;\n        private int asyncFlushSize = 5000;\n        // 单位:秒\n        private long asyncFlushInterval = 60;\n        private boolean async;\n        private String durability;\n        private Map<String, String> config = new HashMap<>();\n\n        public static synchronized Builder me() {\n            return new Builder();\n        }\n\n        public Builder setDurability(String durability) {\n            this.durability = durability;\n            return this;\n        }\n\n        public Builder writeBufferSize(int writeBufferSize) {\n            this.writeBufferSize = writeBufferSize;\n            return this;\n        }\n\n        public Builder conf(String key, String value) {\n            this.config.put(key, value);\n            return this;\n        }\n\n        public Builder conf(Map<String, String> config) {\n            if (MapUtils.isNotEmpty(config)) {\n                for (Map.Entry<String, String> e : config.entrySet()) {\n                    this.config.put(e.getKey(), e.getValue());\n                }\n            }\n            return this;\n        }\n\n        public Builder aync() {\n            this.async = true;\n            return this;\n        }\n\n        public Builder flushInterval(long interval) {\n            this.asyncFlushInterval = interval;\n            return this;\n        }\n\n        public Builder flushSize(int size) {\n            this.asyncFlushSize = size;\n            return this;\n        }\n\n        public Builder async(boolean enable) {\n            this.async = enable;\n            return this;\n        }\n\n        public HBaseWriterConfig build() {\n            return new HBaseWriterConfig(writeBufferSize, asyncFlushSize, asyncFlushInterval, async, durability, config);\n        }\n\n    }\n}\n\n```\n\n&nbsp;addSink\n\n```\n        dataStream.addSink(new SimpleHBaseTableSink(HBaseWriterConfig.Builder.me()\n                .async(true)\n                .build(),\n                \"test_table\")\n        );\n\n```\n\n&nbsp;\n\n### 2.另一种是使用**BufferedMutator**\n\n可以参考\n\n```\nhttps://github.com/phillip2019/flink-parent/blob/master/flink-connectors/flink-connector-hbase/src/main/java/com/aikosolar/bigdata/flink/connectors/hbase/writter/HBaseWriterWithBuffer.java\n\n```\n\n使用**BufferedMutator**进行异步批量插入的方式效率更高\n\n```\nimport org.apache.flink.configuration.Configuration;\nimport org.apache.flink.streaming.api.functions.sink.RichSinkFunction;\nimport org.apache.flink.util.Preconditions;\nimport org.apache.hadoop.hbase.HBaseConfiguration;\nimport org.apache.hadoop.hbase.TableName;\nimport org.apache.hadoop.hbase.client.*;\nimport org.apache.hadoop.hbase.util.Bytes;\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\n\npublic class SimpleHBaseTableSink extends RichSinkFunction<ConsumerRecord<String, String>> {\n\n    private final String tableName;\n    private final HBaseWriterConfig writerConfig;\n\n    private transient TableName tName;\n    private transient Connection connection;\n\n    public SimpleHBaseTableSink(HBaseWriterConfig writerConfig, String tableName) throws Exception {\n        Preconditions.checkNotNull(writerConfig);\n        Preconditions.checkNotNull(tableName);\n\n        this.writerConfig = writerConfig;\n        this.tableName = tableName;\n    }\n\n    @Override\n    public void open(Configuration parameters) throws Exception {\n        this.tName = TableName.valueOf(tableName);\n\n        org.apache.hadoop.conf.Configuration conf = HBaseConfiguration.create();\n        this.writerConfig.getHbaseConfig().forEach(conf::set);\n        this.connection = ConnectionFactory.createConnection(conf);\n    }\n\n    @Override\n    public void invoke(ConsumerRecord<String, String> record, Context context) throws Exception {\n        String rowKey = String.valueOf(record.offset());\n        String value = record.value();\n\n        // 设置缓存1m，当达到1m时数据会自动刷到hbase，替代了hTable.setWriteBufferSize(30 * 1024 * 1024)\n        BufferedMutatorParams params = new BufferedMutatorParams(this.tName);\n        params.writeBufferSize(1024 * 1024);\n        // 创建一个批量异步与hbase通信的对象\n        BufferedMutator mutator = connection.getBufferedMutator(params);\n        Put put = new Put(Bytes.toBytes(rowKey));\n        put.addColumn(Bytes.toBytes(\"test_family\"), Bytes.toBytes(\"test_col\"), Bytes.toBytes(value));\n        // 向hbase插入数据,达到缓存会自动提交,这里也可以通过传入List<put>的方式批量插入\n        mutator.mutate(put);\n        // 不用每次put后就调用flush，最后调用就行，这个方法替代了旧api的hTable.setAutoFlush(false, false)\n        mutator.flush();\n        mutator.close();\n    }\n\n    @Override\n    public void close() throws Exception {\n        if (this.connection != null) {\n            this.connection.close();\n            this.connection = null;\n        }\n    }\n}\n\n```\n\n&nbsp;\n\n一些Hbase sink需要的constants\n\n```\nhttps://github.com/apache/hbase/blob/master/hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java\n\n```\n\n需要引入\n\n```\n<dependency>\n    <groupId>org.apache.hbase</groupId>\n    <artifactId>hbase-common</artifactId>\n    <version>1.2.0-cdh5.15.1</version>\n</dependency>\n\n```\n\n参考flume的sink实现\n\nflume在1.8.0支持hbase的1.x版本，使用\n\n```\nhttps://github.com/apache/flume/blob/trunk/flume-ng-sinks/flume-ng-hbase-sink/src/main/java/org/apache/flume/sink/hbase/HBaseSinkConfigurationConstants.java\n\n```\n\n在1.9.0支持了habse的2.x版本，使用\n\n```\nhttps://github.com/apache/flume/blob/trunk/flume-ng-sinks/flume-ng-hbase2-sink/src/main/java/org/apache/flume/sink/hbase2/HBase2SinkConfigurationConstants.java\n\n```\n\n&nbsp;\n","tags":["Flink","HBase"]},{"title":"Hive学习笔记——metastore listener","url":"/Hive学习笔记——metastore listener.html","content":"除了使用hive hook来记录hive上用户的操作之外，还可以使用hive metastore listener来进行记录，参考：\n\n```\nhttps://towardsdatascience.com/apache-hive-hooks-and-metastore-listeners-a-tale-of-your-metadata-903b751ee99f\n\n```\n\nhive metastore的接口有3种，分别是\n\n<!--more-->\n&nbsp;\n\n\n\n|Property|Abstract class\n| ---- | ---- \n|hive.metastore.pre.event.listeners|org.apache.hadoop.hive.metastore.MetaStorePreEventListener\n|hive.metastore.end.function.listeners|org.apache.hadoop.hive.metastore.MetaStoreEndFunctionListener\n|hive.metastore.event.listeners|org.apache.hadoop.hive.metastore.MetaStoreEventListener\n\n下面的代码中继承了MetaStoreEventListener，实现的功能是在建表和修改表的时候，日志输出一下table的元数据信息\n\n```\npackage com.bigdata.hive;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.hive.metastore.MetaStoreEventListener;\nimport org.apache.hadoop.hive.metastore.events.AlterTableEvent;\nimport org.apache.hadoop.hive.metastore.events.CreateTableEvent;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.IOException;\n\npublic class MyListener extends MetaStoreEventListener {\n\n    private static final Logger LOGGER = LoggerFactory.getLogger(MyListener.class);\n    private static final ObjectMapper objMapper = new ObjectMapper();\n\n    public MyListener(Configuration config) {\n        super(config);\n        logWithHeader(\" created \");\n    }\n\n    @Override\n    public void onCreateTable(CreateTableEvent event) {\n        logWithHeader(event.getTable());\n    }\n\n    @Override\n    public void onAlterTable(AlterTableEvent event) {\n        logWithHeader(event.getOldTable());\n        logWithHeader(event.getNewTable());\n    }\n\n    private void logWithHeader(Object obj) {\n        LOGGER.info(\"[CustomListener][Thread: \" + Thread.currentThread().getName() + \"] | \" + objToStr(obj));\n    }\n\n    private String objToStr(Object obj) {\n        try {\n            return objMapper.writeValueAsString(obj);\n        } catch (IOException e) {\n            LOGGER.error(\"Error on conversion\", e);\n        }\n        return null;\n    }\n\n}\n\n```\n\n进行打包\n\n```\nmvn clean package\n\n```\n\n将jar包放到/var/lib/hive目录下，并修改用户组\n\n```\nlintong@master:/var/lib/hive$ ls -al| grep jar\n-rw-r--r--   1 hive hive 363002613 12月 26 16:30 bigdata-1.0-SNAPSHOT-jar-with-dependencies.jar\n\n```\n\n在CDH界面上配置metastore listener，注意是在metastore的配置项中，而不是hiveserver2的\n\n<img src=\"/images/517519-20211226215619188-233888463.png\" width=\"800\" height=\"447\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;\n\n然后重启hive\n\n之后在HUE上运行建表语句\n\n```\ncreate table test_table2(id int,name string)\n\n```\n\n查看hive metastore的日志，可以看到日志中打印了所创建的hive表的元数据信息\n\n```\nlintong@master:/var/log/hive$ tail -f hadoop-cmf-hive-HIVEMETASTORE-master.log.out | grep Listen\n2021-12-26 21:43:53,737 INFO  com.bigdata.hive.MyListener: [main]: [CustomListener][Thread: main] | \" created \"\n2021-12-26 21:45:15,224 INFO  com.bigdata.hive.MyListener: [pool-5-thread-14]: [CustomListener][Thread: pool-5-thread-14] | {\"tableName\":\"CM_TEST_TABLE\",\"dbName\":\"cloudera_manager_metastore_canary_test_db_hive_hivemetastore_0a5c4e82140edb166b3b3c29b6817024\",\"owner\":null,\"createTime\":1640526315,\"lastAccessTime\":0,\"retention\":0,\"sd\":{\"cols\":[{\"name\":\"s\",\"type\":\"string\",\"comment\":\"test string\",\"setType\":true,\"setComment\":true,\"setName\":true},{\"name\":\"f\",\"type\":\"float\",\"comment\":\"test float\",\"setType\":true,\"setComment\":true,\"setName\":true},{\"name\":\"a\",\"type\":\"array<map<string,struct<p1:int,p2:int>>>\",\"comment\":\"test complex type\",\"setType\":true,\"setComment\":true,\"setName\":true}],\"location\":\"hdfs://master:8020/user/hue/.cloudera_manager_hive_metastore_canary/hive_HIVEMETASTORE_0a5c4e82140edb166b3b3c29b6817024/cm_test_table\",\"inputFormat\":null,\"outputFormat\":null,\"compressed\":false,\"numBuckets\":1,\"serdeInfo\":{\"name\":\"CM_TEST_TABLE\",\"serializationLib\":null,\"parameters\":{\"serialization.format\":\"1\"},\"setSerializationLib\":false,\"setParameters\":true,\"parametersSize\":1,\"setName\":true},\"bucketCols\":[],\"sortCols\":[],\"parameters\":{},\"skewedInfo\":null,\"storedAsSubDirectories\":false,\"setParameters\":true,\"setSkewedInfo\":false,\"colsSize\":3,\"colsIterator\":[{\"name\":\"s\",\"type\":\"string\",\"comment\":\"test string\",\"setType\":true,\"setComment\":true,\"setName\":true},{\"name\":\"f\",\"type\":\"float\",\"comment\":\"test float\",\"setType\":true,\"setComment\":true,\"setName\":true},{\"name\":\"a\",\"type\":\"array<map<string,struct<p1:int,p2:int>>>\",\"comment\":\"test complex type\",\"setType\":true,\"setComment\":true,\"setName\":true}],\"setCols\":true,\"setLocation\":true,\"setInputFormat\":false,\"setOutputFormat\":false,\"setCompressed\":true,\"setNumBuckets\":true,\"setSerdeInfo\":true,\"bucketColsSize\":0,\"bucketColsIterator\":[],\"setBucketCols\":true,\"sortColsSize\":0,\"sortColsIterator\":[],\"setSortCols\":true,\"setStoredAsSubDirectories\":false,\"parametersSize\":0},\"partitionKeys\":[{\"name\":\"p1\",\"type\":\"string\",\"comment\":\"partition-key-1\",\"setType\":true,\"setComment\":true,\"setName\":true},{\"name\":\"p2\",\"type\":\"int\",\"comment\":\"partition-key-2\",\"setType\":true,\"setComment\":true,\"setName\":true}],\"parameters\":{\"transient_lastDdlTime\":\"1640526315\"},\"viewOriginalText\":null,\"viewExpandedText\":null,\"tableType\":null,\"privileges\":null,\"temporary\":false,\"ownerType\":\"USER\",\"partitionKeysSize\":2,\"setParameters\":true,\"setTableType\":false,\"setTableName\":true,\"setOwner\":false,\"setRetention\":true,\"partitionKeysIterator\":[{\"name\":\"p1\",\"type\":\"string\",\"comment\":\"partition-key-1\",\"setType\":true,\"setComment\":true,\"setName\":true},{\"name\":\"p2\",\"type\":\"int\",\"comment\":\"partition-key-2\",\"setType\":true,\"setComment\":true,\"setName\":true}],\"setPartitionKeys\":true,\"setViewOriginalText\":false,\"setViewExpandedText\":false,\"setPrivileges\":false,\"setTemporary\":false,\"setOwnerType\":true,\"setDbName\":true,\"setCreateTime\":true,\"setLastAccessTime\":true,\"setSd\":true,\"parametersSize\":1}\n\n```\n\n&nbsp;\n","tags":["Hive"]},{"title":"mac下安装gradle7.3","url":"/mac下安装gradle7.3.html","content":"gradle和maven类似，是一个构建工具\n\n## gradle安装和配置\n\n1.mac安装gradle\n\n```\nbrew install gradle\n\n```\n\n或者下载gradle的二进制安装包\n\n```\nhttps://gradle.org/releases/\n\n```\n\n然后在~/.bash_profile中配置\n\n```\n# gradle\nexport GRADLE_HOME=/Users/lintong/software/gradle-7.3\nexport PATH=$GRADLE_HOME/bin:$PATH\n\n```\n\n2.查看是否安装成功\n\n```\ngradle -v\n\n------------------------------------------------------------\nGradle 7.3\n------------------------------------------------------------\n\nBuild time:   2021-11-09 20:40:36 UTC\nRevision:     96754b8c44399658178a768ac764d727c2addb37\n\nKotlin:       1.5.31\nGroovy:       3.0.9\nAnt:          Apache Ant(TM) version 1.10.11 compiled on July 10 2021\nJVM:          1.8.0_211 (Oracle Corporation 25.211-b12)\nOS:           Mac OS X 10.16 x86_64\n\n```\n\n使用gradle后，老版本的IDEA 2017.1 对gradle的支持较弱，建议升级到新版本 2021.1\n\n如果需要添加 gradle.properties 配置文件，请放在 $GRADLE_HOME 目录\n\n```\n➜  /Users/lintong/software/gradle-7.3.2 $ ls | grep gradle.properties\ngradle.properties\n\n```\n\n配置文件内容\n\n```\n/Users/lintong/software/gradle-7.3.2 $ cat gradle.properties\nusername=xx\npassword=xx\nmavenUser=xx\nmavenPassword=xx\n```\n\n## gradle处理依赖冲突\n\n使用gradle的时候如果遇到依赖冲突，可以添加如下配置，添加后再编译就能使得有冲突的时候自动失败\n\n```\nconfigurations.all {\n    resolutionStrategy {\n        failOnVersionConflict()\n    }\n}\n\n```\n\n<img src=\"/images/517519-20220726112148097-746512516.png\" width=\"1000\" height=\"504\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n可以看到引入了多个版本的 org.apache.commons:commons-math3，那么就需要找到是哪几个jar包引入了冲突，可以使用如下命令\n\n```\ngradle dependencyInsight --configuration runtimeClasspath --dependency org.apache.commons:commons-math3\n\n```\n\n<img src=\"/images/517519-20220726112326458-1399099016.png\" width=\"1000\" height=\"872\" loading=\"lazy\" />\n\n可以看到在&nbsp;org.apache.spark:spark-hive_2.12:3.1.2, org.apache.hadoop:hadoop-common:3.2.1, org.apache.spark:spark-core_2.12:3.1.2,&nbsp;org.apache.spark:spark-sql_2.12:3.1.2&nbsp;中引入了冲突，且默认使用的版本都是最高的&nbsp;3.6.1，这时候使用exclude来排除冲突，如下\n\n```\nimplementation ('org.apache.spark:spark-hive_2.12:3.1.2'){\n    exclude group:\"org.apache.commons\" , module:\"commons-math3\"\n}\nimplementation ('org.apache.hadoop:hadoop-common:3.2.1') {\n    exclude group:\"org.apache.commons\" , module:\"commons-math3\"\n}\nimplementation ('org.apache.spark:spark-core_2.12:3.1.2') {\n    exclude group:\"org.apache.commons\" , module:\"commons-math3\"\n}\nimplementation ('org.apache.spark:spark-sql_2.12:3.1.2'){\n    exclude group:\"org.apache.commons\" , module:\"commons-math3\"\n}\n\n```\n\n再次&nbsp;&nbsp;gradle clean fatJar，就看不到&nbsp;commons-math3 的冲突了\n\n<img src=\"/images/517519-20220727152910331-1925997657.png\" width=\"1000\" height=\"536\" loading=\"lazy\" />\n\n&nbsp;\n\n也可以强制指定版本的方式来排除依赖冲突，如下\n\n```\n    implementation ('com.google.guava:guava') {\n        version {\n            strictly '30.1.1-jre'\n        }\n    }\n\n```\n\n或者\n\n```\n    implementation ('com.google.guava:guava:30.1.1-jre') {\n        force = true\n    }\n\n```\n\n这样guvua的依赖冲突就解决了\n\n<img src=\"/images/517519-20220727153253947-473020755.png\" alt=\"\" loading=\"lazy\" />&nbsp;\n\n参考：[gradle依赖冲突解决](https://segmentfault.com/a/1190000021587501)&nbsp;和&nbsp;[详解Gradle依赖冲突解决方式](https://cloud.tencent.com/developer/article/1742859)\n\n[Downgrading versions and excluding dependencies](https://docs.gradle.org/current/userguide/dependency_downgrade_and_exclude.html)\n\n&nbsp;\n\n## gradle插件搜索\n\n```\nhttps://plugins.gradle.org/search\n\n```\n\n　　\n\n## implementation、api、compileOnly等的区别\n\n参考：[implementation、api、compileOnly区别详解](https://blog.csdn.net/yuzhiqiang_1993/article/details/78366985)\n\n&nbsp;\n","tags":["gradle"]},{"title":"Spark学习笔记——读写ScyllaDB","url":"/Spark学习笔记——读写ScyllaDB.html","content":"Scylla兼容cassandra API，所以可以使用spark读写cassandra的方法来进行读写\n\n## **1.查看scyllaDB对应的cassandra版本**\n\n```\ncqlsh:my_db> SHOW VERSION\n[cqlsh 5.0.1 | Cassandra 3.0.8 | CQL spec 3.3.1 | Native protocol v4]\n\n```\n\n## **2.查看spark和cassandra对应的版本**\n\n<img src=\"/images/517519-20211109220014493-1994204559.png\" width=\"700\" height=\"643\" loading=\"lazy\" />\n\n参考：https://github.com/datastax/spark-cassandra-connector\n\n## **3.写scyllaDB**\n\ndataset API写scyllaDB\n\n```\nds2.write\n      .mode(\"append\")\n      .format(\"org.apache.spark.sql.cassandra\")\n      .options(Map(\"table\" -> \"my_tb\", \"keyspace\" -> \"my_db\", \"output.consistency.level\" -> \"ALL\", \"ttl\" -> \"8640000\"))\n      .save()\n\n```\n\nRDD API写scyllaDB\n\n```\nimport com.datastax.oss.driver.api.core.ConsistencyLevel\nimport com.datastax.spark.connector._\n\nds.rdd.saveToCassandra(\"my_db\", \"my_tb\", writeConf = WriteConf(ttl = TTLOption.constant(8640000), consistencyLevel = ConsistencyLevel.ALL))\n\n```\n\n注意字段的数量和顺序需要和ScyllaDB表的顺序一致，可以使用下面方式select字段\n\n```\nval columns = Seq[String](\n      \"a\",\n      \"b\",\n      \"c\")\nval colNames = columns.map(name => col(name))\nval colRefs = columns.map(name => toNamedColumnRef(name))\n\nval df2 = df.select(colNames: _*)\ndf2.rdd\n      .saveToCassandra(ks, table, SomeColumns(colRefs: _*), writeConf = WriteConf(ttl = TTLOption.constant(8640000), consistencyLevel = ConsistencyLevel.ALL))\n\n```\n\n不过官方推荐使用DataFrame API，而不是RDD API\n\nIf you have the option we recommend using<!--more-->\n&nbsp;[DataFrames](https://github.com/datastax/spark-cassandra-connector/blob/master/doc/14_data_frames.md)&nbsp;instead of RDDs\n\n```\nhttps://github.com/datastax/spark-cassandra-connector/blob/master/doc/4_mapper.md\n\n```\n\n## **4.读scyllaDB**\n\n```\nval df = spark\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -> \"words\", \"keyspace\" -> \"test\" ))\n  .load()\n\n```\n\n　　\n\n参考：[通过 Spark 创建/插入数据到 Azure Cosmos DB Cassandra API](https://docs.microsoft.com/zh-cn/azure/cosmos-db/cassandra/spark-create-operations)\n\n[Cassandra Optimizations for Apache Spark](https://itnext.io/cassandra-optimizations-with-apache-spark-3ddf7f81ce43)\n\n## **5.cassandra connector参数**\n\n比如如果想实现spark更新scylla表的部分字段，可以将spark.cassandra.output.ignoreNulls设置为true\n\nconnector参数：[https://github.com/datastax/spark-cassandra-connector/blob/master/doc/reference.md](https://github.com/datastax/spark-cassandra-connector/blob/master/doc/reference.md)\n\n参数调优参考：[Spark + Cassandra, All You Need to Know: Tips and Optimizations](https://itnext.io/spark-cassandra-all-you-need-to-know-tips-and-optimizations-d3810cc0bd4e)\n\n&nbsp;\n","tags":["Spark","scyllaDB"]},{"title":"DataGrip2017.1连接Hive","url":"/DataGrip2017.1连接Hive.html","content":"在使用低版本的DataGrip的时候，还没有hive的data source，需要自行添加数据源\n\n1.下载hive driver，如果你使用的EMR的大数据集群的话，下载地址\n\n```\nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/HiveJDBCDriver.html\n\n```\n\n<!--more-->\n&nbsp;添加一个自定义的Driver and Data source\n\n<img src=\"/images/517519-20210915142214669-1252922945.png\" width=\"150\" height=\"343\" loading=\"lazy\" />\n\n起名为Amazon Hive，并保存\n\n<img src=\"/images/517519-20210915142337891-1292196557.png\" width=\"700\" height=\"281\" loading=\"lazy\" />\n\n&nbsp;\n\n2.再添加一个Amazon Hive\n\n<img src=\"/images/517519-20210915142515364-1127631381.png\" width=\"400\" height=\"175\" loading=\"lazy\" />\n\n添加连接hive的ip和port\n\n<img src=\"/images/517519-20210915142600685-12600208.png\" width=\"700\" height=\"253\" loading=\"lazy\" />\n\n然后将all databases勾起来\n\n<img src=\"/images/517519-20210917150850102-1053481528.png\" width=\"300\" height=\"156\" loading=\"lazy\" />\n\n&nbsp;\n\n3.添加成功\n\n<img src=\"/images/517519-20210915142738217-1962278323.png\" width=\"300\" height=\"113\" loading=\"lazy\" />\n\n参考：\n\n```\nhttps://my.oschina.net/u/3034870/blog/3019209\n\n```\n\n&nbsp;\n","tags":["开发工具"]},{"title":"Kafka学习笔记——Consumer API","url":"/Kafka学习笔记——Consumer API.html","content":"参考kafka官方文档，版本1.0.x\n\n```\nhttp://kafka.apache.org/10/documentation.html#consumerapi\n\n```\n\n依赖，选择 Cloudera Rel 中的 1.0.1-kafka-3.1.0\n\n```\n<dependency>\n    <groupId>org.apache.kafka</groupId>\n    <artifactId>kafka-clients</artifactId>\n    <version>1.0.1-kafka-3.1.0</version>\n</dependency>\n\n```\n\nKafka的消费者有2套API，一个是新版的Java API，在 org.apache.kafka.clients 包中\n\n```\nhttp://kafka.apache.org/10/documentation.html#newconsumerconfigs\n\n```\n\n一个是旧版的Scala API，在 kafka.consumer 包中\n\n```\nhttp://kafka.apache.org/10/documentation.html#oldconsumerconfigs\n\n```\n\n其中new consumer api中文含义参考\n\n```\nhttps://tonglin0325.github.io/xml/kafka/1.0.1-kafka-3.1.0/new-consumer-api.xml\n\n```\n\nkafka consumer参数调优：[kafka consumer 参数调优](http://blog.51yip.com/hadoop/2128.html)\n\nKafka消费者不是线程安全的。所有的网络I/O都发生在进行调用的应用程序的线程中。用户有责任确保多线程访问是正确同步的。\n","tags":["kafka"]},{"title":"zigzag编码原理","url":"/zigzag编码原理.html","content":"在Thrift，Protobuf和avro序列化框架中，不约而同使用了zigzag编码来对数字进行编码，从而达到减少数据传输量的目的。\n\nzigzag算法的核心主要是去除二进制数字中的前导0，因为在绝大多数情况下，我们使用到的整数，往往是比较小的。\n\n<img src=\"/images/517519-20230823142929271-1622069254.png\" width=\"500\" height=\"393\" loading=\"lazy\" />\n\n参考：[小而巧的数字压缩算法：zigzag](https://mp.weixin.qq.com/s?__biz=MzA3MDExNzcyNA%3D%3D&amp;mid=2650392086&amp;idx=1&amp;sn=6a2ecfe2548f121a4726d03bf23f4478&amp;scene=0)\n\n在avro编码中，对于字符串Martin，长度为6，而6的二进制为0000 0110，其中首位置的0为符号位，在zigzag编码中，正数的符号位会移动到末尾，其它位往前移动一位，所以会变成0000 1100，即0c，再后面的字节是字符串UTF-8编码后的结果\n\n<img src=\"/images/517519-20230823144536825-1206571882.png\" width=\"500\" height=\"251\" loading=\"lazy\" />\n\n在protobuf编码中，对于字符串的Martin，刚开始的字节表示其id和数据类型，下一个字节表示其长度，后面的字节是字符串UTF-8编码后的结果\n\n<img src=\"/images/517519-20230823171343451-943173252.png\" width=\"500\" height=\"227\" loading=\"lazy\" />\n\n参考：《数据密集型应用系统设计》的 [Schema evolution in Avro, Protocol Buffers and Thrift](https://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html)\n\n[Avro，Protocol Buffer和Thrift中的模式演化(译)](https://zhuanlan.zhihu.com/p/149889589)\n\n<!--more-->\n&nbsp;\n","tags":["avro","Thrift","google"]},{"title":"存储底层数据结构对比","url":"/存储底层数据结构对比.html","content":"该文章对比了常用的一些存储底层所使用的数据结构。\n\n## 1.B+树\n\nMySQL，MongoDB的索引使用的就是B+树\n\nB+树在多读少写（相对而言）的情境下比较有优势。\n\n**B+树的主要优点：**\n\n- 1.结构比较扁平，高度低（一般不超过4层），随机寻道次数少；\n- 2.数据存储密度大，且都位于叶子节点，查询稳定，遍历方便；\n- 3.叶子节点形成有序链表，范围查询转化为顺序读，效率高。相对而言B树必须通过中序遍历才能支持范围查询。\n\n**B+树的缺点：**\n\n- 1.如果写入的数据比较离散，那么寻找写入位置时，子节点有很大可能性不会在内存中，最终会产生大量的随机写，性能下降。\n- 2.如果B+树已经运行了很长时间，写入了很多数据，随着叶子节点分裂，其对应的块会不再顺序存储，而变得分散。这时执行范围查询也会变成随机读，效率降低了。\n\n参考：[从B+树到LSM树，及LSM树在HBase中的应用](https://cloud.tencent.com/developer/article/2016024)\n\n## 2.LSM树(Log-structured Merge-Tree/日志结构的合并树)\n\nRocksDB，HBase，Cassandra，Kudu底层使用的是LSM树\n\n**LSM树的优点**：\n\n- 1.LSM树由于数据按顺序存储，因此可以有效地执行区间查询（从最小值到最大值扫描所有的键），并且由于磁盘是顺序写入的（以顺序方式写入紧凑的SSTable文件，而不必重写树中的多个页），所以LSM-Tree可以支持非常高的写入吞吐量。\n\n**LSM树的缺点**：\n\n- 1.压缩过程中有时会干扰正在进行的读写操作。即使存储引擎尝试增量地执行压缩，并且不影响并发访问，但由于磁盘的并发资源有限，所以当磁盘执行昂贵的压缩操作时，很容易发生读写请求等待的情况。而B-tree的响应延迟则更具确定性。\n\n参考：[LSM树由来、设计思想以及应用到HBase的索引](https://www.cnblogs.com/yanghuahui/p/3483754.html) 和 数据密集型应用系统设计第3章\n\n## 3.跳表\n\nRedis底层使用的是SkipTable\n\n### Mysql的索引为什么使用B+树而不使用跳表?\n\nB+树是多叉树结构，每个结点都是一个16k的数据页，能存放较多索引信息，所以扇出很高。三层左右就可以存储2kw左右的数据(知道结论就行，想知道原因可以看之前的文章)。也就是说查询一次数据，如果这些数据页都在磁盘里，那么最多需要查询三次磁盘IO。\n\n跳表是链表结构，一条数据一个结点，如果最底层要存放2kw数据，且每次查询都要能达到二分查找的效果，2kw大概在2的24次方左右，所以，跳表大概高度在24层左右。最坏情况下，这24层数据会分散在不同的数据页里，也即是查一次数据会经历24次磁盘IO。\n\n因此存放同样量级的数据，B+树的高度比跳表的要少，如果放在mysql数据库上来说，就是磁盘IO次数更少，因此B+树查询更快。\n\n而针对写操作，B+树需要拆分合并索引数据页，跳表则独立插入，并根据随机函数确定层数，没有旋转和维持平衡的开销，因此跳表的写入性能会比B+树要好。\n\n### redis为什么使用跳表而不使用B+树或二叉树呢?\n\nredis支持多种数据结构，里面有个有序集合，也叫ZSET。内部实现就是跳表。那为什么要用跳表而不用B+树等结构呢?\n\n大家知道，redis 是纯纯的内存数据库。\n\n进行读写数据都是操作内存，跟磁盘没啥关系，因此也不存在磁盘IO了，所以层高就不再是跳表的劣势了。\n\n并且前面也提到B+树是有一系列合并拆分操作的，换成红黑树或者其他AVL树的话也是各种旋转，目的也是为了保持树的平衡。\n\n而跳表插入数据时，只需要随机一下，就知道自己要不要往上加索引，根本不用考虑前后结点的感受，也就少了旋转平衡的开销。\n\n因此，redis选了跳表，而不是B+树。\n\n参考：[MySQL的索引为什么使用B+树而不使用跳表？](https://www.51cto.com/article/706701.html)\n","tags":["数据结构"]},{"title":"ElasticSearch学习笔记——插件开发","url":"/ElasticSearch学习笔记——插件开发.html","content":"参考\n\n```\nhttps://dzone.com/articles/elasticsearch5-how-to-build-a-plugin-and-add-a-lis\nhttps://github.com/chrisshayan/es-changes-feed-plugin\nhttps://blog.csdn.net/qq_16164711/article/details/87872383\nhttps://blog.gaiaproject.club/es-develop-plugin/\nhttps://blog.51cto.com/13755625/2117995\n\n```\n\nEs的插件主要有如下几种类型，参考\n\n```\nhttps://github.com/elastic/elasticsearch/tree/master/server/src/main/java/org/elasticsearch/plugins\n\n```\n\n<!--more-->\n&nbsp;\n\n[API Extension Plugins](https://www.elastic.co/guide/en/elasticsearch/plugins/6.8/api.html)&nbsp;API拓展插件：API extension plugins add new functionality to Elasticsearch by adding new APIs or features, usually to do with search or mapping.\n\n[Analysis Plugins](https://www.elastic.co/guide/en/elasticsearch/plugins/6.8/analysis.html)&nbsp;解析器插件：Analysis plugins extend Elasticsearch by adding new analyzers, tokenizers, token filters, or character filters to Elasticsearch.\n\n[Alerting Plugins](https://www.elastic.co/guide/en/elasticsearch/plugins/6.8/alerting.html)&nbsp;告警插件：Alerting plugins allow Elasticsearch to monitor indices and to trigger alerts when thresholds are breached.\n\n[Discovery Plugins](https://www.elastic.co/guide/en/elasticsearch/plugins/6.8/discovery.html)&nbsp;发现插件：Discovery plugins extend Elasticsearch by adding new discovery mechanisms that can be used instead of&nbsp;[Zen Discovery](https://www.elastic.co/guide/en/elasticsearch/reference/6.8/modules-discovery-zen.html).\n\n[Ingest Plugins](https://www.elastic.co/guide/en/elasticsearch/plugins/6.8/ingest.html)&nbsp;摄取插件：The ingest plugins extend Elasticsearch by providing additional ingest node capabilities.\n\n[Management Plugins](https://www.elastic.co/guide/en/elasticsearch/plugins/6.8/management.html)&nbsp;管理插件：Management plugins offer UIs for managing and interacting with Elasticsearch.\n\n[Mapper Plugins](https://www.elastic.co/guide/en/elasticsearch/plugins/6.8/mapper.html)&nbsp;Mapper插件：Mapper plugins allow new field datatypes to be added to Elasticsearch.\n\n[Security Plugins](https://www.elastic.co/guide/en/elasticsearch/plugins/6.8/security.html)&nbsp;安全插件：Security plugins add a security layer to Elasticsearch.\n\n[Snapshot/Restore Repository Plugins](https://www.elastic.co/guide/en/elasticsearch/plugins/6.8/repository.html)&nbsp;快照插件：Repository plugins extend the&nbsp;[Snapshot/Restore](https://www.elastic.co/guide/en/elasticsearch/reference/6.8/modules-snapshots.html)&nbsp;functionality in Elasticsearch by adding repositories backed by the cloud or by distributed file systems:&nbsp;\n\n[Store Plugins](https://www.elastic.co/guide/en/elasticsearch/plugins/6.8/store.html)&nbsp;存储插件：Store plugins offer alternatives to default Lucene stores.\n\n&nbsp;\n\nActionPlugin：An additional extension point for {@link Plugin}s that extends Elasticsearch's scripting functionality\n\n```\nhttps://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/plugins/ActionPlugin.java\n```\n\nCircuitBreakerPlugin 断路器插件：An extension point for {@link Plugin} implementations to add custom circuit breakers\n\n```\nhttps://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/plugins/CircuitBreakerPlugin.java\n\n```\n\nClusterPlugin 集群插件：An extension point for {@link Plugin} implementations to customer behavior of cluster management.\n\n```\nhttps://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/plugins/ClusterPlugin.java\n\n```\n\nEnginePlugin 引擎插件：A plugin that provides alternative engine implementations.\n\n```\nhttps://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/plugins/EnginePlugin.java\n\n```\n\nExtensiblePlugin 拓展插件：This class provides a callback for extensible plugins to be informed of other plugins which extend them.\n\n```\nhttps://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/plugins/ExtensiblePlugin.java\n\n```\n\nIndexStorePlugin 索引存储插件：A plugin that provides alternative directory implementations.\n\n```\nhttps://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/plugins/IndexStorePlugin.java\n\n```\n\nNetworkPlugin 网络插件：Plugin for extending network and transport related classes\n\n```\nhttps://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/plugins/NetworkPlugin.java\n\n```\n\nPersistentTaskPlugin&nbsp;持续任务插件：Plugin for registering persistent tasks executors.\n\n```\nhttps://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/plugins/PersistentTaskPlugin.java\n\n```\n\nReloadablePlugin Reload插件：An extension point for {@link Plugin}s that can be reloaded. There is no clear definition about what reloading a plugin actually means. When a plugin is reloaded it might rebuild any internal members.&nbsp;Plugins usually implement&nbsp;this interface in order to reread the values of {@code SecureSetting}s and then rebuild any dependent internal members.\n\n```\nhttps://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/plugins/ReloadablePlugin.java\n\n```\n\nRestCompatibilityPlugin Rest兼容性插件：An extension point for Compatible API plugin implementation.\n\n```\nhttps://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/plugins/RestCompatibilityPlugin.java\n\n```\n\nScriptPlugin 脚本插件：An additional extension point for {@link Plugin}s that extends Elasticsearch's scripting functionality.\n\n```\nhttps://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/plugins/ScriptPlugin.java\n```\n\nSearchPlugin 搜索插件：Plugin for extending search time behavior.\n\n```\nhttps://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/plugins/SearchPlugin.java\n\n```\n\nSystemIndexPlugin 系统索引插件：Plugin for defining system indices. Extends {@link ActionPlugin} because system indices must be accessed via APIs added by the plugin that owns the system index, rather than standard APIs.\n\n```\nhttps://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/plugins/SystemIndexPlugin.java\n\n```\n\n&nbsp;\n\n编写es插件的时候需要提供2类文件\n\n```\nlintong@master:~/software/apache/elasticsearch-6.2.4/plugins/analysis-ik$ ls\ncommons-codec-1.9.jar    elasticsearch-analysis-ik-6.2.4.jar  httpcore-4.4.4.jar\ncommons-logging-1.2.jar  httpclient-4.5.2.jar                 plugin-descriptor.properties\n\n```\n\n一个是plugin-descriptor.properties文件，里面是插件的一些信息，比如\n\n```\ndescription=自定义插件\nversion=1.0-SNAPSHOT\nname=my plugin\nclassname=com.xxxx.xx.MetadataListenerPlugin\njava.version=1.8\nelasticsearch.version=6.2.4\n\n```\n\n一个是插件的jar包文件\n\n在es启动的过程中，首先会启动节点node，之后PluginsService会加载这个node目录下的插件，可以看到my plugin这个插件已经成功加载\n\n```\n[2021-04-29T10:14:15,061][INFO ][o.e.n.Node               ] [] initializing ...\n[2021-04-29T10:14:15,223][INFO ][o.e.e.NodeEnvironment    ] [gl4ygFd] using [1] data paths, mounts [[/media/xxx/14201D6AD04D90DA (/dev/sdb5)]], net usable_space [421.2gb], net total_space [466gb], types [fuseblk]\n[2021-04-29T10:14:15,223][INFO ][o.e.e.NodeEnvironment    ] [gl4ygFd] heap size [990.7mb], compressed ordinary object pointers [true]\n[2021-04-29T10:14:15,587][INFO ][o.e.n.Node               ] node name [gl4ygFd] derived from node ID [gl4ygFdaRCKyTcLB6SoYJg]; set [node.name] to override\n[2021-04-29T10:14:15,588][INFO ][o.e.n.Node               ] version[6.2.4], pid[1162], build[ccec39f/2018-04-12T20:37:28.497551Z], OS[Linux/4.4.0-165-generic/amd64], JVM[Private Build/OpenJDK 64-Bit Server VM/1.8.0_222/25.222-b10]\n[2021-04-29T10:14:15,590][INFO ][o.e.n.Node               ] JVM arguments [-Xms1g, -Xmx1g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Djava.io.tmpdir=/tmp/elasticsearch.P9YagNAJ, -XX:+HeapDumpOnOutOfMemoryError, -XX:+PrintGCDetails, -XX:+PrintGCDateStamps, -XX:+PrintTenuringDistribution, -XX:+PrintGCApplicationStoppedTime, -Xloggc:logs/gc.log, -XX:+UseGCLogFileRotation, -XX:NumberOfGCLogFiles=32, -XX:GCLogFileSize=64m, -Des.path.home=/home/lintong/software/apache/elasticsearch-6.2.4, -Des.path.conf=/home/lintong/software/apache/elasticsearch-6.2.4/config]\n[2021-04-29T10:14:16,141][INFO ][o.e.p.PluginsService     ] [gl4ygFd] loaded module [aggs-matrix-stats]\n[2021-04-29T10:14:16,141][INFO ][o.e.p.PluginsService     ] [gl4ygFd] loaded module [analysis-common]\n[2021-04-29T10:14:16,141][INFO ][o.e.p.PluginsService     ] [gl4ygFd] loaded module [ingest-common]\n[2021-04-29T10:14:16,142][INFO ][o.e.p.PluginsService     ] [gl4ygFd] loaded module [lang-expression]\n[2021-04-29T10:14:16,142][INFO ][o.e.p.PluginsService     ] [gl4ygFd] loaded module [lang-mustache]\n[2021-04-29T10:14:16,142][INFO ][o.e.p.PluginsService     ] [gl4ygFd] loaded module [lang-painless]\n[2021-04-29T10:14:16,143][INFO ][o.e.p.PluginsService     ] [gl4ygFd] loaded module [mapper-extras]\n[2021-04-29T10:14:16,143][INFO ][o.e.p.PluginsService     ] [gl4ygFd] loaded module [parent-join]\n[2021-04-29T10:14:16,143][INFO ][o.e.p.PluginsService     ] [gl4ygFd] loaded module [percolator]\n[2021-04-29T10:14:16,144][INFO ][o.e.p.PluginsService     ] [gl4ygFd] loaded module [rank-eval]\n[2021-04-29T10:14:16,144][INFO ][o.e.p.PluginsService     ] [gl4ygFd] loaded module [reindex]\n[2021-04-29T10:14:16,144][INFO ][o.e.p.PluginsService     ] [gl4ygFd] loaded module [repository-url]\n[2021-04-29T10:14:16,144][INFO ][o.e.p.PluginsService     ] [gl4ygFd] loaded module [transport-netty4]\n[2021-04-29T10:14:16,144][INFO ][o.e.p.PluginsService     ] [gl4ygFd] loaded module [tribe]\n[2021-04-29T10:14:16,144][INFO ][o.e.p.PluginsService     ] [gl4ygFd] loaded plugin [analysis-ik]\n[2021-04-29T10:14:16,150][INFO ][o.e.p.PluginsService     ] [gl4ygFd] loaded plugin [my plugin]\n\n```\n\n　　\n","tags":["ELK"]},{"title":"HBase学习笔记——rowkey","url":"/HBase学习笔记——rowkey.html","content":"## 1.Airbnb rowkey设计案例\n\n在Airbnb的rowkey设计案例中，使用了**hash法**避免了写入热点问题，其中\n\n<img src=\"/images/517519-20210121102551038-1014294290.png\" alt=\"\" width=\"446\" height=\"306\" loading=\"lazy\" />\n\n**Event_key**标识了一条日志的唯一性，用于将来自Kafka的日志数据进行去重；\n\n**Shard_id**是将Event_key进行hash（可以参考es的路由哈希算法Hashing.murmur3_128）之后，对Shard_num进行取余后的结果，Shard_num感觉应该是当前hbase表region server的总数，由于airbnb在hbase中存储的是实时日志数据，并开启了Hbase的TTL，所以当前hbase表中的数据总量应该是可预测的，即region server数量不会无限增加\n\n**Shard_key**应该就是当前业务的region_start_keys+shard_id，比如当前业务分配的前缀为00000，同时规划了100个table regions给这个业务，即00-99，那么Shard_key的范围就是0000000-0000099\n\n**rowkey**就是Shard_key.Event_key，比如0000000.air_events.canaryevent.016230-a3db-434e\n\n<img src=\"/images/517519-20210121103802880-441983531.png\" alt=\"\" width=\"560\" height=\"281\" loading=\"lazy\" />\n\n参考：\n\n[Reliable and Scalable Data Ingestion at Airbnb](https://www.slideshare.net/HadoopSummit/reliable-and-scalable-data-ingestion-at-airbnb-63920989)\n\n[Apache HBase at Airbnb](https://www.slideshare.net/HBaseCon/apache-hbase-at-airbnb)\n\n[Airbnb软件工程师丁辰 - Airbnb的Streaming ETL](https://myslide.cn/slides/3473)\n\n## **2.rowkey设计原则**\n\n**1.rowkey长度原则**：rowkey是一个二进制码流，可以是任意字符串，最大长度<!--more-->\n&nbsp;**64kb**&nbsp;，实际应用中一般为10-100bytes，以&nbsp;`byte[]`&nbsp;形式保存，一般设计成定长。\n\n建议越短越好，不要超过16个字节，原因如下：\n\n<li>\n数据的持久化文件HFile中是按照KeyValue存储的，如果rowkey过长，比如超过100字节，1000w行数据，光rowkey就要占用100*1000w=10亿个字节，将近1G数据，这样会极大影响HFile的存储效率；\n</li>\n<li>\nMemStore将缓存部分数据到内存，如果rowkey字段过长，内存的有效利用率就会降低，系统不能缓存更多的数据，这样会降低检索效率。\n</li>\n<li>\n目前操作系统都是64位系统，内存8字节对齐，控制在16个字节，8字节的整数倍利用了操作系统的最佳特性。\n</li>\n\n**2.rowkey散列原则**：如果rowkey按照时间戳的方式递增，不要将时间放在二进制码的前面，建议将rowkey的高位作为散列字段，由程序随机生成，低位放时间字段，这样将提高数据均衡分布在每个RegionServer，以实现负载均衡的几率。\n\n如果没有散列字段，首字段直接是时间信息，所有的数据都会集中在一个RegionServer上，这样在数据检索的时候负载会集中在个别的RegionServer上，造成热点问题，会降低查询效率。\n\n**3.rowkey唯一原则**：必须在设计上保证其唯一性，rowkey是按照字典顺序排序存储的，因此，设计rowkey的时候，要充分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到一块。\n\n## **3.热点问题**\n\n　　HBase中的行是按照rowkey的字典顺序排序的，这种设计优化了scan操作，可以将相关的行以及会被一起读取的行存取在临近位置，便于scan。然而糟糕的rowkey设计是热点的源头。\n\n　　热点发生在大量的client直接访问集群的一个或极少数个节点（访问可能是读，写或者其他操作）。大量访问会使热点region所在的单个机器超出自身承受能力，引起性能下降甚至region不可用，这也会影响同一个RegionServer上的其他region，由于主机无法服务其他region的请求。\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;设计良好的数据访问模式以使集群被充分，均衡的利用。为了避免写热点，设计rowkey使得不同行在同一个region，但是在更多数据情况下，数据应该被写入集群的多个region，而不是一个。下面是一些常见的避免热点的方法以及它们的优缺点：\n\n#### 1.加盐\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里所说的加盐不是密码学中的加盐，而是在rowkey的前面增加随机数，具体就是给rowkey分配一个随机前缀以使得它和之前的rowkey的开头不同。分配的前缀种类数量应该和你想使用数据分散到不同的region的数量一致。加盐之后的rowkey就会根据随机生成的前缀分散到各个region上，以避免热点。\n\n#### 2.哈希\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;哈希会使同一行永远用一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是可以预测的。使用确定的哈希可以让客户端重构完整的rowkey，可以使用get操作准确获取某一个行数据。\n\n#### 3.反转\n\n第三种防止热点的方法时反转固定长度或者数字格式的rowkey。这样可以使得rowkey中经常改变的部分（最没有意义的部分）放在前面。这样可以有效的随机rowkey，但是牺牲了rowkey的有序性。\n\n反转rowkey的例子以手机号为rowkey，可以将手机号反转后的字符串作为rowkey，这样的就避免了以手机号那样比较固定开头导致热点问题\n\n#### 4.时间戳反转\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一个常见的数据处理问题是快速获取数据的最近版本，使用反转的时间戳作为rowkey的一部分对这个问题十分有用，可以用 Long.Max_Value - timestamp 追加到key的末尾，例如 [key][reverse_timestamp] , [key] 的最新值可以通过scan [key]获得[key]的第一条记录，因为HBase中rowkey是有序的，第一条记录是最后录入的数据。\n\n## **4.其他一些建议**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;尽量减少行键和列族的大小。在HBase中，value永远和它的key一起传输的。当具体的值在系统间传输时，它的rowkey，列名，时间戳也会一起传输。如果你的rowkey和列名很大，这个时候它们将会占用大量的存储空间。\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;列族尽可能越短越好，最好是一个字符。\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;冗长的属性名虽然可读性好，但是更短的属性名存储在HBase中会更好。\n\n转自：[HBase的rowKey设计技巧](https://blog.csdn.net/weixin_44318830/article/details/103639847)\n","tags":["Airbnb","HBase"]},{"title":"Hive学习笔记——fetch","url":"/Hive学习笔记——fetch.html","content":"在美团点评的文章中，介绍了HiveSQL转化为MapReduce的过程\n\n```\n1、Antlr定义SQL的语法规则，完成SQL词法，语法解析，将SQL转化为抽象语法树AST Tree\n2、遍历AST Tree，抽象出查询的基本组成单元QueryBlock\n3、遍历QueryBlock，翻译为执行操作树OperatorTree\n4、逻辑层优化器进行OperatorTree变换，合并不必要的ReduceSinkOperator，减少shuffle数据量\n5、遍历OperatorTree，翻译为MapReduce任务\n6、物理层优化器进行MapReduce任务的变换，生成最终的执行计划\n\n```\n\n参考：[Hive SQL的编译过程](https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html)\n\n但是不是所有的SQL都有必要转换为MR来执行，比如\n\n```\nselect * from xx.xx limit 1\n\n```\n\nHive只需要直接读取文件，并传输到控制台即可\n\n<!--more-->\n&nbsp;\n\n在hive-default.xml配置文件中，有2个参数，hive.fetch.task.conversion和hive.fetch.task.conversion.threshold\n\n**hive.fetch.task.conversion**属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce\n\n**hive.fetch.task.conversion.threshold**属性表示在输入大小为多少以内的时候fetch task生效，默认1073741824 byte = 1G\n\n```\n<property>\n    <name>hive.fetch.task.conversion</name>\n    <value>more</value>\n    <description>\n      Expects one of [none, minimal, more].\n      Some select queries can be converted to single FETCH task minimizing latency.\n      Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts (which incurs RS), lateral views and joins.\n      0. none : disable hive.fetch.task.conversion\n      1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only\n      2. more  : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)\n    </description>\n</property>\n\n<property>\n  <name>hive.fetch.task.conversion.threshold</name>\n  <value>1073741824</value>\n  <description>\n    Input threshold for applying hive.fetch.task.conversion. If target table is native, input length\n    is calculated by summation of file lengths. If it's not native, storage handler for the table\n    can optionally implement org.apache.hadoop.hive.ql.metadata.InputEstimator interface.\n  </description>\n</property>\n\n```\n\n参考：\n\n[Hive快速入门系列(14) | Hive性能调优 [一]Fetch抓取与本地模式](https://cloud.tencent.com/developer/article/1733563)\n\n[Hive笔记之Fetch Task](https://www.cnblogs.com/cc11001100/p/9434076.html)\n\n&nbsp;\n\n[](https://www.cnblogs.com/cc11001100/p/9434076.html)此外，hive还有**local模式**\n\n配置如下参数，可以开启Hive的本地模式：hive> set hive.exec.mode.local.auto=true;(默认为false)\n\n当一个job满足如下条件才能真正使用本地模式：\n\n1.job的输入数据大小必须小于参数：hive.exec.mode.local.auto.inputbytes.max(默认128MB)\n\n2.job的map数必须小于参数：hive.exec.mode.local.auto.tasks.max(默认4)\n\n3.job的reduce数必须为0或者1\n\n参考：[hive使用本地模式--set hive.exec.mode.local.auto=true;(默认为false)](https://blog.csdn.net/u010002184/article/details/102789922)\n\n　\n","tags":["Hive"]},{"title":"ElasticSearch学习笔记——ik分词添加词库","url":"/ElasticSearch学习笔记——ik分词添加词库.html","content":"前置条件是安装ik分词，请参考\n\n[Elasticsearch学习笔记&mdash;&mdash;分词](https://www.cnblogs.com/tonglin0325/p/10088021.html)\n\n1.在ik分词的config下添加词库文件\n\n```\n~/software/apache/elasticsearch-6.2.4/config/analysis-ik$ ls | grep mydic.dic\nmydic.dic\n\n```\n\n内容为\n\n```\n我给祖国献石油\n\n```\n\n2.配置词库路径，编辑IKAnalyzer.cfg.xml配置文件，添加新增的词库\n\n<img src=\"/images/517519-20210107154921454-1963852513.png\" alt=\"\" loading=\"lazy\" />\n\n3.重启es\n\n4.测试\n\ndata.json\n\n```\n{\n        \"analyzer\":\"ik_max_word\",\n        \"text\": \"我给祖国献石油\"\n}\n\n```\n\n添加之后的ik分词结果\n\n```\ncurl -H 'Content-Type: application/json' http://localhost:9200/_analyze?pretty=true -d@data.json\n{\n  \"tokens\" : [\n    {\n      \"token\" : \"我\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 1,\n      \"type\" : \"CN_CHAR\",\n      \"position\" : 0\n    },\n    {\n      \"token\" : \"给\",\n      \"start_offset\" : 1,\n      \"end_offset\" : 2,\n      \"type\" : \"CN_CHAR\",\n      \"position\" : 1\n    },\n    {\n      \"token\" : \"祖国\",\n      \"start_offset\" : 2,\n      \"end_offset\" : 4,\n      \"type\" : \"CN_WORD\",\n      \"position\" : 2\n    },\n    {\n      \"token\" : \"献\",\n      \"start_offset\" : 4,\n      \"end_offset\" : 5,\n      \"type\" : \"CN_CHAR\",\n      \"position\" : 3\n    },\n    {\n      \"token\" : \"石油\",\n      \"start_offset\" : 5,\n      \"end_offset\" : 7,\n      \"type\" : \"CN_WORD\",\n      \"position\" : 4\n    }\n  ]\n}\n\n```\n\n添加之后的ik分词结果，分词结果的tokens中增加了 \"我给祖国献石油\"\n\n```\ncurl -H 'Content-Type: application/json' http://localhost:9200/_analyze?pretty=true -d@data.json\n{\n  \"tokens\" : [\n    {\n      \"token\" : \"我给祖国献石油\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 7,\n      \"type\" : \"CN_WORD\",\n      \"position\" : 0\n    },\n    {\n      \"token\" : \"祖国\",\n      \"start_offset\" : 2,\n      \"end_offset\" : 4,\n      \"type\" : \"CN_WORD\",\n      \"position\" : 1\n    },\n    {\n      \"token\" : \"献\",\n      \"start_offset\" : 4,\n      \"end_offset\" : 5,\n      \"type\" : \"CN_CHAR\",\n      \"position\" : 2\n    },\n    {\n      \"token\" : \"石油\",\n      \"start_offset\" : 5,\n      \"end_offset\" : 7,\n      \"type\" : \"CN_WORD\",\n      \"position\" : 3\n    }\n  ]\n}\n\n```\n\n　　\n\n<!--more-->\n&nbsp;\n","tags":["ELK"]},{"title":"Flink学习笔记——用户自定义Functions","url":"/Flink学习笔记——用户自定义Functions.html","content":"Flink支持用户自定义 Functions，方法有2个\n\nRef\n\n```\nhttps://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/user_defined_functions.html\n\n```\n\n1. 实现 MapFunction接口\n\n```\nclass MyMapFunction implements MapFunction<String, Integer> {\n  public Integer map(String value) { return Integer.parseInt(value); }\n};\ndata.map(new MyMapFunction());\n\n```\n\n2. 继承 RichMapFunction\n\n```\nclass MyMapFunction extends RichMapFunction<String, Integer> {\n  public Integer map(String value) { return Integer.parseInt(value); }\n};\n\n```\n\n<!--more-->\n&nbsp;\n\n**累加器和计数器**\n\n这个应该和Hadoop和Spark的counter类似，参考\n\n```\nhttps://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/user_defined_functions.html#%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%92%8C%E8%AE%A1%E6%95%B0%E5%99%A8\n\n```\n\n　　\n","tags":["Flink"]},{"title":"Flink学习笔记——Execution Mode","url":"/Flink学习笔记——Execution Mode.html","content":"Flink有3中运行模式，分别是STREAMING，BATCH和AUTOMATIC\n\nRef\n\n```\nhttps://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/datastream_execution_mode.html\n\n```\n\n**1.STREAMING**运行模式 是DataStream默认的运行模式\n\n**2.BATCH**运行模式 也可以在DataStream API上运行\n\n**3.AUTOMATIC**运行模式 是让系统根据source类型自动选择运行模式\n\n可以通过命令行来配置运行模式\n\n```\nbin/flink run -Dexecution.runtime-mode=BATCH examples/streaming/WordCount.jar\n\n```\n\n也可以在代码中配置\n\n```\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nenv.setRuntimeMode(RuntimeExecutionMode.BATCH);\n\n```\n\n<!--more-->\n&nbsp;\n\n在**STREAMING运行模式**中，Flink使用[StateBackend](https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/state/state_backends.html)来控制状态存储和checkpoint的工作，RocksDBStateBackend支持增量Checkpoint，其他2个不支持\n\n在**BATCH****运行模式**中，statebackend是被忽略的，batch模式不支持checkpoint\n\nFlink支持3种**状态后端**（StateBackend），其中\n\n```\n1.MemoryStateBackend\n2.FsStateBackend\n3.RocksDBStateBackend\n\n```\n\n何时使用 RocksDBStateBackend\n\n```\n1.RocksDBStateBackend 最适合用于处理大状态，长窗口，或大键值状态的有状态处理任务。\n2.RocksDBStateBackend 非常适合用于高可用方案。\n3.RocksDBStateBackend 是目前唯一支持增量 checkpoint 的后端。增量 checkpoint 非常使用于超大状态的场景。\n\n```\n\nStateBackend可以参考\n\n[【flink】flink状态后端配置-设置State Backend](https://www.jianshu.com/p/ac0fff780d40)\n\n[Flink 小贴士 (4): 如何选择状态后端](http://wuchong.me/blog/2018/11/21/flink-tips-how-to-choose-state-backends/)\n\n[从RocksDBStateBackend讲述Flink的State机制](https://www.cnblogs.com/lighten/p/13234350.html)\n\n&nbsp;&nbsp;\n\n在**STREAMING运行模式**中，flink使用checkpoint来进行容错，checkpoint参考\n\n```\nhttps://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/state/checkpointing.html\n\n```\n\n在**BATCH运行模式**中，flink会回滚到到之前的stage，只有失败的task才会重启，这比从checkpoint重启所以的task要高效，所以建议如果任务能在BATCK运行模式下运行，就使用BATCH运行模式\n\n&nbsp;\n\n**Broadcast State**\n\n1.&nbsp;**STREAMING运行模式**，一个典型应用就是允许一个控制流接收一个rules，并将其广播到其他的stream中\n\n2.&nbsp;**BATCH运行模式不支持**\n\n&nbsp;\n","tags":["Flink"]},{"title":"Flink学习笔记——DataSet API","url":"/Flink学习笔记——DataSet API.html","content":"Flink中的DataSet任务用于实现data sets的转换，data set通常是固定的数据源，比如可读文件，或者本地集合等。\n\nRef\n\n```\nhttps://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/batch/\n\n```\n\n<!--more-->\n&nbsp;使用DataSet API需要使用 批处理 env\n\n```\nExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n```\n\nDataSet支持的**Data Source**有：File-based，Collection-based，Generic\n\n**1.File-based**\n\n```\nreadTextFile(path) / TextInputFormat - Reads files line wise and returns them as Strings.\n\nreadTextFileWithValue(path) / TextValueInputFormat - Reads files line wise and returns them as StringValues. StringValues are mutable strings.\n\nreadCsvFile(path) / CsvInputFormat - Parses files of comma (or another char) delimited fields. Returns a DataSet of tuples or POJOs. Supports the basic java types and their Value counterparts as field types.\n\nreadFileOfPrimitives(path, Class) / PrimitiveInputFormat - Parses files of new-line (or another char sequence) delimited primitive data types such as String or Integer.\n\nreadFileOfPrimitives(path, delimiter, Class) / PrimitiveInputFormat - Parses files of new-line (or another char sequence) delimited primitive data types such as String or Integer using the given delimiter.\n\n```\n\n**2.Collection-based**\n\n```\nfromCollection(Collection) - Creates a data set from a Java.util.Collection. All elements in the collection must be of the same type.\n\nfromCollection(Iterator, Class) - Creates a data set from an iterator. The class specifies the data type of the elements returned by the iterator.\n\nfromElements(T ...) - Creates a data set from the given sequence of objects. All objects must be of the same type.\n\nfromParallelCollection(SplittableIterator, Class) - Creates a data set from an iterator, in parallel. The class specifies the data type of the elements returned by the iterator.\n\ngenerateSequence(from, to) - Generates the sequence of numbers in the given interval, in parallel.\n\n```\n\n**3.Generic**\n\n```\nreadFile(inputFormat, path) / FileInputFormat - Accepts a file input format.\n\ncreateInput(inputFormat) / InputFormat - Accepts a generic input format.\n\n```\n\n&nbsp;\n\nData Set支持的**transformations**算子\n\n```\nhttps://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/batch/dataset_transformations.html\n\n```\n\n&nbsp;\n\nDataSet支持的**Data Sink**有：\n\n```\nwriteAsText() / TextOutputFormat - Writes elements line-wise as Strings. The Strings are obtained by calling the toString() method of each element.\nwriteAsFormattedText() / TextOutputFormat - Write elements line-wise as Strings. The Strings are obtained by calling a user-defined format() method for each element.\nwriteAsCsv(...) / CsvOutputFormat - Writes tuples as comma-separated value files. Row and field delimiters are configurable. The value for each field comes from the toString() method of the objects.\nprint() / printToErr() / print(String msg) / printToErr(String msg) - Prints the toString() value of each element on the standard out / standard error stream. Optionally, a prefix (msg) can be provided which is prepended to the output. This can help to distinguish between different calls to print. If the parallelism is greater than 1, the output will also be prepended with the identifier of the task which produced the output.\nwrite() / FileOutputFormat - Method and base class for custom file outputs. Supports custom object-to-bytes conversion.\noutput()/ OutputFormat - Most generic output method, for data sinks that are not file based (such as storing the result in a database).\n\n```\n\n　　\n\n&nbsp;\n","tags":["Flink"]},{"title":"Flink学习笔记——DataStream API","url":"/Flink学习笔记——DataStream API.html","content":"Flink中的DataStream任务用于实现data streams的转换，data stream可以来自不同的数据源，比如消息队列，socket，文件等。\n\nRef<!--more-->\n&nbsp;\n\n```\nhttps://ci.apache.org/projects/flink/flink-docs-stable/zh/dev/datastream_api.html\n\n```\n\n&nbsp;使用DataStream API需要使用stream env\n\n```\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n```\n\n&nbsp;\n\nDataStream支持的**Data Source**有：File-based，Socket-based，Collection-based，Custom\n\n1.File-based\n\n```\nreadTextFile(path) - Reads text files, i.e. files that respect the TextInputFormat specification, line-by-line and returns them as Strings.\n\nreadFile(fileInputFormat, path) - Reads (once) files as dictated by the specified file input format.\n\nreadFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo) - This is the method called internally by the two previous ones. It reads files in the path based on the given fileInputFormat. Depending on the provided watchType, this source may periodically monitor (every interval ms) the path for new data (FileProcessingMode.PROCESS_CONTINUOUSLY), or process once the data currently in the path and exit (FileProcessingMode.PROCESS_ONCE). Using the pathFilter, the user can further exclude files from being processed.\n\n```\n\n2.Socket-based\n\n```\nsocketTextStream - Reads from a socket. Elements can be separated by a delimiter\n\n```\n\n3.Collection-based\n\n```\nfromCollection(Collection) - Creates a data stream from the Java Java.util.Collection. All elements in the collection must be of the same type.\n\nfromCollection(Iterator, Class) - Creates a data stream from an iterator. The class specifies the data type of the elements returned by the iterator.\n\nfromElements(T ...) - Creates a data stream from the given sequence of objects. All objects must be of the same type.\n\nfromParallelCollection(SplittableIterator, Class) - Creates a data stream from an iterator, in parallel. The class specifies the data type of the elements returned by the iterator.\n\ngenerateSequence(from, to) - Generates the sequence of numbers in the given interval, in parallel.\n\n```\n\n4.Custom\n\n```\naddSource - Attach a new source function. For example, to read from Apache Kafka you can use addSource(new FlinkKafkaConsumer<>(...)). See connectors for more details\n\n```\n\n&nbsp;\n\n&nbsp;Data Stream支持的**transformations**算子\n\n```\nhttps://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/operators/\n\n```\n\n　　\n\n&nbsp;DataStream支持的**Data Sink**有：\n\n```\nwriteAsText() / TextOutputFormat - Writes elements line-wise as Strings. The Strings are obtained by calling the toString() method of each element.\n\nwriteAsCsv(...) / CsvOutputFormat - Writes tuples as comma-separated value files. Row and field delimiters are configurable. The value for each field comes from the toString() method of the objects.\n\nprint() / printToErr() - Prints the toString() value of each element on the standard out / standard error stream. Optionally, a prefix (msg) can be provided which is prepended to the output. This can help to distinguish between different calls to print. If the parallelism is greater than 1, the output will also be prepended with the identifier of the task which produced the output.\n\nwriteUsingOutputFormat() / FileOutputFormat - Method and base class for custom file outputs. Supports custom object-to-bytes conversion.\n\nwriteToSocket - Writes elements to a socket according to a SerializationSchema\n\naddSink - Invokes a custom sink function. Flink comes bundled with connectors to other systems (such as Apache Kafka) that are implemented as sink functions.\n\n```\n\n　　\n","tags":["Flink"]},{"title":"Flink学习笔记——Environment","url":"/Flink学习笔记——Environment.html","content":"Flink有以下几种Environment\n\n1. 批处理Environment，ExecutionEnvironment\n\n```\nExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n```\n\n2.流处理Environment，StreamExecutionEnvironment\n\n```\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n```\n\n3. 本机Environment，LocalEnvironment\n\n```\nExecutionEnvironment env = LocalEnvironment.getExecutionEnvironment();\n\n```\n\n4. java集合Environment，CollectionEnvironment\n\n```\nExecutionEnvironment env = CollectionEnvironment.getExecutionEnvironment();\n\n```\n\nRef\n\n```\nhttps://www.yuque.com/cuteximi/base/flink-02?language=en-us\n\n```\n\n　　\n\n创建Environment的方法\n\n1. getExecutionEnvironment ，含义就是本地运行就是<!--more-->\n&nbsp;createLocalEnvironment，如果是通过client提交到集群上，就返回集群的环境\n\n```\n Creates an execution environment that represents the context in which the program is currently executed.\n\t * If the program is invoked standalone, this method returns a local execution environment, as returned by\n\t * {@link #createLocalEnvironment()}. If the program is invoked from within the command line client to be\n\t * submitted to a cluster, this method returns the execution environment of this cluster.\n\n```\n\nRef\n\n```\nhttps://github.com/apache/flink/blob/master/flink-java/src/main/java/org/apache/flink/api/java/ExecutionEnvironment.java\n\n```\n\n2.&nbsp;createLocalEnvironment ，返回本地执行环境，需要在调用时指定默认的并行度，比如\n\n```\nLocalStreamEnvironment env1 = StreamExecutionEnvironment.createLocalEnvironment(1);\n\nLocalEnvironment env2 = ExecutionEnvironment.createLocalEnvironment(1);\n\n```\n\n3.&nbsp;createRemoteEnvironment，&nbsp;返回集群执行环境，将 Jar 提交到远程服务器。需要在调用时指定 JobManager 的 IP 和端口号，并指定要在集群中运行的 Jar 包，比如\n\n```\nStreamExecutionEnvironment env1 = StreamExecutionEnvironment.createRemoteEnvironment(\"127.0.0.1\", 8080, \"/path/word_count.jar\");\n\nExecutionEnvironment env2 = ExecutionEnvironment.createRemoteEnvironment(\"127.0.0.1\", 8080, \"/path/word_count.jar\");\n\n```\n\n　　\n\n&nbsp;\n","tags":["Flink"]},{"title":"Flink学习笔记——配置","url":"/Flink学习笔记——配置.html","content":"在Flink任务中，需要加载外置配置参数到任务中，在Flink的开发文档中介绍了，Flink提供了一个名为 **ParameterTool** 的工具来解决这个问题\n\nFlink开发文档:\n\n```\nhttps://github.com/apache/flink/blob/master/docs/dev/application_parameters.zh.md\n\n```\n\n其引入配置的方式有3种：\n\n1.<!--more-->\n&nbsp;**From .properties files**\n\n```\nString propertiesFilePath = \"/home/sam/flink/myjob.properties\";\nParameterTool parameter = ParameterTool.fromPropertiesFile(propertiesFilePath);\n\n```\n\n2.&nbsp;**From the command line arguments**\n\n在args中添加&nbsp;\n\n```\n--input hdfs:///mydata --elements 42\n\n```\n\n在代码中使用\n\n```\npublic static void main(String[] args) {\n\n        // parse input arguments\n        final ParameterTool parameterTool = ParameterTool.fromArgs(args);\n}\n\n```\n\n3.&nbsp;**From system properties**\n\n使用\n\n```\n-Dinput=hdfs:///mydata\n\n```\n\n或者\n\n```\nParameterTool parameter = ParameterTool.fromSystemProperties();\n\n```\n\n&nbsp;\n\nParameterTool有如下几个方法可以**获得参数**\n\n```\nparameter.getRequired(\"input\");\nparameter.get(\"output\", \"myDefaultValue\");\nparameter.getLong(\"expectedCount\", -1L);\nparameter.getNumberOfParameters();\n\n```\n\n　　\n\n**注册全局变量**\n\n```\n// parse input arguments\nParameterTool parameters = ParameterTool.fromSystemProperties();\n// register the parameters globally\nExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\nenv.getConfig().setGlobalJobParameters(parameters);\n\n```\n\n　　\n\n下面使用第1种方法来配置Flink消费Kafka的Topic\n\nRef:\n\n```\nhttps://github.com/apache/flink/blob/master/flink-end-to-end-tests/flink-streaming-kafka-test-base/src/main/java/org/apache/flink/streaming/kafka/test/base/KafkaExampleUtil.java\n\n```\n\n&nbsp;\n\n对于**带有前缀的配置**读取，可以参考flume的前缀配置读取方法&nbsp;getSubProperties()，使用其从整体的配置文件中读取 prefix 开头的配置，并去掉prefix\n\n```\nhttps://github.com/apache/flume/blob/trunk/flume-ng-configuration/src/main/java/org/apache/flume/Context.javab\n```\n\n通用配置读取工具类&nbsp;ParameterToolUtil\n\n```\nimport com.google.common.base.Preconditions;\nimport com.google.common.collect.ImmutableMap;\nimport com.google.common.collect.Maps;\n\nimport java.util.Map;\n\npublic class ParameterToolUtil {\n\n    // 读取parameters中前缀为prefix的配置，并去掉前缀\n    public static Map<String, String> getSubProperties(Map<String, String> parameters, String prefix) {\n        Preconditions.checkArgument(prefix.endsWith(\".\"),\n                \"The given prefix does not end with a period (\" + prefix + \")\");\n        Map<String, String> result = Maps.newHashMap();\n        synchronized (parameters) {\n            for (Map.Entry<String, String> entry : parameters.entrySet()) {\n                String key = entry.getKey();\n                if (key.startsWith(prefix)) {\n                    String name = key.substring(prefix.length());\n                    result.put(name, entry.getValue());\n                }\n            }\n        }\n        return ImmutableMap.copyOf(result);\n    }\n\n}\n\n```\n\n比如如下配置\n\n```\n# kafka source\nkafka.source.bootstrap.servers=localhost:9092\nkafka.source.topics=thrift_log_test\nkafka.source.group.id=test\n\n```\n\n可以这样读取\n\n```\n// 读取kafka相关的配置参数\nProperties kafkaSourceProps = new Properties();\nkafkaSourceProps.putAll(ParameterToolUtil.getSubProperties(parameters, KafkaSourceConstants.KAFKA_SOURCE_PREFIX));\n\n```\n\n其中&nbsp;KafkaSourceConstants.KAFKA_SOURCE_PREFIX 常量为 kafka.source.\n\n使用&nbsp;getSubProperties() 方法读取的时候将会读取带有 prefix 前缀的配置，并自动去掉前缀\n","tags":["Flink"]},{"title":"论文阅读——Twitter日志系统","url":"/论文阅读——Twitter日志系统.html","content":"## 1.业界公司数据平台建设规模\n\n### 1.twitter\n\nTwitter关于日志系统的论文有如下2篇，分别是\n\n《The Unified Logging Infrastructure for Data Analytics at Twitter》和《Scaling Big Data Mining Infrastructure: The Twitter Experience》\n\n```\nhttps://vldb.org/pvldb/vol5/p1771_georgelee_vldb2012.pdf\nhttps://www.kdd.org/exploration_files/V14-02-02-Lin.pdf\n\n```\n\n相关PPT\n\n```\nhttps://www.slideshare.net/Hadoop_Summit/scaling-big-data-mining-infrastructure-twitter-experience\nhttps://slideplayer.com/slide/12451118/\nhttps://blog.twitter.com/engineering/en_us/topics/insights/2016/discovery-and-consumption-of-analytics-data-at-twitter.html\nhttps://www.slideshare.net/kevinweil/hadoop-at-twitter-hadoop-summit-2010\nhttps://www.slideshare.net/kevinweil/protocol-buffers-and-hadoop-at-twitter/1-Twitter_Open_Source_Coming_soon\n\n```\n\n其中《The Unified Logging Infrastructure for Data Analytics at Twitter》这篇Twitter12年的论文中介绍了Twitter的**产品日志基础架构**以及**从应用特定日志到统一的&ldquo;客户端事件&rdquo;日志格式的演进**，其中message都是**Thrift message**。\n\n<img src=\"/images/517519-20201204152111401-1890691389.png\" alt=\"\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n&nbsp;当时（2012年），Twitter的Hadoop集群规模有几百台。\n\n<img src=\"/images/517519-20201204151859741-1440982372.png\" alt=\"\" loading=\"lazy\" />\n\n当然后来其他文章介绍说到了2016年，Twitter的集群规模超过了1W台\n\n```\nhttps://blog.twitter.com/engineering/en_us/topics/insights/2016/discovery-and-consumption-of-analytics-data-at-twitter.html\n\n```\n\n<img src=\"/images/517519-20210113135554507-53416501.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n到现在，我们一般称10台左右的Hadoop集群为小型规模集群，100多台的集群为中型规模集群，上千台规模的为大型集群，上万台规模的为超大规模集群，当集群规模达到上千台之后，一般就需要做联邦了。\n\n### 2.airbnb\n\n就其他互联网公司而言，据资料显示：\n\n**Airbnb**的Hadoop集群规模为1400台+\n\n<img src=\"/images/517519-20201204152259033-1948092928.png\" alt=\"\" loading=\"lazy\" />\n\n```\nairbnb曾洪博 - 《airbnb数据平台实践》\nhttps://myslide.cn/slides/3043\n\n```\n\n### 3.字节跳动\n\n**字节跳动**的Hadoop集群规模为**多集群上万台**\n\n<img src=\"/images/517519-20201204152630789-554906070.png\" alt=\"\" loading=\"lazy\" />\n\n```\n字节跳动 EB 级 HDFS 实践\nhttps://www.infoq.cn/article/aLE9ObVYUEmDvRCzCTBr\n\n```\n\n<img src=\"/images/517519-20201204160337243-458449818.png\" alt=\"\" loading=\"lazy\" />\n\nTwitter在论文那种提到，在大规模日志的生产者，管理这些数据的基础架构，分析pipeline的工程师，数据科学家之间缺乏一下互动，意思即机器和人都能正确地理解一份数据，\n\n比如一个字段内容为\"123\"，基础架构服务能知道它是string，而不是int；日志的生产者生产了一份数据，数据分析师能知道日志中每个字段的含义等等。这在Airbnb的PPT《**Airbnb的核心日志系统**》中也提到了\n\n## 2.日志序列化方案选择\n\n### 引入日志schema的好处\n\n在字段定义上，文章中提到了字段格式的问题，就是驼峰，下划线，还有id，uid。。。这些问题还是比较常见的\n\n<img src=\"/images/517519-20201205000556239-1584401941.png\" width=\"600\" height=\"174\" loading=\"lazy\" />\n\n&nbsp;\n\n还有就是日志格式问题，非结构化，半结构化，即使是json格式的日志也存在字段可以动态变化，字段是否是optional的问题\n\n<img src=\"/images/517519-20201205001138710-717175048.png\" width=\"600\" height=\"319\" loading=\"lazy\" />\n\n在《Scaling Big Data Mining Infrastructure: The Twitter Experience》这篇文章中，作者提到了几点：\n\n1.不要使用mysql来存储日志\n\n2.使用HDFS来存储日志，每个HDFS目录下保持，少量的文件数以及大文件\n\n<img src=\"/images/517519-20201205005407184-2139502432.png\" width=\"600\" height=\"137\" loading=\"lazy\" />\n\n&nbsp;至于存储在HDFS的压缩格式，《Hadoop at Twitter (Hadoop Summit 2010)》中介绍的是lzo压一切，至于lzo和其他压缩方式的对比，可以参考我的文章《[MapReduce中的InputFormat](https://www.cnblogs.com/tonglin0325/p/13750952.html)》\n\n<img src=\"/images/517519-20201209144521041-1145635929.png\" alt=\"\" loading=\"lazy\" />\n\n3.从需要正则表达式解析的纯文本(plain-text)日志格式到json格式，再到Thrift格式\n\n<img src=\"/images/517519-20201205005813367-652689412.png\" width=\"500\" height=\"258\" loading=\"lazy\" />\n\n4.使用IDL语言定义日志格式，此外Twitter还开发了Elephant bird来自动生成和Hadoop，Pig交互的代码，有了这些，基础架构组件和开发人员都能很好地理解日志\n\n<img src=\"/images/517519-20201205010026554-1380196751.png\" width=\"500\" height=\"404\" loading=\"lazy\" />\n\n### 1.Airbnb（Thrift）\n\n<img src=\"/images/517519-20201204161429933-1426778961.png\" alt=\"\" loading=\"lazy\" />\n\n```\nhttps://myslide.cn/slides/635\n\n```\n\n### 2.Twitter（Thrift）\n\nTwitter使用Facebook开源的Scribe进行日志采集，同时Twitter在论文中提到，每一种日志包含了2个string，一个是category，一个是message。其中category有配置中的元数据决定，包含了这个数据是从哪个写入的。\n\n<img src=\"/images/517519-20201204170242699-2012970077.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n同时，Twitter使用ZooKeeper对scribe的agent进行管理，方案很常见，就是在zk注册一个临时节点，当节点超时和zk通信超时一段时间后，临时节点就会消失，即agent失联\n\n有点类似filebeat的中心化管理（Beats central management），除外Apache Flume也可以使用Zookeeper作为配置中心\n\n```\nhttps://www.elastic.co/guide/en/beats/filebeat/7.10/configuration-central-management.html\n\n```\n\n不过Twitter后来使用Flume替换了Scribe\n\n<img src=\"/images/517519-20210113140912675-1811175177.png\" alt=\"\" loading=\"lazy\" />\n\n```\nhttps://www.slideshare.net/prasadwagle/extracting-insights-from-data-at-twitter\n\n```\n\n<img src=\"/images/517519-20201204170501191-1291813574.png\" alt=\"\" loading=\"lazy\" />\n\n当日志落盘到HDFS上的时候，系统将会根据日志中的category，自动将其写到一个对应的HDFS路径，即/logs/category\n\n<img src=\"/images/517519-20201204172112069-265759442.png\" alt=\"\" loading=\"lazy\" />\n\n<img src=\"/images/517519-20201205000727917-386467285.png\" width=\"500\" height=\"121\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;在日志定义中，由于Twitter广泛使用的是Thrift，所以其也就沿用了Thrift作为其日志定义语言，除了Twitter，使用Thrift来定义日志格式的公司还有Airbnb和Xiaomi；而使用Protobuf有百度，字节跳动，快手，友盟；使用avro的有uber和linkedin\n\n<img src=\"/images/517519-20201204172932521-1090217995.png\" alt=\"\" loading=\"lazy\" />\n\n### 3.百度（Protobuf）\n\n<img src=\"/images/517519-20201207160012689-1391300189.png\" alt=\"\" loading=\"lazy\" />\n\n```\n从日志统计到大数据分析（六）&mdash;&mdash;秦天下\nhttps://zhuanlan.zhihu.com/p/20401637\n\n```\n\n### 4.字节跳动（Protobuf）\n\n<img src=\"/images/517519-20201204173457387-498409907.png\" alt=\"\" loading=\"lazy\" />\n\n```\n今日头条数据平台架构师王烨 - 今日头条大数据平台的演进\nhttps://myslide.cn/slides/3497\n\n```\n\n### 5.快手（Protobuf）\n\n<img src=\"/images/517519-20201209143810168-484482273.png\" alt=\"\" loading=\"lazy\" />\n\n```\nFlink在快手实时多维分析场景的应用\nhttps://cloud.tencent.com/developer/news/643334\n\n```\n\n### 6.Uber（Avro）\n\n<img src=\"/images/517519-20201228173251901-1100648214.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n```\nChaperone：Uber是如何对Kafka进行端到端审计的\nhttps://www.infoq.cn/article/2016/12/uber-chaperone-kafka\n\n```\n\n### 7.Linkedin（Avro）\n\n<img src=\"/images/517519-20210306140333959-1063561614.png\" width=\"700\" height=\"568\" loading=\"lazy\" />\n\n### 8.友盟（Protobuf）\n\n<img src=\"/images/517519-20230702140052020-970432773.png\" width=\"800\" height=\"210\" loading=\"lazy\" />\n\n参考：[Hive的序列化/反序列化(SerDe)&nbsp;](https://www.jianshu.com/p/9aa7a681941a)\n\n### 9.知乎（Protobuf）\n\n<img src=\"/images/517519-20230703164723592-971989049.png\" width=\"800\" height=\"645\" loading=\"lazy\" />\n\n参考：[Protobuf 在知乎大数据场景的应用](https://zhuanlan.zhihu.com/p/586120009)\n\n&nbsp;\n\n### 10.腾讯TEG（Protobuf）\n\n<img src=\"/images/517519-20230703165050337-1879579954.png\" width=\"800\" height=\"320\" loading=\"lazy\" />\n\n参考：[[TDW]Protobuf在腾讯数据仓库TDW的使用](https://www.jianshu.com/p/d9d82bfece88)\n\n&nbsp;\n","tags":["twitter","Paper"]},{"title":"SpringBoot学习笔记——Redis Template","url":"/SpringBoot学习笔记——Redis Template.html","content":"Springboot可以通过redis template和redis进行交互，使用方法如下\n\n<!--more-->\n&nbsp;\n\n可以参考这个系列的文章：\n\n[【快学springboot】11.整合redis实现session共享](https://www.toutiao.com/i6685965269333967373/?group_id=6685965269333967373)\n\n[【快学springboot】13.操作redis之String数据结构](https://cloud.tencent.com/developer/article/1464682)\n\n[【快学springboot】14.操作redis之list](https://cloud.tencent.com/developer/article/1464683)\n\n还有python版本的redis实战\n\n```\nhttps://github.com/7-sevens/Developer-Books/blob/master/Redis/Redis%E5%AE%9E%E6%88%98.pdf\n\n```\n\n在pom中引入\n\n```\n<!-- redis -->\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-data-redis</artifactId>\n</dependency>\n\n```\n\n并在application.conf中配置\n\n```\n# redis\nspring.redis.host=localhost\nspring.redis.port=6379\nspring.redis.password=xxxx\nspring.redis.database=1\nspring.redis.jedis.pool.max-active=10\nspring.redis.timeout=3000ms\n\n```\n\n初始化rest template\n\n```\n@Autowired\nprivate RedisTemplate<String, String> redisTemplate;\n\n```\n\n&nbsp;redis有以下基本类型\n\n<img src=\"/images/517519-20201201140643334-1970344613.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n**STRING操作**\n\n[【快学springboot】13.操作redis之String数据结构](https://cloud.tencent.com/developer/article/1464682)\n\n<img src=\"/images/517519-20201201141228331-90583297.png\" alt=\"\" width=\"993\" height=\"355\" loading=\"lazy\" />\n\n&nbsp;\n\n**LIST操作**\n\n[【快学springboot】14.操作redis之list](https://cloud.tencent.com/developer/article/1464683)\n\n<img src=\"/images/517519-20201201141423516-1444000138.png\" alt=\"\" width=\"996\" height=\"459\" loading=\"lazy\" />\n\n&nbsp;\n\n**SET操作**\n\n<img src=\"/images/517519-20201201142103937-79655728.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n**HASH操作**\n\n[SpringBoot系列教程之RedisTemplate Hash数据结构使用教程](https://segmentfault.com/a/1190000017555774)\n\n&nbsp;\n\n**ZSET操作**\n\n[SpringBoot高级篇Redis之ZSet数据结构使用姿势](https://juejin.cn/post/6844903737786368014)\n\n&nbsp;\n\n在使用redis实现**自动补全**功能\n\npython版本可以参考《redis实战》或者&nbsp;[基于Redis的自动补全算法](https://www.dazhuanlan.com/2020/01/02/5e0d526ddecbf/)\n\njava版本可以参考：[使用Redis实现中英文自动补全功能详解](https://cloud.tencent.com/developer/article/1517594)\n\nhue的自动补全可以参考：[聊一聊代码的智能提示](https://github.com/879479119/879479119.github.io/issues/9)\n\n**基本原理**就是利用了redis的zset类型，当zset的value都是0的时候，key的排序为字典序，\n\n这时候如果查询的前缀是\"ab\"，这时往zset中插入2个string，\"aa{\"和\"ab{\"，并使用zrange拿到这两字符的index，由于z的下一个字符为{，所以这2个index之间的字符必定前缀为\"ab\"\n\n但是如果这样的话在并对的条件下会有问题，所以真实实现中一般会在&nbsp;\"aa{\"和\"ab{\" 后面拼接上uuid，防止覆盖掉别task的成员标记，取到2个index后就立即删除这两个key\n","tags":["SpringBoot","Redis"]},{"title":"Hadoop学习笔记——配置文件","url":"/Hadoop学习笔记——配置文件.html","content":"下载hadoop的原生版本，版本选择2.6.0，下载地址\n\n```\nhttps://archive.apache.org/dist/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz\n\n```\n\n解压后可以看到\n\n<img src=\"/images/517519-20201120142942474-74759388.png\" width=\"400\" height=\"199\" loading=\"lazy\" />\n\n其中配置文件在 /etc/hadoop目录下\n\n<img src=\"/images/517519-20201120144226142-858775669.png\" alt=\"\" width=\"496\" height=\"182\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n&nbsp;\n\n解压后默认的配置文件都是空的，需要自行配置，下面为配置文件的作用\n\n其中比较重要的是前4个配置文件，分别为**core-site.xml，<strong><strong>hdfs-site.xml，<strong>mapred-site.xml，<strong>yarn-site.xml**</strong></strong></strong></strong>\n\n可以**<strong><strong><strong><strong>参考文档**</strong></strong></strong></strong>，该文档的版本为4.3.0**<strong><strong><strong><strong>，：**</strong></strong></strong></strong>[**Hadoop参数汇总**](https://segmentfault.com/a/1190000000709725)\n\n默认的配置文件可以参考**官方文档**\n\n```\nhttps://hadoop.apache.org/docs/r2.6.0/\n\n```\n\n左下角有配置的链接\n\n&nbsp;<img src=\"/images/517519-20201125193557119-312766049.png\" width=\"200\" height=\"125\" loading=\"lazy\" />\n\n1.**core-site.xml**\n\nHadoop全局配置参数，例如HDFS和MapReduce常用的I/O设置等\n\n```\nhttps://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-common/core-default.xml\n\n```\n\n配置含义和调优建议可以参考\n\n```\nhttp://tonglin0325.github.io/xml/hadoop/2.6.0/core-default.xml\n\n```\n\n&nbsp;\n\n2.**<strong>hdfs-site.xml**</strong>\n\n**<strong><img src=\"/images/517519-20201204103223013-954452230.png\" width=\"600\" height=\"405\" loading=\"lazy\" />**</strong>\n\n&nbsp;\n\nhdfs配置文件\n\n```\nhttps://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml\n\n```\n\n配置含义和调优建议可以参考\n\n```\nhttp://tonglin0325.github.io/xml/hadoop/2.6.0/hdfs-default.xml\n\n```\n\n&nbsp;\n\n3.**mapred-site.xml**\n\nmapreduce配置文件\n\n```\nhttps://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml\n\n```\n\n配置含义和调优建议可以参考\n\n```\nhttp://tonglin0325.github.io/xml/hadoop/2.6.0/mapred-default.xml\n\n```\n\n&nbsp;\n\n4.**yarn-site.xml**\n\n**<img src=\"/images/517519-20201204102149957-1780427858.png\" width=\"534\" height=\"368\" loading=\"lazy\" />**\n\n　　YARN是Hadoop 2.0中的资源管理系统，它的基本设计思想是将MRv1中的JobTracker拆分成了两个独立的服务：一个全局的**资源管理器ResourceManager**和**每个应用程序特有的ApplicationMaster**。\n\n其中ResourceManager负责整个系统的资源管理和分配，而ApplicationMaster负责单个应用程序的管理。\n\n　　YARN总体上仍然是master/slave结构，在整个资源管理框架中，resourcemanager为master，nodemanager是slave。Resourcemanager负责对各个nademanger上资源进行统一管理和调度。当用户提交一个应用程序时，需要提供一个用以跟踪和管理这个程序的ApplicationMaster，它负责向ResourceManager申请资源，并要求NodeManger启动可以占用一定资源的任务。由于不同的ApplicationMaster被分布到不同的节点上，因此它们之间不会相互影响。\n\n　　YARN的基本组成结构，YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等几个组件构成。\n\n　　ResourceManager是Master上一个独立运行的进程，负责集群统一的资源管理、调度、分配等等；NodeManager是Slave上一个独立运行的进程，负责上报节点的状态；App Master和Container是运行在Slave上的组件，Container是yarn中分配资源的一个单位，包涵内存、CPU等等资源，yarn以Container为单位分配资源。\n\n　　Client向ResourceManager提交的每一个应用程序都必须有一个Application Master，它经过ResourceManager分配资源后，运行于某一个Slave节点的Container中，具体做事情的Task，同样也运行与某一个Slave节点的Container中。RM，NM，AM乃至普通的Container之间的通信，都是用RPC机制。\n\n**1、Resourcemanager**\n\n　　RM是一个全局的资源管理器，集群只有一个，负责整个系统的资源管理和分配，包括处理客户端请求、启动/监控APP master、监控nodemanager、资源的分配与调度。它主要由两个组件构成：**调度器（Scheduler）**和**应用程序管理器（Applications Manager，ASM）**。\n\n**（1） 调度器&nbsp;Scheduler**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。需要注意的是，该调度器是一个&ldquo;纯调度器&rdquo;，它不再从事任何与具体应用程序相关的工作，比如不负责监控或者跟踪应用的执行状态等，也不负责重新启动因应用执行失败或者硬件故障而产生的失败任务，这些均交由应用程序相关的ApplicationMaster完成。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念&ldquo;资源容器&rdquo;（Resource Container，简称Container）表示，Container是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需要设计新的调度器，YARN提供了多种直接可用的调度器，比如Fair Scheduler和Capacity Scheduler等。\n\n（2） **应用程序管理器&nbsp;Applications Manager**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动它等。\n\n**2、ApplicationMaster（AM）**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 管理YARN内运行的应用程序的每个实例。\n\n功能：\n\n　　数据切分\n\n　　为应用程序申请资源并进一步分配给内部任务。\n\n　　任务监控与容错\n\n　　负责协调来自resourcemanager的资源，并通过nodemanager监视容器的执行和资源使用情况。\n\n**3、NodeManager（NM）**\n\nNodemanager整个集群有多个，负责每个节点上的资源和使用。\n\n功能：\n\n　　单个节点上的资源管理和任务。\n\n　　处理来自于resourcemanager的命令。\n\n　　处理来自域app master的命令。\n\n　　Nodemanager管理着抽象容器，这些抽象容器代表着一些特定程序使用针对每个节点的资源。\n\n　　Nodemanager定时地向RM汇报本节点上的资源使用情况和各个Container的运行状态（cpu和内存等资源）\n\n**4、****Container**\n\n　　Container是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当AM向RM申请资源时，RM为AM返回的资源便是用Container表示的。YARN会为每个任务分配一个Container，且该任务只能使用该Container中描述的资源。需要注意的是，Container不同于MRv1中的slot，它是一个动态资源划分单位，是根据应用程序的需求动态生成的。目前为止，YARN仅支持CPU和内存两种资源，且使用了轻量级资源隔离机制Cgroups进行资源隔离。\n\n功能：\n\n　　对task环境的抽象\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 描述一系列信息\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 任务运行资源的集合（cpu、内存、io等）\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 任务运行环境\n\n参考：[YARN架构设计详解](https://www.cnblogs.com/wcwen1990/p/6737985.html)\n\nyarn配置文件\n\n```\nhttps://hadoop.apache.org/docs/r2.6.0/hadoop-yarn/hadoop-yarn-common/yarn-default.xml\n\n```\n\n配置含义和调优建议可以参考\n\n```\nhttp://tonglin0325.github.io/xml/hadoop/2.6.0/yarn-default.xml\n\n```\n\n&nbsp;\n\n5.**httpfs-site.xml**\n\nhttpfs模块配置文件\n\n&nbsp;\n\n6.**capacity-scheduler.xml**\n\n配置了yarn资源调度器运行中的各项参数\n\n&nbsp;\n\n7.**hadoop-policy.xml**\n\n用于配置服务级别授权\n\n&nbsp;\n\n8.**kms-acls.xml**\n\n&nbsp;\n\n9.**kms-site.xml**\n\n&nbsp;\n","tags":["Hadoop"]},{"title":"Hive学习笔记——SerDe","url":"/Hive学习笔记——SerDe.html","content":"**SerDe** 是Serializer 和 Deserializer 的简称，它提供了Hive和各种数据格式交互的方式。\n\nAmazon的Athena可以理解是Amazon对标hive的一款产品，其中对SerDe的介绍如下\n\n```\nhttps://docs.aws.amazon.com/zh_cn/athena/latest/ug/serde-about.html\n\n```\n\n<img src=\"/images/517519-20201117102839829-297329757.png\" alt=\"\" loading=\"lazy\" />\n\n对于Hive中经常使用的SerDe如下，参考了\n\n[Hive_10. Hive中常用的 SerDe 和 当前社区的状态](https://blog.csdn.net/Mike_H/article/details/50161555)\n\n1.LazySimpleSerDe，用来处理文本格式文件：TEXTFILE<!--more-->\n&nbsp;\n\n```\nhttps://github.com/apache/hive/blob/master/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java\n\n```\n\n2.RegexSerDe，可以使用java正则表达式来处理文本格式文件：TEXTFILE\n\n```\nhttps://github.com/apache/hive/blob/master/serde/src/java/org/apache/hadoop/hive/serde2/RegexSerDe.java\n\n```\n\n3.JSONSerDe，可以使用它来处理json格式的文本文件：TEXTFILE\n\n```\nhttps://github.com/rcongiu/Hive-JSON-Serde/blob/master/json-serde/src/main/java/org/openx/data/jsonserde/JsonSerDe.java\n\n```\n\n第三方的jar包，下载连接\n\n```\nhttp://www.congiu.net/hive-json-serde/1.3.8/hdp23/json-serde-1.3.8-jar-with-dependencies.jar\nhttp://www.congiu.net/hive-json-serde/1.3.8/hdp23/json-udf-1.3.8-jar-with-dependencies.jar\n\n```\n\n4.OpenCSVSerDe，可以使用它来处理csv格式的文本文件：TEXTFILE\n\n```\nhttps://github.com/apache/hive/blob/master/serde/src/java/org/apache/hadoop/hive/serde2/OpenCSVSerde.java\n\n```\n\n5.ParquetHiveSerDe，可以使用它来处理Parquet格式的文件：PARQUET\n\n```\nhttps://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java\n\n```\n\n6.AvroSerDe，可以使用它来处理Avro格式数据的serde：AVRO\n\n```\nhttps://github.com/apache/hive/blob/master/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerDe.java\n\n```\n\n7.HBaseSerDe，可以使用它来集成Hive和Hbase\n\n```\nhttps://github.com/apache/hive/blob/master/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseSerDe.java\n\n```\n\n8.EsSerDe，可以使用它来集成Hive和Es\n\n```\nhttps://github.com/elastic/elasticsearch-hadoop/blob/master/hive/src/main/java/org/elasticsearch/hadoop/hive/EsSerDe.java\n\n```\n\n9.ThriftDeserializer，可以使用它来集成thrift和hive\n\n```\nhttps://github.com/apache/hive/blob/master/serde/src/java/org/apache/hadoop/hive/serde2/thrift/ThriftDeserializer.java\n\n```\n\n10.MySQLSerDe\n\n```\nhttps://cwiki.apache.org/confluence/display/Hive/JDBC+Storage+Handler\n\n```\n\n使用hive查询mysql需要注意调整分区参数，否则可能会导致数据重复或者连接过多，比如\n\n```\nCREATE EXTERNAL TABLE IF NOT EXISTS `xx.xx`(\n  `a` int COMMENT 'from deserializer',\n  `b` string COMMENT 'from deserializer',\n  `c` int COMMENT 'from deserializer'\n)\nSTORED BY 'org.apache.hive.storage.jdbc.JdbcStorageHandler'\nTBLPROPERTIES (\n    \"hive.sql.database.type\" = \"MYSQL\",\n    \"hive.sql.jdbc.driver\" = \"com.mysql.jdbc.Driver\",\n    \"hive.sql.jdbc.url\" = \"jdbc:mysql://xxx:3306/xx_db\",\n    \"hive.sql.dbcp.username\" = \"xx_username\",\n    \"hive.sql.dbcp.password\" = \"xx_pws\",\n    \"hive.sql.table\" = \"xx_table\",\n    \"hive.sql.dbcp.maxActive\" = \"1\",\n    \"hive.sql.query\"=\"SELECT a,b,c FROM xx_table\"\n)\n\n```\n\n11.MongoSerde\n\n```\nhttps://github.com/mongodb/mongo-hadoop/\nhttps://blog.csdn.net/zz60708320/article/details/102722361\n\n```\n\n　　\n\n&nbsp;\n\n&nbsp;\n\n　　\n\n&nbsp;\n","tags":["Hive"]},{"title":"MapReduce中的OutputFormat","url":"/MapReduce中的OutputFormat.html","content":"OutputFormat在hadoop源码中是一个抽象类 public abstract class OutputFormat<K, V>，其定义了reduce任务的输出格式\n\n```\nhttps://github.com/apache/hadoop/blob/master/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/OutputFormat.java\n\n```\n\n可以参考文章\n\n[MapReduce快速入门系列(12) | MapReduce之OutputFormat](https://cloud.tencent.com/developer/article/1733319)\n\n<!--more-->\n&nbsp;\n\n常用的**OutputFormat**可以查看源码\n\n```\nhttps://github.com/apache/hadoop/tree/master/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output\n\n```\n\n<img src=\"/images/517519-20201116111552740-1380467690.png\" alt=\"\" loading=\"lazy\" />\n\n1.**文本输出TextOutputFormat**，是hadoop的默认实现输出功能的类\n\n```\nhttps://github.com/apache/hadoop/blob/master/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java\n\n```\n\n**TextOutputFormat**实现了**FileOutputFormat**，**<strong>FileOutputFormat**</strong>也一个抽象类，是OutputFormat的子类，源码在\n\n```\nhttps://github.com/apache/hadoop/blob/release-2.6.0/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputFormat.java\n\n```\n\n**<i>**其中一个比较重要的是&nbsp;RecordWriter接口，其中有两个方法，一个是write方法，另一个是close方法\n\n```\nhttps://github.com/apache/hadoop/blob/master/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/RecordWriter.java\n\n```\n\n源码\n\n```\npublic interface RecordWriter<K, V> {\n  /** \n   * Writes a key/value pair.\n   *\n   * @param key the key to write.\n   * @param value the value to write.\n   * @throws IOException\n   */      \n  void write(K key, V value) throws IOException;\n\n  /** \n   * Close this `RecordWriter` to future operations.\n   * \n   * @param reporter facility to report progress.\n   * @throws IOException\n   */ \n  void close(Reporter reporter) throws IOException;\n}\n\n```\n\n在&nbsp;**TextOutputFormat**&nbsp;实现类中如下实现了**&nbsp;****LineRecordWriter<K, V>**，源码\n\n```\nprotected static class LineRecordWriter<K, V>\n    extends RecordWriter<K, V> {\n    private static final String utf8 = \"UTF-8\";\n    private static final byte[] newline;\n    static {\n      try {\n        newline = \"\\n\".getBytes(utf8);\n      } catch (UnsupportedEncodingException uee) {\n        throw new IllegalArgumentException(\"can't find \" + utf8 + \" encoding\");\n      }\n    }\n\n    protected DataOutputStream out;\n    private final byte[] keyValueSeparator;\n\n    public LineRecordWriter(DataOutputStream out, String keyValueSeparator) {\n      this.out = out;\n      try {\n        this.keyValueSeparator = keyValueSeparator.getBytes(utf8);\n      } catch (UnsupportedEncodingException uee) {\n        throw new IllegalArgumentException(\"can't find \" + utf8 + \" encoding\");\n      }\n    }\n\n    public LineRecordWriter(DataOutputStream out) {\n      this(out, \"\\t\");\n    }\n\n    /**\n     * Write the object to the byte stream, handling Text as a special\n     * case.\n     * @param o the object to print\n     * @throws IOException if the write throws, we pass it on\n     */\n    private void writeObject(Object o) throws IOException {\n      if (o instanceof Text) {\n        Text to = (Text) o;\n        out.write(to.getBytes(), 0, to.getLength());\n      } else {\n        out.write(o.toString().getBytes(utf8));\n      }\n    }\n\n    public synchronized void write(K key, V value)\n      throws IOException {\n\n      boolean nullKey = key == null || key instanceof NullWritable;\n      boolean nullValue = value == null || value instanceof NullWritable;\n      if (nullKey &amp;&amp; nullValue) {\n        return;\n      }\n      if (!nullKey) {\n        writeObject(key);\n      }\n      if (!(nullKey || nullValue)) {\n        out.write(keyValueSeparator);\n      }\n      if (!nullValue) {\n        writeObject(value);\n      }\n      out.write(newline);\n    }\n\n    public synchronized \n    void close(TaskAttemptContext context) throws IOException {\n      out.close();\n    }\n  }\n\n```\n\n**<ii> **另外一个比较重要是 **getRecordWriter**&nbsp;抽象方法，当实现&nbsp;**FileOutputFormat抽象类&nbsp;**的时候需要实现这个方法，从job当中获取&nbsp;**RecordWriter<K, V>**\n\n```\npublic abstract RecordWriter<K, V> \n     getRecordWriter(TaskAttemptContext job\n                     ) throws IOException, InterruptedException;\n\n```\n\n在&nbsp;**TextOutputFormat** 实现类中如下实现了&nbsp;**getRecordWriter**，\n\n其中使用了**LineRecordWriter**，源码\n\n```\npublic RecordWriter<K, V> \n         getRecordWriter(TaskAttemptContext job\n                         ) throws IOException, InterruptedException {\n    Configuration conf = job.getConfiguration();\n    boolean isCompressed = getCompressOutput(job);\n    String keyValueSeparator= conf.get(SEPERATOR, \"\\t\");\n    CompressionCodec codec = null;\n    String extension = \"\";\n    if (isCompressed) {\n      Class<? extends CompressionCodec> codecClass = \n        getOutputCompressorClass(job, GzipCodec.class);\n      codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, conf);\n      extension = codec.getDefaultExtension();\n    }\n    Path file = getDefaultWorkFile(job, extension);\n    FileSystem fs = file.getFileSystem(conf);\n    if (!isCompressed) {\n      FSDataOutputStream fileOut = fs.create(file, false);\n      return new LineRecordWriter<K, V>(fileOut, keyValueSeparator);\n    } else {\n      FSDataOutputStream fileOut = fs.create(file, false);\n      return new LineRecordWriter<K, V>(new DataOutputStream\n                                        (codec.createOutputStream(fileOut)),\n                                        keyValueSeparator);\n    }\n  }\n\n```\n\n&nbsp;\n\n2.**二进制输出SequenceFileOutputFormat**\n\n```\nhttps://github.com/apache/hadoop/blob/master/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java\n\n```\n\n**<strong>SequenceFileOutputFormat**</strong>和**TextOutputFormat**一样，同样实现了**FileOutputFormat**\n\n**<strong><i>**</strong>其中一个比较重要的是&nbsp;**getSequenceWriter方法**，返回二进制文件的Writer\n\n```\nhttps://github.com/apache/hadoop/blob/master/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java\n\n```\n\n源码\n\n```\n  protected SequenceFile.Writer getSequenceWriter(TaskAttemptContext context,\n      Class<?> keyClass, Class<?> valueClass) \n      throws IOException {\n    Configuration conf = context.getConfiguration();\n\t    \n    CompressionCodec codec = null;\n    CompressionType compressionType = CompressionType.NONE;\n    if (getCompressOutput(context)) {\n      // find the kind of compression to do\n      compressionType = getOutputCompressionType(context);\n      // find the right codec\n      Class<?> codecClass = getOutputCompressorClass(context, \n                                                     DefaultCodec.class);\n      codec = (CompressionCodec) \n        ReflectionUtils.newInstance(codecClass, conf);\n    }\n    // get the path of the temporary output file \n    Path file = getDefaultWorkFile(context, \"\");\n    FileSystem fs = file.getFileSystem(conf);\n    return SequenceFile.createWriter(fs, conf, file,\n             keyClass,\n             valueClass,\n             compressionType,\n             codec,\n             context);\n  }\n\n```\n\n**<ii>&nbsp;**另外一个比较重要是&nbsp;**getRecordWriter**&nbsp;抽象方法，源码\n\n```\n  public RecordWriter<K, V> \n         getRecordWriter(TaskAttemptContext context\n                         ) throws IOException, InterruptedException {\n    final SequenceFile.Writer out = getSequenceWriter(context,\n      context.getOutputKeyClass(), context.getOutputValueClass());\n\n    return new RecordWriter<K, V>() {\n\n        public void write(K key, V value)\n          throws IOException {\n\n          out.append(key, value);\n        }\n\n        public void close(TaskAttemptContext context) throws IOException { \n          out.close();\n        }\n      };\n  }\n\n```\n\n　　\n\n3.**用于输出thrift对象的Parquet文件**的**ParquetThriftOutputFormat**，参考项目：[adobe-research](https://github.com/adobe-research)/[spark-parquet-thrift-example](https://github.com/adobe-research/spark-parquet-thrift-example)\n\n```\nhttps://github.com/adobe-research/spark-parquet-thrift-example/blob/master/src/main/scala/SparkParquetThriftApp.scala\n\n```\n\n代码\n\n```\n    ParquetThriftOutputFormat.setThriftClass(job, classOf[SampleThriftObject])\n    ParquetOutputFormat.setWriteSupportClass(job, classOf[SampleThriftObject])\n    sc.parallelize(sampleData)\n      .map(obj => (null, obj))\n      .saveAsNewAPIHadoopFile(\n        parquetStore,\n        classOf[Void],\n        classOf[SampleThriftObject],\n        classOf[ParquetThriftOutputFormat[SampleThriftObject]],\n        job.getConfiguration\n      )\n\n```\n\n&nbsp;\n\n4.用于文本格式hive表的**HiveIgnoreKeyTextOutputFormat**\n\n```\nhttps://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/io/HiveIgnoreKeyTextOutputFormat.java\n\n```\n\n其实现了&nbsp;HiveOutputFormat\n\n```\nhttps://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/io/HiveOutputFormat.java\n\n```\n\nHiveIgnoreKeyTextOutputFormat的key为null，源码\n\n```\n    @Override\n    public synchronized void write(K key, V value) throws IOException {\n      this.mWriter.write(null, value);\n    }\n\n```\n\n　　\n\n　　\n\n&nbsp;\n","tags":["Hadoop"]},{"title":"Filebeat的http endpoint input","url":"/Filebeat的http endpoint input.html","content":"Filebeat的input终于支持了http，可以使用post请求向filebeat的input传输数据，不过现在还是处于beta版本\n\n<img src=\"/images/517519-20201010183609657-1152999418.png\" alt=\"\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n参考\n\n```\nhttps://www.elastic.co/guide/en/beats/filebeat/7.x/filebeat-input-http_endpoint.html\n\n```\n\n下载filebeat最新版本，然后解压\n\n```\nwget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.9.2-linux-x86_64.tar.gz\n\n```\n\n配置 filebeat_http.yml\n\n```\nfilebeat.inputs:\n- type: http_endpoint\n  enabled: true\n  listen_address: localhost\n  listen_port: 19080\n\n#----------------------------------File output--------------------------------#\noutput.file:\n  path: \"/tmp/filebeat\"\n  filename: filebeat\n\n```\n\n给配置文件赋权\n\n```\nchmod go-w ./filebeat_http.yml\n\n```\n\n启动filebeat\n\n```\n./filebeat -e -c filebeat_http.yml\n\n```\n\n向19080端口发送post请求\n\n```\ncurl -XPOST http://localhost:19080 -H 'Content-Type:application/json' -d'{\"a\":\"b\"}'\n{\"message\": \"success\"}\n\n```\n\n不支持get请求\n\n<img src=\"/images/517519-20201010191720866-1666279438.png\" alt=\"\" loading=\"lazy\" />\n\n去/tmp/filebeat中查看\n\n```\nlintong@master:/tmp/filebeat$ tail -n 100 filebeat \n{\"@timestamp\":\"2020-10-10T09:44:22.294Z\",\"@metadata\":{\"beat\":\"filebeat\",\"type\":\"_doc\",\"version\":\"7.9.2\"},\"json\":{\"a\":\"b\"},\"input\":{\"type\":\"http_endpoint\"},\"ecs\":{\"version\":\"1.5.0\"},\"host\":{\"name\":\"master\"},\"agent\":{\"hostname\":\"master\",\"ephemeral_id\":\"20bf06ed-3421-44b8-aeaf-bb5d54565eb3\",\"id\":\"78e98f32-c8ce-4505-b75e-c98f373a2205\",\"name\":\"master\",\"type\":\"filebeat\",\"version\":\"7.9.2\"}}\n\n```\n\n&nbsp;\n","tags":["filebeat"]},{"title":"maven打包scala+java工程","url":"/maven打包scala+java工程.html","content":"在 scala和java混合编程的时候，需要添加一些额外的配置到pom中，才能将scala文件的class加到最终的jar中\n\n```\n    <build>\n        <pluginManagement>\n            <plugins>\n                <plugin>\n                    <groupId>org.scala-tools</groupId>\n                    <artifactId>maven-scala-plugin</artifactId>\n                    <version>2.15.2</version>\n                    <executions>\n                        <execution>\n                            <id>scala-compile</id>\n                            <goals>\n                                <goal>compile</goal>\n                            </goals>\n                            <configuration>\n                                <!--includes是一个数组，包含要编译的code-->\n                                <includes>\n                                    <include>**/*.scala</include>\n                                </includes>\n                            </configuration>\n                        </execution>\n                        <execution>\n                            <id>scala-test-compile</id>\n                            <goals>\n                                <goal>testCompile</goal>\n                            </goals>\n                        </execution>\n                    </executions>\n                </plugin>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-assembly-plugin</artifactId>\n                    <version>3.1.0</version>\n                    <configuration>\n                        <descriptorRefs>\n                            <descriptorRef>jar-with-dependencies</descriptorRef>\n                        </descriptorRefs>\n                    </configuration>\n                    <executions>\n                        <execution>\n                            <id>make-assembly</id>\n                            <phase>package</phase>\n                            <goals>\n                                <goal>single</goal>\n                            </goals>\n                        </execution>\n                    </executions>\n                </plugin>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-compiler-plugin</artifactId>\n                    <version>3.7.0</version>\n                    <configuration>\n                        <source>1.8</source>\n                        <target>1.8</target>\n                    </configuration>\n                </plugin>\n            </plugins>\n        </pluginManagement>\n   </build>\n\n```\n\n<!--more-->\n&nbsp;打包的命令\n\n```\n mvn clean scala:compile compile package assembly:single -Pproduction -Dmaven.test.skip=true\n\n```\n\n&nbsp;\n\n或者\n\n```\n    <build>\n            <plugin>\n                <groupId>org.scala-tools</groupId>\n                <artifactId>maven-scala-plugin</artifactId>\n                <version>2.15.2</version>\n                <executions>\n                    <execution>\n                        <id>scala-compile</id>\n                        <goals>\n                            <goal>compile</goal>\n                        </goals>\n                        <configuration>\n                            <!--includes是一个数组，包含要编译的code-->\n                            <includes>\n                                <include>**/*.scala</include>\n                            </includes>\n                        </configuration>\n                    </execution>\n                    <execution>\n                        <id>scala-test-compile</id>\n                        <goals>\n                            <goal>testCompile</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-assembly-plugin</artifactId>\n                <version>3.1.0</version>\n                <configuration>\n                    <descriptorRefs>\n                        <descriptorRef>jar-with-dependencies</descriptorRef>\n                    </descriptorRefs>\n                </configuration>\n                <executions>\n                    <execution>\n                        <id>make-assembly</id>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>single</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>3.7.0</version>\n                <configuration>\n                    <source>1.8</source>\n                    <target>1.8</target>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n\n```\n\n打包\n\n```\nmvn package assembly:single\n\n```\n\n　　\n","tags":["Maven"]},{"title":"使用thrift的java client调用python server","url":"/使用thrift的java client调用python server.html","content":"参考：[Thrift 连接 Java 与 Python，附 Java 通用工厂方法](http://www.ciphermagic.cn/thrift-python-java.html)\n\n上面这篇文章的例子是使用java client调用python server中的helloString方法来打印client传输过去的字符串\n\nthrift文件，hello.thrift\n\n```\nservice Hello {\n    string helloString(1:string word)\n}\n\n```\n\n**Server端**\n\n生成Python server端代码\n\n```\nthrift --gen py hello.thrift\n\n```\n\n<!--more-->\n&nbsp;python server端代码，其中包括生成的hello文件夹中的代码，以及server代码\n\n```\nfrom thrift.protocol import TBinaryProtocol\nfrom thrift.server import TServer\nfrom thrift.transport import TSocket\nfrom thrift.transport import TTransport\n\nfrom hello import Hello\n\n\nclass HelloHandler:\n    def __init__(self):\n        pass\n\n    def helloString(self, word):\n        ret = \"hello Thrift! Received: \" + word\n        return ret\n\n\n# handler processer类\nhandler = HelloHandler()\nprocessor = Hello.Processor(handler)\ntransport = TSocket.TServerSocket(\"127.0.0.1\", 8989)\n# 传输方式，使用buffer\ntfactory = TTransport.TBufferedTransportFactory()\n# 传输的数据类型：二进制\npfactory = TBinaryProtocol.TBinaryProtocolFactory()\n# 创建一个thrift 服务~\nserver = TServer.TThreadPoolServer(processor, transport, tfactory, pfactory)\nprint(\"Starting thrift server in python...\")\nserver.serve()\nprint(\"done!\")\n\n```\n\n&nbsp;\n\n**Client端**\n\n生成java client代码\n\n&nbsp;\n\n```\nthrift --gen java hello.thrift\n\n```\n\npom文件\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>org.example</groupId>\n    <artifactId>thrift-example</artifactId>\n    <version>1.0-SNAPSHOT</version>\n\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.thrift</groupId>\n            <artifactId>libthrift</artifactId>\n            <version>0.10.0</version>\n        </dependency>\n    </dependencies>\n\n</project>\n\n```\n\njava client的代码\n\n```\npackage com.example.tutorial;\n\nimport org.apache.thrift.protocol.TBinaryProtocol;\nimport org.apache.thrift.protocol.TProtocol;\nimport org.apache.thrift.transport.TSocket;\nimport org.apache.thrift.transport.TTransport;\nimport org.apache.thrift.transport.TTransportException;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.lang.reflect.Constructor;\nimport java.lang.reflect.Method;\n\npublic class ThriftFactory {\n\n    private ThriftFactory() {\n    }\n\n    private static final Logger LOG = LoggerFactory.getLogger(ThriftFactory.class);\n    private static TProtocol protocol;\n    private static TTransport transport;\n\n    /**\n     * 获取二进制 protocol\n     *\n     * @return 二进制 protocol\n     */\n    public static TProtocol getTProtocol() {\n        // 单例获取 protocol\n        if (protocol == null) {\n            protocol = new TBinaryProtocol(getTTransport());\n        }\n        return protocol;\n    }\n\n    /**\n     * 获取传输对象\n     *\n     * @return 传输对象\n     */\n    public static TTransport getTTransport() {\n        // 单例获取 transport\n        if (transport == null) {\n            // 应改成从配置文件读取\n            String ip = \"127.0.0.1\";\n            Integer port = 8989;\n            transport = new TSocket(ip, port);\n        }\n        return transport;\n    }\n\n    /**\n     * 获取客户端实例\n     *\n     * @param clazz 客户端类\n     * @param <T>   泛型\n     * @return 客户端实例\n     */\n    public static <T> T getClient(Class<T> clazz) {\n        T instance = null;\n        try {\n            //获取有参构造器\n            Constructor c = clazz.getConstructor(TProtocol.class);\n            // 实例化客户端，需要传入 protocol\n            instance = (T) c.newInstance(getTProtocol());\n        } catch (Exception e) {\n            LOG.error(\"\", e);\n            throw new RuntimeException(e.getMessage());\n        }\n        return instance;\n    }\n\n    /**\n     * 发起请求\n     *\n     * @param clazz      客户端类\n     * @param methodName 方法名，客户端中不能有重载的方法\n     * @param param      方法参数\n     * @param <T>        泛型\n     * @return 方法返回值\n     */\n    public static <T> Object doRequest(Class<T> clazz, String methodName, Object... param) {\n        Object result = null;\n        try {\n            // 获取客户端实例\n            T instance = getClient(clazz);\n            Method[] methods = clazz.getMethods();\n            for (Method method : methods) {\n                // 获取指定的方法\n                if (method.getName().equals(methodName)) {\n                    open();\n                    result = method.invoke(instance, param);\n                    close();\n                    break;\n                }\n            }\n        } catch (Exception e) {\n            LOG.error(\"\", e);\n            throw new RuntimeException(e.getMessage());\n        }\n        return result;\n    }\n\n    /**\n     * 打开传输\n     */\n    public static void open() {\n        try {\n            getTTransport().open();\n        } catch (TTransportException e) {\n            LOG.error(\"\", e);\n            throw new RuntimeException(e.getMessage());\n        }\n    }\n\n    /**\n     * 关闭传输\n     */\n    public static void close() {\n        getTTransport().close();\n    }\n\n}\n\n```\n\nclient主函数\n\n```\npackage com.example.tutorial;\n\npublic class ThriftExample {\n\n    public static void main(String[] args) {\n        String msg = (String) ThriftFactory.doRequest(com.example.tutorial.Hello.Client.class, \"helloString\", \"测试\");\n        System.out.println(msg);\n    }\n\n}\n\n```\n\n运行python server\n\n运行java client，调用了python的helloString方法\n\n<img src=\"/images/517519-20201009134617438-830998538.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["Thrift"]},{"title":"MapReduce中的InputFormat","url":"/MapReduce中的InputFormat.html","content":"InputFormat在hadoop源码中是一个抽象类 public abstract class InputFormat<K, V>\n\n```\nhttps://github.com/apache/hadoop/blob/master/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/InputFormat.java\n\n```\n\n可以参考文章\n\n```\nhttps://cloud.tencent.com/developer/article/1043622\n\n```\n\n其中有两个抽象方法\n\n```\n  public abstract \n    List<InputSplit> getSplits(JobContext context\n                               ) throws IOException, InterruptedException;\n\n```\n\n和\n\n```\n  public abstract \n    RecordReader<K,V> createRecordReader(InputSplit split,\n                                         TaskAttemptContext context\n                                        ) throws IOException, \n                                                 InterruptedException;\n\n```\n\n**InputFormat**做的事情就是将inputfile使用**getSplits方法**切分成List<InputSplit>，之后使用**createRecordReader方法**将每个split 解析成records, 再依次将record解析成<K,V>对\n\n<!--more-->\n&nbsp;\n\n1.**getSplits方法**负责将输入的文件做一个逻辑上的切分，切分成一个List<InputSplit>，InputSplit的源码在\n\n```\nhttps://github.com/apache/hadoop/blob/master/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/InputSplit.java\n\n```\n\n&nbsp;在下文中提到 **`InputSplit`**是一个逻辑概念，并没有对实际文件进行切分，它只包含一些元数据信息，比如数据的起始位置，数据长度，数据所在的节点等\n\n```\nhttps://cloud.tencent.com/developer/article/1481777\n\n```\n\n2.**`createRecordReader`方法 将每个&nbsp;split&nbsp; 解析成records, 再依次将record解析成<K,V>对**\n\n&nbsp;\n\n常用的InputFormat如下图\n\n<img src=\"/images/517519-20201111173019261-371084873.png\" alt=\"\" loading=\"lazy\" />\n\n<1>其中 **`TextInputFormat`** 格式的文件，每一行是一个record，key是这一行的byte的offset，即key是LongWritable，value是Text\n\n比如我们使用下面的建表语句来创建一张hive表\n\n```\nCREATE TABLE `default.test_1`(\n      `key` string,\n      `value` string)\nSTORED AS TEXTFILE\n\n```\n\n&nbsp;其hive表对应的inputformat即为 org.apache.hadoop.mapred.TextInputFormat\n\n```\nCREATE TABLE `default.test_1`(\n\t  `key` string, \n\t  `value` string)\n\tROW FORMAT SERDE \n\t  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' \n\tSTORED AS INPUTFORMAT \n\t  'org.apache.hadoop.mapred.TextInputFormat' \n\tOUTPUTFORMAT \n\t  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n\tLOCATION\n\t  'hdfs://xxx-nameservice/user/hive/warehouse/test_1'\n\tTBLPROPERTIES (\n\t  'transient_lastDdlTime'='1605247462')\n\n```\n\n<2>其中 **KeyValue`TextInputFormat`** 格式是最简单的 Hadoop 输入格式之一，可以用于从文本文件中读取键值对数据。每一行都会被独立处理，键和值之间用制表符隔开。\n\n参考：[Spark学习笔记&mdash;&mdash;数据读取和保存](https://www.cnblogs.com/tonglin0325/p/6682367.html)\n\n比如文件的内容为，中间以tab分隔\n\n<img src=\"/images/517519-20201113154632190-996469566.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n如果使用 **`TextInputFormat`**` 来读取的话`\n\n```\n  def main(args: Array[String]): Unit = {\n    val conf = new SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n    val job = Job.getInstance()\n    val data = sc.newAPIHadoopFile(\"file:///home/lintong/下载/test.log\",\n      classOf[TextInputFormat],\n      classOf[LongWritable],\n      classOf[Text],\n      job.getConfiguration)\n    data.foreach(println)\n  }\n\n```\n\n&nbsp;输出为，其中_1为LongWritable，即byte的offset，_2为每行的文件内容\n\n<img src=\"/images/517519-20201113154813343-496035637.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n如果使用 **KeyValue`TextInputFormat`** 来读取的话\n\n```\n  def main(args: Array[String]): Unit = {\n    val conf = new SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n    val job = Job.getInstance()\n    val data = sc.newAPIHadoopFile(\"file:///home/lintong/下载/test.log\",\n      classOf[KeyValueTextInputFormat],\n      classOf[Text],\n      classOf[Text],\n      job.getConfiguration)\n    data.foreach(println)\n  }\n\n```\n\n&nbsp;输出为其中_1为Text，即tab分隔之前的内容，_2为tab分隔之后的内容，如果有多个tab的话，只会切分第一个tab\n\n<img src=\"/images/517519-20201113154952009-2023481365.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;<3> 对于 **`SequenceFileInputFormat`** 格式，这是一种专门用于二进制文件的input format，key和value的类型由用户决定\n\n在hive中如果使用thrift和PB序列化格式的hive表，可以参考Twitter的elephant-bird项目\n\n```\nhttps://github.com/twitter/elephant-bird/wiki/How-to-use-Elephant-Bird-with-Hive\n\n```\n\n下面给出一个例子\n\n```\nCREATE EXTERNAL TABLE `default.test`(\n      `bbb` string COMMENT 'from deserializer',\n      `aaa` string COMMENT 'from deserializer')\n    COMMENT 'aas'\n    PARTITIONED BY (\n      `ds` string COMMENT '日期分区')\n    ROW FORMAT SERDE\n      'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer'\n    WITH SERDEPROPERTIES (\n      'serialization.class'='com.xxx.xxx.xxx.tables.v1.XXXX',\n      'serialization.format'='org.apache.thrift.protocol.TCompactProtocol')\n    STORED AS INPUTFORMAT\n      'org.apache.hadoop.mapred.SequenceFileInputFormat'\n    OUTPUTFORMAT\n      'org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'\n    LOCATION\n      'hdfs://master:8020/user/hive/warehouse/test'\n    TBLPROPERTIES (\n      'transient_lastDdlTime'='xxxxxx')\n\n```\n\n&nbsp;如果使用spark来读写sequencefile文件\n\n```\ndef main(args: Array[String]) {\n    val conf = new SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n \n    //写sequenceFile，\n    val rdd = sc.parallelize(List((\"Panda\", 3), (\"Kay\", 6), (\"Snail\", 2)))\n    rdd.saveAsSequenceFile(\"file:///home/lintong/下载/output\")\n \n    //读sequenceFile\n    val output = sc.sequenceFile(\"file:///home/lintong/下载/output\", classOf[Text], classOf[IntWritable]).\n      map{case (x, y) => (x.toString, y.get())}\n    output.foreach(println)\n \n  }\n\n```\n\n输出的文件格式为二进制，如下\n\n<img src=\"/images/517519-20201113160810677-567983998.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;读取结果如下\n\n<img src=\"/images/517519-20201113160933290-1992319925.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;对于那些不支持split的文件格式，提升mapreduce任务的map数并不能加快处理的速度，参考hadoop权限指南的表格\n\n<img src=\"/images/517519-20201111163056598-1210829799.png\" alt=\"\" loading=\"lazy\" />\n\n<img src=\"/images/517519-20201113163206877-1827565106.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;\n\n注意：其中**gzip压缩格式**适合用于冷数据上；**snappy**和**lzo压缩格式**适合用于热数据上；\n\n**lzo格式**在创建在index索引文件后，可以支持切分；\n\n此外，从cloudera的文章 [Choosing and Configuring Data Compression](https://docs.cloudera.com/cloudera-manager/7.1.0/managing-clusters/topics/cm-choosing-configuring-data-compression.html) 中，可以看出\n\n**gzip**和**snappy格式**的块是不能切分的，但是使用了avro这种**容器文件格式**的snappy块的文件是可以切分的；snappy压缩格式也建议是使用在二进制文件和avro数据文件中，不建议使用在文本格式文件中，比如json格式\n\n<img src=\"/images/517519-20201111165456938-1196281628.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n**&nbsp;facebook**&nbsp;提供的各种压缩算法的benchmark\n\n```\nhttps://github.com/facebook/zstd#benchmarks\n\n```\n\n<img src=\"/images/517519-20201221111426247-356129254.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;其中zstd也是可以切分的\n\n```\nhttps://medium.com/@anirbangoswami_22783/parquet-zstd-vs-gzip-740a83571ecd\n\n```\n\n　　\n","tags":["Hadoop"]},{"title":"Ubuntu16.04安装protobuf","url":"/Ubuntu16.04安装protobuf.html","content":"## 1.proto2\n\n1.protobuf的github地址\n\n```\nhttps://github.com/protocolbuffers/protobuf\n\n```\n\n去releases下载需要的版本\n\n```\nhttps://github.com/protocolbuffers/protobuf/releases\n\n```\n\n选择2.5.0的版本\n\n```\nhttps://github.com/protocolbuffers/protobuf/releases/tag/v2.5.0\n\n```\n\n下载\n\n```\nwget https://github.com/protocolbuffers/protobuf/releases/download/v2.5.0/protobuf-2.5.0.tar.gz\n\n```\n\n编译安装\n\n```\n./autogen.sh\n./configure\nmake\nmake check\nsudo make install\n\n```\n\n安装完毕，查看版本\n\n```\nprotoc --version\nlibprotoc 2.5.0\n\n```\n\n参考google的javatutorial\n\n```\nhttps://developers.google.com/protocol-buffers/docs/javatutorial\n\n```\n\npb的数据类型如下\n\n```\nhttps://developers.google.com/protocol-buffers/docs/proto\n\n```\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20200917170923391-1137294845.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;\n\n下载文件addressbook.proto\n\n```\nhttps://github.com/protocolbuffers/protobuf/blob/v2.5.0/examples/addressbook.proto\n\n```\n\n&nbsp;编译成java代码\n\n```\nprotoc -I=./ --java_out=./ ./addressbook.proto # protoc -I=$SRC_DIR --java_out=$DST_DIR $SRC_DIR/addressbook.proto\n或者\nprotoc --java_out=. addressbook.proto\n\n```\n\n生成文件\n\n```\ntree -L 4\n.\n├── addressbook.proto\n└── com\n    └── example\n        └── tutorial\n            └── AddressBookProtos.java\n\n```\n\n## 2.proto3\n\n如果想使用proto3的话，需要添加如下语法，否则编译器会使用proto2\n\n```\nsyntax = \"proto3\";\n\n```\n\n参考\n\n```\nhttps://github.com/protocolbuffers/protobuf/blob/main/examples/addressbook.proto\n\n```\n\n以及\n\n```\nhttps://docs.confluent.io/platform/current/schema-registry/fundamentals/serdes-develop/serdes-protobuf.html\n\n```\n\n## 3.proto定义\n\n```\nmessage HelloRequest {\n  string name = 1; // 转换为json的时候，\"name\": \"\"\n  optional string name1 = 2; // 转换为json的时候，将没有key\n  google.protobuf.StringValue name2 = 3; // 转换为json的时候，\"name2\": null\n}\n\n```\n\n&nbsp;\n","tags":["google"]},{"title":"aws s3原理和常用命令","url":"/aws s3原理和常用命令.html","content":"## **1.概念**\n\n**Amazon s3**全称Amazon Simple Storage Service，是一个对象存储，不是一个file system，所以在使用s3的时候，list dir会很慢\n\nkv存储：[从零开始写KV数据库：基于哈希索引](https://zhuanlan.zhihu.com/p/351897096)\n\n<!--more-->\n&nbsp;\n\n比如如下的s3路径\n\n```\ns3://BucketName/Project/WordFiles/123.txt\n\n```\n\n其中BucketName是s3的桶名\n\nbucketname/Project/WordFiles/是分区前缀prefix\n\n123.txt是对象名字\n\ns3://BucketName/Project/WordFiles/123.txt是键前缀\n\n&nbsp;\n\n参考：[对于 Amazon S3 请求速率，前缀和嵌套文件夹之间有何区别？ S3 存储桶中可以有多少个前缀？](https://aws.amazon.com/cn/premiumsupport/knowledge-center/s3-prefix-nested-folders-difference/#:~:text=A%20prefix%20is%20the%20complete,%2FProject%2FWordFiles%2F%E2%80%9D.)\n\n[S3 - What Exactly Is A Prefix? And what Ratelimits apply?](https://stackoverflow.com/questions/52443839/s3-what-exactly-is-a-prefix-and-what-ratelimits-apply)\n\n[Organizing objects using prefixes](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-prefixes.html)\n\n## 2.s3性能\n\nAmazon S3 是一个非常大的分布式系统，在应用程序对 Amazon S3 读写数据时，您可以将请求性能扩展到每秒数千个事务。\n\nAmazon S3 性能不是按存储桶定义的，而是按存储桶中的[前缀](https://docs.aws.amazon.com/general/latest/gr/glos-chap.html#keyprefix)。在一个存储桶中，对于每个前缀，应用程序每秒可以处理至少 3500 个 PUT/COPY/POST/DELETE 请求或 5500 个 GET/HEAD 请求。\n\n此外，存储桶中的前缀数量没有限制，因此您可以利用并行处理，横向扩展读取或写入性能。\n\n例如，如果您在 S3 存储桶中创建 10 个前缀来并行执行读取操作，则可以将读取性能扩展到每秒 55000 个读取请求。同样，您也可以利用多个前缀写入数据来扩展写入性能。\n\n参考：[Best practices design patterns: optimizing Amazon S3 performance](https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html)\n\n[从 Amazon EMR 和 AWS Glue 访问 Amazon S3 中数据的性能优化最佳实践](https://aws.amazon.com/cn/blogs/china/best-practices-to-optimize-data-access-performance-from-amazon-emr-and-aws-glue-to-amazon-s3/)\n\n&nbsp;\n\n## **3.对象存储和文件系统的区别**\n\n参考：[是否可以将 Amazon S3 而不是 HDFS 作为 Hadoop 存储？](https://aws.amazon.com/cn/premiumsupport/knowledge-center/configure-emr-s3-hadoop-storage/)\n\n```\nhttps://hadoop.apache.org/docs/stable2/hadoop-project-dist/hadoop-common/filesystem/introduction.html#Object_Stores_vs._Filesystems\n\n```\n\n脉脉上老哥的解答\n\n<img src=\"/images/517519-20211008140425771-1072765003.png\" width=\"300\" height=\"302\" loading=\"lazy\" />\n\n&nbsp;<img src=\"/images/517519-20211008140251893-1108561049.png\" width=\"300\" height=\"230\" loading=\"lazy\" />\n\n对于hdfs这种的fs而言，rename和list操作很快，而s3作为KV存储，rename和list操作很慢，因此s3使用了dynamodb使用了索引对齐做加速，尤其是rename操作，s3的rename等于copy+delete\n\n## **4.s3协议的区别**\n|Generation|Usage|Description\n|First|s3:\\\\|s3&nbsp;which is also called classic&nbsp;(s3:&nbsp;filesystem for reading from or storing objects in Amazon S3 This has been deprecated and recommends to use either the second or third generation library.\n|Second|s3n:\\\\|s3n&nbsp;uses native s3 object and makes easy to use it with Hadoop and other files systems.\n|Third|s3a:\\\\|s3a&nbsp;&ndash; This is a replacement of&nbsp;s3n&nbsp;which supports larger files and improves in performance.\n\n**在EMR中，还是建议使用s3协议**\n\n**<img src=\"/images/517519-20220620225617902-1226295500.png\" width=\"700\" height=\"135\" loading=\"lazy\" />**\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n\n**s3和cdh hdfs之间数据迁移**，参考\n\n```\nhttp://bdlabs.edureka.co/static/help/topics/cdh_admin_distcp_data_cluster_migrate.html\n\n```\n\n**HDP的s3 guide**\n\n```\nhttps://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.0.0/bk_cloud-data-access/content/s3-get-started.html\n\n```\n\n**s3常用命令**，参考：\n\n```\nhttps://docs.aws.amazon.com/zh_cn/cli/latest/userguide/cli-services-s3.html\n\n```\n\n&nbsp;\n\n## **5.常用命令**\n\n1.安装awscli\n\n```\npip install awscli\n\n```\n\n版本\n\n```\naws --version\naws-cli/1.18.143 Python/3.6.2 Linux/4.4.0-165-generic botocore/1.18.2\n\n```\n\n有些在CDH的s3自动安装的awscli版本可能会过低，导致一些命令不支持，比如\n\n```\naws --version\naws-cli/1.4.2 Python/3.4.2 Linux/4.9.0-0.bpo.6-amd64\n\n```\n\n在~/.aws目录下配置region和账号密码\n\n```\n~/.aws$ ls\nconfig  credentials\n\n```\n\nconfig\n\n```\n[default]\nregion = ap-northeast-1\n\n```\n\ncredentials\n\n```\n[default]\naws_access_key_id = XXXX\naws_secret_access_key = XXXX\n\n```\n\n2.查看文件夹\n\n```\naws s3 ls s3://xxxxx/logs/\n\n```\n\n3. 递归删除s3文件夹\n\n```\naws s3 rm --recursive s3://xxxxx/logs/test\n\n```\n\n4.下载对象存储的前1024个字节的文件\n\n```\naws s3api get-object --bucket {bucket-name} --key xxx/xxx/xxx_log/2021-09-22/xx-xx.gz --range bytes=0000-1024 my_data_range\n\n```\n\n5.下载s3文件到本地\n\n```\naws s3 cp s3://{bucket_name}/xxxx/xx/xxxx/2021-10-21/00/xxxxx.bin.gz ./\n\n```\n\n6.上传文件到s3\n\n```\naws s3 cp ~/test.json s3://xxx-bucket/new_dic/\n\n```\n\n　　\n","tags":["AWS"]},{"title":"Ubuntu16.04安装openldap和phpldapadmin","url":"/Ubuntu16.04安装openldap和phpldapadmin.html","content":"安装openldap，参考：\n\n```\nhttps://www.alibabacloud.com/blog/how-to-install-openldap-and-phpldapadmin-on-ubuntu-16-04_594318\nhttps://www.cnblogs.com/hzw97/p/11592244.html#_lab2_3_0\n\n```\n\n先卸载干净\n\n```\nsudo apt remove --purge slapd ldap-utils\n\n```\n\n再安装\n\n```\nsudo apt-get install slapd ldap-utils\n\n```\n\n设置管理员密码，直接确定，后面配置admin的时候会再次配置\n\n<img src=\"/images/517519-20211031212435032-848971249.png\" width=\"500\" height=\"239\" loading=\"lazy\" />\n\n配置ldap admin账号密码\n\n```\nsudo dpkg-reconfigure slapd\n\n```\n\n<img src=\"/images/517519-20211031212551991-885205825.png\" width=\"600\" height=\"216\" loading=\"lazy\" />\n\n配置dc\n\n<img src=\"/images/517519-20211031212634396-1154026409.png\" width=\"600\" height=\"182\" loading=\"lazy\" />\n\n组织名\n\n<img src=\"/images/517519-20211031212709924-228195399.png\" width=\"600\" height=\"218\" loading=\"lazy\" />\n\n设置admin账号密码\n\n<img src=\"/images/517519-20211031212820091-1733752138.png\" width=\"500\" height=\"220\" loading=\"lazy\" />\n\ndatabase选择MDB\n\n<img src=\"/images/517519-20211031212858297-1196615075.png\" width=\"700\" height=\"302\" loading=\"lazy\" />\n\n否\n\n<img src=\"/images/517519-20211031212934218-1000041861.png\" width=\"400\" height=\"185\" loading=\"lazy\" />\n\n是\n\n<img src=\"/images/517519-20211031213428290-1922290766.png\" width=\"700\" height=\"193\" loading=\"lazy\" />\n\n否\n\n<img src=\"/images/517519-20211031213557091-1741767788.png\" width=\"700\" height=\"172\" loading=\"lazy\" />\n\n查看ldap状态\n\n```\nsudo systemctl status slapd\n\n```\n\n<!--more-->\n&nbsp;\n\n安装phpldapadmin\n\n1.首先卸载掉老的phpldapadmin\n\n```\nsudo apt remove --purge phpldapadmin\n\n```\n\n2.安装\n\n```\nsudo apt-get install phpldapadmin\n\n```\n\n3.修改config.php\n\n```\nsudo vim /etc/phpldapadmin/config.php\n\n$servers->setValue('server','base',array('dc=hadoop,dc=com'));\n$servers->setValue('login','bind_id','cn=admin,dc=hadoop,dc=com');\n\n```\n\n&nbsp;\n\n如果出现报错\n\n```\nConf phpldapadmin disabled.\napache2_invoke postrm: Disable configuration phpldapadmin.conf\napache2.service is not active, cannot reload.\ninvoke-rc.d: initscript apache2, action \"reload\" failed\n\n```\n\n&nbsp;<img src=\"/images/517519-20200913134843137-571281073.png\" width=\"500\" height=\"134\" loading=\"lazy\" />\n\n然后访问页面，出现的是404\n\n```\nhttp://master/phpldapadmin\n\n```\n\n<img src=\"/images/517519-20200913135355694-1218301642.png\" width=\"600\" height=\"111\" loading=\"lazy\" />\n\n那是phpldapadmin安装失败\n\n有可能是apache2的80端口被nginx占用导致的，输出命令查看80端口的占用情况\n\n```\nsudo netstat -tulpn | grep :80\n\n```\n\n&nbsp;可以看到nginx占用了apache2的80端口，但是ldap又需要使用apache2\n\n&nbsp;<img src=\"/images/517519-20200913141432864-506115724.png\" width=\"600\" height=\"267\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;查看apache2的状态\n\n```\n/etc/init.d/apache2 reload\n\n```\n\n&nbsp;<img src=\"/images/517519-20200913141906577-645772149.png\" width=\"800\" height=\"61\" loading=\"lazy\" />\n\n所以apache2服务是有问题的\n\n关闭nginx\n\n```\nsudo service nginx stop\n\n```\n\n打开apache2\n\n```\nsudo service apache2 start\n\n```\n\n&nbsp;<img src=\"/images/517519-20200913142301735-655420957.png\" width=\"600\" height=\"49\" loading=\"lazy\" />\n\n或者也可以修改apache2的默认端口\n\n```\nvim /etc/apache2/ports.conf\n\n```\n\n修改80成其他的，然后\n\n```\nvim /etc/apache2/sites-enabled/000-default.conf\n\n```\n\n修改 <VirtualHost *:80>\n\n然后重启apache2\n\n```\nsudo service apache2 restart\n\n```\n\n重新安装phpldapadmin，正常安装\n\n```\napt-get install phpldapadmin\n正在读取软件包列表... 完成\n正在分析软件包的依赖关系树\n正在读取状态信息... 完成\n下列软件包是自动安装的并且现在不需要了：\n  gimp-data gimp-help-common gimp-help-en lame libamd2.4.1 libbabl-0.1-0 libcamd2.4.1 libccolamd2.9.1 libcholmod3.0.6\n  libgegl-0.3-0 libgimp2.0 libjavascriptcoregtk-1.0-0 libqgsttools-p1 libqt4-svg libqt5multimedia5-plugins\n  libqt5multimediawidgets5 libumfpack5.7.1 libwebkitgtk-1.0-0 libwebkitgtk-1.0-common\n使用'sudo apt autoremove'来卸载它(它们)。\n下列【新】软件包将被安装：\n  phpldapadmin\n升级了 0 个软件包，新安装了 1 个软件包，要卸载 0 个软件包，有 0 个软件包未被升级。\n需要下载 0 B/727 kB 的归档。\n解压缩后会消耗 5,104 kB 的额外空间。\n正在预设定软件包 ...\n正在选中未选择的软件包 phpldapadmin。\n(正在读取数据库 ... 系统当前共安装有 288553 个文件和目录。)\n正准备解包 .../phpldapadmin_1.2.2-5.2ubuntu2.1_all.deb  ...\n正在解包 phpldapadmin (1.2.2-5.2ubuntu2.1) ...\n正在设置 phpldapadmin (1.2.2-5.2ubuntu2.1) ...\n\nCreating config file /etc/phpldapadmin/config.php with new version\napache2_invoke: Enable configuration phpldapadmin.conf\n\n```\n\n访问页面，成功\n\n```\nhttp://master/phpldapadmin\n\n```\n\n&nbsp;<img src=\"/images/517519-20200913142646604-908498246.png\" width=\"1000\" height=\"307\" loading=\"lazy\" />\n\n账号就是admin和刚刚安装ldap的时候设置的password\n\n往ldap上导入os上的组织\n\n可以选择自己新建ou，选择 Generic: Organisational Unit，新建Group和People两个ou\n\n<img src=\"/images/517519-20200913170422890-1420762396.png\" width=\"600\" height=\"322\" loading=\"lazy\" />\n\n&nbsp;\n\n也可以使用命令批量导入os上的ou\n\n首先安装迁移工具 migrationtools\n\n```\nsudo apt-get install migrationtools\n\n```\n\n修改配置\n\n```\nvim /etc/migrationtools/migrate_common.ph\n\n```\n\n将下面\n\n```\n# Default DNS domain\n$DEFAULT_MAIL_DOMAIN =  \"padl.com\";\n\n# Default base \n$DEFAULT_BASE =  \"dc=padl,dc=com\";\n\n```\n\n修改成\n\n```\n# Default DNS domain\n$DEFAULT_MAIL_DOMAIN = \"example.com\";\n\n# Default base\n$DEFAULT_BASE = \"dc=example,dc=com\";\n\n```\n\n参考了\n\n```\nhttps://www.cnblogs.com/hzw97/p/11592244.html#_lab2_3_0\n\n```\n\n&nbsp;生成文件模板\n\n```\nlintong@master:~/下载$ /usr/share/migrationtools/migrate_base.pl > ./linux_base.ldif\n\n```\n\n将模板中将ldap上已有的entry去掉，否则会导入失败\n\n```\ndn: dc=example,dc=com\ndc: example\nobjectClass: top\nobjectClass: domain\n\n```\n\n如下报错\n\n```\nadding new entry \"dc=example,dc=com\"\nldap_add: Already exists (68)\n\n```\n\n去掉后再次导入，XXXXXX是bind_password\n\n```\nsudo ldapadd -x -D \"cn=admin,dc=example,dc=com\" -w XXXXXX -f ./linux_base.ldif\n\n```\n\n&nbsp;导入成功\n\n```\nadding new entry \"ou=Protocols,dc=example,dc=com\"\n\nadding new entry \"nisMapName=netgroup.byuser,dc=example,dc=com\"\n\nadding new entry \"ou=Services,dc=example,dc=com\"\n\nadding new entry \"ou=Networks,dc=example,dc=com\"\n\nadding new entry \"ou=People,dc=example,dc=com\"\n\nadding new entry \"nisMapName=netgroup.byhost,dc=example,dc=com\"\n\nadding new entry \"ou=Rpc,dc=example,dc=com\"\n\nadding new entry \"ou=Netgroup,dc=example,dc=com\"\n\nadding new entry \"ou=Group,dc=example,dc=com\"\n\nadding new entry \"ou=Mounts,dc=example,dc=com\"\n\nadding new entry \"ou=Aliases,dc=example,dc=com\"\n\nadding new entry \"ou=Hosts,dc=example,dc=com\"\n\n```\n\n&nbsp;去phpldapadmin上查看\n\n<img src=\"/images/517519-20200913171906185-885521727.png\" width=\"400\" height=\"552\" loading=\"lazy\" />\n\n&nbsp;导入os用户，`/etc/passwd文件中记录了linux上所有的os用户`\n\n```\ncat /etc/passwd > ./people.txt\n/usr/share/migrationtools/migrate_passwd.pl ./people.txt ./people.ldif\n\n```\n\n迁移到ldap中\n\n```\nsudo ldapadd -x -D \"cn=admin,dc=example,dc=com\" -w XXXXXX -f ./people.ldif\n\n```\n\n&nbsp;去phpldapadmin上查看\n\n<img src=\"/images/517519-20200913174730034-609924589.png\" width=\"300\" height=\"685\" loading=\"lazy\" />\n\n导入os组\n\n```\ncat /etc/group > ./group.txt\n/usr/share/migrationtools/migrate_group.pl ./group.txt ./group.ldif\n\n```\n\n&nbsp;迁移到ldap中\n\n```\nsudo ldapadd -x -D \"cn=admin,dc=example,dc=com\" -w XXXXXX -f ./group.ldif\n\n```\n\n&nbsp;成功\n\n<img src=\"/images/517519-20200913182121545-1317847895.png\" width=\"300\" height=\"759\" loading=\"lazy\" />\n\n&nbsp;ldap查询entry，-W是交互式输入密码，-w \"密码\"\n\n```\nldapsearch -x -LLL -H ldap://master:389 -D \"cn=admin,dc=example,dc=com\" -W -b \"dc=example,dc=com\"  -s base\n\n```\n\n&nbsp;<img src=\"/images/517519-20200913235617167-1373349770.png\" width=\"300\" height=\"154\" loading=\"lazy\" />\n\n&nbsp;\n\nldap的备份和迁移参考：[How To Backup and Restore OpenLDAP](https://tylersguides.com/articles/backup-restore-openldap/)\n","tags":["ldap"]},{"title":"JVM调优常用命令","url":"/JVM调优常用命令.html","content":"1.查看java进程，jps命令可以列出正在运行的虚拟机进程\n\n```\njps -l\n1005373 sun.tools.jps.Jps\n1000153 org.apache.flume.node.Application\n\n```\n\n2.查看flume进程java虚拟机的统计信息\n\n```\njstat -gcutil 1028479\n  S0     S1     E      O      M     CCS    YGC     YGCT    FGC    FGCT     GCT   \n 71.69   0.00  25.63   0.14  96.56  89.78    116    1.074     0    0.000    1.074\n\n```\n\n某springboot web服务进程java虚拟机的统计信息\n\n```\njstat -gcutil 29\n  S0     S1     E      O      M     CCS    YGC     YGCT    FGC    FGCT     GCT   \n 76.84   0.00  47.11  55.27  96.06  93.04     28   17.787     4    8.589   26.376\n\n```\n\n**对应指标的中文含义**\n\nS0：Survivor0的占用比例<br />S1：Survivor1的占用比例<br />E：新生代Eden区的占用比例<br />O：老年代的占用比例<br />M：方法区的占用比例<br />CCS：压缩类空间的占用比例<br />YGC：年轻代垃圾回收次数<br />YGCT：年轻代垃圾回收消耗时间<br />FGC：老年代垃圾回收次数<br />FGCT：老年代垃圾回收消耗时间<br />GCT：垃圾回收消耗总时间\n\n3.查看jvm使用的是什么垃圾收集器\n\n```\njava -XX:+PrintCommandLineFlags -version\n-XX:InitialHeapSize=2147483648 -XX:MaxHeapSize=32210157568 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseParallelGC \njava version \"1.8.0_121\"\nJava(TM) SE Runtime Environment (build 1.8.0_121-b13)\nJava HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode)\n\n```\n\n使用的是并行垃圾收集器\n\n<!--more-->\n&nbsp;\n\n**JVM内存空间组成**\n\nGithub：[JVM 内存结构](https://github.com/doocs/jvm/blob/main/docs/01-jvm-memory-structure.md)\n\nJava 虚拟机的内存空间分为 5 个部分：\n\n```\n程序计数器\nJava 虚拟机栈\n本地方法栈\n堆\n方法区\n\n```\n\n**<img src=\"/images/517519-20210304145732157-1186399695.png\" alt=\"\" width=\"545\" height=\"412\" loading=\"lazy\" />**\n\n图来自：[【JVM学习】&mdash;&mdash;本地方法栈、堆](https://segmentfault.com/a/1190000037428080)&nbsp;\n\n&nbsp;\n\n**配置参数**\n\n参考：[JVM内存调优总结 -Xms -Xmx -Xmn -Xss 参数设置](https://blog.csdn.net/shadow_zed/article/details/88047808)\n\n```\n堆设置\n-Xms :初始堆大小\n-Xmx :最大堆大小\n-XX:NewSize=n :设置年轻代大小\n-XX:NewRatio=n: 设置年轻代和年老代的比值。如:为3，表示年轻代与年老代比值为1：3，年轻代占整个年轻代年老代和的1/4\n-XX:SurvivorRatio=n :年轻代中Eden区与两个Survivor区的比值。注意Survivor区有两个。如：3，表示Eden：Survivor=3：2，一个Survivor区占整个年轻代的1/5\n-XX:MaxPermSize=n :设置持久代大小\n收集器设置\n-XX:+UseSerialGC :设置串行收集器\n-XX:+UseParallelGC :设置并行收集器\n-XX:+UseParalledlOldGC :设置并行年老代收集器\n-XX:+UseConcMarkSweepGC :设置并发收集器\n垃圾回收统计信息\n-XX:+PrintGC\n-XX:+PrintGCDetails\n-XX:+PrintGCTimeStamps\n-Xloggc:filename\n并行收集器设置\n-XX:ParallelGCThreads=n :设置并行收集器收集时使用的CPU数。并行收集线程数。\n-XX:MaxGCPauseMillis=n :设置并行收集最大暂停时间\n-XX:GCTimeRatio=n :设置垃圾回收时间占程序运行时间的百分比。公式为1/(1+n)\n并发收集器设置\n-XX:+CMSIncrementalMode :设置为增量模式。适用于单CPU情况。\n-XX:ParallelGCThreads=n :设置并发收集器年轻代收集方式为并行收集时，使用的CPU数。并行收集线程数。\n\n```\n\n&nbsp;\n\n1.**程序计数器**\n\n程序计数器（Program Counter Register)，在JVM规范中，每个线程都有自己的程序计数器。这是一块比较小的内存空间，存储当前线程正在执行的Java方法的JVM指令地址，即字节码的行号。如果正在执行Native方法，则这个计数器为空。该内存区域是唯一一个在Java虚拟机规范中没有规定任何OOM情况的内存区域。\n\n&nbsp;\n\n2.**Java虚拟机栈**\n\nJava虚拟机栈(Java Virtal Machine Stack)，同样也是属于线程私有区域，每个线程在创建的时候都会创建一个虚拟机栈，生命周期与线程一致，线程退出时，线程的虚拟机栈也回收。虚拟机栈内部保持一个个的栈帧，每次方法调用都会进行压栈，JVM对栈帧的操作只有出栈和压栈两种，方法调用结束时会进行出栈操作。\n\n&nbsp;\n\n3.**本地方法栈**\n\n本地方法栈（Native Method Stack）与虚拟机栈类似，本地方法栈是在调用本地方法时使用的栈，每个线程都有一个本地方法栈。\n\n&nbsp;\n\n4.**堆内存**\n\n使用jmap查看JVM的**堆内存**情况\n\n堆（Heap），几乎所有创建的Java对象实例，都是被直接分配到堆上的。堆被所有的线程所共享，在堆上的区域，会被垃圾回收器做进一步划分，例如新生代、老年代的划分。Java虚拟机在启动的时候，可以使用&ldquo;Xmx&rdquo;之类的参数指定堆区域的大小。\n\n**堆内存**分成&nbsp;**Young Generation**（年轻代）和 **Old Generation**（老年代）\n\n此外，**Young Generation**（年轻代）还分成&nbsp;Eden Space ， From Space 和 To Space（也叫 Survivor0空间 和 Survivor1空间）\n\n**JVM参数设置参考：**[JVM系列三:JVM参数设置、分析](https://www.cnblogs.com/redcreen/archive/2011/05/04/2037057.html)\n\n```\njmap -heap 29\nAttaching to process ID 29, please wait...\nDebugger attached successfully.\nServer compiler detected.\nJVM version is 25.121-b13\n\nusing thread-local object allocation.\nParallel GC with 33 thread(s)\n\nHeap Configuration:\n   MinHeapFreeRatio         = 0\n   MaxHeapFreeRatio         = 100\n   MaxHeapSize              = 32210157568 (30718.0MB)\n   NewSize                  = 715653120 (682.5MB)\n   MaxNewSize               = 10736369664 (10239.0MB)\n   OldSize                  = 1431830528 (1365.5MB)\n   NewRatio                 = 2\n   SurvivorRatio            = 8\n   MetaspaceSize            = 21807104 (20.796875MB)\n   CompressedClassSpaceSize = 1073741824 (1024.0MB)\n   MaxMetaspaceSize         = 17592186044415 MB\n   G1HeapRegionSize         = 0 (0.0MB)\n\nHeap Usage:\n**PS Young Generation**\nEden Space:\n   capacity = 5859442688 (5588.0MB)\n   used     = 2972839024 (2835.1202239990234MB)\n   free     = 2886603664 (2752.8797760009766MB)\n   50.73586657120658% used\nFrom Space:\n   capacity = 505937920 (482.5MB)\n   used     = 388770920 (370.7608413696289MB)\n   free     = 117167000 (111.7391586303711MB)\n   76.84162515432723% used\nTo Space:\n   capacity = 558891008 (533.0MB)\n   used     = 0 (0.0MB)\n   free     = 558891008 (533.0MB)\n   0.0% used\n**PS Old Generation**\n   capacity = 2802843648 (2673.0MB)\n   used     = 1549069632 (1477.3079223632812MB)\n   free     = 1253774016 (1195.6920776367188MB)\n   55.26778609664352% used\n\n```\n\n&nbsp;\n\n5.**方法区**\n\n方法区（Method Area)。方法区与堆一样，也是所有的线程所共享，存储被虚拟机加载的元（Meta）数据，包括类信息、常量、静态变量、即时编译器编译后的代码等数据。这里需要注意的是运行时常量池也在方法区中。根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。由于早期HotSpot JVM的实现，将CG分代收集拓展到了方法区，因此很多人会将方法区称为永久代。Oracle JDK8中已永久代移除永久代，同时增加了元数据区（Metaspace）。\n\n&nbsp;\n\n6.**运行时常量池**\n\n运行时常量池（Run-Time Constant Pool)，这是方法区的一部分，受到方法区内存的限制，当常量池无法再申请到内存时，会抛出OutOfMemoryError异常。\n\n&nbsp;\n\n7.**直接内存**\n\n直接内存（Direct Memory），直接内存并不属于Java规范规定的属于Java虚拟机运行时数据区的一部分。Java的NIO可以使用Native方法直接在java堆外分配内存，使用DirectByteBuffer对象作为这个堆外内存的引用。\n","tags":["JVM"]},{"title":"Nexus上传jar包","url":"/Nexus上传jar包.html","content":"添加maven proxy\n\n比如中央仓库\n\n```\nhttps://repo1.maven.org/maven2/\n\n```\n\n比如cloudera的仓库\n\n```\nhttps://repository.cloudera.com/artifactory/cloudera-repos\n\n```\n\nmaven-central\n\n<img src=\"/images/517519-20200814150244430-1762524826.png\" width=\"600\" height=\"682\" loading=\"lazy\" />\n\nmaven-cloudera\n\n<img src=\"/images/517519-20200814150347345-1063922310.png\" width=\"600\" height=\"647\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n将这些添加到maven-public中<br /><img src=\"/images/517519-20200814150501579-1273640339.png\" width=\"500\" height=\"239\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["Nexus"]},{"title":"Zk学习笔记——权限控制","url":"/Zk学习笔记——权限控制.html","content":"参考：从Paxos到Zookeeper分布式一致性原理和实践\n\n使用的zk依赖是cdh5.16.2的3.4.5\n\n```\n<!-- zookeeper -->\n<dependency>\n    <groupId>org.apache.zookeeper</groupId>\n    <artifactId>zookeeper</artifactId>\n    <version>3.4.5-cdh5.16.2</version>\n</dependency>\n\n```\n\nZookeeper提供了多种**权限控制模式**，分别是world，auth，digest，ip和super。\n\n下面介绍模式scheme中的**digest**\n\n使用如下语句对zk session添加权限，其中的username:password是账号密码\n\n```\nzk1.addAuthInfo(\"digest\", \"username:password\".getBytes());\n\n```\n\n如果操作zk节点没有权限的话，会抛出NoAuthException\n\n```\nException in thread \"main\" org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /app6\n\n```\n\n代码\n\n```\npackage com.bigdata.zookeeper;\n\nimport org.apache.zookeeper.CreateMode;\nimport org.apache.zookeeper.ZooDefs;\nimport org.apache.zookeeper.ZooKeeper;\n\npublic class AuthExample {\n\n    private static ZooKeeper zk1;\n    private static ZooKeeper zk2;\n\n    public static void main(String[] args) throws Exception {\n        // zk1 session\n        zk1 = new ZooKeeper(\"master:2181\", 5000, null);\n        zk1.addAuthInfo(\"digest\", \"username:password\".getBytes());\n        // 创建一个节点\n        String path = \"/app6\";\n        zk1.create(path, \"123\".getBytes(), ZooDefs.Ids.CREATOR_ALL_ACL, CreateMode.EPHEMERAL);\n\n        // zk2 session无权限\n        zk2 = new ZooKeeper(\"master:2181\", 5000, null);\n//        System.out.println(new String(zk2.getData(path, false, null)));\n\n        // zk2 session有权限\n        zk2.addAuthInfo(\"digest\", \"username:password\".getBytes());\n        System.out.println(new String(zk2.getData(path, false, null)));\n    }\n\n}\n\n```\n\n没有权限的话，zkui也会报错\n\n<img src=\"/images/517519-20200802233851058-803782956.png\" width=\"800\" height=\"197\" loading=\"lazy\" />\n\n使用zookeeper-client访问\n\n```\nlintong@master:/opt/cloudera/parcels/CDH/bin$ ./zookeeper-client\nConnecting to localhost:2181\n\n```\n\n查看，仍然没有权限\n\n```\n[zk: localhost:2181(CONNECTED) 0] ls /\n[cluster, controller, brokers, zookeeper, admin, isr_change_notification, log_dir_event_notification, ngdata, controller_epoch, kafka-manager, solr, app6, consumers, hive_zookeeper_namespace_hive, latest_producer_id_block, app2, config, app1, hbase, app4, app3]\n[zk: localhost:2181(CONNECTED) 1] ls /app6\nAuthentication is not valid : /app6\n\n```\n\n设置密码并查看\n\n```\n[zk: localhost:2181(CONNECTED) 3] addauth digest username:password\n\n[zk: localhost:2181(CONNECTED) 7] get /app6\n123\ncZxid = 0x139e88\nctime = Sun Aug 02 23:38:30 CST 2020\nmZxid = 0x139e88\nmtime = Sun Aug 02 23:38:30 CST 2020\npZxid = 0x139e88\ncversion = 0\ndataVersion = 0\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 3\nnumChildren = 0\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["zookeeper"]},{"title":"guava学习笔记","url":"/guava学习笔记.html","content":"## 1.**CaseFormat**\n\n**CaseFormat**是guava中用于字符串格式转换的工具**，**有以下几种类型\n\nUPPER_CAMEL，比如 UpperCamel\n\nUPPER_UNDERSCORE，比如 UPPER_UNDERSCORE\n\nLOWER_CAMEL，比如 lowerCamel\n\nLOWER_HYPHEN，比如 lower-hyphen\n\nLOWER_UNDERSCORE，比如 lower_underscore\n\n<!--more-->\n&nbsp;\n\n可以使用to进行转换\n\n```\n// 从大写驼峰转小写驼峰\nSystem.out.println(CaseFormat.UPPER_CAMEL.to(CaseFormat.LOWER_CAMEL, \"UpperCamel\"));\n\n```\n\n也可以使用converter进行转换\n\n```\nConverter<String, String> converter = CaseFormat.UPPER_CAMEL.converterTo(CaseFormat.LOWER_CAMEL);\nSystem.out.println(converter.convert(\"UpperCamel\"));\n\n```\n\n## 2.**guava cache**\n\n**guava cache**是一种非常优秀本地缓存解决方案，提供了基于容量，时间和引用的缓存回收方式\n\nguava cache作为本地缓存，对于redis，redis将会受到网卡等原因，所以缓存的方案可以是DB+redis+local cache\n\n使用的场景：\n\n```\n1.愿意消耗一些内存空间来提升速度\n2.预料到某些键会被多次查询\n3.缓存中存放的数据总量不会超出内存容量\n\n```\n\n参考：[https://www.jianshu.com/p/38bd5f1cf2f2](https://www.jianshu.com/p/38bd5f1cf2f2)\n\n其他的本地缓存方案还有：caffeine\n\n## 3.**Preconditions**\n\n可以使用Preconditions来优雅的进行参数的校验，比如\n\n```\nPreconditions.checkNotNull(jedisPoolConfig, \"Redis pool config should not be Null\");\nPreconditions.checkArgument(minIdle >= 0, \"minIdle value can not be negative\");\n\n```\n\n## 4.**Hash**\n\n可以使用guava来对进行哈希\n\n下面的例子使用了md5，还有其他的哈希方法，比如murmur3（es使用的路由哈希算法），sha256，crc32等\n\n```\nHashCode hash = Hashing.md5().hashString(\"123456\"); // 等同于Hashing.md5().hashString(\"123456\", Charsets.UTF_16LE);\nSystem.out.println(hash.toString());\n\nHashCode hash2 = Hashing.md5().hashString(\"123456\", StandardCharsets.UTF_8); // 等同于Hashing.md5().hashBytes(\"123456\".getBytes()).toString()\nSystem.out.println(hash2.toString());\n\n```\n\n分别输出\n\n```\nce0bfd15059b68d67688884d7a3d3e8c\ne10adc3949ba59abbe56e057f20f883e\n\n```\n\n在Linux上使用md5sum，等同于使用&nbsp;Hashing.md5().hashString(\"123456\", StandardCharsets.UTF_8)\n\n```\necho -n '123456' | md5sum\ne10adc3949ba59abbe56e057f20f883e  -\n\n```\n\n## 5.EventBus\n\nEventBus是guava提供的一种消息发布-订阅类库\n\n参考：[Guava-EventBus使用详解](https://www.jianshu.com/p/93486a604c34)\n\n## 6.RateLimiter\n\n```\nRateLimiter limiter = RateLimiter.create(1.0); // 这里的1表示每秒允许处理的量为1个\nfor (int i = 1; i <= 10; i++) { \n    limiter.acquire();// 请求RateLimiter, 超过permits会被阻塞\n    System.out.println(\"call execute..\" + i);\n}\n\n```\n\n参考：[Guava RateLimiter实现接口API限流](https://blog.csdn.net/fanrenxiang/article/details/80949079)\n\n## 7.去掉字符串的空格和换行\n\n```\nCharMatcher.breakingWhitespace().removeFrom(str)\n\n```\n\n　　\n\n&nbsp;\n","tags":["google"]},{"title":"Elasticsearch学习笔记——别名","url":"/Elasticsearch学习笔记——别名.html","content":"**别名**是一个指针或者名称，可以对应一个或者多个具体的索引。\n\n**别名的创建**，这样就给一个名为es的索引添加了一个别名：alias_test\n\n```\nlintong@lintongdeMacBook-Pro ~ $ curl -XPUT 'http://master:9200/es/_alias/alias_test'\n{\"acknowledged\":true}%\n\n```\n\n添加了别名之后，在查询es索引的时候\n\n```\ncurl 'http://master:9200/alias_test/_search?q=_id:1'\n\n```\n\n就等同于\n\n```\ncurl 'http://master:9200/es/_search?q=_id:1'\n\n```\n\n**别名的删除**\n\n```\nlintong@lintongdeMacBook-Pro ~ $ curl -XDELETE 'http://master:9200/es/_alias/alias_test'\n{\"acknowledged\":true}%\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["ELK"]},{"title":"Kafka学习笔记——存储结构","url":"/Kafka学习笔记——存储结构.html","content":"1，由cdh安装的kafka的默认存储路径如图所示在/var/local/kafka/data，一般会进行修改\n\n<img src=\"/images/517519-20200719165104346-185256693.png\" width=\"500\" height=\"116\" loading=\"lazy\" />\n\nkafka配置参考：[apache kafka系列之server.properties配置文件参数说明](https://blog.csdn.net/lizhitao/article/details/25667831)\n\n路径下文件如下\n\n<img src=\"/images/517519-20200719165325582-1321225246.png\" width=\"700\" height=\"50\" loading=\"lazy\" />\n\n如果是多个路径的话，使用,进行分隔，比如/data01/kafka/data,/data02/kafka/data，注意data的权限需要是kafka用户和kafka组\n\n对应kafka manager上的topic\n\n<img src=\"/images/517519-20200719165438343-1809914824.png\" width=\"400\" height=\"199\" loading=\"lazy\" />\n\n具体的topic的目录下的文件\n\n```\nlintong@master:/var/local/kafka/data/test_topic-0$ ls\n00000000000000000187.index  00000000000000000187.snapshot   leader-epoch-checkpoint\n00000000000000000187.log    00000000000000000187.timeindex\n\n```\n\n其中\n\n187就是这个日志数据文件开始的offset\n\n00000000000000000187.log是**日志数据文件**\n\n可以使用解码命令查看日志片段中的内容\n\n```\n/opt/cloudera/parcels/KAFKA/bin/kafka-run-class kafka.tools.DumpLogSegments --files ./00000000000000000187.log\n20/07/19 17:28:55 INFO utils.Log4jControllerRegistration$: Registered kafka:type=kafka.Log4jController MBean\nDumping ./00000000000000000187.log\nStarting offset: 187\nbaseOffset: 187 lastOffset: 187 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 0 CreateTime: 1595149737820 size: 71 magic: 2 compresscodec: NONE crc: 1738564593 isvalid: true\nbaseOffset: 188 lastOffset: 188 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 71 CreateTime: 1595149744449 size: 74 magic: 2 compresscodec: NONE crc: 3794338178 isvalid: true\nbaseOffset: 189 lastOffset: 189 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 145 CreateTime: 1595149757514 size: 75 magic: 2 compresscodec: NONE crc: 3084259622 isvalid: true\n\n```\n\nlog.segment.bytes设置是1G，如果log文件的大小达到1G之后会生成另外一个log文件\n\n<img src=\"/images/517519-20200719224704351-504957439.png\" width=\"400\" height=\"90\" />\n\n**该参数在1.0.1及以下的kafka有bug**，可能会影响消费者消费topic的数据，但是不影响生产者，参考：[https://issues.apache.org/jira/browse/KAFKA-6292](https://issues.apache.org/jira/browse/KAFKA-6292)\n\n当kafka的broker读取segment文件的时候，会判断当前当前读取的segment的偏移量position在继续读取一段`HEADER_SIZE_UP_TO_MAGIC`之后和该segment文件最大可读取的偏移量end之间的大小\n\n当调高了log.segment.bytes=2G，注意此处2G=有符号INT的最大值=2147483647，有可能导致position+`HEADER_SIZE_UP_TO_MAGIC`的大小超过int最大值，从而成为负数，小于end，返回null，并导致从log文件的末尾开始读取数据\n\n参考kafka 1.0.1版本源码：[https://github.com/apache/kafka/blob/1.0.1/clients/src/main/java/org/apache/kafka/common/record/FileLogInputStream.java](https://github.com/apache/kafka/blob/1.0.1/clients/src/main/java/org/apache/kafka/common/record/FileLogInputStream.java)\n\n<img src=\"/images/517519-20200720144452996-1799650750.png\" width=\"800\" height=\"431\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\nflume消费者报错：\n\n```\n24 Mar 2020 01:55:05,055 WARN  [PollableSourceRunner-KafkaSource-source1] (org.apache.kafka.clients.consumer.internals.Fetcher.handleFetchResponse:600)  - Unknown error fetching data for topic-partition xxxx-17\n\n```\n\nkafka端报错：\n\n```\n2020-03-24 15:57:60,265 ERROR kafka.server.ReplicaManager: [ReplicaManager broker=xxx] Error processing fetch operatio\nn on partition xxx-17, offset 4095495430\norg.apache.kafka.common.KafkaException: java.io.EOFException: Failed to read `log header` from file channel `sun.nio.ch\n.FileChannelImpl@73e125b5`. Expected to read 18 bytes, but reached end of file after reading 0 bytes. Started read from\n position 2147454346.\n    at org.apache.kafka.common.record.RecordBatchIterator.makeNext(RecordBatchIterator.java:40)\n    at org.apache.kafka.common.record.RecordBatchIterator.makeNext(RecordBatchIterator.java:24)\n    at org.apache.kafka.common.utils.AbstractIterator.maybeComputeNext(AbstractIterator.java:79)\n    at org.apache.kafka.common.utils.AbstractIterator.hasNext(AbstractIterator.java:45)\n    at org.apache.kafka.common.record.FileRecords.searchForOffsetWithSize(FileRecords.java:287)\n    at kafka.log.LogSegment.translateOffset(LogSegment.scala:174)\n    at kafka.log.LogSegment.read(LogSegment.scala:226)\n    at kafka.log.Log$$anonfun$read$2.apply(Log.scala:1002)\n    at kafka.log.Log$$anonfun$read$2.apply(Log.scala:958)\n    at kafka.log.Log.maybeHandleIOException(Log.scala:1669)\n    at kafka.log.Log.read(Log.scala:958)\n    at kafka.server.ReplicaManager.kafka$server$ReplicaManager$$read$1(ReplicaManager.scala:900)\n    at kafka.server.ReplicaManager$$anonfun$readFromLocalLog$1.apply(ReplicaManager.scala:962)\n    at kafka.server.ReplicaManager$$anonfun$readFromLocalLog$1.apply(ReplicaManager.scala:961)\n    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n    at kafka.server.ReplicaManager.readFromLocalLog(ReplicaManager.scala:961)\n    at kafka.server.ReplicaManager.readFromLog$1(ReplicaManager.scala:790)\n    at kafka.server.ReplicaManager.fetchMessages(ReplicaManager.scala:803)\n    at kafka.server.KafkaApis.handleFetchRequest(KafkaApis.scala:601)\n    at kafka.server.KafkaApis.handle(KafkaApis.scala:99)\n    at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:65)\n    at java.lang.Thread.run(Thread.java:748)\n\n```\n\n&nbsp;\n\n00000000000000000187.index是**偏移量索引文件**\n\n因为**index文件**中key存储是的offset，而value存储的是对应log文件中的偏移量，所以**index文件**的**作用**是通过二分查找帮助client更快地找到所需offset的message在log文件中的位置\n\n参考：https://mrbird.cc/Kafka%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6.html\n\n```\n/opt/cloudera/parcels/KAFKA/bin/kafka-run-class kafka.tools.DumpLogSegments --files ./00000000000000000187.index\n20/07/19 19:05:08 INFO utils.Log4jControllerRegistration$: Registered kafka:type=kafka.Log4jController MBean\nDumping ./00000000000000000187.index\noffset: 187 position: 0\n\n```\n\n00000000000000000187.timeindex是**时间戳索引文件**\n\n```\n/opt/cloudera/parcels/KAFKA/bin/kafka-run-class kafka.tools.DumpLogSegments --files ./00000000000000000187.timeindex\n20/07/19 18:47:31 INFO utils.Log4jControllerRegistration$: Registered kafka:type=kafka.Log4jController MBean\nDumping ./00000000000000000187.timeindex\ntimestamp: 0 offset: 187\nFound timestamp mismatch in :/var/local/kafka/data/test_topic-0/./00000000000000000187.timeindex\n  Index timestamp: 0, log timestamp: 1595149737820\n  Index timestamp: 0, log timestamp: 1595149737820\nFound out of order timestamp in :/var/local/kafka/data/test_topic-0/./00000000000000000187.timeindex\n  Index timestamp: 0, Previously indexed timestamp: 0\n\n```\n\n1595149737820毫秒转换成unix时间是2020/7/19 17:8:57，即这个日志文件开始的时间戳\n\nindex文件和log文件的大小还可以在创建topic的时候设置，如下\n\n**segment.index.bytes**参数决定了index文件大小达到多大之后进行切分，默认大小是10M，如下\n\n<img src=\"/images/517519-20200720135812263-53834084.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n**segment.bytes**参数决定了log文件大小达到多大之后进行切分\n\n<img src=\"/images/517519-20200719234856123-1415520140.png\" width=\"700\" height=\"328\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;\n\n可能还会有其他文件，比如`：`\n\n`.snapshot(快照文件)、.deleted()、.cleaned(日志清理临时文件)<code>、.swap(Log Compaction之后的临时文件)、leader-epoch-checkpoint（保存了每一任leader开始写入消息时的offset，会定时更新。 follower被选为leader时会根据这个确定哪些消息可用）```</code>``\n\n&nbsp;\n\n**kafka在写入和读取上都做了优化**\n\n参考：[https://juejin.im/post/5cd2db8951882530b11ee976](https://juejin.im/post/5cd2db8951882530b11ee976)\n\n写入：**顺序写入、MMFile（Memory Mapped Files，内存映射文件）**\n\n读取：**基于sendfile实现Zero Copy、批量压缩**\n\n&nbsp;\n\n[Kafka中的索引机制](https://www.jianshu.com/p/abe1af87a889)\n\n其中介绍了跳表\n","tags":["kafka"]},{"title":"分布式协议——Paxos、Raft和ZAB","url":"/分布式协议——Paxos、Raft和ZAB.html","content":"参考：[分布式系统协议Paxos、Raft和ZAB](https://zhuanlan.zhihu.com/p/147691282)\n\n**Paxos算法**是一种提高分布式系统容错率的一致性算法\n\nPaxos 算法的步骤是这样：\n\n1.首先有两种角色，一个是&ldquo;提议者&rdquo;，一个是&ldquo;接受者&rdquo;。提议者可以向接受者提出提议，然后接受者表达意见。\n\n2.因为存在多个提议者，如果同时表达意见会出现意见不一致的情况，所以首先需要尽快选出一个领导者，让意见统一。\n\n3.然后领导者会给接受者发出提议，如果一个提议被大多数接受者接纳，这个提议就通过了。\n\n<!--more-->\n&nbsp;\n\n**Raft协议**\n\nRaft 协议的每个副本都会处于三种状态之一：Leader、Follower、Candidate。\n\n**Leader**：所有请求的处理者，Leader 副本接受 client 的更新请求，本地处理后再同步至多个其他副本。\n\n**Follower**：请求的被动更新者，从 Leader 接受更新请求，然后写入本地日志文件。\n\n**Candidate**：如果 Follower 副本在一段时间内没有收到 Leader 副本的心跳，则判断 Leader 可能已经故障，此时启动选主过程，此时副本会变成 Candidate 状态，直到选主结束。\n\n&nbsp;\n\n每一个副本都会维护一个 term，类似于一个逻辑时钟，每发生一个动作就会递增，通过比较每个提议的 term，副本会默认使用最新的 term，防止发生冲突。如果一个 Leader 或者 Candidate 发现自己的 term 不是最新的了，就会自动降级到 Follower，而如果一个 Follower 接收到低于自己当前 term 的提议，就会直接抛弃。\n\n&nbsp;\n\n参考：从Paxos到Zookeeper分布式一致性原理与实践\n\n**ZAB（Zookeeper Atomic Broadcast）协议**是为分布式协调服务Zookeeper专门设计的一种支持奔溃恢复的原子广播协议。\n\nZookeeper使用ZAB协议来保持集群中各副本之间数据的一致性。\n\nZookeeper使用一个单一的主进程来接收并处理客户端的所有事务请求，并采用ZAB的原子广播协议，将服务器数据的状态变更以事务Proposal（事务提议）的形式广播到所有的副本进程上去。\n\n&nbsp;\n\nZAB协议保证了：\n\n1.同一时刻集群中只有一个主进程来广播服务的状态变更，这样能够很好地处理客户端大量的并发请求\n\n2.在分布式环境中，顺序执行的状态变更前后会存在一定的依赖关系。因此，ZAB协议保证了一个全局的变更序列被顺序执行。\n\n&nbsp;\n\n**ZAB协议**包括了两种基本的模式：**崩溃恢复**和**消息广播**\n\n**崩溃恢复**：1.如果leader服务器出现奔溃退出或机器重启，或者是集群中已经不存在过半的服务器与该leader服务器保持正常通信，那么在重新开始新一轮的原子广播操作之前，所有进程首先会使用奔溃恢复协议来使彼此达到一个一致的状态，此时整个ZAB流程就会从消息广播模式进入奔溃恢复模式。\n\n**消息广播**：2.ZAB协议中的消息广播使用的是一个原子广播协议，类似于一个二阶段提交的过程。在整个消息广播过程中，leader服务器会为每个事务请求生成对应的proposal（提案）来进行广播，每个follower服务器在接收到这个事务proposal之后，都会首先讲其以事务日志的形式写到磁盘中，并且在成功写入后反馈给leader服务器一个Ack响应。当leader服务器接收到超过半数follower的Ack响应后，就会广播一个Commit消息给所有的follower服务器以通知其进行事务提交，同时leader自身也会完成对事务的提交。\n\n在ZAB协议的**事务编号ZXID**设计中，ZXID是一个64位的数字。\n\n其中**低32位**是一个递增的计数器，对客户端的每个事务请求，leader服务器在产生一个新的事务proposal的时候，都会对该计数器进行+1\n\n其中**高32位**表示了leader周期的epoch编号，每选举一个新leader，就会从这个新的leader服务器中取得最大事务proposal的ZXID，从中解析出对应的epoch值，对其进行+1\n\n&nbsp;\n","tags":["zookeeper"]},{"title":"Zk学习笔记——应用场景","url":"/Zk学习笔记——应用场景.html","content":"**1.Master选举**\n\n在分布式系统中，需要选举一台机器作为master或者leader。\n\n这时候，可以选择一个跟节点，比如/master，然后多台机器同时像这个节点创建一个子节点/master/lock，利用zookeeper的特性，最终只有一台机器能否创建成功，成功的那台机器就是Master；\n\n其他机器注册watch到这个子节点，然后当master宕机的时候，其他机器就会重新开始选举。\n\n<!--more-->\n&nbsp;\n\n**2.分布式锁**\n\n分布式锁的场景通常是多个进程需要在某个节点保证同步，比如保证只有一个进程进到某个函数里面\n\n下面例子中的**查询余额**和**更新余额**操作就需要用锁锁住，保证一个线程做完了查询和更新操作之后，才能有另外一个线程来做查询和更新操作\n\n这个使用的锁称为**排他锁**（Exclusive Locks），又称为**写锁**或者独占锁。当一个事务1对数据对象1加了排他锁，那么在整个加锁期间，只允许事务1对对象1进行读取或者更新\n\n```\n一个账户有100元\n进程A，查询账户余额，等于100元\n进程B，查询账户余额，等于100元\n进程B，取出100元，并更新，100-100=0\n进程A，存入100元，并更新，100+100=200（此时发生错误）\n\n```\n\n其他的锁还有**共享锁**（Shared Locks），又称之为**读锁**。 如果事务1对数据对象1加了共享锁，那个当前事务只能对数据对象1进行读操作，其他事务也只能对这个对象加共享锁，直到所有共享锁都被释放。\n\n&nbsp;\n\n**3.分布式计数器**\n\n比如统计系统的在线人数\n\n&nbsp;\n\n**4.分布式Barrier**\n\nbarrier是一种控制多线程之间同步的经典方法\n\n在一个JVM里面，可以使用java的CyclicBarrier，会等待所有线程同步后，再执行其他业务逻辑\n\n参考：[Java多线程&mdash;&mdash;其他工具类CyclicBarrier、CountDownLatch和Exchange](https://www.cnblogs.com/tonglin0325/p/6265379.html)\n\n在分布式的场景下，可以使用zk来实现\n\n&nbsp;\n\n5.hadoop\n\n在Hadoop中，Zookeeper主要用于实现HA，其中HDFS的NameNode和ResourceManager都是基于Zookeeper实现的HA。\n\n&nbsp;\n\n6.Hbase\n\n在Hbase中，Zookeeper主要用于分布式状态协调，即regionserver挂掉时，通知到整个hbase系统，以及客户端。\n\n&nbsp;\n\n7.Kafka\n","tags":["zookeeper"]},{"title":"Zk学习笔记——ZkClient","url":"/Zk学习笔记——ZkClient.html","content":"ZkClient是开源的zk客户端，对Zookeeper原生的java api进行了封装，实现了诸如session超时重连，watcher反复注册等功能。\n\n依赖的话有\n\n```\n<dependency>\n    <groupId>com.101tec</groupId>\n    <artifactId>zkclient</artifactId>\n    <version>0.10</version>\n</dependency>\n\n```\n\n和\n\n```\n<dependency>\n    <groupId>com.github.sgroschupf</groupId>\n    <artifactId>zkclient</artifactId>\n    <version>0.1</version>\n</dependency>\n\n```\n\n前者使用较为广泛，使用其的项目包括kafka，dubbo等\n\n<img src=\"/images/517519-20200712000528264-560947125.png\" width=\"800\" height=\"326\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n&nbsp;\n\n1.创建，获取和删除节点\n\n依赖\n\n```\n        <!-- zookeeper -->\n        <dependency>\n            <groupId>org.apache.zookeeper</groupId>\n            <artifactId>zookeeper</artifactId>\n            <version>3.4.5-cdh5.16.2</version>\n        </dependency>\n        <dependency>\n            <groupId>com.101tec</groupId>\n            <artifactId>zkclient</artifactId>\n            <version>0.10</version>\n        </dependency>\n\n```\n\n&nbsp;代码\n\n```\npackage com.bigdata.zookeeper;\n\nimport org.I0Itec.zkclient.ZkClient;\n\npublic class ZkClientExample {\n\n    public static void main(String[] args) throws Exception {\n\n        ZkClient zkClient = new ZkClient(\"master:2181\", 5000);\n        System.out.println(\"Zookeeper session established\");\n        String path = \"/app4/app4-1\";\n        // true表示可以递归创建节点，重复节点不会报错\n        zkClient.createPersistent(path, true);\n        // 获取\n        System.out.println(zkClient.getChildren(\"/app4\"));\n\n        // 删除节点\n//        zkClient.delete(path);\n        // 递归删除\n        zkClient.deleteRecursive(\"/app4\");\n    }\n\n}\n\n```\n\n2.zkClient引入了listener，客户端可以通过注册相关的事件监听来实现对ZooKeeper服务端事件的订阅\n\nlistener是异步的，可能触发会有延迟\n\n代码，chiild listener，监听子节点是否发生变化\n\n```\npackage com.bigdata.zookeeper;\n\nimport org.I0Itec.zkclient.IZkChildListener;\nimport org.I0Itec.zkclient.ZkClient;\n\nimport java.util.List;\n\npublic class ZkClientExample {\n\n    public static void main(String[] args) throws Exception {\n\n        ZkClient zkClient = new ZkClient(\"master:2181\", 5000);\n        System.out.println(\"Zookeeper session established\");\n        String path = \"/app4/app4-1\";\n\n        // listener\n        zkClient.subscribeChildChanges(\"/app4\", new Listerner());\n\n        // true表示可以递归创建节点，重复节点不会报错\n        zkClient.createPersistent(path, true);\n        // 如果注释的话，listener的执行可能会在删除节点执行之后，所以输出可能是空的子节点\n        Thread.sleep(10000);\n        // 获取\n        System.out.println(zkClient.getChildren(\"/app4\"));\n\n\n        // 删除节点\n        zkClient.delete(path);\n\n        Thread.sleep(10000);\n        // 递归删除\n//        zkClient.deleteRecursive(\"/app4\");\n\n\n    }\n\n}\n\nclass Listerner implements IZkChildListener {\n\n    @Override\n    public void handleChildChange(String s, List<String> list) throws Exception {\n        System.out.println(\"parent path: \" + s + \" changed, current children: \" + list);\n    }\n}\n\n```\n\n输出\n\n<img src=\"/images/517519-20200712125426122-267100941.png\" width=\"800\" height=\"144\" loading=\"lazy\" />\n\n&nbsp;\n\n3.代码，data listener，监听节点的data是否发生变化\n\nZkClient的默认序列化类是org.I0Itec.zkclient.serialize.SerializableSerializer\n\n在zkui上查看会有乱码，比如\n\n<img src=\"/images/517519-20200712140219071-319348547.png\" width=\"800\" height=\"162\" loading=\"lazy\" />\n\n所以重写了序列化类，并且使用writeData和readData修改和读取节点数据\n\n```\npackage com.bigdata.zookeeper;\n\nimport org.I0Itec.zkclient.IZkChildListener;\nimport org.I0Itec.zkclient.IZkDataListener;\nimport org.I0Itec.zkclient.ZkClient;\nimport org.I0Itec.zkclient.exception.ZkMarshallingError;\nimport org.I0Itec.zkclient.serialize.ZkSerializer;\nimport org.apache.commons.io.Charsets;\n\nimport java.util.List;\n\npublic class ZkClientExample {\n\n    public static void main(String[] args) throws Exception {\n\n        ZkClient zkClient = new ZkClient(\"master:2181\", 5000);\n        zkClient.setZkSerializer(new MyZkSerializer());\n        System.out.println(\"Zookeeper session established\");\n        String path = \"/app4/app4-1\";\n\n        // child listener\n        zkClient.subscribeChildChanges(\"/app4\", new ChildListerner());\n        // data listener\n        zkClient.subscribeDataChanges(path, new DataListerner());\n\n        // true表示可以递归创建节点，重复节点不会报错\n        zkClient.createPersistent(path, true);\n        // 设置data\n        zkClient.writeData(path, \"12345\");\n        System.out.println(zkClient.readData(path).toString());\n        // 如果注释的话，listener的执行可能会在删除节点执行之后，所以输出可能是空的子节点\n        Thread.sleep(10000);\n        // 获取\n        System.out.println(zkClient.getChildren(\"/app4\"));\n\n\n        // 删除节点\n        zkClient.delete(path);\n\n        Thread.sleep(10000);\n        // 递归删除\n//        zkClient.deleteRecursive(\"/app4\");\n\n\n    }\n\n}\n\nclass MyZkSerializer implements ZkSerializer {\n    public Object deserialize(byte[] bytes) throws ZkMarshallingError {\n        return new String(bytes, Charsets.UTF_8);\n    }\n\n    public byte[] serialize(Object obj) throws ZkMarshallingError {\n        return String.valueOf(obj).getBytes(Charsets.UTF_8);\n    }\n}\n\n\nclass ChildListerner implements IZkChildListener {\n\n    @Override\n    public void handleChildChange(String s, List<String> list) throws Exception {\n        System.out.println(\"parent path: \" + s + \" changed, current children: \" + list);\n    }\n}\n\nclass DataListerner implements IZkDataListener {\n\n\n    @Override\n    public void handleDataChange(String s, Object o) throws Exception {\n        System.out.println(\"data path: \" + s + \" changed, current data: \" + o);\n    }\n\n    @Override\n    public void handleDataDeleted(String s) throws Exception {\n        System.out.println(\"data path: \" + s + \" deleted\");\n    }\n}\n\n```\n\n输出\n\n<img src=\"/images/517519-20200712140334039-111953819.png\" width=\"800\" height=\"219\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["zookeeper"]},{"title":"Zk学习笔记——修改节点","url":"/Zk学习笔记——修改节点.html","content":"参考：从Paxos到Zookeeper分布式一致性原理和实践\n\n使用的zk依赖是cdh5.16.2的3.4.5\n\n```\n<!-- zookeeper -->\n<dependency>\n    <groupId>org.apache.zookeeper</groupId>\n    <artifactId>zookeeper</artifactId>\n    <version>3.4.5-cdh5.16.2</version>\n</dependency>\n\n```\n\n代码，创建一个临时节点，并赋值，此时初始的节点version=0\n\n使用version=-1对这个节点进行修改，然后查看这个stat，version=1，\n\n使用version=1对这个节点进行修改，成功，然后查看这个stat，version=2，\n\n再次使用version=1对这个节点进行修改，失败，抛出BadVersionException\n\n```\npackage com.bigdata.zookeeper;\n\n\nimport org.apache.zookeeper.*;\nimport org.apache.zookeeper.data.Stat;\n\nimport java.util.List;\nimport java.util.concurrent.CountDownLatch;\n\npublic class ZkExample implements Watcher {\n\n    public static CountDownLatch connectedSemaphore = new CountDownLatch(1);\n    private static ZooKeeper zk;\n\n    public static void main(String[] args) throws Exception {\n        zk = new ZooKeeper(\"master:2181\", 5000, new ZkExample());\n        System.out.println(zk.getState());\n        try {\n            connectedSemaphore.await();\n            // 创建一个节点\n            String path = \"/app6\";\n            zk.create(path, \"123\".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL);\n            zk.getData(path, true, null);\n\n            // version=-1，修改节点\n            Stat stat = zk.setData(path, \"555\".getBytes(), -1);\n            System.out.println(stat.getVersion());\n\n            // version=1，修改节点\n            Stat stat2 = zk.setData(path, \"55555\".getBytes(), stat.getVersion());\n            System.out.println(stat2.getVersion());\n\n            // version=1，再次修改节点\n            Stat stat3 = zk.setData(path, \"555555555\".getBytes(), stat.getVersion());\n\n            Thread.sleep(10000); // 10秒延时\n\n        } catch (InterruptedException e) {\n            System.out.println(\"Zk session established\" + e);\n        }\n    }\n\n    @Override\n    public void process(WatchedEvent watchedEvent) {\n        System.out.println(watchedEvent);\n        if (Event.KeeperState.SyncConnected == watchedEvent.getState()) {\n            if (Event.EventType.None == watchedEvent.getType() &amp;&amp; null == watchedEvent.getPath()) {\n                connectedSemaphore.countDown();\n            } else if (watchedEvent.getType() == Event.EventType.NodeChildrenChanged) {\n                try {\n                    System.out.println(zk.getChildren(watchedEvent.getPath(), true));\n                } catch (Exception e) {\n\n                }\n            }\n\n        }\n    }\n}\n \n```\n\n输出\n\n<img src=\"/images/517519-20200711141244182-1393013749.png\" width=\"800\" height=\"175\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n删除节点\n\n和修改节点一样，version=-1的时候可以正常删除节点，不等于-1的时候需要判断version是不是相等还能正常删除\n\n```\npackage com.bigdata.zookeeper;\n\n\nimport org.apache.zookeeper.*;\nimport org.apache.zookeeper.data.Stat;\n\nimport java.util.List;\nimport java.util.concurrent.CountDownLatch;\n\npublic class ZkExample implements Watcher {\n\n    public static CountDownLatch connectedSemaphore = new CountDownLatch(1);\n    //    private static Stat stat = new Stat();\n    private static ZooKeeper zk;\n\n    public static void main(String[] args) throws Exception {\n        zk = new ZooKeeper(\"master:2181\", 5000, new ZkExample());\n        System.out.println(zk.getState());\n        try {\n            connectedSemaphore.await();\n            // 创建一个节点\n            String path = \"/app6\";\n            zk.create(path, \"123\".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL);\n            zk.getData(path, true, null);\n\n            // version=-1，修改节点\n            Stat stat = zk.setData(path, \"555\".getBytes(), -1);\n            System.out.println(stat.getVersion());\n\n            // version=1，修改节点\n            Stat stat2 = zk.setData(path, \"55555\".getBytes(), stat.getVersion());\n            System.out.println(stat2.getVersion());\n\n            // version=-1，删除节点，抛异常\n//            zk.delete(path, stat.getVersion());\n            // version=-1，删除节点，正常\n            zk.delete(path, stat2.getVersion());\n\n\n            Thread.sleep(10000); // 10秒延时\n\n        } catch (InterruptedException e) {\n            System.out.println(\"Zk session established\" + e);\n        }\n    }\n\n    @Override\n    public void process(WatchedEvent watchedEvent) {\n        System.out.println(watchedEvent);\n        if (Event.KeeperState.SyncConnected == watchedEvent.getState()) {\n            if (Event.EventType.None == watchedEvent.getType() &amp;&amp; null == watchedEvent.getPath()) {\n                connectedSemaphore.countDown();\n            } else if (watchedEvent.getType() == Event.EventType.NodeChildrenChanged) {\n                try {\n                    System.out.println(zk.getChildren(watchedEvent.getPath(), true));\n                } catch (Exception e) {\n\n                }\n            }\n\n        }\n    }\n}\n\n```\n\n&nbsp;\n","tags":["zookeeper"]},{"title":"Zk学习笔记——检测节点是否存在","url":"/Zk学习笔记——检测节点是否存在.html","content":"参考：从Paxos到Zookeeper分布式一致性原理和实践\n\n使用的zk依赖是cdh5.16.2的3.4.5\n\n```\n<!-- zookeeper -->\n<dependency>\n    <groupId>org.apache.zookeeper</groupId>\n    <artifactId>zookeeper</artifactId>\n    <version>3.4.5-cdh5.16.2</version>\n</dependency>\n\n```\n\n<!--more-->\n&nbsp;代码，exist函数来检查节点是否存在，同时会注册一个watch\n\n```\npackage com.bigdata.zookeeper;\n\n\nimport org.apache.zookeeper.*;\nimport org.apache.zookeeper.data.Stat;\n\nimport java.util.List;\nimport java.util.concurrent.CountDownLatch;\n\npublic class ZkExample implements Watcher {\n\n    public static CountDownLatch connectedSemaphore = new CountDownLatch(1);\n    //    private static Stat stat = new Stat();\n    private static ZooKeeper zk;\n\n    public static void main(String[] args) throws Exception {\n        zk = new ZooKeeper(\"master:2181\", 5000, new ZkExample());\n        System.out.println(zk.getState());\n        try {\n            connectedSemaphore.await();\n            // 创建一个节点\n            String path = \"/app6\";\n            // 注册watch\n            Stat stat = zk.exists(path, true);\n\n            zk.create(path, \"123\".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL);\n\n            // version=-1，修改节点\n            zk.setData(path, \"555\".getBytes(), -1);\n            // 注册watch\n            zk.exists(path, true);\n            System.out.println(stat.getVersion());\n\n            // version=1，修改节点\n            Stat stat2 = zk.setData(path, \"55555\".getBytes(), stat.getVersion());\n            // 注册watch\n            zk.exists(path, true);\n            System.out.println(stat2.getVersion());\n            \n            // version=-1，删除节点，正常\n            zk.delete(path, stat2.getVersion());\n\n\n            Thread.sleep(10000); // 10秒延时\n\n        } catch (InterruptedException e) {\n            System.out.println(\"Zk session established\" + e);\n        }\n    }\n\n    @Override\n    public void process(WatchedEvent watchedEvent) {\n        System.out.println(watchedEvent);\n        if (Event.KeeperState.SyncConnected == watchedEvent.getState()) {\n            if (Event.EventType.None == watchedEvent.getType() &amp;&amp; null == watchedEvent.getPath()) {\n                connectedSemaphore.countDown();\n            } else if (watchedEvent.getType() == Event.EventType.NodeChildrenChanged ||\n                    watchedEvent.getType() == Event.EventType.NodeDeleted || watchedEvent.getType() == Event.EventType.NodeDataChanged) {\n                try {\n                    System.out.println(zk.getChildren(watchedEvent.getPath(), true));\n//\n                } catch (Exception e) {\n                    System.out.println(e);\n                }\n            }\n\n        }\n    }\n}\n\n```\n\n&nbsp;输出\n\n<img src=\"/images/517519-20200711161047598-1740537149.png\" width=\"800\" height=\"230\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["zookeeper"]},{"title":"Pac代理文件","url":"/Pac代理文件.html","content":"用于将本地特定规则的请求转发到某个ip port代理\n\nmac系统需要编辑~/.$hadow$ocksX-NG/gfwli$t.js中的FindProxyForURL函数\n\n```\nvar proxy = \"PROXY 127.0.0.1:xxxx; DIRECT;\";\n\nvar rules = [\n  \"||xxx.xxx.com\",\n];\n\n/*\n* This file is part of Adblock Plus <http://adblockplus.org/>,\n* Copyright (C) 2006-2014 Eyeo GmbH\n*\n* Adblock Plus is free software: you can redistribute it and/or modify\n* it under the terms of the GNU General Public License version 3 as\n* published by the Free Software Foundation.\n*\n* Adblock Plus is distributed in the hope that it will be useful,\n* but WITHOUT ANY WARRANTY; without even the implied warranty of\n* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n* GNU General Public License for more details.\n*\n* You should have received a copy of the GNU General Public License\n* along with Adblock Plus.  If not, see <http://www.gnu.org/licenses/>.\n*/\n\nfunction createDict()\n{\n    var result = {};\n    result.__proto__ = null;\n    return result;\n}\n\nfunction getOwnPropertyDescriptor(obj, key)\n{\n    if (obj.hasOwnProperty(key))\n    {\n        return obj[key];\n    }\n    return null;\n}\n\nfunction extend(subclass, superclass, definition)\n{\n    if (Object.__proto__)\n    {\n        definition.__proto__ = superclass.prototype;\n        subclass.prototype = definition;\n    }\n    else\n    {\n        var tmpclass = function(){}, ret;\n        tmpclass.prototype = superclass.prototype;\n        subclass.prototype = new tmpclass();\n        subclass.prototype.constructor = superclass;\n        for (var i in definition)\n        {\n            if (definition.hasOwnProperty(i))\n            {\n                subclass.prototype[i] = definition[i];\n            }\n        }\n    }\n}\n\nfunction Filter(text)\n{\n    this.text = text;\n    this.subscriptions = [];\n}\nFilter.prototype = {\n    text: null,\n    subscriptions: null,\n    toString: function()\n    {\n        return this.text;\n    }\n};\nFilter.knownFilters = createDict();\nFilter.elemhideRegExp = /^([^\\/\\*\\|\\@\"!]*?)#(\\@)?(?:([\\w\\-]+|\\*)((?:\\([\\w\\-]+(?:[$^*]?=[^\\(\\)\"]*)?\\))*)|#([^{}]+))$/;\nFilter.regexpRegExp = /^(@@)?\\/.*\\/(?:\\$~?[\\w\\-]+(?:=[^,\\s]+)?(?:,~?[\\w\\-]+(?:=[^,\\s]+)?)*)?$/;\nFilter.optionsRegExp = /\\$(~?[\\w\\-]+(?:=[^,\\s]+)?(?:,~?[\\w\\-]+(?:=[^,\\s]+)?)*)$/;\nFilter.fromText = function(text)\n{\n    if (text in Filter.knownFilters)\n    {\n        return Filter.knownFilters[text];\n    }\n    var ret;\n    if (text[0] == \"!\")\n    {\n        ret = new CommentFilter(text);\n    }\n    else\n    {\n        ret = RegExpFilter.fromText(text);\n    }\n    Filter.knownFilters[ret.text] = ret;\n    return ret;\n};\n\nfunction InvalidFilter(text, reason)\n{\n    Filter.call(this, text);\n    this.reason = reason;\n}\nextend(InvalidFilter, Filter, {\n    reason: null\n});\n\nfunction CommentFilter(text)\n{\n    Filter.call(this, text);\n}\nextend(CommentFilter, Filter, {\n});\n\nfunction ActiveFilter(text, domains)\n{\n    Filter.call(this, text);\n    this.domainSource = domains;\n}\nextend(ActiveFilter, Filter, {\n    domainSource: null,\n    domainSeparator: null,\n    ignoreTrailingDot: true,\n    domainSourceIsUpperCase: false,\n    getDomains: function()\n    {\n        var prop = getOwnPropertyDescriptor(this, \"domains\");\n        if (prop)\n        {\n            return prop;\n        }\n        var domains = null;\n        if (this.domainSource)\n        {\n            var source = this.domainSource;\n            if (!this.domainSourceIsUpperCase)\n            {\n                source = source.toUpperCase();\n            }\n            var list = source.split(this.domainSeparator);\n            if (list.length == 1 &amp;&amp; list[0][0] != \"~\")\n            {\n                domains = createDict();\n                domains[\"\"] = false;\n                if (this.ignoreTrailingDot)\n                {\n                    list[0] = list[0].replace(/\\.+$/, \"\");\n                }\n                domains[list[0]] = true;\n            }\n            else\n            {\n                var hasIncludes = false;\n                for (var i = 0; i < list.length; i++)\n                {\n                    var domain = list[i];\n                    if (this.ignoreTrailingDot)\n                    {\n                        domain = domain.replace(/\\.+$/, \"\");\n                    }\n                    if (domain == \"\")\n                    {\n                        continue;\n                    }\n                    var include;\n                    if (domain[0] == \"~\")\n                    {\n                        include = false;\n                        domain = domain.substr(1);\n                    }\n                    else\n                    {\n                        include = true;\n                        hasIncludes = true;\n                    }\n                    if (!domains)\n                    {\n                        domains = createDict();\n                    }\n                    domains[domain] = include;\n                }\n                domains[\"\"] = !hasIncludes;\n            }\n            this.domainSource = null;\n        }\n        return this.domains;\n    },\n    sitekeys: null,\n    isActiveOnDomain: function(docDomain, sitekey)\n    {\n        if (this.getSitekeys() &amp;&amp; (!sitekey || this.getSitekeys().indexOf(sitekey.toUpperCase()) < 0))\n        {\n            return false;\n        }\n        if (!this.getDomains())\n        {\n            return true;\n        }\n        if (!docDomain)\n        {\n            return this.getDomains()[\"\"];\n        }\n        if (this.ignoreTrailingDot)\n        {\n            docDomain = docDomain.replace(/\\.+$/, \"\");\n        }\n        docDomain = docDomain.toUpperCase();\n        while (true)\n        {\n            if (docDomain in this.getDomains())\n            {\n                return this.domains[docDomain];\n            }\n            var nextDot = docDomain.indexOf(\".\");\n            if (nextDot < 0)\n            {\n                break;\n            }\n            docDomain = docDomain.substr(nextDot + 1);\n        }\n        return this.domains[\"\"];\n    },\n    isActiveOnlyOnDomain: function(docDomain)\n    {\n        if (!docDomain || !this.getDomains() || this.getDomains()[\"\"])\n        {\n            return false;\n        }\n        if (this.ignoreTrailingDot)\n        {\n            docDomain = docDomain.replace(/\\.+$/, \"\");\n        }\n        docDomain = docDomain.toUpperCase();\n        for (var domain in this.getDomains())\n        {\n            if (this.domains[domain] &amp;&amp; domain != docDomain &amp;&amp; (domain.length <= docDomain.length || domain.indexOf(\".\" + docDomain) != domain.length - docDomain.length - 1))\n            {\n                return false;\n            }\n        }\n        return true;\n    }\n});\n\nfunction RegExpFilter(text, regexpSource, contentType, matchCase, domains, thirdParty, sitekeys)\n{\n    ActiveFilter.call(this, text, domains, sitekeys);\n    if (contentType != null)\n    {\n        this.contentType = contentType;\n    }\n    if (matchCase)\n    {\n        this.matchCase = matchCase;\n    }\n    if (thirdParty != null)\n    {\n        this.thirdParty = thirdParty;\n    }\n    if (sitekeys != null)\n    {\n        this.sitekeySource = sitekeys;\n    }\n    if (regexpSource.length >= 2 &amp;&amp; regexpSource[0] == \"/\" &amp;&amp; regexpSource[regexpSource.length - 1] == \"/\")\n    {\n        var regexp = new RegExp(regexpSource.substr(1, regexpSource.length - 2), this.matchCase ? \"\" : \"i\");\n        this.regexp = regexp;\n    }\n    else\n    {\n        this.regexpSource = regexpSource;\n    }\n}\nextend(RegExpFilter, ActiveFilter, {\n    domainSourceIsUpperCase: true,\n    length: 1,\n    domainSeparator: \"|\",\n    regexpSource: null,\n    getRegexp: function()\n    {\n        var prop = getOwnPropertyDescriptor(this, \"regexp\");\n        if (prop)\n        {\n            return prop;\n        }\n        var source = this.regexpSource.replace(/\\*+/g, \"*\").replace(/\\^\\|$/, \"^\").replace(/\\W/g, \"\\\\$&amp;\").replace(/\\\\\\*/g, \".*\").replace(/\\\\\\^/g, \"(?:[\\\\x00-\\\\x24\\\\x26-\\\\x2C\\\\x2F\\\\x3A-\\\\x40\\\\x5B-\\\\x5E\\\\x60\\\\x7B-\\\\x7F]|$)\").replace(/^\\\\\\|\\\\\\|/, \"^[\\\\w\\\\-]+:\\\\/+(?!\\\\/)(?:[^\\\\/]+\\\\.)?\").replace(/^\\\\\\|/, \"^\").replace(/\\\\\\|$/, \"$\").replace(/^(\\.\\*)/, \"\").replace(/(\\.\\*)$/, \"\");\n        var regexp = new RegExp(source, this.matchCase ? \"\" : \"i\");\n        this.regexp = regexp;\n        return regexp;\n    },\n    contentType: 2147483647,\n    matchCase: false,\n    thirdParty: null,\n    sitekeySource: null,\n    getSitekeys: function()\n    {\n        var prop = getOwnPropertyDescriptor(this, \"sitekeys\");\n        if (prop)\n        {\n            return prop;\n        }\n        var sitekeys = null;\n        if (this.sitekeySource)\n        {\n            sitekeys = this.sitekeySource.split(\"|\");\n            this.sitekeySource = null;\n        }\n        this.sitekeys = sitekeys;\n        return this.sitekeys;\n    },\n    matches: function(location, contentType, docDomain, thirdParty, sitekey)\n    {\n        if (this.getRegexp().test(location) &amp;&amp; this.isActiveOnDomain(docDomain, sitekey))\n        {\n            return true;\n        }\n        return false;\n    }\n});\nRegExpFilter.prototype[\"0\"] = \"#this\";\nRegExpFilter.fromText = function(text)\n{\n    var blocking = true;\n    var origText = text;\n    if (text.indexOf(\"@@\") == 0)\n    {\n        blocking = false;\n        text = text.substr(2);\n    }\n    var contentType = null;\n    var matchCase = null;\n    var domains = null;\n    var sitekeys = null;\n    var thirdParty = null;\n    var collapse = null;\n    var options;\n    var match = text.indexOf(\"$\") >= 0 ? Filter.optionsRegExp.exec(text) : null;\n    if (match)\n    {\n        options = match[1].toUpperCase().split(\",\");\n        text = match.input.substr(0, match.index);\n        for (var _loopIndex6 = 0; _loopIndex6 < options.length; ++_loopIndex6)\n        {\n            var option = options[_loopIndex6];\n            var value = null;\n            var separatorIndex = option.indexOf(\"=\");\n            if (separatorIndex >= 0)\n            {\n                value = option.substr(separatorIndex + 1);\n                option = option.substr(0, separatorIndex);\n            }\n            option = option.replace(/-/, \"_\");\n            if (option in RegExpFilter.typeMap)\n            {\n                if (contentType == null)\n                {\n                    contentType = 0;\n                }\n                contentType |= RegExpFilter.typeMap[option];\n            }\n            else if (option[0] == \"~\" &amp;&amp; option.substr(1) in RegExpFilter.typeMap)\n            {\n                if (contentType == null)\n                {\n                    contentType = RegExpFilter.prototype.contentType;\n                }\n                contentType &amp;= ~RegExpFilter.typeMap[option.substr(1)];\n            }\n            else if (option == \"MATCH_CASE\")\n            {\n                matchCase = true;\n            }\n            else if (option == \"~MATCH_CASE\")\n            {\n                matchCase = false;\n            }\n            else if (option == \"DOMAIN\" &amp;&amp; typeof value != \"undefined\")\n            {\n                domains = value;\n            }\n            else if (option == \"THIRD_PARTY\")\n            {\n                thirdParty = true;\n            }\n            else if (option == \"~THIRD_PARTY\")\n            {\n                thirdParty = false;\n            }\n            else if (option == \"COLLAPSE\")\n            {\n                collapse = true;\n            }\n            else if (option == \"~COLLAPSE\")\n            {\n                collapse = false;\n            }\n            else if (option == \"SITEKEY\" &amp;&amp; typeof value != \"undefined\")\n            {\n                sitekeys = value;\n            }\n            else\n            {\n                return new InvalidFilter(origText, \"Unknown option \" + option.toLowerCase());\n            }\n        }\n    }\n    if (!blocking &amp;&amp; (contentType == null || contentType &amp; RegExpFilter.typeMap.DOCUMENT) &amp;&amp; (!options || options.indexOf(\"DOCUMENT\") < 0) &amp;&amp; !/^\\|?[\\w\\-]+:/.test(text))\n    {\n        if (contentType == null)\n        {\n            contentType = RegExpFilter.prototype.contentType;\n        }\n        contentType &amp;= ~RegExpFilter.typeMap.DOCUMENT;\n    }\n    try\n    {\n        if (blocking)\n        {\n            return new BlockingFilter(origText, text, contentType, matchCase, domains, thirdParty, sitekeys, collapse);\n        }\n        else\n        {\n            return new WhitelistFilter(origText, text, contentType, matchCase, domains, thirdParty, sitekeys);\n        }\n    }\n    catch (e)\n    {\n        return new InvalidFilter(origText, e);\n    }\n};\nRegExpFilter.typeMap = {\n    OTHER: 1,\n    SCRIPT: 2,\n    IMAGE: 4,\n    STYLESHEET: 8,\n    OBJECT: 16,\n    SUBDOCUMENT: 32,\n    DOCUMENT: 64,\n    XBL: 1,\n    PING: 1,\n    XMLHTTPREQUEST: 2048,\n    OBJECT_SUBREQUEST: 4096,\n    DTD: 1,\n    MEDIA: 16384,\n    FONT: 32768,\n    BACKGROUND: 4,\n    POPUP: 268435456,\n    ELEMHIDE: 1073741824\n};\nRegExpFilter.prototype.contentType &amp;= ~ (RegExpFilter.typeMap.ELEMHIDE | RegExpFilter.typeMap.POPUP);\n\nfunction BlockingFilter(text, regexpSource, contentType, matchCase, domains, thirdParty, sitekeys, collapse)\n{\n    RegExpFilter.call(this, text, regexpSource, contentType, matchCase, domains, thirdParty, sitekeys);\n    this.collapse = collapse;\n}\nextend(BlockingFilter, RegExpFilter, {\n    collapse: null\n});\n\nfunction WhitelistFilter(text, regexpSource, contentType, matchCase, domains, thirdParty, sitekeys)\n{\n    RegExpFilter.call(this, text, regexpSource, contentType, matchCase, domains, thirdParty, sitekeys);\n}\nextend(WhitelistFilter, RegExpFilter, {\n});\n\nfunction Matcher()\n{\n    this.clear();\n}\nMatcher.prototype = {\n    filterByKeyword: null,\n    keywordByFilter: null,\n    clear: function()\n    {\n        this.filterByKeyword = createDict();\n        this.keywordByFilter = createDict();\n    },\n    add: function(filter)\n    {\n        if (filter.text in this.keywordByFilter)\n        {\n            return;\n        }\n        var keyword = this.findKeyword(filter);\n        var oldEntry = this.filterByKeyword[keyword];\n        if (typeof oldEntry == \"undefined\")\n        {\n            this.filterByKeyword[keyword] = filter;\n        }\n        else if (oldEntry.length == 1)\n        {\n            this.filterByKeyword[keyword] = [oldEntry, filter];\n        }\n        else\n        {\n            oldEntry.push(filter);\n        }\n        this.keywordByFilter[filter.text] = keyword;\n    },\n    remove: function(filter)\n    {\n        if (!(filter.text in this.keywordByFilter))\n        {\n            return;\n        }\n        var keyword = this.keywordByFilter[filter.text];\n        var list = this.filterByKeyword[keyword];\n        if (list.length <= 1)\n        {\n            delete this.filterByKeyword[keyword];\n        }\n        else\n        {\n            var index = list.indexOf(filter);\n            if (index >= 0)\n            {\n                list.splice(index, 1);\n                if (list.length == 1)\n                {\n                    this.filterByKeyword[keyword] = list[0];\n                }\n            }\n        }\n        delete this.keywordByFilter[filter.text];\n    },\n    findKeyword: function(filter)\n    {\n        var result = \"\";\n        var text = filter.text;\n        if (Filter.regexpRegExp.test(text))\n        {\n            return result;\n        }\n        var match = Filter.optionsRegExp.exec(text);\n        if (match)\n        {\n            text = match.input.substr(0, match.index);\n        }\n        if (text.substr(0, 2) == \"@@\")\n        {\n            text = text.substr(2);\n        }\n        var candidates = text.toLowerCase().match(/[^a-z0-9%*][a-z0-9%]{3,}(?=[^a-z0-9%*])/g);\n        if (!candidates)\n        {\n            return result;\n        }\n        var hash = this.filterByKeyword;\n        var resultCount = 16777215;\n        var resultLength = 0;\n        for (var i = 0, l = candidates.length; i < l; i++)\n        {\n            var candidate = candidates[i].substr(1);\n            var count = candidate in hash ? hash[candidate].length : 0;\n            if (count < resultCount || count == resultCount &amp;&amp; candidate.length > resultLength)\n            {\n                result = candidate;\n                resultCount = count;\n                resultLength = candidate.length;\n            }\n        }\n        return result;\n    },\n    hasFilter: function(filter)\n    {\n        return filter.text in this.keywordByFilter;\n    },\n    getKeywordForFilter: function(filter)\n    {\n        if (filter.text in this.keywordByFilter)\n        {\n            return this.keywordByFilter[filter.text];\n        }\n        else\n        {\n            return null;\n        }\n    },\n    _checkEntryMatch: function(keyword, location, contentType, docDomain, thirdParty, sitekey)\n    {\n        var list = this.filterByKeyword[keyword];\n        for (var i = 0; i < list.length; i++)\n        {\n            var filter = list[i];\n            if (filter == \"#this\")\n            {\n                filter = list;\n            }\n            if (filter.matches(location, contentType, docDomain, thirdParty, sitekey))\n            {\n                return filter;\n            }\n        }\n        return null;\n    },\n    matchesAny: function(location, contentType, docDomain, thirdParty, sitekey)\n    {\n        var candidates = location.toLowerCase().match(/[a-z0-9%]{3,}/g);\n        if (candidates === null)\n        {\n            candidates = [];\n        }\n        candidates.push(\"\");\n        for (var i = 0, l = candidates.length; i < l; i++)\n        {\n            var substr = candidates[i];\n            if (substr in this.filterByKeyword)\n            {\n                var result = this._checkEntryMatch(substr, location, contentType, docDomain, thirdParty, sitekey);\n                if (result)\n                {\n                    return result;\n                }\n            }\n        }\n        return null;\n    }\n};\n\nfunction CombinedMatcher()\n{\n    this.blacklist = new Matcher();\n    this.whitelist = new Matcher();\n    this.resultCache = createDict();\n}\nCombinedMatcher.maxCacheEntries = 1000;\nCombinedMatcher.prototype = {\n    blacklist: null,\n    whitelist: null,\n    resultCache: null,\n    cacheEntries: 0,\n    clear: function()\n    {\n        this.blacklist.clear();\n        this.whitelist.clear();\n        this.resultCache = createDict();\n        this.cacheEntries = 0;\n    },\n    add: function(filter)\n    {\n        if (filter instanceof WhitelistFilter)\n        {\n            this.whitelist.add(filter);\n        }\n        else\n        {\n            this.blacklist.add(filter);\n        }\n        if (this.cacheEntries > 0)\n        {\n            this.resultCache = createDict();\n            this.cacheEntries = 0;\n        }\n    },\n    remove: function(filter)\n    {\n        if (filter instanceof WhitelistFilter)\n        {\n            this.whitelist.remove(filter);\n        }\n        else\n        {\n            this.blacklist.remove(filter);\n        }\n        if (this.cacheEntries > 0)\n        {\n            this.resultCache = createDict();\n            this.cacheEntries = 0;\n        }\n    },\n    findKeyword: function(filter)\n    {\n        if (filter instanceof WhitelistFilter)\n        {\n            return this.whitelist.findKeyword(filter);\n        }\n        else\n        {\n            return this.blacklist.findKeyword(filter);\n        }\n    },\n    hasFilter: function(filter)\n    {\n        if (filter instanceof WhitelistFilter)\n        {\n            return this.whitelist.hasFilter(filter);\n        }\n        else\n        {\n            return this.blacklist.hasFilter(filter);\n        }\n    },\n    getKeywordForFilter: function(filter)\n    {\n        if (filter instanceof WhitelistFilter)\n        {\n            return this.whitelist.getKeywordForFilter(filter);\n        }\n        else\n        {\n            return this.blacklist.getKeywordForFilter(filter);\n        }\n    },\n    isSlowFilter: function(filter)\n    {\n        var matcher = filter instanceof WhitelistFilter ? this.whitelist : this.blacklist;\n        if (matcher.hasFilter(filter))\n        {\n            return !matcher.getKeywordForFilter(filter);\n        }\n        else\n        {\n            return !matcher.findKeyword(filter);\n        }\n    },\n    matchesAnyInternal: function(location, contentType, docDomain, thirdParty, sitekey)\n    {\n        var candidates = location.toLowerCase().match(/[a-z0-9%]{3,}/g);\n        if (candidates === null)\n        {\n            candidates = [];\n        }\n        candidates.push(\"\");\n        var blacklistHit = null;\n        for (var i = 0, l = candidates.length; i < l; i++)\n        {\n            var substr = candidates[i];\n            if (substr in this.whitelist.filterByKeyword)\n            {\n                var result = this.whitelist._checkEntryMatch(substr, location, contentType, docDomain, thirdParty, sitekey);\n                if (result)\n                {\n                    return result;\n                }\n            }\n            if (substr in this.blacklist.filterByKeyword &amp;&amp; blacklistHit === null)\n            {\n                blacklistHit = this.blacklist._checkEntryMatch(substr, location, contentType, docDomain, thirdParty, sitekey);\n            }\n        }\n        return blacklistHit;\n    },\n    matchesAny: function(location, docDomain)\n    {\n        var key = location + \" \" + docDomain + \" \";\n        if (key in this.resultCache)\n        {\n            return this.resultCache[key];\n        }\n        var result = this.matchesAnyInternal(location, 0, docDomain, null, null);\n        if (this.cacheEntries >= CombinedMatcher.maxCacheEntries)\n        {\n            this.resultCache = createDict();\n            this.cacheEntries = 0;\n        }\n        this.resultCache[key] = result;\n        this.cacheEntries++;\n        return result;\n    }\n};\nvar defaultMatcher = new CombinedMatcher();\n\nvar direct = 'DIRECT;';\n\nfor (var i = 0; i < rules.length; i++) {\n    defaultMatcher.add(Filter.fromText(rules[i]));\n}\n\nfunction FindProxyForURL(url, host) {\n    if (defaultMatcher.matchesAny(url, host) instanceof BlockingFilter) {\n        return proxy;\n    }\n    return direct;\n}\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["Linux"]},{"title":"Zk学习笔记——创建节点","url":"/Zk学习笔记——创建节点.html","content":"参考：从Paxos到Zookeeper分布式一致性原理和实践\n\n使用的zk依赖是cdh5.16.2的3.4.5\n\n```\n<!-- zookeeper -->\n<dependency>\n    <groupId>org.apache.zookeeper</groupId>\n    <artifactId>zookeeper</artifactId>\n    <version>3.4.5-cdh5.16.2</version>\n</dependency>\n\n```\n\nZk在创建节点的有同步创建和异步创建，但是Zk是不支持递归创建节点，即父节点必须存储。\n\n在创建同名的节点的时候，会抛出NodeExistsException异常。\n\nZookeeper节点内容只支持字节数组（byte[]）\n\n同步创建节点代码，其中创建了一个永久节点，key是/app1，value是123\n\n```\npackage com.bigdata.zookeeper;\n\n\nimport org.apache.zookeeper.*;\nimport java.util.concurrent.CountDownLatch;\n\npublic class ZkExample implements Watcher {\n\n    public static CountDownLatch connectedSemaphore = new CountDownLatch(1);\n\n    public static void main(String[] args) throws Exception {\n        ZooKeeper zk = new ZooKeeper(\"master:2181\", 5000, new ZkExample());\n        System.out.println(zk.getState());\n        try {\n            connectedSemaphore.await();\n            String path = zk.create(\"/app1\", \"123\".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            System.out.println(\"success create znode: \" + path);\n        } catch (InterruptedException e) {\n            System.out.println(\"Zk session established\" + e);\n        }\n    }\n\n    @Override\n    public void process(WatchedEvent watchedEvent) {\n        System.out.println(watchedEvent);\n        if (Event.KeeperState.SyncConnected == watchedEvent.getState()) {\n            connectedSemaphore.countDown();\n        }\n    }\n}\n\n```\n\n输出，使用Zkui查看zk中的数据，能发现多出了/app1的节点，节点的值是123\n\n<img src=\"/images/517519-20200705192913887-2060462579.png\" width=\"800\" height=\"308\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\nZk中节点的类型分为持久节点（PERSISTENT），持久顺序节点（PERSISTENT_SEQUENTIAL），临时节点（EPHEMERAL），临时顺序节点（EPHEMERAL_SEQUENTIAL）\n\n区别如下\n\n<img src=\"/images/517519-20200705193417372-2124206183.png\" width=\"600\" height=\"505\" loading=\"lazy\" />\n\n异步创建节点代码，\n\n```\npackage com.bigdata.zookeeper;\n\n\nimport org.apache.zookeeper.*;\n\nimport java.util.concurrent.CountDownLatch;\n\npublic class ZkExample implements Watcher {\n\n    public static CountDownLatch connectedSemaphore = new CountDownLatch(1);\n\n    public static void main(String[] args) throws Exception {\n        ZooKeeper zk = new ZooKeeper(\"master:2181\", 5000, new ZkExample());\n        System.out.println(zk.getState());\n        try {\n            connectedSemaphore.await();\n            zk.create(\"/app2\", \"123\".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE,\n                    CreateMode.PERSISTENT, new IStringCallback(), \"This is context\");\n            Thread.sleep(Integer.MAX_VALUE);\n        } catch (InterruptedException e) {\n            System.out.println(\"Zk session established\" + e);\n        }\n    }\n\n    @Override\n    public void process(WatchedEvent watchedEvent) {\n        System.out.println(watchedEvent);\n        if (Event.KeeperState.SyncConnected == watchedEvent.getState()) {\n            connectedSemaphore.countDown();\n        }\n    }\n}\n\nclass IStringCallback implements AsyncCallback.StringCallback {\n\n    @Override\n    public void processResult(int i, String s, Object o, String s1) {\n        System.out.println(\"Create path result : [\" + i + \", \" + s + \", \" + o + \", real path name : \" + s1);\n    }\n}\n\n```\n\n如果创建成功，返回的状态码是0\n\n<img src=\"/images/517519-20200705201956495-1491196186.png\" width=\"600\" height=\"91\" loading=\"lazy\" />\n\n&nbsp;\n\n如果节点已经存在，返回的状态码是-110\n\n<img src=\"/images/517519-20200705202051036-671869392.png\" width=\"600\" height=\"79\" loading=\"lazy\" />\n\n具体介绍\n\n<img src=\"/images/517519-20200705202156327-661020717.png\" width=\"600\" height=\"475\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["zookeeper"]},{"title":"Zk学习笔记——读取节点","url":"/Zk学习笔记——读取节点.html","content":"参考：从Paxos到Zookeeper分布式一致性原理和实践\n\n使用的zk依赖是cdh5.16.2的3.4.5\n\n```\n<!-- zookeeper -->\n<dependency>\n    <groupId>org.apache.zookeeper</groupId>\n    <artifactId>zookeeper</artifactId>\n    <version>3.4.5-cdh5.16.2</version>\n</dependency>\n\n```\n\n代码，在初始化zk的时候，会触发一个watchEvent，将CountDownLatch-1=0，从而开始读取/app1的value，此时为12345，\n\n然后将其修改成123，会再次触发一个watchEvent，此时watchEvent的type是NodeDataChanged，之后10秒延时过后，输出/app1的value，此时为123\n\n读取节点的getData放中的true表示是否注册一个watch，注意这里的watch是一次性的，触发通过之后就失效，需要反复注册watch\n\n```\npackage com.bigdata.zookeeper;\n\n\nimport org.apache.zookeeper.AsyncCallback;\nimport org.apache.zookeeper.WatchedEvent;\nimport org.apache.zookeeper.Watcher;\nimport org.apache.zookeeper.ZooKeeper;\nimport org.apache.zookeeper.data.Stat;\n\nimport java.util.concurrent.CountDownLatch;\n\npublic class ZkExample implements Watcher {\n\n    public static CountDownLatch connectedSemaphore = new CountDownLatch(1);\n    private static Stat stat = new Stat();\n\n    public static void main(String[] args) throws Exception {\n        ZooKeeper zk = new ZooKeeper(\"master:2181\", 5000, new ZkExample());\n        System.out.println(zk.getState());\n        try {\n            connectedSemaphore.await();\n            System.out.println(new String(zk.getData(\"/app1\", true, stat)));\n            Thread.sleep(10000); // 10秒延时\n            System.out.println(new String(zk.getData(\"/app1\", true, stat)));\n        } catch (InterruptedException e) {\n            System.out.println(\"Zk session established\" + e);\n        }\n    }\n\n    @Override\n    public void process(WatchedEvent watchedEvent) {\n        System.out.println(watchedEvent);\n        if (Event.KeeperState.SyncConnected == watchedEvent.getState()) {\n            connectedSemaphore.countDown();\n        }\n    }\n}\n\n```\n\n输出\n\n<img src=\"/images/517519-20200705234908254-1042989337.png\" width=\"800\" height=\"241\" loading=\"lazy\" />\n\n异步代码\n\n```\npackage com.bigdata.zookeeper;\n\n\nimport org.apache.zookeeper.*;\nimport org.apache.zookeeper.data.Stat;\n\nimport java.util.List;\nimport java.util.concurrent.CountDownLatch;\n\npublic class ZkExample implements Watcher {\n\n    public static CountDownLatch connectedSemaphore = new CountDownLatch(1);\n    private static Stat stat = new Stat();\n    private static ZooKeeper zk = null;\n\n    public static void main(String[] args) throws Exception {\n        zk = new ZooKeeper(\"master:2181\", 5000, new ZkExample());\n        System.out.println(zk.getState());\n        try {\n            connectedSemaphore.await();\n            // 注册一个watch，将能异步获取到为空的子节点\n            zk.getChildren(\"/app1\", true, new IChildren2Callback(), null);\n            // 创建一个子节点\n            zk.create(\"/app1/app12\", \"\".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n\n            Thread.sleep(10000); // 10秒延时\n\n        } catch (InterruptedException e) {\n            System.out.println(\"Zk session established\" + e);\n        }\n    }\n\n    @Override\n    public void process(WatchedEvent watchedEvent) {\n        System.out.println(watchedEvent);\n        if (Event.KeeperState.SyncConnected == watchedEvent.getState()) {\n            if (Event.EventType.None == watchedEvent.getType() &amp;&amp; null == watchedEvent.getPath()) {\n                connectedSemaphore.countDown();\n            } else if (watchedEvent.getType() == Event.EventType.NodeChildrenChanged) {\n                try {\n                    System.out.println(zk.getChildren(watchedEvent.getPath(), true));\n                } catch (Exception e) {\n\n                }\n            }\n\n        }\n    }\n}\n\nclass IChildren2Callback implements AsyncCallback.Children2Callback {\n\n    @Override\n    public void processResult(int i, String s, Object o, List<String> list, Stat stat) {\n        System.out.println(\"Get children znode : [response code: \" + i + \", path: \" + s +\n                \", children list\" + list + \", stat : \" + stat);\n    }\n}\n\n```\n\n输出\n\n<img src=\"/images/517519-20200707004148013-1342056918.png\" width=\"800\" height=\"122\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n","tags":["zookeeper"]},{"title":"Zk学习笔记——连接Zookeeper","url":"/Zk学习笔记——连接Zookeeper.html","content":"参考：从Paxos到Zookeeper分布式一致性原理和实践\n\n使用的zk依赖是cdh5.16.2的3.4.5\n\n```\n        <!-- zookeeper -->\n        <dependency>\n            <groupId>org.apache.zookeeper</groupId>\n            <artifactId>zookeeper</artifactId>\n            <version>3.4.5-cdh5.16.2</version>\n        </dependency>\n\n```\n\n代码，其中CountDownLatch计数器参考：[Java多线程&mdash;&mdash;其他工具类CyclicBarrier、CountDownLatch和Exchange](https://www.cnblogs.com/tonglin0325/p/6265379.html)\n\n初始化的计数器为1，当zk client连接到zk集群后，zk集群返回的状态为connected，计数器-1，触发InterruptedException\n\n```\npackage com.bigdata.zookeeper;\n\nimport org.apache.zookeeper.WatchedEvent;\nimport org.apache.zookeeper.Watcher;\nimport org.apache.zookeeper.ZooKeeper;\n\nimport java.util.concurrent.CountDownLatch;\n\npublic class ZkExample implements Watcher {\n\n    public static CountDownLatch connectedSemaphore = new CountDownLatch(1);\n\n    public static void main(String[] args) throws Exception {\n        ZooKeeper zk = new ZooKeeper(\"master:2181\", 5000, new ZkExample());\n        System.out.println(zk.getState());\n        try {\n            connectedSemaphore.await();\n        } catch (InterruptedException e) {\n            System.out.println(\"Zk session established\");\n        }\n    }\n\n    @Override\n    public void process(WatchedEvent watchedEvent) {\n        System.out.println(watchedEvent);\n        if (Event.KeeperState.SyncConnected == watchedEvent.getState()) {\n            connectedSemaphore.countDown();\n        }\n    }\n}\n\n```\n\n输出\n\n<img src=\"/images/517519-20200705170926401-1195013054.png\" width=\"800\" height=\"213\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n","tags":["zookeeper"]},{"title":"小米手机卸载小爱同学","url":"/小米手机卸载小爱同学.html","content":"## 1.将系统进行root\n\n手机的miui版本需要是开发版，并且进行了root，这样才能获取到卸载系统自带软件的权限\n\n尤其是小爱同学这个语音助手，消耗了过多的系统资源，使得手机变得卡顿，所以建议卸载掉\n\n像通过安全中心进行卸载的方式是不能完全卸载掉的，会随着系统后台再次进行安装，所以只能获取root权限后，使用root权限进行删除\n\n如果需要获取root权限，就需要刷开发版的rom，下载ROM请去\n\n```\nhttps://xiaomirom.com/\n\n```\n\n## 2.在手机上安装termux app\n\n这个app用于在手机上执行终端命令\n\n```\nhttps://github.com/termux/termux-app/releases\n\n```\n\n## 3.手机安装termux-adb\n\n这个用于在termux上连上手机adb模式，开启调试\n\n参考：https://github.com/MasterDevX/Termux-ADB\n\n```\napt update &amp;&amp; apt install wget &amp;&amp; wget https://github.com/MasterDevX/Termux-ADB/raw/master/RemoveTools.sh &amp;&amp; bash RemoveTools.sh\n\n```\n\n## 4.使用pm uninstall命令将小爱同学卸载掉\n\n然后如图所示在termux终端中连上adb，输入命令进行卸载操作，就可以卸载小爱同学了\n\n<img src=\"/images/517519-20200614132915932-1570750100.png\" width=\"600\" height=\"1072\" loading=\"lazy\" />\n\n如果还需要卸载其他的系统应用的话，参考下面文章\n\n```\nhttps://powersee.github.io/2019/09/MIUI-adb/\n\n```\n\n卸载某些系统应用会导致系统异常，下面列出了我验证过的可以卸载的应用\n\n```\npm uninstall com.xiaomi.mimobile.noti\npm uninstall com.xiaomi.jr.security\npm uninstall -k --user 0 com.miui.voiceassist\npm uninstall -k --user 0 com.miui.yellowpage\npm uninstall -k --user 0 com.miui.bugreport\npm uninstall -k --user 0 com.miui.hybrid\npm uninstall -k --user 0 com.xiaomi.ab\npm uninstall -k --user 0 com.xiaomi.vipaccount\npm uninstall -k --user 0 com.miui.milivetalk\npm uninstall -k --user 0 com.xiaomi.payment\npm uninstall -k --user 0 com.miui.hybrid.accessory\npm uninstall -k --user 0 com.sohu.inputmethod.sogou.xiaomi\npm uninstall -k --user 0 com.xiaomi.gamecenter\npm uninstall -k --user 0 com.miui.analytics\npm uninstall -k --user 0 com.mipay.wallet\npm uninstall -k --user 0 com.xiaomi.joyose\npm uninstall -k --user 0 com.xiaomi.miplay\npm uninstall -k --user 0 com.miui.video\npm uninstall -k --user 0 com.miui.klo.bugreport\npm uninstall -k --user 0 com.baidu.input_mi\npm uninstall -k --user 0 com.xiaomi.pass\npm uninstall -k --user 0 com.miui.virtualsim\npm uninstall -k --user 0 com.miui.player\npm uninstall -k --user 0 com.miui.compass\npm uninstall -k --user 0 com.miui.systemAdSolution\npm uninstall -k --user 0 com.android.browser\npm uninstall -k --user 0 com.miui.contentextension\npm uninstall -k --user 0 com.xiaomi.gamecenter.sdk.service\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["Android"]},{"title":"Flink学习笔记——Flink on YARN","url":"/Flink学习笔记——Flink on YARN.html","content":"Flink集群部署的方式有以下几种，在本文中主要介绍Flink on yarn：\n\n```\nYarn\nMesos\nDocker/Kubernetes\nStandalone\n\n```\n\n<img src=\"/images/517519-20200531120817189-925422057.png\" alt=\"\" width=\"444\" height=\"249\" />\n\n<!--more-->\n&nbsp;\n\n参考：\n\n```\nhttps://www.slideshare.net/tillrohrmann/redesigning-apache-flinks-distributed-architecture-flink-forward-2017\n\n```\n\n&nbsp;\n\n**Flink on yarn**有2种运行方式：\n\n<img src=\"/images/517519-20200601234545677-741835783.png\" alt=\"\" width=\"435\" height=\"235\" />\n\n&nbsp;\n\n**Flink架构**\n\n<img src=\"/images/517519-20201214143920541-325306389.png\" alt=\"\" width=\"903\" height=\"609\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;\n\n第一种是**Session mode**\n\n**定义参考：《Stream Processing with Apache Flink: Fundamentals, Implementation, and Operation of Streaming Applications》**\n\n```\nhttps://books.google.com.sg/books?id=7OiRDwAAQBAJ&amp;pg=PT265&amp;lpg=PT265&amp;dq=flink+job+mode+session+mode&amp;source=bl&amp;ots=0fMg-gnzyf&amp;sig=ACfU3U1-W4Az3WWJiUBwTpm0tGbsi2UpGw&amp;hl=zh-CN&amp;sa=X&amp;ved=2ahUKEwiMwJjS-uDpAhVTIqYKHbgABkwQ6AEwBnoECAoQAQ#v=onepage&amp;q=flink%20job%20mode%20session%20mode&amp;f=false\n\n```\n\n&nbsp;**<img src=\"/images/517519-20200601235257382-1371433206.png\" alt=\"\" width=\"500\" height=\"604\" />**\n\n&nbsp;\n\n&nbsp;\n\n即启动一个yarn session，然后submit flink job到这个yarn session上\n\n<img src=\"/images/517519-20200601000219537-554271676.png\" alt=\"\" width=\"800\" height=\"207\" />\n\n有2个步骤：\n\n1.启动一个yarn session\n\n<img src=\"/images/517519-20200601000330749-1865374546.png\" alt=\"\" width=\"800\" height=\"783\" />\n\n&nbsp;\n\n&nbsp;\n\n2.提交任务到flink集群上\n\n<img src=\"/images/517519-20200601000439004-1695565829.png\" alt=\"\" width=\"800\" height=\"614\" />\n\n&nbsp;\n\n参考：\n\n```\nhttps://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/deployment/yarn_setup.html\n\n```\n\n&nbsp;\n\n第二种是**Job mode**\n\n**<strong><img src=\"/images/517519-20200601234733603-1728592271.png\" alt=\"\" width=\"500\" height=\"83\" />**</strong>\n\n即提交一个独立的flink任务到yarn上面，一旦这个job结束了，Flink集群上的资源就会释放\n\n<img src=\"/images/517519-20200601000646279-2072249952.png\" alt=\"\" width=\"800\" height=\"313\" />\n\n&nbsp;\n\n**flink kerberos参考**\n\n```\nhttps://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/security/security-kerberos.html\n\n```\n\n即在conf/flink-conf.yaml中配置\n\n```\nsecurity.kerberos.login.use-ticket-cache: false\nsecurity.kerberos.login.keytab: /path/xxx.keytab\nsecurity.kerberos.login.principal: xxx\nsecurity.kerberos.login.contexts: Client,KafkaClient\n\n```\n\n　　\n","tags":["Flink"]},{"title":"使用frp+ss访问内网机器服务","url":"/使用frp+ss访问内网机器服务.html","content":"1.在[使用frp进行内网穿透](https://www.cnblogs.com/tonglin0325/p/12815487.html)的基础上，在内网机器的frpc.ini配置中添加\n\n```\n[web]\ntype = tcp\nlocal_ip = master\nlocal_port = 内网端口\nremote_port = 外网机器端口\n\n```\n\n启动\n\n```\n./frpc -c frpc.ini\n\n```\n\n2.在内网机器上启动ss-server\n\n修改配置/etc/shadow$ocks-libev/config.json，必须是0.0.0.0\n\n```\n{\n    \"server\":\"0.0.0.0\",\n    \"server_port\":内网端口,\n    \"local_port\":1080,\n    \"password\":\"密码\",\n    \"timeout\":60,\n    \"method\":\"chacha20-ietf-poly1305\"\n}\n\n```\n\n启动\n\n```\nss-server -c /etc/shadow$ocks-libev/config.json\n\n```\n\n3.配置代理\n\n<img src=\"/images/517519-20200502165548613-237501905.png\" alt=\"\" width=\"600\" height=\"341\" />\n\n<!--more-->\n&nbsp;\n\n这样就能访问内网机器上的服务了\n\n&nbsp;\n","tags":["Linux"]},{"title":"使用frp进行内网穿透","url":"/使用frp进行内网穿透.html","content":"1.前提：1台有公网ip的服务器（1核1G），1台在内网的服务器（16G）\n\n2.在公网机器上安装frp，并启动frp server\n\n下载并解压\n\n```\nwget https://github.com/fatedier/frp/releases/download/v0.33.0/frp_0.33.0_linux_amd64.tar.gz\n\n```\n\n配置文件frps.ini\n\n```\n[common]\nbind_port = xxxx\ntoken = ssssss\n\n```\n\n其中bind_port是用于和client端通信的；token是密码；vhost_http_port是当client端配置了web http的服务的时候，通过server访问的端口；vhost_https_port是当client端配置了web https的服务的时候，通过server访问的端口\n\n启动\n\n```\n./frps -c frps.ini\n\n```\n\n<img src=\"/images/517519-20200501235344532-1380346097.png\" alt=\"\" width=\"600\" height=\"87\" />\n\n<!--more-->\n&nbsp;\n\n3.在内网机器上安装frp，并启动frp client\n\n配置文件frpc.ini\n\n```\n[common]\nserver_addr = 公网机器host\nserver_port = xxxx\ntoken = ssssss\n[ssh]\ntype = tcp\nlocal_ip = 内网机器host\nlocal_port = 22\nremote_port = 6000\n\n```\n\n启动\n\n```\n./frpc -c frpc.ini\n\n```\n\n<img src=\"/images/517519-20200501235626898-1476902527.png\" alt=\"\" width=\"1000\" height=\"87\" />\n\n&nbsp;\n\n这时就可以通过外网机器的6000端口来ssh到内网机器上了\n\n```\nssh -p 6000 user@外网机器host\n\n```\n\n&nbsp;\n","tags":["Linux"]},{"title":"CDH修改机器ip","url":"/CDH修改机器ip.html","content":"如果机器ip变更的话,cdh将无法正常启动,需要修改两个地方的ip地址\n\n1.数据库scm库的HOSTS表,将IP_ADDRESS字段的ip修改成变更后的ip地址\n\n<img src=\"/images/517519-20200411104916148-1677176555.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n2.修改/etc/cloudera-scm-agent/config.ini中的ip地址\n\n<img src=\"/images/517519-20200411105053605-1545574810.png\" alt=\"\" />\n\n&nbsp;\n\n然后进行重启\n\n&nbsp;\n","tags":["CDH"]},{"title":"Python学习笔记——sqlalchemy","url":"/Python学习笔记——sqlalchemy.html","content":"**sqlalchemy**是Python的**ORM**框架\n\n1.安装sqlalchemy\n\n<img src=\"/images/517519-20200408162646722-997313354.png\" alt=\"\" width=\"600\" height=\"491\" />\n\n2.安装mysql-connector\n\n<img src=\"/images/517519-20200408172931138-1720758907.png\" alt=\"\" width=\"600\" height=\"482\" />\n\n<!--more-->\n&nbsp;\n","tags":["Python"]},{"title":"Hive学习笔记——hive hook","url":"/Hive学习笔记——hive hook.html","content":"Hive hook是hive的钩子函数，可以嵌入HQL执行的过程中运行，比如下面的这几种情况\n\n<img src=\"/images/517519-20200321193657092-1621988024.png\" alt=\"\" width=\"400\" height=\"309\" />\n\n参考\n\n```\nhttps://www.slideshare.net/julingks/apache-hive-hooksminwookim130813\n\n```\n\n有了Hook，可以实现例如非法SQL拦截，SQL收集和审计等功能，业界的案例可以参考Airbnb的reair\n\n```\nhttps://github.com/airbnb/reair\n\n```\n\n该项目中就使用了Hive的hook函数实现了一个Audit Log Hook，将提交到hiveserver2上的query写入到MySQL当中收集起来\n\n```\nhttps://github.com/airbnb/reair/blob/master/hive-hooks/src/main/java/com/airbnb/reair/hive/hooks/CliAuditLogHook.java\n\n```\n\n<!--more-->\n&nbsp;\n\n这些hook函数的hive sql运行过程中的执行顺序\n\n```\nDriver.run()\n\n=> HiveDriverRunHook.preDriverRun()(hive.exec.driver.run.hooks)\n\n=> Driver.compile()\n\n=> HiveSemanticAnalyzerHook.preAnalyze()(hive.semantic.analyzer.hook)\n\n=> SemanticAnalyze(QueryBlock, LogicalPlan, PhyPlan, TaskTree)\n\n=> HiveSemanticAnalyzerHook.postAnalyze()(hive.semantic.analyzer.hook)\n\n=> QueryString redactor(hive.exec.query.redactor.hooks)\n\n=> QueryPlan Generation\n\n=> Authorization\n\n=> Driver.execute()\n\n=> ExecuteWithHookContext.run() || PreExecute.run() (hive.exec.pre.hooks)\n\n=> TaskRunner\n\n=> if failed, ExecuteWithHookContext.run()(hive.exec.failure.hooks)\n\n=> ExecuteWithHookContext.run() || PostExecute.run() (hive.exec.post.hooks)\n\n=> HiveDriverRunHook.postDriverRun()(hive.exec.driver.run.hooks)\n\n```\n\n参考\n\n```\nhttps://my.oschina.net/kavn/blog/1514648\n\n```\n\n&nbsp;\n\n**1.ExecuteWithHookContext接口**\n\n下面将实现ExecuteWithHookContext接口来实现一个钩子函数，其他的hook还有实现HiveSemanticAnalyzerHook接口，继承AbstractSemanticAnalyzerHook抽象类等\n\nExecuteWithHookContext可以实现3种类型的hook，分别是pre-execution,post-execution和execution-failure，这个在hive sql的执行过程中已经处于最后几个步骤了\n\n<img src=\"/images/517519-20200328153748662-155259969.png\" alt=\"\" width=\"650\" height=\"122\" />\n\n&nbsp;\n\n依赖\n\n需要注意依赖的版本需要和集群保持一致，我的cdh集群的版本为cdh5.16.2\n\n如果使用的是apache版本的1.1.0版本的hive-exec的话，代码的内容会和cloudera的1.1.0-cdh5.16.2的hive-exec会有些不同，比如\n\n1.1.0-cdh5.16.2版本的TOK_QUERY=789\n\n<img src=\"/images/517519-20200331103303038-1025368439.png\" alt=\"\" width=\"600\" height=\"205\" />\n\n但是1.1.0版本的TOK_QUERY=777\n\n<img src=\"/images/517519-20200331103513186-1595124067.png\" alt=\"\" width=\"600\" height=\"330\" />\n\n&nbsp;\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <parent>\n        <artifactId>interview-parent</artifactId>\n        <groupId>com.interview</groupId>\n        <version>1.0-SNAPSHOT</version>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n\n    <artifactId>interview-bigdata</artifactId>\n\n    <dependencies>\n        <!-- logback -->\n        <dependency>\n            <groupId>org.slf4j</groupId>\n            <artifactId>log4j-over-slf4j</artifactId>\n            <version>1.7.25</version>\n        </dependency>\n        <!--hive-->\n        <dependency>\n            <groupId>org.apache.hive</groupId>\n            <artifactId>hive-exec</artifactId>\n            <version>1.1.0-cdh5.16.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hive</groupId>\n            <artifactId>hive-metastore</artifactId>\n            <version>1.1.0-cdh5.16.2</version>\n        </dependency>\n        <!--hadoop-->\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-common</artifactId>\n            <version>2.6.0-cdh5.16.2</version>\n        </dependency>\n\n    </dependencies>\n\n    <!--<build>-->\n        <!--<plugins>-->\n            <!--<plugin>-->\n                <!--<groupId>org.apache.maven.plugins</groupId>-->\n                <!--<artifactId>maven-shade-plugin</artifactId>-->\n                <!--<executions>-->\n                    <!--<!&ndash; Run shade goal on package phase &ndash;>-->\n                    <!--<execution>-->\n                        <!--<phase>package</phase>-->\n                        <!--<goals>-->\n                            <!--<goal>shade</goal>-->\n                        <!--</goals>-->\n                        <!--<configuration>-->\n                            <!--<filters>-->\n                                <!--<filter>-->\n                                    <!--<!&ndash; Do not copy the signatures in the META-INF folder.-->\n                                    <!--Otherwise, this might cause SecurityExceptions when using the JAR. &ndash;>-->\n                                    <!--<artifact>*:*</artifact>-->\n                                    <!--<excludes>-->\n                                        <!--<exclude>META-INF/*.SF</exclude>-->\n                                        <!--<exclude>META-INF/*.DSA</exclude>-->\n                                        <!--<exclude>META-INF/*.RSA</exclude>-->\n                                    <!--</excludes>-->\n                                <!--</filter>-->\n                            <!--</filters>-->\n\n                            <!--<createDependencyReducedPom>false</createDependencyReducedPom>-->\n                        <!--</configuration>-->\n                    <!--</execution>-->\n                <!--</executions>-->\n            <!--</plugin>-->\n\n            <!--<plugin>-->\n                <!--<groupId>org.apache.maven.plugins</groupId>-->\n                <!--<artifactId>maven-compiler-plugin</artifactId>-->\n                <!--<configuration>-->\n                    <!--<source>1.8</source>-->\n                    <!--<target>1.8</target>-->\n                <!--</configuration>-->\n            <!--</plugin>-->\n\n        <!--</plugins>-->\n    <!--</build>-->\n\n</project>\n\n```\n\n&nbsp;\n\n代码，只是简单的打印了一行日志\n\n```\npackage com.bigdata.hive;\n\nimport org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext;\nimport org.apache.hadoop.hive.ql.hooks.HookContext;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\npublic class MyHiveHook implements ExecuteWithHookContext {\n\n    private static Logger logger = LoggerFactory.getLogger(MyHiveHook.class);\n\n    public void run(HookContext hookContext) throws Exception {\n        logger.info(\"this is my hive hook\");\n    }\n}\n\n```\n\n打包\n\n```\nmvn clean package\n\n```\n\n将打好的jar包上传到所有hiveserver2所在机器的/var/lib/hive目录下，或者找一个hdfs目录\n\n```\nroot@master:/var/lib/hive# ls\nexamples.desktop  interview-bigdata-1.0-SNAPSHOT.jar\n\n```\n\n修改owner成hive\n\n```\nsudo chown hive:hive ./interview-bigdata-1.0-SNAPSHOT.jar\n\n```\n\n去cloudera manager中配置hive的辅助jar目录和hook函数\n\njar目录\n\n<img src=\"/images/517519-20200321223815028-474591254.png\" alt=\"\" width=\"600\" height=\"241\" />\n\n&nbsp;\n\nhook函数，此处配置成hive.exec.pre.hooks，此时添加的hook函数将在sql执行之前运行\n\n&nbsp;<img src=\"/images/517519-20200321223726000-124218584.png\" alt=\"\" width=\"1000\" height=\"837\" />\n\n&nbsp;\n\n第一次配置之后需要重启hive集群，之后替换jar包的时候就只需要在hue中执行reload命令即可\n\n执行sql\n\n<img src=\"/images/517519-20200321223922239-224576797.png\" alt=\"\" width=\"900\" height=\"407\" />\n\n&nbsp;\n\n查看hiveserver2日志\n\n```\ntail -n 100 /var/log/hive/hadoop-cmf-hive-HIVESERVER2-master.log.out\n\n```\n\n可以看到打印的日志\n\n<img src=\"/images/517519-20200321224111206-227149050.png\" alt=\"\" width=\"900\" height=\"247\" />\n\n下面尝试获取一下截取query，并进行打印\n\n参考了\n\n```\nhttps://towardsdatascience.com/apache-hive-hooks-and-metastore-listeners-a-tale-of-your-metadata-903b751ee99f\n\n```\n\n&nbsp;代码，替换jar包的时候需要重启hive才能生效\n\n```\npackage com.bigdata.hive;\n\nimport org.apache.hadoop.hive.metastore.api.Database;\nimport org.apache.hadoop.hive.ql.QueryPlan;\nimport org.apache.hadoop.hive.ql.hooks.*;\nimport org.apache.hadoop.hive.ql.plan.HiveOperation;\n\nimport org.codehaus.jackson.map.ObjectMapper;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.util.HashSet;\nimport java.util.Set;\n\npublic class MyHiveHook implements ExecuteWithHookContext {\n\n    private static Logger logger = LoggerFactory.getLogger(MyHiveHook.class);\n    private static final HashSet<String> OPERATION_NAMES = new HashSet<>();\n\n    static {\n        OPERATION_NAMES.add(HiveOperation.CREATETABLE.getOperationName());\n        OPERATION_NAMES.add(HiveOperation.ALTERDATABASE.getOperationName());\n        OPERATION_NAMES.add(HiveOperation.ALTERDATABASE_OWNER.getOperationName());\n        OPERATION_NAMES.add(HiveOperation.ALTERTABLE_ADDCOLS.getOperationName());\n        OPERATION_NAMES.add(HiveOperation.ALTERTABLE_LOCATION.getOperationName());\n        OPERATION_NAMES.add(HiveOperation.ALTERTABLE_PROPERTIES.getOperationName());\n        OPERATION_NAMES.add(HiveOperation.ALTERTABLE_RENAME.getOperationName());\n        OPERATION_NAMES.add(HiveOperation.ALTERTABLE_RENAMECOL.getOperationName());\n        OPERATION_NAMES.add(HiveOperation.ALTERTABLE_REPLACECOLS.getOperationName());\n        OPERATION_NAMES.add(HiveOperation.CREATEDATABASE.getOperationName());\n        OPERATION_NAMES.add(HiveOperation.DROPDATABASE.getOperationName());\n        OPERATION_NAMES.add(HiveOperation.DROPTABLE.getOperationName());\n    }\n\n    @Override\n    public void run(HookContext hookContext) throws Exception {\n        assert (hookContext.getHookType() == HookContext.HookType.POST_EXEC_HOOK);\n\n        QueryPlan plan = hookContext.getQueryPlan();\n\n        String operationName = plan.getOperationName();\n        logWithHeader(\"Query executed: \" + plan.getQueryString());\n        logWithHeader(\"Operation: \" + operationName);\n        if (OPERATION_NAMES.contains(operationName)\n                &amp;&amp; !plan.isExplain()) {\n            logWithHeader(\"Monitored Operation\");\n            Set<ReadEntity> inputs = hookContext.getInputs();\n            Set<WriteEntity> outputs = hookContext.getOutputs();\n\n            for (Entity entity : inputs) {\n                logWithHeader(\"Hook metadata input value: \" +  toJson(entity));\n            }\n\n            for (Entity entity : outputs) {\n                logWithHeader(\"Hook metadata output value: \" +  toJson(entity));\n            }\n\n        } else {\n            logWithHeader(\"Non-monitored Operation, ignoring hook\");\n        }\n    }\n\n    private static String toJson(Entity entity) throws Exception {\n        ObjectMapper mapper = new ObjectMapper();\n        switch (entity.getType()) {\n            case DATABASE:\n                Database db = entity.getDatabase();\n                return mapper.writeValueAsString(db);\n            case TABLE:\n                return mapper.writeValueAsString(entity.getTable().getTTable());\n        }\n        return null;\n    }\n\n    private void logWithHeader(Object obj){\n        logger.info(\"[CustomHook][Thread: \"+Thread.currentThread().getName()+\"] | \" + obj);\n    }\n}\n\n```\n\n&nbsp;输出，可以看到select * from test，执行的将会成为两个operation，\n\n一个是SWITCHDATABASE\n\n```\n2020-03-22 23:50:33,374 INFO  org.apache.hadoop.hive.ql.log.PerfLogger: [HiveServer2-Handler-Pool: Thread-39]: <PERFLOG method=PreHook.com.bigdata.hive.MyHiveHook from=org.apache.hadoop.hive.ql.Driver>\n2020-03-22 23:50:33,374 INFO  com.bigdata.hive.MyHiveHook: [HiveServer2-Handler-Pool: Thread-39]: [CustomHook][Thread: HiveServer2-Handler-Pool: Thread-39] | Query executed: USE `default`\n2020-03-22 23:50:33,374 INFO  com.bigdata.hive.MyHiveHook: [HiveServer2-Handler-Pool: Thread-39]: [CustomHook][Thread: HiveServer2-Handler-Pool: Thread-39] | Operation: SWITCHDATABASE\n2020-03-22 23:50:33,374 INFO  com.bigdata.hive.MyHiveHook: [HiveServer2-Handler-Pool: Thread-39]: [CustomHook][Thread: HiveServer2-Handler-Pool: Thread-39] | Non-monitored Operation, ignoring hook\n2020-03-22 23:50:33,375 INFO  org.apache.hadoop.hive.ql.log.PerfLogger: [HiveServer2-Handler-Pool: Thread-39]: </PERFLOG method=PreHook.com.bigdata.hive.MyHiveHook start=1584892233374 end=1584892233375 duration=1 from=org.apache.hadoop.hive.ql.Driver>\n\n```\n\n&nbsp;一个是QUERY\n\n```\n2020-03-22 23:50:35,282 INFO  org.apache.hadoop.hive.ql.Driver: [HiveServer2-Background-Pool: Thread-50]: Executing command(queryId=hive_20200322235050_83cdb414-52bd-4990-9d20-87f5dc0d76dc): SELECT * from test\n2020-03-22 23:50:35,283 INFO  org.apache.hadoop.hive.ql.log.PerfLogger: [HiveServer2-Background-Pool: Thread-50]: <PERFLOG method=PreHook.com.bigdata.hive.MyHiveHook from=org.apache.hadoop.hive.ql.Driver>\n2020-03-22 23:50:35,283 INFO  com.bigdata.hive.MyHiveHook: [HiveServer2-Background-Pool: Thread-50]: [CustomHook][Thread: HiveServer2-Background-Pool: Thread-50] | Query executed: SELECT * from test\n2020-03-22 23:50:35,283 INFO  com.bigdata.hive.MyHiveHook: [HiveServer2-Background-Pool: Thread-50]: [CustomHook][Thread: HiveServer2-Background-Pool: Thread-50] | Operation: QUERY\n2020-03-22 23:50:35,283 INFO  com.bigdata.hive.MyHiveHook: [HiveServer2-Background-Pool: Thread-50]: [CustomHook][Thread: HiveServer2-Background-Pool: Thread-50] | Non-monitored Operation, ignoring hook\n2020-03-22 23:50:35,283 INFO  org.apache.hadoop.hive.ql.log.PerfLogger: [HiveServer2-Background-Pool: Thread-50]: </PERFLOG method=PreHook.com.bigdata.hive.MyHiveHook start=1584892235283 end=1584892235283 duration=0 from=org.apache.hadoop.hive.ql.Driver>\n\n```\n\n如果执行的是建表语句，比如\n\n```\nCREATE TABLE `test1`(\n\t  `id` int, \n\t  `name` string)\n\tROW FORMAT SERDE \n\t  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' \n\tSTORED AS INPUTFORMAT \n\t  'org.apache.hadoop.mapred.TextInputFormat' \n\tOUTPUTFORMAT \n\t  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n\tLOCATION\n\t  'hdfs://master:8020/user/hive/warehouse/test'\n\tTBLPROPERTIES (\n\t  'COLUMN_STATS_ACCURATE'='true', \n\t  'numFiles'='3', \n\t  'numRows'='6', \n\t  'rawDataSize'='36', \n\t  'totalSize'='42', \n\t  'transient_lastDdlTime'='1584893066')\n\n```\n\n那么hook函数的输出将会是\n\n```\n2020-03-23 00:08:16,486 INFO  org.apache.hadoop.hive.ql.log.PerfLogger: [HiveServer2-Background-Pool: Thread-94]: <PERFLOG method=PreHook.com.bigdata.hive.MyHiveHook from=org.apache.hadoop.hive.ql.Driver>\n2020-03-23 00:08:16,486 INFO  com.bigdata.hive.MyHiveHook: [HiveServer2-Background-Pool: Thread-94]: [CustomHook][Thread: HiveServer2-Background-Pool: Thread-94] | Query executed: CREATE TABLE `test1`(\n          `id` int,\n          `name` string)\n        ROW FORMAT SERDE\n          'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n        STORED AS INPUTFORMAT\n          'org.apache.hadoop.mapred.TextInputFormat'\n        OUTPUTFORMAT\n          'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n        LOCATION\n          'hdfs://master:8020/user/hive/warehouse/test'\n        TBLPROPERTIES (\n          'COLUMN_STATS_ACCURATE'='true',\n          'numFiles'='3',\n          'numRows'='6',\n          'rawDataSize'='36',\n          'totalSize'='42',\n          'transient_lastDdlTime'='1584893066')\n2020-03-23 00:08:16,486 INFO  com.bigdata.hive.MyHiveHook: [HiveServer2-Background-Pool: Thread-94]: [CustomHook][Thread: HiveServer2-Background-Pool: Thread-94] | Operation: CREATETABLE\n2020-03-23 00:08:16,486 INFO  com.bigdata.hive.MyHiveHook: [HiveServer2-Background-Pool: Thread-94]: [CustomHook][Thread: HiveServer2-Background-Pool: Thread-94] | Monitored Operation\n2020-03-23 00:08:16,487 INFO  com.bigdata.hive.MyHiveHook: [HiveServer2-Background-Pool: Thread-94]: [CustomHook][Thread: HiveServer2-Background-Pool: Thread-94] | Hook metadata input value: null\n2020-03-23 00:08:16,497 INFO  com.bigdata.hive.MyHiveHook: [HiveServer2-Background-Pool: Thread-94]: [CustomHook][Thread: HiveServer2-Background-Pool: Thread-94] | Hook metadata output value: {\"description\":\"Default Hive database\",\"name\":\"default\",\"parameters\":{},\"ownerType\":\"ROLE\",\"setName\":true,\"setDescription\":true,\"locationUri\":\"hdfs://master:8020/user/hive/warehouse\",\"setLocationUri\":true,\"setOwnerName\":true,\"ownerName\":\"public\",\"setPrivileges\":false,\"setOwnerType\":true,\"parametersSize\":0,\"setParameters\":true,\"privileges\":null}\n2020-03-23 00:08:16,519 INFO  com.bigdata.hive.MyHiveHook: [HiveServer2-Background-Pool: Thread-94]: [CustomHook][Thread: HiveServer2-Background-Pool: Thread-94] | Hook metadata output value: {\"lastAccessTime\":0,\"parameters\":{},\"owner\":\"hive\",\"ownerType\":\"USER\",\"dbName\":\"default\",\"tableType\":\"MANAGED_TABLE\",\"tableName\":\"test1\",\"setTableName\":true,\"setOwner\":true,\"retention\":0,\"setRetention\":false,\"partitionKeysSize\":0,\"partitionKeysIterator\":[],\"setPartitionKeys\":true,\"viewOriginalText\":null,\"setViewOriginalText\":false,\"viewExpandedText\":null,\"setViewExpandedText\":false,\"setTableType\":true,\"setPrivileges\":false,\"setTemporary\":false,\"setOwnerType\":true,\"setDbName\":true,\"createTime\":1584893296,\"setCreateTime\":true,\"setLastAccessTime\":false,\"setSd\":true,\"parametersSize\":0,\"setParameters\":true,\"privileges\":null,\"sd\":{\"location\":null,\"parameters\":{},\"numBuckets\":-1,\"inputFormat\":\"org.apache.hadoop.mapred.SequenceFileInputFormat\",\"outputFormat\":\"org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\",\"compressed\":false,\"sortCols\":[],\"parametersSize\":0,\"setParameters\":true,\"cols\":[],\"colsSize\":0,\"serdeInfo\":{\"setSerializationLib\":true,\"name\":null,\"parameters\":{\"serialization.format\":\"1\"},\"setName\":false,\"parametersSize\":1,\"setParameters\":true,\"serializationLib\":\"org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe\"},\"skewedInfo\":{\"skewedColNamesSize\":0,\"skewedColNamesIterator\":[],\"setSkewedColNames\":true,\"skewedColValuesSize\":0,\"skewedColValuesIterator\":[],\"setSkewedColValues\":true,\"skewedColValueLocationMapsSize\":0,\"setSkewedColValueLocationMaps\":true,\"skewedColValueLocationMaps\":{},\"skewedColNames\":[],\"skewedColValues\":[]},\"bucketCols\":[],\"setNumBuckets\":true,\"setSerdeInfo\":true,\"bucketColsSize\":0,\"bucketColsIterator\":[],\"setBucketCols\":true,\"sortColsSize\":0,\"sortColsIterator\":[],\"setSortCols\":true,\"setSkewedInfo\":true,\"storedAsSubDirectories\":false,\"setStoredAsSubDirectories\":false,\"colsIterator\":[],\"setCols\":true,\"setLocation\":false,\"setInputFormat\":true,\"setOutputFormat\":true,\"setCompressed\":false},\"temporary\":false,\"partitionKeys\":[]}\n2020-03-23 00:08:16,519 INFO  org.apache.hadoop.hive.ql.log.PerfLogger: [HiveServer2-Background-Pool: Thread-94]: </PERFLOG method=PreHook.com.bigdata.hive.MyHiveHook start=1584893296486 end=1584893296519 duration=33 from=org.apache.hadoop.hive.ql.Driver>\n2020-03-23 00:08:16,519 INFO  org.apache.hadoop.hive.ql.log.PerfLogger: [HiveServer2-Background-Pool: Thread-94]: </PERFLOG method=TimeToSubmit start=1584893296477 end=1584893296519 duration=42 from=org.apache.hadoop.hive.ql.Driver>\n\n```\n\n如果执行的是修改字段的语句，比如\n\n```\nALTER TABLE test1 CHANGE id id String COMMENT \"test\"\n\n```\n\n那么hook函数的输出会是\n\n```\n2020-03-28 14:19:12,912 INFO  org.apache.hadoop.hive.ql.log.PerfLogger: [HiveServer2-Handler-Pool: Thread-39]: </PERFLOG method=compile start=1585376352892 end=1585376352912 duration=20 from=org.apache.hadoop.hive.ql.Driver>\n2020-03-28 14:19:12,912 INFO  org.apache.hadoop.hive.ql.Driver: [HiveServer2-Handler-Pool: Thread-39]: Completed compiling command(queryId=hive_20200328141919_d2739f08-478e-4f95-949b-e0bd176e4eab); Time taken: 0.02 seconds\n2020-03-28 14:19:12,913 INFO  org.apache.hadoop.hive.ql.log.PerfLogger: [HiveServer2-Background-Pool: Thread-79]: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>\n2020-03-28 14:19:12,913 INFO  org.apache.hadoop.hive.ql.log.PerfLogger: [HiveServer2-Background-Pool: Thread-79]: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>\n2020-03-28 14:19:12,913 INFO  org.apache.hadoop.hive.ql.log.PerfLogger: [HiveServer2-Background-Pool: Thread-79]: <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>\n2020-03-28 14:19:12,922 INFO  org.apache.hadoop.hive.ql.log.PerfLogger: [HiveServer2-Background-Pool: Thread-79]: </PERFLOG method=acquireReadWriteLocks start=1585376352913 end=1585376352922 duration=9 from=org.apache.hadoop.hive.ql.Driver>\n2020-03-28 14:19:12,922 INFO  org.apache.hadoop.hive.ql.log.PerfLogger: [HiveServer2-Background-Pool: Thread-79]: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>\n2020-03-28 14:19:12,922 INFO  org.apache.hadoop.hive.ql.Driver: [HiveServer2-Background-Pool: Thread-79]: Executing command(queryId=hive_20200328141919_d2739f08-478e-4f95-949b-e0bd176e4eab): ALTER TABLE test1 CHANGE id id String COMMENT \"test\"\n2020-03-28 14:19:12,923 INFO  org.apache.hadoop.hive.ql.log.PerfLogger: [HiveServer2-Background-Pool: Thread-79]: <PERFLOG method=PreHook.com.bigdata.hive.MyHiveHook from=org.apache.hadoop.hive.ql.Driver>\n2020-03-28 14:19:12,923 INFO  com.bigdata.hive.MyHiveHook: [HiveServer2-Background-Pool: Thread-79]: [CustomHook][Thread: HiveServer2-Background-Pool: Thread-79] | Query executed: ALTER TABLE test1 CHANGE id id String COMMENT \"test\"\n2020-03-28 14:19:12,923 INFO  com.bigdata.hive.MyHiveHook: [HiveServer2-Background-Pool: Thread-79]: [CustomHook][Thread: HiveServer2-Background-Pool: Thread-79] | Operation: ALTERTABLE_RENAMECOL\n2020-03-28 14:19:12,923 INFO  com.bigdata.hive.MyHiveHook: [HiveServer2-Background-Pool: Thread-79]: [CustomHook][Thread: HiveServer2-Background-Pool: Thread-79] | Monitored Operation\n2020-03-28 14:19:12,928 INFO  com.bigdata.hive.MyHiveHook: [HiveServer2-Background-Pool: Thread-79]: [CustomHook][Thread: HiveServer2-Background-Pool: Thread-79] | Hook metadata input value: {\"lastAccessTime\":0,\"ownerType\":\"USER\",\"parameters\":{\"last_modified_time\":\"1585376257\",\"totalSize\":\"0\",\"numRows\":\"-1\",\"rawDataSize\":\"-1\",\"COLUMN_STATS_ACCURATE\":\"false\",\"numFiles\":\"0\",\"transient_lastDdlTime\":\"1585376257\",\"last_modified_by\":\"hive\"},\"owner\":\"hive\",\"tableName\":\"test1\",\"dbName\":\"default\",\"tableType\":\"MANAGED_TABLE\",\"privileges\":null,\"sd\":{\"location\":\"hdfs://master:8020/user/hive/warehouse/test\",\"parameters\":{},\"inputFormat\":\"org.apache.hadoop.mapred.TextInputFormat\",\"outputFormat\":\"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\",\"compressed\":false,\"numBuckets\":-1,\"sortCols\":[],\"cols\":[{\"comment\":\"test\",\"name\":\"id\",\"type\":\"string\",\"setName\":true,\"setType\":true,\"setComment\":true},{\"comment\":null,\"name\":\"name\",\"type\":\"string\",\"setName\":true,\"setType\":true,\"setComment\":false}],\"colsSize\":2,\"serdeInfo\":{\"setSerializationLib\":true,\"name\":null,\"parameters\":{\"serialization.format\":\"1\"},\"serializationLib\":\"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\",\"parametersSize\":1,\"setParameters\":true,\"setName\":false},\"skewedInfo\":{\"skewedColNamesSize\":0,\"skewedColNamesIterator\":[],\"setSkewedColNames\":true,\"skewedColValuesSize\":0,\"skewedColValuesIterator\":[],\"setSkewedColValues\":true,\"skewedColValueLocationMapsSize\":0,\"setSkewedColValueLocationMaps\":true,\"skewedColValueLocationMaps\":{},\"skewedColNames\":[],\"skewedColValues\":[]},\"bucketCols\":[],\"parametersSize\":0,\"setParameters\":true,\"colsIterator\":[{\"comment\":\"test\",\"name\":\"id\",\"type\":\"string\",\"setName\":true,\"setType\":true,\"setComment\":true},{\"comment\":null,\"name\":\"name\",\"type\":\"string\",\"setName\":true,\"setType\":true,\"setComment\":false}],\"setCols\":true,\"setLocation\":true,\"setInputFormat\":true,\"setOutputFormat\":true,\"setCompressed\":true,\"setNumBuckets\":true,\"setSerdeInfo\":true,\"bucketColsSize\":0,\"bucketColsIterator\":[],\"setBucketCols\":true,\"sortColsSize\":0,\"sortColsIterator\":[],\"setSortCols\":true,\"setSkewedInfo\":true,\"storedAsSubDirectories\":false,\"setStoredAsSubDirectories\":true},\"temporary\":false,\"partitionKeys\":[],\"setTableName\":true,\"setOwner\":true,\"retention\":0,\"setRetention\":true,\"partitionKeysSize\":0,\"partitionKeysIterator\":[],\"setPartitionKeys\":true,\"viewOriginalText\":null,\"setViewOriginalText\":false,\"viewExpandedText\":null,\"setViewExpandedText\":false,\"setTableType\":true,\"setPrivileges\":false,\"setTemporary\":false,\"setOwnerType\":true,\"setDbName\":true,\"createTime\":1584893296,\"setCreateTime\":true,\"setLastAccessTime\":true,\"setSd\":true,\"parametersSize\":8,\"setParameters\":true}\n2020-03-28 14:19:12,935 INFO  com.bigdata.hive.MyHiveHook: [HiveServer2-Background-Pool: Thread-79]: [CustomHook][Thread: HiveServer2-Background-Pool: Thread-79] | Hook metadata output value: {\"lastAccessTime\":0,\"ownerType\":\"USER\",\"parameters\":{\"last_modified_time\":\"1585376257\",\"totalSize\":\"0\",\"numRows\":\"-1\",\"rawDataSize\":\"-1\",\"COLUMN_STATS_ACCURATE\":\"false\",\"numFiles\":\"0\",\"transient_lastDdlTime\":\"1585376257\",\"last_modified_by\":\"hive\"},\"owner\":\"hive\",\"tableName\":\"test1\",\"dbName\":\"default\",\"tableType\":\"MANAGED_TABLE\",\"privileges\":null,\"sd\":{\"location\":\"hdfs://master:8020/user/hive/warehouse/test\",\"parameters\":{},\"inputFormat\":\"org.apache.hadoop.mapred.TextInputFormat\",\"outputFormat\":\"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\",\"compressed\":false,\"numBuckets\":-1,\"sortCols\":[],\"cols\":[{\"comment\":\"test\",\"name\":\"id\",\"type\":\"string\",\"setName\":true,\"setType\":true,\"setComment\":true},{\"comment\":null,\"name\":\"name\",\"type\":\"string\",\"setName\":true,\"setType\":true,\"setComment\":false}],\"colsSize\":2,\"serdeInfo\":{\"setSerializationLib\":true,\"name\":null,\"parameters\":{\"serialization.format\":\"1\"},\"serializationLib\":\"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\",\"parametersSize\":1,\"setParameters\":true,\"setName\":false},\"skewedInfo\":{\"skewedColNamesSize\":0,\"skewedColNamesIterator\":[],\"setSkewedColNames\":true,\"skewedColValuesSize\":0,\"skewedColValuesIterator\":[],\"setSkewedColValues\":true,\"skewedColValueLocationMapsSize\":0,\"setSkewedColValueLocationMaps\":true,\"skewedColValueLocationMaps\":{},\"skewedColNames\":[],\"skewedColValues\":[]},\"bucketCols\":[],\"parametersSize\":0,\"setParameters\":true,\"colsIterator\":[{\"comment\":\"test\",\"name\":\"id\",\"type\":\"string\",\"setName\":true,\"setType\":true,\"setComment\":true},{\"comment\":null,\"name\":\"name\",\"type\":\"string\",\"setName\":true,\"setType\":true,\"setComment\":false}],\"setCols\":true,\"setLocation\":true,\"setInputFormat\":true,\"setOutputFormat\":true,\"setCompressed\":true,\"setNumBuckets\":true,\"setSerdeInfo\":true,\"bucketColsSize\":0,\"bucketColsIterator\":[],\"setBucketCols\":true,\"sortColsSize\":0,\"sortColsIterator\":[],\"setSortCols\":true,\"setSkewedInfo\":true,\"storedAsSubDirectories\":false,\"setStoredAsSubDirectories\":true},\"temporary\":false,\"partitionKeys\":[],\"setTableName\":true,\"setOwner\":true,\"retention\":0,\"setRetention\":true,\"partitionKeysSize\":0,\"partitionKeysIterator\":[],\"setPartitionKeys\":true,\"viewOriginalText\":null,\"setViewOriginalText\":false,\"viewExpandedText\":null,\"setViewExpandedText\":false,\"setTableType\":true,\"setPrivileges\":false,\"setTemporary\":false,\"setOwnerType\":true,\"setDbName\":true,\"createTime\":1584893296,\"setCreateTime\":true,\"setLastAccessTime\":true,\"setSd\":true,\"parametersSize\":8,\"setParameters\":true}\n\n```\n\n&nbsp;\n\n**2.HiveSemanticAnalyzerHook接口**\n\n**参考：https://www.iteye.com/blog/crazymatrix-2092830**\n\n实现HiveSemanticAnalyzerHook接口可以在hive执行语法分析前后插入hook函数，即preAnalyze和postAnalyze\n\n有时，用户写的sql会带有上下文，比如select * from test，这时test这张表会属于用户当前session中的某个库，比如use default，可以使用\n\n```\nprivate final SessionState ss = SessionState.get();\n\n```\n\n&nbsp;来获得当前用户session中的库\n\n```\npackage com.bigdata.hive;\n\nimport java.io.Serializable;\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport org.apache.commons.lang.StringUtils;\nimport org.apache.hadoop.hive.metastore.api.FieldSchema;\nimport org.apache.hadoop.hive.ql.exec.Task;\nimport org.apache.hadoop.hive.ql.metadata.Hive;\nimport org.apache.hadoop.hive.ql.metadata.HiveException;\nimport org.apache.hadoop.hive.ql.metadata.Table;\nimport org.apache.hadoop.hive.ql.parse.*;\nimport org.apache.hadoop.hive.ql.session.SessionState;\nimport org.apache.hadoop.hive.ql.session.SessionState.LogHelper;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\npublic class MyHiveHook2 implements HiveSemanticAnalyzerHook {\n\n    private final static String NO_PARTITION_WARNING = \"WARNING: HQL is not efficient, Please specify partition condition! HQL:%s ;USERNAME:%s\";\n\n    private final SessionState ss = SessionState.get();\n    private final LogHelper console = SessionState.getConsole();\n    private Hive hive = null;\n    private String username;\n    private String currentDatabase = \"default\";\n    private String hql;\n    private String whereHql;\n    private String tableAlias;\n    private String tableName;\n    private String tableDatabaseName;\n    private Boolean needCheckPartition = false;\n\n    private static Logger logger = LoggerFactory.getLogger(MyHiveHook2.class);\n\n\n    @Override\n    public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context, ASTNode ast) throws SemanticException {\n        try {\n            logger.info(context.getCommand()); // 当前query\n            logger.info(ss.getUserName());  // 当前user\n            hql = StringUtils.replaceChars(context.getCommand(), '\\n', ' ');\n            logger.info(\"hql: \" + hql);\n            if (hql.contains(\"where\")) {\n                whereHql = hql.substring(hql.indexOf(\"where\"));\n            }\n            \n            username = context.getUserName();\n            logger.info(ast.getToken().getText()); // TOK_QUERY\n            logger.info(String.valueOf(ast.getToken().getType())); // token code\n            logger.info(\"\" + (ast.getToken().getType() == HiveParser.TOK_QUERY));\n            if (ast.getToken().getType() == HiveParser.TOK_QUERY) {\n                try {\n                    hive = context.getHive();\n\n                    currentDatabase = hive.getDatabaseCurrent().getName();\n                    logger.info(\"current database: \" + currentDatabase); // session db\n                } catch (HiveException e) {\n                    throw new SemanticException(e);\n                }\n\n                extractFromClause((ASTNode) ast.getChild(0));\n\n                String dbname = StringUtils.isEmpty(tableDatabaseName) ? currentDatabase : tableDatabaseName;\n                String tbname = tableName;\n\n                String[] parts = tableName.split(\".\");\n                if (parts.length == 2) {\n                    dbname = parts[0];\n                    tbname = parts[1];\n                }\n                logger.info(\"this is hive database name: \" + dbname); // current db\n                logger.info(\"this is hive table name: \" + tbname);  // current table\n                Table t = hive.getTable(dbname, tbname);\n                if (t.isPartitioned()) {\n                    if (StringUtils.isBlank(whereHql)) {\n                        console.printError(String.format(NO_PARTITION_WARNING, hql, username));\n                    } else {\n                        List<FieldSchema> partitionKeys = t.getPartitionKeys();\n                        List<String> partitionNames = new ArrayList<String>();\n                        for (int i = 0; i < partitionKeys.size(); i++) {\n                            partitionNames.add(partitionKeys.get(i).getName().toLowerCase());\n                        }\n\n                        if (!containsPartCond(partitionNames, whereHql, tableAlias)) {\n                            console.printError(String.format(NO_PARTITION_WARNING, hql, username));\n                        }\n                    }\n                }\n\n\n            }\n        } catch (Exception ex) {\n            logger.info(\"error: \", ex);\n        }\n        return ast;\n    }\n\n    private boolean containsPartCond(List<String> partitionKeys, String sql, String alias) {\n        for (String pk : partitionKeys) {\n            if (sql.contains(pk)) {\n                return true;\n            }\n            if (!StringUtils.isEmpty(alias) &amp;&amp; sql.contains(alias + \".\" + pk)) {\n                return true;\n            }\n        }\n        return false;\n    }\n\n    private void extractFromClause(ASTNode ast) {\n        if (HiveParser.TOK_FROM == ast.getToken().getType()) {\n            ASTNode refNode = (ASTNode) ast.getChild(0);\n            if (refNode.getToken().getType() == HiveParser.TOK_TABREF &amp;&amp; ast.getChildCount() == 1) {\n                ASTNode tabNameNode = (ASTNode) (refNode.getChild(0));\n                int refNodeChildCount = refNode.getChildCount();\n                if (tabNameNode.getToken().getType() == HiveParser.TOK_TABNAME) {\n                    if (tabNameNode.getChildCount() == 2) {\n                        tableDatabaseName = tabNameNode.getChild(0).getText().toLowerCase();\n                        tableName = BaseSemanticAnalyzer.getUnescapedName((ASTNode) tabNameNode.getChild(1))\n                                .toLowerCase();\n                    } else if (tabNameNode.getChildCount() == 1) {\n                        tableName = BaseSemanticAnalyzer.getUnescapedName((ASTNode) tabNameNode.getChild(0))\n                                .toLowerCase();\n                    } else {\n                        return;\n                    }\n\n                    if (refNodeChildCount == 2) {\n                        tableAlias = BaseSemanticAnalyzer.unescapeIdentifier(refNode.getChild(1).getText())\n                                .toLowerCase();\n                    }\n                    needCheckPartition = true;\n                }\n            }\n        }\n    }\n\n    @Override\n    public void postAnalyze(HiveSemanticAnalyzerHookContext context,\n                            List<Task<? extends Serializable>> rootTasks) throws SemanticException {\n        logger.info(context.getCommand());\n\n    }\n\n}\n\n```\n\n&nbsp;输出是\n\n```\n2020-04-01 00:41:41,485 INFO  org.apache.hadoop.hive.ql.Driver: [HiveServer2-Handler-Pool: Thread-39]: Compiling command(queryId=hive_20200401004141_bf4c578a-6872-4947-8396-7c11b0530539): select * from test\n2020-04-01 00:41:41,486 INFO  org.apache.hadoop.hive.ql.log.PerfLogger: [HiveServer2-Handler-Pool: Thread-39]: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>\n2020-04-01 00:41:41,501 INFO  org.apache.hadoop.hive.ql.log.PerfLogger: [HiveServer2-Handler-Pool: Thread-39]: </PERFLOG method=parse start=1585672901486 end=1585672901501 duration=15 from=org.apache.hadoop.hive.ql.Driver>\n2020-04-01 00:41:41,502 INFO  org.apache.hadoop.hive.ql.log.PerfLogger: [HiveServer2-Handler-Pool: Thread-39]: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>\n2020-04-01 00:41:41,550 INFO  com.bigdata.hive.MyHiveHook2: [HiveServer2-Handler-Pool: Thread-39]: select * from test\n2020-04-01 00:41:41,551 INFO  com.bigdata.hive.MyHiveHook2: [HiveServer2-Handler-Pool: Thread-39]: hive\n2020-04-01 00:41:41,551 INFO  com.bigdata.hive.MyHiveHook2: [HiveServer2-Handler-Pool: Thread-39]: hql: select * from test\n2020-04-01 00:41:41,551 INFO  com.bigdata.hive.MyHiveHook2: [HiveServer2-Handler-Pool: Thread-39]: TOK_QUERY\n2020-04-01 00:41:41,551 INFO  com.bigdata.hive.MyHiveHook2: [HiveServer2-Handler-Pool: Thread-39]: 789\n2020-04-01 00:41:41,551 INFO  com.bigdata.hive.MyHiveHook2: [HiveServer2-Handler-Pool: Thread-39]: true\n2020-04-01 00:41:41,561 INFO  com.bigdata.hive.MyHiveHook2: [HiveServer2-Handler-Pool: Thread-39]: current database: default\n2020-04-01 00:41:41,561 INFO  com.bigdata.hive.MyHiveHook2: [HiveServer2-Handler-Pool: Thread-39]: this is hive database name: default\n2020-04-01 00:41:41,561 INFO  com.bigdata.hive.MyHiveHook2: [HiveServer2-Handler-Pool: Thread-39]: this is hive table name: test\n2020-04-01 00:41:41,621 INFO  org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: [HiveServer2-Handler-Pool: Thread-39]: Starting Semantic Analysis\n2020-04-01 00:41:41,623 INFO  org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: [HiveServer2-Handler-Pool: Thread-39]: Completed phase 1 of Semantic Analysis\n2020-04-01 00:41:41,623 INFO  org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: [HiveServer2-Handler-Pool: Thread-39]: Get metadata for source tables\n2020-04-01 00:41:41,648 INFO  org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: [HiveServer2-Handler-Pool: Thread-39]: Get metadata for subqueries\n2020-04-01 00:41:41,656 INFO  org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: [HiveServer2-Handler-Pool: Thread-39]: Get metadata for destination tables\n2020-04-01 00:41:41,708 INFO  hive.ql.Context: [HiveServer2-Handler-Pool: Thread-39]: New scratch dir is hdfs://master:8020/tmp/hive/hive/3fc884ec-0f81-49e7-ae87-45ce85ca4139/hive_2020-04-01_00-41-41_485_2813780198360550808-1\n2020-04-01 00:41:41,710 INFO  org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: [HiveServer2-Handler-Pool: Thread-39]: Completed getting MetaData in Semantic Analysis\n\n```\n\n&nbsp;\n","tags":["Hive"]},{"title":"Flink学习笔记——读写kafka","url":"/Flink学习笔记——读写kafka.html","content":"Flink的**kafka connector**文档\n\n```\nhttps://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/kafka.html\n\n```\n\nFlink写入kafka时候需要实现**序列化**和**反序列化**\n\n部分代码参考了\n\n```\nhttps://github.com/apache/flink/blob/master/flink-end-to-end-tests/flink-streaming-kafka-test/src/main/java/org/apache/flink/streaming/kafka/test/KafkaExample.java\n\n```\n\n以及\n\n```\nhttps://juejin.im/post/5d844d11e51d4561e0516bbd\nhttps://developer.aliyun.com/article/686809\n\n```\n\n**1.依赖**，其中\n\n**flink-java**提供了flink的java api，包括dataset执行环境，format，一些算子\n\n```\nhttps://github.com/apache/flink/tree/master/flink-java/src/main/java/org/apache/flink/api/java\n\n```\n\n**flink-streaming-java**提供了flink的java streaming api，包括stream执行环境，一些算子\n\n```\nhttps://github.com/apache/flink/tree/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api\n\n```\n\n**flink-connector-kafka**提供了kafka的连接器\n\n```\nhttps://github.com/apache/flink/tree/master/flink-connectors/flink-connector-kafka\n\n```\n\n## **1.pom文件依赖**\n\n```\n        <!-- log4j -->\n        <dependency>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n            <version>1.7.7</version>\n            <scope>runtime</scope>\n        </dependency>\n        <dependency>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n            <version>1.2.17</version>\n            <scope>runtime</scope>\n        </dependency>\n        <!--flink-->\n        <dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-java</artifactId>\n            <version>1.10.0</version>\n            <exclusions>\n                <exclusion>\n                    <groupId>org.slf4j</groupId>\n                    <artifactId>slf4j-log4j12</artifactId>\n                </exclusion>\n                <exclusion>\n                    <groupId>log4j</groupId>\n                    <artifactId>*</artifactId>\n                </exclusion>\n            </exclusions>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-streaming-java_2.11</artifactId>\n            <version>1.10.0</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-connector-kafka_2.11</artifactId>\n            <version>1.10.0</version>\n        </dependency>\n\n```\n\n## **2.Kafka Consumer**\n\n作为kafka consumer有几个比较重要的配置参数\n\n<!--more-->\n&nbsp;\n\n### 2.1 消费kafka内容并打印\n\n这里选用SimpleStringSchema序列化方式，只会打印message\n\n```\n    public static void main(String[] args) throws Exception {\n\n        ParameterTool pt = ParameterTool.fromArgs(args);\n        if (pt.getNumberOfParameters() != 1) {\n            throw new Exception(\"Missing parameters!\\n\" +\n                    \"Usage: --conf-file <conf-file>\");\n        }\n        String confFile = pt.get(\"conf-file\");\n        pt = ParameterTool.fromPropertiesFile(confFile);\n\n        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        env.getConfig().setGlobalJobParameters(pt);\n\n\n        // 消费kafka\n        DataStream input = env\n                .addSource(\n                        new FlinkKafkaConsumer(\n                                pt.getProperties().getProperty(\"input.topic\"),\n                                new SimpleStringSchema(),\n                                pt.getProperties()\n                        )\n                );\n\n        // 打印\n        input.print();\n\n        env.execute(\"Kafka consumer Example\");\n    }\n\n```\n\nidea args配置\n\n```\n--conf-file ./conf/xxxxx.conf\n\n```\n\n<img src=\"/images/517519-20220109230437664-1994629053.png\" width=\"400\" height=\"157\" loading=\"lazy\" />\n\nxxxxx.conf内容\n\n```\n# kafka source config\nbootstrap.servers=master:9092\ninput.topic=test_topic\ngroup.id=test\n\n```\n\n往kafka的topic中灌入数据，控制台会打印出刚刚输入的数据\n\n<img src=\"/images/517519-20201218162530647-501452900.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n### 2.2&nbsp;消费kafka内容并打印\n\n如果想要在消费kafka的时候，得到除message之外的其他信息，比如这条消息的offset，topic，partition等，可以使用&nbsp;JSONKeyValueDeserializationSchema，JSONKeyValueDeserializationSchema将以json格式来反序列化byte数组\n\n使用&nbsp;**JSONKeyValueDeserializationSchema** 的时候需要保证输入kafka的数据是**json格式**的，否则会有报错\n\n```\nCaused by: org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'asdg': was expecting (JSON String, Number, Array, Object or token 'null', 'true' or 'false')\n\n```\n\n实现\n\n```\nFlinkKafkaConsumer<ConsumerRecord<String, String>> consumer = new FlinkKafkaConsumer(pt.getProperties().getProperty(\"input.topic\"),\n                new JSONKeyValueDeserializationSchema(false), pt.getProperties());\n\n```\n\n如果为false，输出\n\n```\n2> {\"value\":123123}\n\n```\n\n如果为true，输出\n\n```\n2> {\"value\":123123,\"metadata\":{\"offset\":18,\"topic\":\"xxxxx\",\"partition\":0}}\n\n```\n\nmap输出offset\n\n```\n        // 打印\n        input.map(new MapFunction<ObjectNode, String>() {\n            @Override\n            public String map(ObjectNode value) {\n                return value.get(\"metadata\").get(\"offset\").asText();\n            }\n        }).print();\n\n```\n\n　　\n\n如果还要获得其他信息，比如kafka消息的key，也可以自行实现&nbsp;KafkaDeserializationSchema，参考如下\n\n```\nhttps://blog.csdn.net/jsjsjs1789/article/details/105099742\nhttps://blog.csdn.net/weixin_40954192/article/details/107561435\n\n```\n\nkafka的key的用途有2个：一是作为消息的附加信息，二是可以用来决定消息应该写到kafka的哪个partition\n\n```\npublic class KafkaConsumerRecordDeserializationSchema implements KafkaDeserializationSchema<ConsumerRecord<String, String>> {\n\n    @Override\n    public boolean isEndOfStream(ConsumerRecord<String, String> nextElement) {\n        return false;\n    }\n\n    @Override\n    public ConsumerRecord<String, String> deserialize(ConsumerRecord<byte[], byte[]> record) throws Exception {\n        return new ConsumerRecord<String, String>(\n                record.topic(),\n                record.partition(),\n                record.offset(),\n                record.key() != null ? new String(record.key()) : null,\n                record.value() != null ? new String(record.value()) : null);\n    }\n\n    @Override\n    public TypeInformation<ConsumerRecord<String, String>> getProducedType() {\n        return TypeInformation.of(new TypeHint<ConsumerRecord<String, String>>() {\n        });\n    }\n}\n\n```\n\n然后\n\n```\nFlinkKafkaConsumer<ConsumerRecord<String, String>> consumer =\n                new FlinkKafkaConsumer<>(pt.getProperties().getProperty(\"input.topic\"), new KafkaConsumerRecordDeserializationSchema(), pt.getProperties());\n\n```\n\n输出　　\n\n```\n2> ConsumerRecord(topic = xxxx, partition = 0, leaderEpoch = null, offset = 19, NoTimestampType = -1, serialized key size = -1, serialized value size = -1, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = 1111111)\n\n```\n\n　　\n\n## **3.Kafka Producer**\n\nFlinkKafkaProducer有多个版本，参考：[你真的了解Flink Kafka source吗？](https://jiamaoxiang.top/2020/04/02/%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3Flink-Kafka-connector%E5%90%97%EF%BC%9F/)\n\n<img src=\"/images/517519-20210715172445932-1918956871.png\" width=\"600\" height=\"712\" loading=\"lazy\" />\n\nFlinkKafkaProducer可以参考\n\n```\nhttps://www.programcreek.com/java-api-examples/?api=org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer\n```\n\n### 3.1 从kafka的topic读取数据 ，之后写到另外一个kafka的topic中\n\n使用&nbsp;**SimpleStringSchema**，这里FlinkKafkaProducer会显示过期，但不影响功能\n\n```\n    public static void main(String[] args) throws Exception {\n\n        ParameterTool pt = ParameterTool.fromArgs(args);\n        if (pt.getNumberOfParameters() != 1) {\n            throw new Exception(\"Missing parameters!\\n\" +\n                    \"Usage: --conf-file <conf-file>\");\n        }\n        String confFile = pt.get(\"conf-file\");\n        pt = ParameterTool.fromPropertiesFile(confFile);\n\n        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        env.getConfig().setGlobalJobParameters(pt);\n        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n\n        FlinkKafkaConsumer<ConsumerRecord<String, String>> consumer = new FlinkKafkaConsumer(\n                pt.getProperties().getProperty(\"input.topic\"),\n                new SimpleStringSchema(),\n                pt.getProperties()\n        );\n\n        // 消费kafka\n        DataStream input = env.addSource(consumer);\n\n\n        // 打印\n        input.print();\n\n        // producer\n        FlinkKafkaProducer<String> producer = new FlinkKafkaProducer<>(\n                pt.getProperties().getProperty(\"output.topic\"),\n                new SimpleStringSchema(),\n                pt.getProperties()\n        );\n\n        // 往kafka中写数据\n        input.addSink(producer);\n\n        env.execute(\"Kafka consumer Example\");\n    }\n\n```\n\n配置文件\n\n```\n# kafka config\nbootstrap.servers=localhost:9092\ninput.topic=thrift_log_test\noutput.topic=test\ngroup.id=test\n\n```\n\n输出\n\n<img src=\"/images/517519-20201223200803899-1119910148.png\" alt=\"\" loading=\"lazy\" />\n\n如果想写到文件，可以setParallelism是控制输出的文件数量，1是写成1个文件，大于1会是文件夹下面的多个文件\n\n```\ninput.writeAsText(\"file:///Users/lintong/coding/java/flink-demo/conf/test.log\").setParallelism(1);\n\n```\n\n　　\n\n### 3.2 也可以自行实现&nbsp;KafkaSerializationSchema 接口来序列化string\n\n```\nimport org.apache.flink.streaming.connectors.kafka.KafkaSerializationSchema;\nimport org.apache.kafka.clients.producer.ProducerRecord;\n\nimport java.nio.charset.StandardCharsets;\n\npublic class KafkaProducerStringSerializationSchema implements KafkaSerializationSchema<String> {\n\n    private String topic;\n\n    public KafkaProducerStringSerializationSchema(String topic) {\n        super();\n        this.topic = topic;\n    }\n\n    @Override\n    public ProducerRecord<byte[], byte[]> serialize(String element, Long timestamp) {\n        return new ProducerRecord<>(topic, element.getBytes(StandardCharsets.UTF_8));\n    }\n\n}\n\n```\n\n然后\n\n```\n    public static void main(String[] args) throws Exception {\n\n        ParameterTool pt = ParameterTool.fromArgs(args);\n        if (pt.getNumberOfParameters() != 1) {\n            throw new Exception(\"Missing parameters!\\n\" +\n                    \"Usage: --conf-file <conf-file>\");\n        }\n        String confFile = pt.get(\"conf-file\");\n        pt = ParameterTool.fromPropertiesFile(confFile);\n\n        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        env.getConfig().setGlobalJobParameters(pt);\n        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n\n        // consumer\n        FlinkKafkaConsumer<ConsumerRecord<String, String>> consumer = new FlinkKafkaConsumer(\n                pt.getProperties().getProperty(\"input.topic\"),\n                new SimpleStringSchema(),\n                pt.getProperties()\n        );\n\n        // 消费kafka\n        DataStream input = env.addSource(consumer);\n\n        // 打印\n        input.print();\n\n        // producer\n        FlinkKafkaProducer<String> producer = new FlinkKafkaProducer<>(\n                pt.getProperties().getProperty(\"output.topic\"),\n                new KafkaProducerStringSerializationSchema(pt.getProperties().getProperty(\"output.topic\")),\n                pt.getProperties(),\n                FlinkKafkaProducer.Semantic.EXACTLY_ONCE\n        );\n\n        // 往kafka中写数据\n        input.addSink(producer);\n\n        env.execute(\"Kafka consumer Example\");\n    }\n\n```\n\n　　\n\n### 3.3 使用&nbsp;ProducerRecord<String, String> 序列化\n\n```\npublic class KafkaProducerRecordSerializationSchema implements KafkaSerializationSchema<ProducerRecord<String, String>> {\n\n    @Override\n    public ProducerRecord<byte[], byte[]> serialize(ProducerRecord<String, String> element, Long timestamp) {\n        return new ProducerRecord<>(element.topic(), element.value().getBytes(StandardCharsets.UTF_8));\n    }\n\n}\n\n```\n\n然后\n\n```\n    public static void main(String[] args) throws Exception {\n\n        ParameterTool pt = ParameterTool.fromArgs(args);\n        if (pt.getNumberOfParameters() != 1) {\n            throw new Exception(\"Missing parameters!\\n\" +\n                    \"Usage: --conf-file <conf-file>\");\n        }\n        String confFile = pt.get(\"conf-file\");\n        pt = ParameterTool.fromPropertiesFile(confFile);\n\n        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        env.getConfig().setGlobalJobParameters(pt);\n        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n\n        // consumer\n        FlinkKafkaConsumer<ConsumerRecord<String, String>> consumer = new FlinkKafkaConsumer<>(\n                pt.getProperties().getProperty(\"input.topic\"),\n                new KafkaConsumerRecordDeserializationSchema(),\n                pt.getProperties()\n        );\n\n        // 消费kafka\n        DataStream input = env.addSource(consumer);\n\n        String outputTopic = pt.getProperties().getProperty(\"output.topic\");\n\n        // 转换\n        DataStream output = input.map(new MapFunction<ConsumerRecord<String, String>, ProducerRecord<String, String>>() {\n            @Override\n            public ProducerRecord<String, String> map(ConsumerRecord<String, String> value) throws Exception {\n                ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outputTopic, value.value());\n                return producerRecord;\n            }\n        });\n\n        FlinkKafkaProducer<ProducerRecord<String, String>> producer = new FlinkKafkaProducer<>(\n                pt.getProperties().getProperty(\"output.topic\"),\n                new KafkaProducerRecordSerializationSchema(),\n                pt.getProperties(),\n                FlinkKafkaProducer.Semantic.EXACTLY_ONCE\n        );\n\n        // 往kafka中写数据\n        output.addSink(producer);\n\n        output.print();\n\n        env.execute(\"Kafka consumer Example\");\n    }\n\n```\n\n　　\n\n也可以参考官方文档\n\n```\nhttps://github.com/apache/flink/blob/master/flink-end-to-end-tests/flink-streaming-kafka-test/src/main/java/org/apache/flink/streaming/kafka/test/KafkaExample.java\n\n```\n\n代码\n\n```\npackage com.bigdata.flink;\n\nimport org.apache.flink.api.common.restartstrategy.RestartStrategies;\nimport org.apache.flink.api.java.utils.ParameterTool;\nimport org.apache.flink.streaming.api.TimeCharacteristic;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n\npublic class KafkaExampleUtil {\n\n    public static StreamExecutionEnvironment prepareExecutionEnv(ParameterTool parameterTool)\n            throws Exception {\n\n        if (parameterTool.getNumberOfParameters() < 5) {\n            System.out.println(\"Missing parameters!\\n\" +\n                    \"Usage: Kafka --input-topic <topic> --output-topic <topic> \" +\n                    \"--bootstrap.servers <kafka brokers> \" +\n                    \"--zookeeper.connect <zk quorum> --group.id <some id>\");\n            throw new Exception(\"Missing parameters!\\n\" +\n                    \"Usage: Kafka --input-topic <topic> --output-topic <topic> \" +\n                    \"--bootstrap.servers <kafka brokers> \" +\n                    \"--zookeeper.connect <zk quorum> --group.id <some id>\");\n        }\n\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        env.getConfig().setRestartStrategy(RestartStrategies.fixedDelayRestart(4, 10000));\n        env.enableCheckpointing(5000); // create a checkpoint every 5 seconds\n        env.getConfig().setGlobalJobParameters(parameterTool); // make parameters available in the web interface\n        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n\n        return env;\n    }\n\n\n}\n\n```\n\n代码\n\n```\npackage com.bigdata.flink;\n\nimport org.apache.flink.api.common.serialization.SimpleStringSchema;\nimport org.apache.flink.api.java.utils.ParameterTool;\nimport org.apache.flink.streaming.api.datastream.DataStream;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\nimport org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\nimport org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n\npublic class KafkaExample {\n\n    public static void main(String[] args) throws Exception {\n        // parse input arguments\n        final ParameterTool parameterTool = ParameterTool.fromArgs(args);\n        StreamExecutionEnvironment env = KafkaExampleUtil.prepareExecutionEnv(parameterTool);\n\n        // 消费kafka\n        DataStream input = env\n                .addSource(new FlinkKafkaConsumer(\"test_topic\", new SimpleStringSchema(), parameterTool.getProperties()));\n\n        // 打印\n        input.print();\n\n        // 往kafka中写数据\n        FlinkKafkaProducer<String> myProducer = new FlinkKafkaProducer<>(\n                parameterTool.getProperties().getProperty(\"bootstrap.servers\"),            // broker list\n                \"test_source\",                  // target topic\n                new SimpleStringSchema());   // serialization schema\n\n        input.map(line -> line + \"test\").addSink(myProducer);\n\n        env.execute(\"Modern Kafka Example\");\n    }\n\n}\n\n```\n\n配置\n\n```\n--input-topic test_topic --output-topic test_source --bootstrap.servers master:9092 --zookeeper.connect master:2181 --group.id test_group\n\n```\n\n往kafka topic中放数据\n\n```\n/opt/cloudera/parcels/KAFKA/bin/kafka-console-producer --broker-list master:9092 --topic test_topic\n\n```\n\n输出\n\n<img src=\"/images/517519-20200315152259900-1724316756.png\" alt=\"\" width=\"600\" height=\"129\" />\n\n消费flink程序写入的topic\n\n```\n/opt/cloudera/parcels/KAFKA/bin/kafka-console-consumer --bootstrap-server master:9092 --topic test_source\n\n```\n\n输出\n\n<img src=\"/images/517519-20200315224503265-985486934.png\" alt=\"\" width=\"700\" height=\"105\" />\n\n&nbsp;\n\n## **4.数据重复问题**\n\nFlink自带Exactly Once语义，对于支持事务的存储，可以实现数据的不重不丢。Kafka在**0.11.0**版本的时候，支持了事务，参考：[【干货】Kafka 事务特性分析](https://zhuanlan.zhihu.com/p/42046847)\n\n&nbsp;\n\n要使用Flink实现Exactly Once，需要注意，参考：[Flink exactly-once 实战笔记](https://www.huaweicloud.com/articles/1b84394b589e493b4e3d8af1d567e40f.html)\n\n1. kafka的Producer写入数据的时候需要通过事务来写入，即使用Exactly-once语义的FlinkKafkaProducer；\n\n2. 是kafka的consumer消费的时候，需要给消费者加上参数isolation.level=read_committed来保证未commit的消息对消费者不可见\n\n&nbsp;\n\nKafka端到端一致性需要注意的点，参考：[Flink Kafka端到端精准一致性测试](https://zhuanlan.zhihu.com/p/272087368)\n\n1. Flink任务需要开启checkpoint配置为CheckpointingMode.**EXACTLY_ONCE**\n\n2. Flink任务FlinkKafkaProducer需要指定参数Semantic.EXACTLY_ONCE\n\n3. Flink任务FlinkKafkaProducer配置需要配置transaction.timeout.ms,checkpoint间隔(代码指定)<transaction.timeout.ms(默认为1小时)<transaction.max.timeout.ms(默认为15分钟)\n\n4. 消费端在消费FlinkKafkaProducer的topic时需要指定isolation.level(默认为read_uncommitted)为read_committed\n\n&nbsp;\n\n关于flink读写kafka&nbsp;Exactly Once的最佳实践，参考：[Best Practices for Using Kafka Sources/Sinks in Flink Jobs](https://ververica.zendesk.com/hc/en-us/articles/360013269680-Best-Practices-for-Using-Kafka-Sources-Sinks-in-Flink-Jobs)\n\n1. 在FlinkKafkaProducer开启了Semantic.EXACTLY_ONCE之后，如果遇到一下报错\n\n```\nUnexpected error in InitProducerIdResponse; The transaction timeout is larger than the maximum value allowed by the broker (as configured by max.transaction.timeout.ms).\n\n```\n\n则需要调小Producer的transaction.timeout.ms参数，其默认值为1 hour，比如调整成\n\n```\ntransaction.timeout.ms=300000\n\n```\n\n2. 开启Semantic.EXACTLY_ONCE之后，需要保证transactional.id是唯一的\n\n3. 设置做checkpoint的间隔时间，比如\n\n```\nStreamExecutionEnvironment env = ...;\nenv.enableCheckpointing(1000); // unit is millisecond\n\n```\n\n4. 并发checkpoint，默认的FlinkKafkaProducer有一个5个KafkaProducers的线程池，支持并发做4个checkpoint\n\n5. 需要注意kafka connect的版本\n\n6. 当kafka集群不可用的时候，避免刷日志\n\n```\nmin.insync.replicas<br />reconnect.backoff.max.ms\nreconnect.backoff.ms\n\n```\n\n　　\n\n关于kafka的事务机制和read_committed，参考：[Kafka Exactly-Once 之事务性实现](http://matt33.com/2018/11/04/kafka-transaction/#Server-%E5%A4%84%E7%90%86-read-committed-%E7%B1%BB%E5%9E%8B%E7%9A%84-Fetch-%E8%AF%B7%E6%B1%82)\n\n&nbsp;\n\n## **5.flink读写kafka端到端exactly once**\n\n```\nsocket stream 写入数据 -> flink读取socket流式数据 -> 事务写kafka -> flink使用isolation.level=read_committed来消费kafka数据 -> console打印数据\n\n```\n\n由于使用了checkpoint机制，在消费kafka的时候，只有当flink周期性做checkpoint成功后，才会提交offset；如果当flink任务挂掉的时候，对于未提交事务的消息，消费者是不可见的\n\n&nbsp;\n","tags":["Flink"]},{"title":"Flink学习笔记——scala shell","url":"/Flink学习笔记——scala shell.html","content":"Flink也和和spark-shell类似的交互式开发模式\n\n```\nbin/start-scala-shell.sh yarn\nStarting Flink Shell:\n20/03/14 14:34:07 INFO configuration.GlobalConfiguration: Loading configuration property: jobmanager.rpc.address, localhost\n20/03/14 14:34:07 INFO configuration.GlobalConfiguration: Loading configuration property: jobmanager.rpc.port, 6123\n20/03/14 14:34:07 INFO configuration.GlobalConfiguration: Loading configuration property: jobmanager.heap.size, 1024m\n20/03/14 14:34:07 INFO configuration.GlobalConfiguration: Loading configuration property: taskmanager.memory.process.size, 1568m\n20/03/14 14:34:07 INFO configuration.GlobalConfiguration: Loading configuration property: taskmanager.numberOfTaskSlots, 1\n20/03/14 14:34:07 INFO configuration.GlobalConfiguration: Loading configuration property: parallelism.default, 1\n20/03/14 14:34:07 INFO configuration.GlobalConfiguration: Loading configuration property: jobmanager.execution.failover-strategy, region\n20/03/14 14:34:07 INFO cli.FlinkYarnSessionCli: Found Yarn properties file under /tmp/.yarn-properties-lintong.\n20/03/14 14:34:07 WARN cli.FlinkYarnSessionCli: The configuration directory ('/home/lintong/software/apache/flink-1.10.0/conf') already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.\n\nConnecting to Flink cluster (host: localhost, port: 6123).\n\n\n                         ▒▓██▓██▒\n                     ▓████▒▒█▓▒▓███▓▒\n                  ▓███▓░░        ▒▒▒▓██▒  ▒\n                ░██▒   ▒▒▓▓█▓▓▒░      ▒████\n                ██▒         ░▒▓███▒    ▒█▒█▒\n                  ░▓█            ███   ▓░▒██\n                    ▓█       ▒▒▒▒▒▓██▓░▒░▓▓█\n                  █░ █   ▒▒░       ███▓▓█ ▒█▒▒▒\n                  ████░   ▒▓█▓      ██▒▒▒ ▓███▒\n               ░▒█▓▓██       ▓█▒    ▓█▒▓██▓ ░█░\n         ▓░▒▓████▒ ██         ▒█    █▓░▒█▒░▒█▒\n        ███▓░██▓  ▓█           █   █▓ ▒▓█▓▓█▒\n      ░██▓  ░█░            █  █▒ ▒█████▓▒ ██▓░▒\n     ███░ ░ █░          ▓ ░█ █████▒░░    ░█░▓  ▓░\n    ██▓█ ▒▒▓▒          ▓███████▓░       ▒█▒ ▒▓ ▓██▓\n ▒██▓ ▓█ █▓█       ░▒█████▓▓▒░         ██▒▒  █ ▒  ▓█▒\n ▓█▓  ▓█ ██▓ ░▓▓▓▓▓▓▓▒              ▒██▓           ░█▒\n ▓█    █ ▓███▓▒░              ░▓▓▓███▓          ░▒░ ▓█\n ██▓    ██▒    ░▒▓▓███▓▓▓▓▓██████▓▒            ▓███  █\n▓███▒ ███   ░▓▓▒░░   ░▓████▓░                  ░▒▓▒  █▓\n█▓▒▒▓▓██  ░▒▒░░░▒▒▒▒▓██▓░                            █▓\n██ ▓░▒█   ▓▓▓▓▒░░  ▒█▓       ▒▓▓██▓    ▓▒          ▒▒▓\n▓█▓ ▓▒█  █▓░  ░▒▓▓██▒            ░▓█▒   ▒▒▒░▒▒▓█████▒\n ██░ ▓█▒█▒  ▒▓▓▒  ▓█                █░      ░░░░   ░█▒\n ▓█   ▒█▓   ░     █░                ▒█              █▓\n  █▓   ██         █░                 ▓▓        ▒█▓▓▓▒█░\n   █▓ ░▓██░       ▓▒                  ▓█▓▒░░░▒▓█░    ▒█\n    ██   ▓█▓░      ▒                    ░▒█▒██▒      ▓▓\n     ▓█▒   ▒█▓▒░                         ▒▒ █▒█▓▒▒░░▒██\n      ░██▒    ▒▓▓▒                     ▓██▓▒█▒ ░▓▓▓▓▒█▓\n        ░▓██▒                          ▓░  ▒█▓█  ░░▒▒▒\n            ▒▓▓▓▓▓▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒░░▓▓  ▓░▒█░\n\n              F L I N K - S C A L A - S H E L L\n\n```\n\n读文件\n\n```\nscala> val dataSet = benv.readTextFile(\"hdfs://master:8020/user/lintong/logs/test/test.log\")\n20/03/14 14:49:07 INFO configuration.GlobalConfiguration: Loading configuration property: jobmanager.rpc.address, localhost\n20/03/14 14:49:07 INFO configuration.GlobalConfiguration: Loading configuration property: jobmanager.rpc.port, 6123\n20/03/14 14:49:07 INFO configuration.GlobalConfiguration: Loading configuration property: jobmanager.heap.size, 1024m\n20/03/14 14:49:07 INFO configuration.GlobalConfiguration: Loading configuration property: taskmanager.memory.process.size, 1568m\n20/03/14 14:49:07 INFO configuration.GlobalConfiguration: Loading configuration property: taskmanager.numberOfTaskSlots, 1\n20/03/14 14:49:07 INFO configuration.GlobalConfiguration: Loading configuration property: parallelism.default, 1\n20/03/14 14:49:07 INFO configuration.GlobalConfiguration: Loading configuration property: jobmanager.execution.failover-strategy, region\ndataSet: org.apache.flink.api.scala.DataSet[String] = org.apache.flink.api.scala.DataSet@13e5b262\n\n```\n\n打印\n\n```\nscala> dataSet.print()\n20/03/14 14:49:10 INFO java.ExecutionEnvironment: The job has 0 registered types and 0 default Kryo serializers\n20/03/14 14:49:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n20/03/14 14:49:11 INFO client.RMProxy: Connecting to ResourceManager at master/192.168.2.105:8032\n20/03/14 14:49:11 INFO yarn.YarnClusterDescriptor: No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar\n20/03/14 14:49:11 INFO yarn.YarnClusterDescriptor: Found Web Interface master:36441 of application 'application_1584163852090_0002'.\n1\n2\n3\n4\n\n```\n\n退出\n\n```\nscala> :q\n good bye ..\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["Flink"]},{"title":"Flink学习笔记——SocketWindowWordCount","url":"/Flink学习笔记——SocketWindowWordCount.html","content":"参考Flink官方代码的example\n\n```\nhttps://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/socket/SocketWindowWordCount.java\n\n```\n\n<!--more-->\n&nbsp;\n\n引入pom\n\n```\n        <!--flink-->\n        <dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-java</artifactId>\n            <version>1.8.0</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-streaming-java_2.11</artifactId>\n            <version>1.8.0</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-scala_2.11</artifactId>\n            <version>1.8.0</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-streaming-scala_2.11</artifactId>\n            <version>1.8.0</version>\n        </dependency>\n\n```\n\n代码\n\n```\npackage com.xxx.xx.flink;\n\n\nimport org.apache.flink.api.common.functions.FlatMapFunction;\nimport org.apache.flink.api.common.functions.ReduceFunction;\nimport org.apache.flink.api.java.utils.ParameterTool;\nimport org.apache.flink.streaming.api.datastream.DataStream;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\nimport org.apache.flink.streaming.api.windowing.time.Time;\nimport org.apache.flink.util.Collector;\n\npublic class SocketWindowWordCount {\n\n    /**\n     * Data type for words with count.\n     */\n    public static class WordWithCount {\n\n        public String word;\n        public long count;\n\n        public WordWithCount() {\n        }\n\n        public WordWithCount(String word, long count) {\n            this.word = word;\n            this.count = count;\n        }\n\n        @Override\n        public String toString() {\n            return word + \" : \" + count;\n        }\n    }\n\n    public static void main(String[] args) throws Exception {\n        // the host and the port to connect to\n        final String hostname;\n        final int port;\n        try {\n            final ParameterTool params = ParameterTool.fromArgs(args);\n            hostname = params.has(\"hostname\") ? params.get(\"hostname\") : \"localhost\";\n            port = 9999;\n        } catch (Exception e) {\n            System.err.println(\"No port specified. Please run 'SocketWindowWordCount \" +\n                    \"--hostname <hostname> --port <port>', where hostname (localhost by default) \" +\n                    \"and port is the address of the text server\");\n            System.err.println(\"To start a simple text server, run 'netcat -l <port>' and \" +\n                    \"type the input text into the command line\");\n            return;\n        }\n\n        // get the execution environment\n        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n        // get input data by connecting to the socket\n        DataStream<String> text = env.socketTextStream(hostname, port, \"\\n\");\n\n        // parse the data, group it, window it, and aggregate the counts\n        DataStream<WordWithCount> windowCounts = text\n\n                .flatMap(new FlatMapFunction<String, WordWithCount>() {\n                    @Override\n                    public void flatMap(String value, Collector<WordWithCount> out) {\n                        for (String word : value.split(\"\\\\s\")) {\n                            out.collect(new WordWithCount(word, 1L));\n                        }\n                    }\n                })\n\n                .keyBy(\"word\")\n                .timeWindow(Time.seconds(5))\n\n                .reduce(new ReduceFunction<WordWithCount>() {\n                    @Override\n                    public WordWithCount reduce(WordWithCount a, WordWithCount b) {\n                        return new WordWithCount(a.word, a.count + b.count);\n                    }\n                });\n\n        // print the results with a single thread, rather than in parallel\n        windowCounts.print().setParallelism(1);\n\n        env.execute(\"Socket Window WordCount\");\n\n    }\n\n}\n\n```\n\n运行\n\n```\nnc -lk 9999\n1 2 3 4\n\n```\n\n结果\n\n<img src=\"/images/517519-20200313151415208-781098157.png\" alt=\"\" width=\"1200\" height=\"212\" />\n\n&nbsp;\n","tags":["Flink"]},{"title":"Flink学习笔记——WordCount","url":"/Flink学习笔记——WordCount.html","content":"参考Flink官方example\n\n```\nhttps://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/wordcount/WordCount.java\n\n```\n\npom\n\n```\n        <!--flink-->\n        <dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-java</artifactId>\n            <version>1.8.0</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-streaming-java_2.11</artifactId>\n            <version>1.8.0</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-scala_2.11</artifactId>\n            <version>1.8.0</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-streaming-scala_2.11</artifactId>\n            <version>1.8.0</version>\n        </dependency>\n\n```\n\n代码\n\n```\npackage com.xxx.xx.flink;\n\nimport org.apache.flink.api.common.functions.FlatMapFunction;\nimport org.apache.flink.api.java.tuple.Tuple2;\nimport org.apache.flink.api.java.utils.ParameterTool;\nimport org.apache.flink.streaming.api.datastream.DataStream;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\nimport org.apache.flink.util.Collector;\nimport org.apache.flink.util.Preconditions;\n\n/**\n * Created by lintong on 20-3-13.\n */\npublic class WordCount {\n\n    public static class WordCountData {\n\n        public static final String[] WORDS = new String[]{\n                \"To be, or not to be,--that is the question:--\",\n                \"Whether 'tis nobler in the mind to suffer\",\n                \"The slings and arrows of outrageous fortune\",\n                \"Or to take arms against a sea of troubles,\",\n                \"And by opposing end them?--To die,--to sleep,--\",\n                \"No more; and by a sleep to say we end\",\n                \"The heartache, and the thousand natural shocks\",\n                \"That flesh is heir to,--'tis a consummation\",\n                \"Devoutly to be wish'd. To die,--to sleep;--\",\n                \"To sleep! perchance to dream:--ay, there's the rub;\",\n                \"For in that sleep of death what dreams may come,\",\n                \"When we have shuffled off this mortal coil,\",\n                \"Must give us pause: there's the respect\",\n                \"That makes calamity of so long life;\",\n                \"For who would bear the whips and scorns of time,\",\n                \"The oppressor's wrong, the proud man's contumely,\",\n                \"The pangs of despis'd love, the law's delay,\",\n                \"The insolence of office, and the spurns\",\n                \"That patient merit of the unworthy takes,\",\n                \"When he himself might his quietus make\",\n                \"With a bare bodkin? who would these fardels bear,\",\n                \"To grunt and sweat under a weary life,\",\n                \"But that the dread of something after death,--\",\n                \"The undiscover'd country, from whose bourn\",\n                \"No traveller returns,--puzzles the will,\",\n                \"And makes us rather bear those ills we have\",\n                \"Than fly to others that we know not of?\",\n                \"Thus conscience does make cowards of us all;\",\n                \"And thus the native hue of resolution\",\n                \"Is sicklied o'er with the pale cast of thought;\",\n                \"And enterprises of great pith and moment,\",\n                \"With this regard, their currents turn awry,\",\n                \"And lose the name of action.--Soft you now!\",\n                \"The fair Ophelia!--Nymph, in thy orisons\",\n                \"Be all my sins remember'd.\"\n        };\n    }\n\n    // *************************************************************************\n    // PROGRAM\n    // *************************************************************************\n\n    public static void main(String[] args) throws Exception {\n\n        final ParameterTool params = ParameterTool.fromArgs(args);\n\n        // set up the execution environment\n        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n        // make parameters available in the web interface\n        env.getConfig().setGlobalJobParameters(params);\n\n        // get input data\n        DataStream<String> text = null;\n        if (params.has(\"input\")) {\n            text = env.readTextFile(params.get(\"input\"));\n            Preconditions.checkNotNull(text, \"Input DataStream should not be null.\");\n        } else {\n            System.out.println(\"Executing WordCount example with default input data set.\");\n            System.out.println(\"Use --input to specify file input.\");\n            // get default test text data\n            text = env.fromElements(WordCountData.WORDS);\n        }\n\n        DataStream<Tuple2<String, Integer>> counts =\n                // split up the lines in pairs (2-tuples) containing: (word,1)\n                text.flatMap(new Tokenizer())\n                        // group by the tuple field \"0\" and sum up tuple field \"1\"\n                        .keyBy(0).sum(1);\n\n        // emit result\n        if (params.has(\"output\")) {\n            counts.writeAsText(params.get(\"output\"));\n        } else {\n            System.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n            counts.print();\n        }\n        // execute program\n        env.execute(\"Streaming WordCount\");\n    }\n\n    // *************************************************************************\n    // USER FUNCTIONS\n    // *************************************************************************\n\n    /**\n     * Implements the string tokenizer that splits sentences into words as a\n     * user-defined FlatMapFunction. The function takes a line (String) and\n     * splits it into multiple pairs in the form of \"(word,1)\" ({@code Tuple2<String,\n     * Integer>}).\n     */\n    public static final class Tokenizer implements FlatMapFunction<String, Tuple2<String, Integer>> {\n\n        @Override\n        public void flatMap(String value, Collector<Tuple2<String, Integer>> out) {\n            // normalize and split the line\n            String[] tokens = value.toLowerCase().split(\"\\\\W+\");\n\n            // emit the pairs\n            for (String token : tokens) {\n                if (token.length() > 0) {\n                    out.collect(new Tuple2<>(token, 1));\n                }\n            }\n        }\n    }\n\n}\n\n```\n\n运行参数\n\n<img src=\"/images/517519-20200313155734660-595476051.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n&nbsp;结果\n\n<img src=\"/images/517519-20200313155753633-1061631172.png\" alt=\"\" />\n\n&nbsp;\n","tags":["Flink"]},{"title":"CDH学习笔记——cloudera manager API","url":"/CDH学习笔记——cloudera manager API.html","content":"可以使用CM提供的api查询cdh集群的信息\n\n```\nhttp://cloudera.github.io/cm_api/\n\n```\n\n7.0.3的api文档\n\n```\n https://archive.cloudera.com/cm7/7.0.3/generic/jar/cm_api/apidocs/index.html\n\n```\n\n**查询impala query的api**\n\n```\nhttps://archive.cloudera.com/cm7/7.0.3/generic/jar/cm_api/apidocs/json_ApiImpalaQuery.html\n\n```\n\n比如\n\n```\nhttps://xxxx:7180/api/v9/clusters/dev-cdh/services/impala/impalaQueries?from=2020-03-10T06:26:01.927Z\n\n```\n\n支持的参数如图所示\n\n<img src=\"/images/517519-20200312200226264-520049914.png\" alt=\"\" />\n\n**查询yarn上query的api**\n\n```\nhttps://archive.cloudera.com/cm7/7.0.3/generic/jar/cm_api/apidocs/resource_YarnApplicationsResource.html\n\n```\n\n比如\n\n```\nhttps://xxxx:7180/api/v9/clusters/dev-cdh/services/yarn/yarnApplications\n\n```\n\n支持的参数如图所示，和impala的一样\n\n<img src=\"/images/517519-20200316160930294-2106981338.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n查询组件的日志log的api\n\n```\nhttps://archive.cloudera.com/cm7/7.0.3/generic/jar/cm_api/apidocs/resource_RolesResource.html\n\n```\n\n比如\n\n```\nhttps://xxxx:7180/api/v9/clusters/dev-cdh/services/hive/roles/hive-HIVESERVER2-xxxx/logs/full\n\n```\n\n其中的roleName通过下面的接口请求\n\n```\nhttps://xxxx:7180/api/v9/clusters/dev-cdh/services/hive/roles\n\n```\n\n&nbsp;<img src=\"/images/517519-20200327153933807-1320400074.png\" alt=\"\" />\n\n&nbsp;\n","tags":["CDH"]},{"title":"Ubuntu16.04安装Kitematic","url":"/Ubuntu16.04安装Kitematic.html","content":"1.下载安装文件\n\n```\nhttps://github.com/docker/kitematic/releases\n\n```\n\n解压并安装\n\n```\nsudo dpkg -i ./Kitematic-0.17.10_amd64.deb\n\n```\n\n启动，然后可以启动容器\n\n<img src=\"/images/517519-20200311110421472-1939029830.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n&nbsp;并可以修改端口映射\n\n<img src=\"/images/517519-20200311110500121-676069318.png\" alt=\"\" />\n\n如果kitematic的my images无法显示的话，那是由于版本问题，在0.17.3版本是正常的，更高的版本无法显示\n\n<img src=\"/images/517519-20210508165550591-31476583.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;启动后可以使用docker exec <container id> bash进入容器\n\n```\ndocker exec -it a3fc85ae144d0c65691fe6d83e4d1fc87a65d4c83f2af5332835cbeae66f74a8 bash\nroot@f65ca23e3250:/home# ls\napp.jar\n\n```\n\ncontainer id可以在界面或者使用docker ps命令查看\n\n```\ndocker ps\nCONTAINER ID   IMAGE                      COMMAND                  CREATED        STATUS              PORTS                                                                                  NAMES\na3fc85ae144d\ndocker exec -it a3fc85ae144d bash\nroot@f65ca23e3250:/home#\n\n```\n\n<img src=\"/images/517519-20210510102245279-1197261190.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["docker"]},{"title":"Ubuntu16.04安装flink-1.10.0","url":"/Ubuntu16.04安装flink-1.10.0.html","content":"本来想cdh集成flink，但是我的cdh版本为5.16.2，参考了下面的issue可能cdh版本太低，至少要cdh6\n\n```\nhttps://github.com/pkeropen/flink-parcel/issues\n\n```\n\n进行独立安装\n\n```\nwget https://archive.apache.org/dist/flink/flink-1.10.0/flink-1.10.0-bin-scala_2.11.tgz\n\n```\n\n安装路径\n\n```\n/home/lintong/software/apache/flink-1.10.0\n\n```\n\n/etc/profile添加，并source /etc/profile\n\n```\n#flink\nexport FLINK_HOME=/home/lintong/software/apache/flink-1.10.0\nexport PATH=${FLINK_HOME}/bin:$PATH\n\n```\n\n下载flink-shaded-hadoop-2-uber-2.7.5-7.0.jar包，放到flink的lib目录下\n\n```\nwget https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.7.5-7.0/flink-shaded-hadoop-2-uber-2.7.5-7.0.jar\n\n```\n\n不然flink on yarn的时候会报\n\n```\nError: A JNI error has occurred, please check your installation and try again\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/hadoop/yarn/exceptions/YarnException\n\tat java.lang.Class.getDeclaredMethods0(Native Method)\n\tat java.lang.Class.privateGetDeclaredMethods(Class.java:2701)\n\tat java.lang.Class.privateGetMethodRecursive(Class.java:3048)\n\tat java.lang.Class.getMethod0(Class.java:3018)\n\tat java.lang.Class.getMethod(Class.java:1784)\n\tat sun.launcher.LauncherHelper.validateMainClass(LauncherHelper.java:544)\n\tat sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:526)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.exceptions.YarnException\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\t... 7 more\n\n```\n\n启动yarn-session\n\n```\nyarn-session.sh -n 3 -s 5 -jm 1024 -tm 4096 -d\n\n```\n\nyarn-seesion参数\n\n```\n  -n ： 指定TaskManager的数量；\n  -d: 以分离模式运行；\n  -id：指定yarn的任务ID；\n  -j:Flink jar文件的路径;\n  -jm：JobManager容器的内存（默认值：MB）;\n  -nl：为YARN应用程序指定YARN节点标签;\n  -nm:在YARN上为应用程序设置自定义名称;\n  -q:显示可用的YARN资源（内存，内核）;\n  -qu:指定YARN队列;\n  -s:指定TaskManager中slot的数量;\n  -st:以流模式启动Flink;\n  -tm:每个TaskManager容器的内存（默认值：MB）;\n  -z:命名空间，用于为高可用性模式创建Zookeeper子路径;\n\n```\n\n去CDH上查看，第一个是正在运行，第二个是结束\n\n<img src=\"/images/517519-20200310235657051-1377564747.png\" alt=\"\" width=\"1200\" height=\"316\" />\n\n去appliance id进到yarn的app页面\n\n<img src=\"/images/517519-20200313000607455-1448641822.png\" alt=\"\" width=\"1200\" height=\"359\" />\n\n<!--more-->\n&nbsp;\n\n再点击ApplicationMaster进到Flink Dashboard页面\n\n<img src=\"/images/517519-20200313000752358-1539984855.png\" alt=\"\" width=\"1400\" height=\"599\" />\n\n再提交Flink任务到yarn上\n\n```\nlintong@master:~/software/apache/flink-1.10.0$ bin/flink run examples/batch/WordCount.jar\n\n```\n\n再查看Flink Dashboard\n\n<img src=\"/images/517519-20200314135802643-110981648.png\" alt=\"\" width=\"1300\" height=\"603\" />\n\n&nbsp;\n\n输出\n\n<img src=\"/images/517519-20200314135848038-475244500.png\" alt=\"\" width=\"1000\" height=\"586\" />\n\n&nbsp;\n\n&nbsp;\n\n如果页面中的Available task slot一直为0的话，运行任务报，那说明yarn的资源不够\n\n```\nCaused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not allocate the required slot within slot request timeout. Please make sure that the cluster has enough resources.\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:452)\n\t... 45 more\nCaused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException\n\tat java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)\n\tat java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)\n\tat java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)\n\tat java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)\n\t... 25 more\nCaused by: java.util.concurrent.TimeoutException\n\t... 23 more\n\n```\n\n去cdh上调整yarn.nodemanager.resource.memory-mb，容器内存，我这边从2G调整成4G就可以运行flink任务\n\n<img src=\"/images/517519-20200314140031592-1868830400.png\" alt=\"\" width=\"600\" height=\"250\" />\n\n&nbsp;\n","tags":["Flink"]},{"title":"Yarn学习笔记——MR任务","url":"/Yarn学习笔记——MR任务.html","content":"1.hive sql提交到yarn上面执行之后，将会成为MR任务执行\n\n正在运行的MR任务的application查看的url，不同类似的任务查看的url可能会不同，比如Spark，Flink等\n\n```\nhttp://xxxx:8088/cluster/app/application_158225xxxxx_0316\n\n```\n\n<img src=\"/images/517519-20200310150834878-452941309.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n运行结束的MR任务的查看url\n\n```\nhttp://xxxx:19888/jobhistory/job/job_1582255xxxx_0316\n\n```\n\n&nbsp;<img src=\"/images/517519-20200310151046333-502648623.png\" alt=\"\" />\n\n具体hive sql的具体执行用户，sql内容等信息到配置进行查看\n\n```\nhttp://xxxx:19888/ws/v1/history/mapreduce/jobs/job_15822552xxxxx_0298/conf\n\n```\n\n如执行用户hive.server2.proxy.user\n\n<img src=\"/images/517519-20200310151523765-211406411.png\" alt=\"\" />\n\n&nbsp;\n\n具体执行的sql语句hive.query.string\n\n<img src=\"/images/517519-20200310151618917-1541847282.png\" alt=\"\" />\n\n&nbsp;\n\nApplication和Job的区别\n\n```\nJob：源于Hadoop 1.0的概念，一般指用户提交的MapReduce作业。当然，具体到客户端提交的spark作业，也会根据action，将作业的DAG划分成多个job。这个以后再具体说。\nApplication：从Hadoop 2.0引入，即可以指传统的MapReduce作业，也可以指其它计算框架的作业，如Spark、Storm作业等，甚至是一系列作业组成的有向无环图。\n\n```\n\n&nbsp;\n","tags":["YARN"]},{"title":"Yarn学习笔记——常用命令","url":"/Yarn学习笔记——常用命令.html","content":"1.yarn top，查看yarn上面的资源使用情况\n\n<img src=\"/images/517519-20200310143812487-1384791057.png\" alt=\"\" />\n\n2.队列使用状态\n\n```\nqueue -status root.xxx_common\nQueue Information : \nQueue Name : root.xxx_common\n\tState : RUNNING\n\tCapacity : 100.0%\n\tCurrent Capacity : 21.7%\n\tMaximum Capacity : -100.0%\n\tDefault Node Label expression : \n\tAccessible Node Labels :\n\n```\n\n3.查看yarn上运行的任务列表，如果集群有krb认证的话，需要先kinit，认证后可以看到所有正在运行的任务\n\n```\nyarn application -list\n\n```\n\n结果\n\n```\nTotal number of applications (application-types: [] and states: [SUBMITTED, ACCEPTED, RUNNING]):12\n                Application-Id\t    Application-Name\t    Application-Type\t      User\t     Queue\t             State\t       Final-State\t       Progress\t                       Tracking-URL\napplication_15771778xxxxx_0664\txx-flink-test\t        Apache Flink\t  xxx-xx\troot.xxx_common\t           RUNNING\t         UNDEFINED\t           100%\t         http://xxx-76:35437\napplication_15771778xxxxx_0663\txx-flink-debug\t        Apache Flink\t        xx\troot.xxx_common\t           RUNNING\t         UNDEFINED\t           100%\t         http://xxx-79:42443\napplication_15771778xxxxx_0641\t           xxx-flink\t        Apache Flink\t xxx-xx\troot.xxx_common\t           RUNNING\t         UNDEFINED\t           100%\t         http://xxx-76:38067\napplication_15771778xxxxx_0182\t        common_flink\t        Apache Flink\t        xx\troot.xxx_common\t           RUNNING\t         UNDEFINED\t           100%\t         http://xxx-79:38583\napplication_15822552xxxxx_0275\t             testjar\t        XXX-FLINK\txxx\troot.xxx_common\t           RUNNING\t         UNDEFINED\t           100%\t         http://xxx-78:36751\napplication_15822552xxxxx_0259\t            flinksql\t        XXX-FLINK\t      hdfs\troot.xxx_common\t           RUNNING\t         UNDEFINED\t           100%\t         http://xxx-77:37127\napplication_15822552xxxxx_0026\t           kudu-test\t        Apache Flink\t      hdfs\troot.xxx_common\t           RUNNING\t         UNDEFINED\t           100%\t         http://xxx-78:43071\napplication_15822552xxxxx_0307\t        xxx_statistic\t       XXX Flink\txxx\troot.xxx_common\t           RUNNING\t         UNDEFINED\t           100%\t         http://xxx:18000\napplication_15822552xxxxx_0308\t       xxx-statistic\t       XXX Flink\txxx\troot.xxx_common\t          ACCEPTED\t         UNDEFINED\t             0%\t                                N/A\napplication_15810489xxxxx_0003\t xxx-flink\t        Apache Flink\t        xx\troot.xxx_common\t           RUNNING\t         UNDEFINED\t           100%\t          http://xxx-78:8081\napplication_15810489xxxxx_0184\t        common_flink\t        Apache Flink\t        xx\troot.xxx_common\t           RUNNING\t         UNDEFINED\t           100%\t         http://xxx-76:35659\napplication_15810489xxxxx_0154\tFlink session cluster\t        Apache Flink\t      hdfs\troot.xxx_common\t           RUNNING\t         UNDEFINED\t           100%\t         http://xxx-80:38797\n\n```\n\n使用状态进行筛选\n\n```\nyarn application -list -appStates RUNNING\nTotal number of applications (application-types: [] and states: [RUNNING]):12\n                Application-Id\t    Application-Name\t    Application-Type\t      User\t     Queue\t             State\t       Final-State\t       Progress\t                       Tracking-URL\napplication_157717780xxxx_0664\txx-flink-test\t        Apache Flink\t  xxx-xx\troot.xxx_common\t           RUNNING\t         UNDEFINED\t           100%\t         http://xxxxx-xx:35437\n\n```\n\n4.查看任务状态信息\n\n```\nyarn application -status application_1582255xxxx_0314\nApplication Report : \n\tApplication-Id : application_1582255xxxx_0314\n\tApplication-Name : select count(*) from tb1 (Stage-1)\n\tApplication-Type : MAPREDUCE\n\tUser : hive\n\tQueue : root.xxxx_common\n\tStart-Time : 1583822835423\n\tFinish-Time : 1583822860082\n\tProgress : 100%\n\tState : FINISHED\n\tFinal-State : SUCCEEDED\n\tTracking-URL : http://xxx-xxxx-xx:19888/jobhistory/job/job_15822552xxxx_0314\n\tRPC Port : 32829\n\tAM Host : xxxx-xxxx-xx\n\tAggregate Resource Allocation : 162810 MB-seconds, 78 vcore-seconds\n\tLog Aggregation Status : SUCCEEDED\n\tDiagnostics :\n\n```\n\n<!--more-->\n&nbsp;\n\n5.查看yarn上任务的log\n\n```\nxxx@xxx-xx:~$ yarn logs -applicationId application_1582255xxxxx_0259\n/tmp/logs/xxx/logs/application_1582255xxxxx_0259 does not exist.\n\n```\n\n你使用哪里keytab进行认证，就只能查看改用户下的application的日志，比如是xxx的keytab，就只能查看/tmp/logs/xxx目录下的日志\n\nhive的MR任务的运行用户都是hive，所以要先使用hive.keytab进行认证\n\n```\nkinit -kt /var/lib/hive/hive.keytab hive\n\n```\n\n查看日志\n\n```\nyarn logs -applicationId application_1582255xxxxx_0313\n\n```\n\n内容大致\n\n```\nContainer: container_e49_15822552xxxx_0313_01_000003 on xxxxxxx-xx_8041\n=============================================================================\nLogType:stderr\nLog Upload Time:Tue Mar 10 14:04:57 +0800 2020\nLogLength:0\nLog Contents:\n\nLogType:stdout\nLog Upload Time:Tue Mar 10 14:04:57 +0800 2020\nLogLength:0\nLog Contents:\n\nLogType:syslog\nLog Upload Time:Tue Mar 10 14:04:57 +0800 2020\nLogLength:71619\nLog Contents:\n2020-03-10 14:04:42,509 INFO [main] org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2020-03-10 14:04:42,569 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2020-03-10 14:04:42,569 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started\n2020-03-10 14:04:42,609 INFO [main] org.apache.hadoop.mapred.YarnChild: Executing with tokens:\n2020-03-10 14:04:42,609 INFO [main] org.apache.hadoop.mapred.YarnChild: Kind: mapreduce.job, Service: job_1582255xxxxx_0313, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@505fc5a4)\n2020-03-10 14:04:42,770 INFO [main] org.apache.hadoop.mapred.YarnChild: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:xxx-nameservice, Ident: (token for hive: HDFS_DELEGATION_TOKEN owner=hive/xxxxx-xx@XXX-XXXXX-XX, renewer=yarn, realUser=, issueDate=1583820254237, maxDate=1584425054237, sequenceNumber=32956, masterKeyId=647)\n2020-03-10 14:04:42,800 INFO [main] org.apache.hadoop.mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.\n2020-03-10 14:04:42,955 INFO [main] org.apache.hadoop.mapred.YarnChild: mapreduce.cluster.local.dir for child: /data02/yarn/nm/usercache/hive/appcache/application_15822xxxx_0313,/data03/yarn/nm/usercache/hive/appcache/application_1582255xxxx_0313,/data04/yarn/nm/usercache/hive/appcache/application_1582xxxx_0313,/data05/yarn/nm/usercache/hive/appcache/application_15822552xxxx_0313,/data06/yarn/nm/usercache/hive/appcache/application_158225xxxxx_0313,/data07/yarn/nm/usercache/hive/appcache/application_158225xxxxxx_0313,/data08/yarn/nm/usercache/hive/appcache/application_158225xxxx_0313,/data09/yarn/nm/usercache/hive/appcache/application_1582255xxxx_0313,/data10/yarn/nm/usercache/hive/appcache/application_158225xxxx_0313,/data11/yarn/nm/usercache/hive/appcache/application_158225xxxx_0313,/data12/yarn/nm/usercache/hive/appcache/application_158225xxxx_0313,/data01/yarn/nm/usercache/hive/appcache/application_158225xxx_0313\n2020-03-10 14:04:43,153 INFO [main] org.apache.hadoop.conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n2020-03-10 14:04:43,633 INFO [main] org.apache.hadoop.mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n2020-03-10 14:04:43,716 INFO [main] org.apache.hadoop.mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@62f4ff3b\n2020-03-10 14:04:43,847 INFO [fetcher#10] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.snappy]\n2020-03-10 14:04:43,872 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]\n\n```\n\n6.yarn kill application\n\n```\nyarn application -kill application_1583941025138_0001\n20/03/12 00:04:05 INFO client.RMProxy: Connecting to ResourceManager at master/192.168.2.105:8032\nKilling application application_1583941025138_0001\n20/03/12 00:04:05 INFO impl.YarnClientImpl: Killed application application_1583941025138_0001\n\n```\n\nyarn kill job\n\n```\nyarn job -kill job_xxxxx\n\n```\n\n参考：https://www.jianshu.com/p/f510a1f8e5f0\n\n7.mapreduce任务临时文件没有删除文件\n\n[MapReduce作业运行结束后有部分临时文件在HDFS临时目录下残留](http://support-it.huawei.com/docs/zh-cn/fusioninsight-all/maintenance-guide/zh-cn_topic_0222554324.html)\n\n8.查看YARN所有的节点\n\n```\n yarn node -all -list\n\n```\n\n查看YARN所有RUNNING状态的节点，节点状态总共有下面几种：NEW,RUNNING,UNHEALTHY,DECOMMISSIONED,LOST,REBOOTED,DEC,OMMISSIONING,SHUTDOWN\n\n```\nyarn node -list -states RUNNING\n\n```\n\n9.ssh到所有的YARN节点上，首先过滤出所有的ip，然后使用xpanes来批量ssh\n\n```\nlist=`yarn node -list -states RUNNING | awk '{print $1}' |egrep \"(^ip.*8041$)\" | awk -F \":\" '{print $1}'`\necho $list\n\nxpanes -c \"ssh {}\" ip1 ip2\n\n```\n\n10.查看YARN resource manager的状态，resource manager在开启高可用的情况下，有一个节点是active状态，其他的节点的standby状态\n\n```\nyarn rmadmin -getAllServiceState\nip-1:8033 active\nip-2:8033 standby\nip-3:8033 standby\n\n```\n\n参考：[https://docs.amazonaws.cn/emr/latest/ManagementGuide/emr-plan-ha-applications.html](https://docs.amazonaws.cn/emr/latest/ManagementGuide/emr-plan-ha-applications.html)\n\n&nbsp;\n","tags":["YARN"]},{"title":"Elasticsearch的Shard和Segment","url":"/Elasticsearch的Shard和Segment.html","content":"**Shard是什么？**\n\n在下面的文档中进行了介绍\n\n```\nhttps://www.elastic.co/guide/cn/elasticsearch/guide/current/kagillion-shards.html\n\n```\n\n1.一个分片的底层即为一个 Lucene 索引，会消耗一定文件句柄、内存、以及 CPU 运转。<!--more-->\n&nbsp;&nbsp;&nbsp;&nbsp;\n\n2.每一个搜索请求都需要命中索引中的每一个分片，如果每一个分片都处于不同的节点还好， 但如果多个分片都需要在同一个节点上竞争使用相同的资源就有些糟糕了。&nbsp;&nbsp;&nbsp;&nbsp;\n\n3.用于计算相关度的词项统计信息是基于分片的。如果有许多分片，每一个都只有很少的数据会导致很低的相关度。\n\n&nbsp;\n\n在官方文档中介绍了clusters, nodes, and shards的关系\n\n```\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/scalability.html\n\n```\n\n1.一个Es的索引实际上是一个或者多个的物理shard的组合\n\n2.Shard有两种：一种是primaries and replicas，即主分片和副本分片。索引中的每一个文档都属于一个主分片，副本分片是主分片的拷贝\n\nreplica shard是primary shard的副本，负责容错，以及承担读请求负载\n\n每个document肯定只存在于某一个primary shard以及其对应的replica shard中，不可能存在于多个primary shard\n\nprimary shard不能和自己的replica shard放在同一个节点上，否则宕机会出现数据丢失\n\n3.Es通过分散一个index中的文档到到多个shards中，以及分散这些shards到不同的节点来实现高可用\n\n4.主分片的数量在index创建的时候就决定好了， 副本分片的数量可以随时改变\n\n5.当增减节点的时候，shard会在集群中自动平衡\n\n6.primary shard的默认数量是5，replica默认是1，默认有10个shard，5个primary shard，5个replica shard，如下\n\n查看某个index的shard命令\n\n```\nhttp://localhost:9200/xxxx/_search_shards\n```\n\n<img src=\"/images/517519-20200224150001069-933953652.png\" width=\"300\" height=\"315\" />\n\n查看集群的shard可以通过下面命令\n\n```\nhttp://localhost:9200/_cat/shards\n\n```\n\n&nbsp;\n\n参考：\n\n[Elasticsearch 的 Shard 和 Segment](https://blog.csdn.net/likui1314159/article/details/53217750)\n\n[elasticsearch：shard和replica机制](https://blog.csdn.net/qq_37502106/article/details/80584041)\n\n[高可用 Elasticsearch 集群的分片管理 （Shard）](https://www.jianshu.com/p/210465322e18)\n\n&nbsp;\n\n**Segment是什么？**\n\n每个shard（分片）包含多个segment（段），每一个segment都是一个倒排索引\n\n在查询的时，会把所有的segment查询结果汇总归并后最为最终的分片查询结果返回\n\n1.segment是不可变的，物理上你并不能从中删除信息，所以在删除文档的时候，是在文档上面打上一个删除的标记，然后在执行段合并的时候，进行删除\n\n2.索引segment段的个数越多，搜索性能越低且消耗内存更多\n\n&nbsp;\n\n参考：\n\n[Elasticsearch段合并](https://www.jianshu.com/p/2f65d801d367)\n","tags":["ELK"]},{"title":"mac使用karabiner-elements修改键位","url":"/mac使用karabiner-elements修改键位.html","content":"在最新的mac catalina系统中，已经从karabiner更名为karabiner-elements，安装的版本为Karabiner-Elements-12.9.0\n\n下载地址：https://pqrs.org/osx/karabiner/\n\n<img src=\"/images/517519-20200223113738102-801024441.png\" alt=\"\" width=\"800\" height=\"581\" />\n\n<!--more-->\n&nbsp;\n\n安装后会有两个应用，一个是Karabiner-Elements，一个是Karabiner-EventViewer，修改键位使用的是Karabiner-Elements\n\n安装的时候会提示添加权限\n\n&nbsp;<img src=\"/images/517519-20200223113953945-1673594611.png\" alt=\"\" width=\"800\" height=\"289\" />\n\n&nbsp;\n\n&nbsp;\n\n安装后可以在下面的网址导入你想要修改的组合键方案，组合键的是complex_modifications，单个键是simple_modifications\n\n```\nhttps://pqrs.org/osx/karabiner/complex_modifications/#modifier-keys\n\n```\n\n比如我就对调了command+tab和ctrl+tab\n\n<img src=\"/images/517519-20200223114302246-69789218.png\" alt=\"\" width=\"800\" height=\"185\" />\n\n&nbsp;\n\n外接键盘修改了ctrl command option的位置\n\n<img src=\"/images/517519-20200223114339731-969019500.png\" alt=\"\" width=\"800\" height=\"192\" />\n\n&nbsp;\n\niterm2修改home和end快捷键command+a成command+ ->，以及commend+e成command+ <-\n\n参考：https://www.itranslater.com/qa/details/2111127749946508288\n\n<img src=\"/images/517519-20200630233009272-516830420.png\" width=\"400\" height=\"97\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp; \n\n下载的规则在\n\n```\n~/.config/karabiner/assets/complex_modifications\n\n```\n\n&nbsp;\n","tags":["mac"]},{"title":"mac修改brew源","url":"/mac修改brew源.html","content":"参考：https://juejin.im/post/5daec26a51882575d50cd0aa\n\n1.查看brew当前源\n\n```\ngit -C \"$(brew --repo)\" remote -v\norigin\thttps://github.com/Homebrew/brew (fetch)\norigin\thttps://github.com/Homebrew/brew (push)\n\n```\n\n2.改成清华的源\n\n```\ngit -C \"$(brew --repo)\" remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.git\ngit -C \"$(brew --repo homebrew/core)\" remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-core.git\ngit -C \"$(brew --repo homebrew/cask)\" remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-cask.git\nbrew update\n\n```\n\n已经修改成清华的源\n\n```\ngit -C \"$(brew --repo)\" remote -v\norigin\thttps://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.git (fetch)\norigin\thttps://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.git (push)\n\n```\n\n或者改成中科大的源\n\n```\n# 替换 Homebrew\ngit -C \"$(brew --repo)\" remote set-url origin https://mirrors.ustc.edu.cn/brew.git\n# 替换 Homebrew Core\ngit -C \"$(brew --repo homebrew/core)\" remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git\n# 替换 Homebrew Cask\ngit -C \"$(brew --repo homebrew/cask)\" remote set-url origin https://mirrors.ustc.edu.cn/homebrew-cask.git\n# 更新\nbrew update\n\n```\n\n如果想还原\n\n```\ngit -C \"$(brew --repo)\" remote set-url origin https://github.com/Homebrew/brew.git\ngit -C \"$(brew --repo homebrew/core)\" remote set-url origin https://github.com/Homebrew/homebrew-core.git\ngit -C \"$(brew --repo homebrew/cask)\" remote set-url origin https://github.com/Homebrew/homebrew-cask.git\nbrew update\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["mac"]},{"title":"mac环境变量不生效问题","url":"/mac环境变量不生效问题.html","content":"1.在 ~/.zshrc 中添加\n\n```\nsource ~/.bash_profile \n\n```\n\n参考：https://blog.csdn.net/qq_18505715/article/details/83276208\n\n<!--more-->\n&nbsp;\n\n2.比如mac的git命令补全不生效，可以参考如下文章添加\n\nhttps://blog.csdn.net/WinWill2012/article/details/71774461\n\n```\nsource ~/.git-completion.bash\n\n```\n\n&nbsp;\n\n3.调整mac终端颜色和全路径显示\n\n```\nexport PS1=\"%n@%m %0~ $ \"\n\nexport CLICOLOR=1\nexport LSCOLORS=ExFxBxDxCxegedabagacad\nalias ls='ls -GFh'\n```\n\n如果安装的oh my zsh，那么修改全路径显示的方式是\n\n```\nvim ~/.oh-my-zsh/themes/robbyrussell.zsh-theme\n```\n\n修改成\n\n```\n#PROMPT+=' %{$fg[cyan]%}%c%{$reset_color%} $(git_prompt_info)'\nPROMPT+='${ret_status} %{$fg[cyan]%}%d%{$reset_color%} $(git_prompt_info)$ '\n```\n\n就会变成\n\n<img src=\"/images/517519-20211008235317070-2109341511.png\" width=\"500\" height=\"65\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;\n\n4.Big Sur在使用mvn package的时候回遇到\n\n```\nNo compiler is provided in this environment. Perhaps you are running on a JRE rather than a JDK?\n```\n\n解决方法是在~/.bash_profile中加入\n\n```\n#java\nexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home\nexport JRE_HOME=${JAVA_HOME}/jre\nexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib\nexport PATH=${JAVA_HOME}/bin:$PATH\n\n# maven\nexport M2_HOME=/Users/lintong/software/apache-maven-3.6.3\nexport M2=$M2_HOME/bin\nexport PATH=$M2:$PATH\n\n```\n\n　　\n\n5.iterm2连接远程终端中文乱码，在 ~/.zshrc 中添加\n\n```\nexport LANG='UTC-8' export LC_ALL='en_US.UTF-8'\n\n```\n\n然后\n\n```\nsource ~/.zshrc\n\n```\n\n　　\n\n6.oh my zsh添加提示插件\n\n```\nhttps://github.com/zsh-users/zsh-autosuggestions/blob/master/INSTALL.md\n\n```\n\n&nbsp;\n","tags":["mac"]},{"title":"分屏工具xpanes","url":"/分屏工具xpanes.html","content":"参考：https://www.ctolib.com/greymd-tmux-xpanes.html\n\n```\nbrew install tmux-xpanes\n\n```\n\n<!--more-->\n&nbsp;或者\n\n```\n# Install `add-apt-repository` command, if necessary.\n$ sudo apt install software-properties-common\n\n$ sudo add-apt-repository ppa:greymd/tmux-xpanes\n$ sudo apt update\n$ sudo apt install tmux-xpanes\n\n```\n\n使用\n\n```\nxpanes -c \"ssh {}\" xxxx-{52..60}\n\n```\n\n&nbsp;\n","tags":["开发工具"]},{"title":"MySQL自增id不连续问题","url":"/MySQL自增id不连续问题.html","content":"项目中有一张表是记录人员，在每个新用户调用接口认证通过了之后，会有一个往该表插入这个新用户信息的操作。\n\n但是在线上环境中，发现该表的自增id不连续，且间隔都是差了2，比如上一个人的id是10，下一个人的id就是12，而在前端页面中，一个用户认证通过后，会调用3个接口，初步排查是MySQL并发操作导致了自增id不连续的情况\n\n在这篇文章中，列举了导致自增id不连续的几个原因，这次遇到的就是第一种情况，因为个人的信息中我设置了唯一索引，参考：[MySQL实战45讲Day38----自增主键不是连续的原因](https://www.jianshu.com/p/957f605a646c)\n\n<1>、唯一键冲突是导致自增主键id不连续的第一种原因\n\n<2>、事务回滚是导致自增主键id不连续的第二种原因\n\n<3>、批量申请自增id的策略是导致自增主键id不连续的第三种原因\n\n在这篇文章中提到了MySQL默认的innodb_autoinc_lock_mode=1，当innodb_autoinc_lock_mode=1和innodb_autoinc_lock_mode=2的情况下，自增id可能会出现不连续\n\n在innodb_autoinc_lock_mode=0的时候，自增id是连续的，但是会导致锁表，影响并发性能\n\n参考：[INNODB自增主键的一些问题](https://www.cnblogs.com/zhoujinyi/p/3433823.html)\n\n[再谈MySQL auto_increment空洞问题](https://www.jianshu.com/p/369559f399d0)\n\n解决方法：\n\n1.[MySQL中自增主键不连续之解决方案。](https://blog.csdn.net/yang5726685/article/details/78161105)\n\n<!--more-->\n&nbsp;\n","tags":["MySQL"]},{"title":"Ubuntu16.04安装zkui","url":"/Ubuntu16.04安装zkui.html","content":"1.git clone\n\n```\ngit clone git@github.com:DeemOpen/zkui.git\n\n```\n\n2.打包\n\n```\nmvn clean install\n\n```\n\n3.复制config.cfg到target目录，并修改zk地址\n\n4.配置 supervisor zkui.conf\n\n```\n[program:zkui]\ndirectory=/home/lintong/software/zkui/target\ncommand = java -jar zkui-2.0-SNAPSHOT-jar-with-dependencies.jar\nuser = lintong\nautostart = false\nautoresart = true\nstderr_logfile = /var/log/supervisor/zkui.stderr.log\nstdout_logfile = /var/log/supervisor/zkui.stdout.log\n\n```\n\n5.启动zookeeper\n\n6.启动zkui\n\n<img src=\"/images/517519-20200120111847848-1657225752.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n7.登录 localhost:9090，默认登录账号 admin manager\n\n&nbsp;<img src=\"/images/517519-20200120112030775-750368950.png\" alt=\"\" />\n\n&nbsp;\n","tags":["zookeeper"]},{"title":"antlr解析hive语句","url":"/antlr解析hive语句.html","content":"hive是使用antlr来解析的\n\nparser要做的事情，是从无结构的字符串里面，解码产生有结构的数据结构（a [parser](https://en.wikipedia.org/wiki/Parser) is a function accepting strings as input and returning some structure as output），参考 [Parser_combinator wiki](https://en.wikipedia.org/wiki/Parser_combinator)\n\nparser分成两种，一种是**parser combinator**，一种是**parser generator**，区别可以参考 [王垠的文章&mdash;&mdash;对 Parser 的误解](https://www.yinwang.org/blog-cn/2015/09/19/parser)\n\n<!--more-->\n&nbsp;\n\n**1.parser combinator**是需要手写parser，a **parser combinator** is a [higher-order function](https://en.wikipedia.org/wiki/Higher-order_function) that accepts several parsers as input and returns a new parser as its output，比如Thrift的Parser\n\n```\nhttps://github.com/apache/thrift/blob/master/compiler/cpp/src/thrift/main.cc\n\n```\n\n&nbsp;\n\n**2.parser generator**是需要你用某种指定的描述语言来表示出语法，然后自动把他们转换成parser的代码，比如**Antlr**里面的**g4语法文件**，**calcite**的**ftl语法文件**，hue使用的**jison**以及**flex**和**cup**等，缺点是由于代码是生成的，排错比较困难\n\n使用了**Antlr**的parser有Hive，Presto，Spark SQL\n\n美团点评的文章\n\n```\nhttps://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html\n\n```\n\n以及hive源码的测试用例\n\n```\nhttps://github.com/apache/hive/blob/branch-1.1/ql/src/test/org/apache/hadoop/hive/ql/parse/TestHiveDecimalParse.java\n\n```\n\nhive的g4文件如下\n\n老版本的hive\n\n```\nhttps://github.com/apache/hive/blob/59d8665cba4fe126df026f334d35e5b9885fc42c/parser/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g\n\n```\n\n新版本的hive\n\n```\nhttps://github.com/apache/hive/blob/master/hplsql/src/main/antlr4/org/apache/hive/hplsql/Hplsql.g4\n\n```\n\nspark的g4文件如下\n\n```\nhttps://github.com/apache/spark/blob/master/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4\n\n```\n\nPresto的g4文件如下\n\n```\nhttps://github.com/prestodb/presto/blob/master/presto-parser/src/main/antlr4/com/facebook/presto/sql/parser/SqlBase.g4\n\n```\n\nconfluent的kSql的g4文件\n\n```\nhttps://github.com/confluentinc/ksql/blob/master/ksqldb-parser/src/main/antlr4/io/confluent/ksql/parser/SqlBase.g4\n\n```\n\n　　\n\n使用了**Apache Calcite**的parser有Apache Flink，Mybatis，Apache Storm等\n\n参考：[Apache Calcite 为什么能这么流行](https://zhuanlan.zhihu.com/p/67560995)\n\nFlink的ftl文件如下\n\n```\nhttps://github.com/apache/flink/blob/master/flink-table/flink-sql-parser/src/main/codegen/includes/parserImpls.ftl\n\n```\n\nMybatis的mapper模板生成\n\n```\nhttps://github.com/abel533/Mapper/blob/master/generator/src/main/resources/generator/mapper.ftl\n\n```\n\nStorm的ftl文件如下\n\n```\nhttps://github.com/apache/storm/blob/master/sql/storm-sql-core/src/codegen/includes/parserImpls.ftl\n\n```\n\n&nbsp;\n\n以及使用了**flex和cup**的impala，如何使用impala的parser来解析query可以参考另一篇文章：[使用Impala parser解析SQL](https://www.cnblogs.com/tonglin0325/p/5243824.html)\n\nparser的测试用例\n\n```\nhttps://github.com/cloudera/Impala/blob/master/fe/src/test/java/com/cloudera/impala/analysis/ParserTest.java\n\n```\n\n源码\n\n```\nhttps://github.com/apache/impala/blob/master/fe/src/main/jflex/sql-scanner.flex\n\n```\n\n&nbsp;和\n\n```\nhttps://github.com/apache/impala/blob/master/fe/src/main/cup/sql-parser.cup\n\n```\n\nimpala也用了少量的antlr\n\n```\nhttps://github.com/apache/impala/blob/master/fe/src/main/java/org/apache/impala/analysis/ToSqlUtils.java\n\n```\n\n&nbsp;\n\n还有hue使用的jison，jison是JavaScript语言的语法分析器\n\n```\nhttps://github.com/cloudera/hue/tree/master/desktop/core/src/desktop/js/parse/jison\n\n```\n\n&nbsp;\n\n以hive的Hplsql.g4为例，来解析一句sql\n\n```\nantlr4 Hplsql.g4\njavac Hplsql*.java\n\n```\n\n解析select语句\n\n```\ngrun Hplsql r -tokens\nWarning: TestRig moved to org.antlr.v4.gui.TestRig; calling automatically\nselect * from db1.tb1;\n[@0,0:5='select',<T_SELECT>,1:0]\n[@1,7:7='*',<'*'>,1:7]\n[@2,9:12='from',<T_FROM>,1:9]\n[@3,14:16='db1',<L_ID>,1:14]\n[@4,17:17='.',<'.'>,1:17]\n[@5,18:20='tb1',<L_ID>,1:18]\n[@6,21:21=';',<';'>,1:21]\n[@7,23:22='<EOF>',<EOF>,2:0]\nNo method for rule r or it has arguments\n\n```\n\n可以看到`打印出token流`\n\n解析建表语句\n\n```\ngrun Hplsql r -tokens\nWarning: TestRig moved to org.antlr.v4.gui.TestRig; calling automatically\nCREATE TABLE IF NOT EXISTS db1.tb1 (\n  `f1` string,\n  `f2` bigint,\n  `f3` string,\n  `f4` string,\n  `f5` string)\npartitioned by(ds string)\nstored as parquet\nTBLPROPERTIES (\"parquet.compression\"=\"SNAPPY\");\n[@0,0:5='CREATE',<T_CREATE>,1:0]\n[@1,7:11='TABLE',<T_TABLE>,1:7]\n[@2,13:14='IF',<T_IF>,1:13]\n[@3,16:18='NOT',<T_NOT>,1:16]\n[@4,20:25='EXISTS',<T_EXISTS>,1:20]\n[@5,27:29='db1',<L_ID>,1:27]\n[@6,30:30='.',<'.'>,1:30]\n[@7,31:33='tb1',<L_ID>,1:31]\n[@8,35:35='(',<'('>,1:35]\n[@9,39:42='`f1`',<L_ID>,2:2]\n[@10,44:49='string',<T_STRING>,2:7]\n[@11,50:50=',',<','>,2:13]\n[@12,54:57='`f2`',<L_ID>,3:2]\n[@13,59:64='bigint',<T_BIGINT>,3:7]\n[@14,65:65=',',<','>,3:13]\n[@15,69:72='`f3`',<L_ID>,4:2]\n[@16,74:79='string',<T_STRING>,4:7]\n[@17,80:80=',',<','>,4:13]\n[@18,84:87='`f4`',<L_ID>,5:2]\n[@19,89:94='string',<T_STRING>,5:7]\n[@20,95:95=',',<','>,5:13]\n[@21,99:102='`f5`',<L_ID>,6:2]\n[@22,104:109='string',<T_STRING>,6:7]\n[@23,110:110=')',<')'>,6:13]\n[@24,112:122='partitioned',<L_ID>,7:0]\n[@25,124:125='by',<T_BY>,7:12]\n[@26,126:126='(',<'('>,7:14]\n[@27,127:128='ds',<L_ID>,7:15]\n[@28,130:135='string',<T_STRING>,7:18]\n[@29,136:136=')',<')'>,7:24]\n[@30,138:143='stored',<T_STORED>,8:0]\n[@31,145:146='as',<T_AS>,8:7]\n[@32,148:154='parquet',<L_ID>,8:10]\n[@33,156:168='TBLPROPERTIES',<L_ID>,9:0]\n[@34,170:170='(',<'('>,9:14]\n[@35,171:191='\"parquet.compression\"',<L_ID>,9:15]\n[@36,192:192='=',<'='>,9:36]\n[@37,193:200='\"SNAPPY\"',<L_ID>,9:37]\n[@38,201:201=')',<')'>,9:45]\n[@39,202:202=';',<';'>,9:46]\n[@40,204:203='<EOF>',<EOF>,10:0]\nNo method for rule r or it has arguments\n\n```\n\n&nbsp;\n\n上面介绍了antlr如果解析hive语句，而在hive中使用的就是由antlr编译出来的java代码来解析hive语句\n\n接下来介绍如何使用java代码解析hive语句，首先引用依赖\n\n```\n<dependency>\n    <groupId>org.apache.hive</groupId>\n    <artifactId>hive-exec</artifactId>\n    <version>1.1.0-cdh5.16.2</version>\n</dependency>\n\n```\n\n代码\n\n```\nimport com.google.common.collect.Lists;\nimport com.google.common.collect.Maps;\nimport org.apache.hadoop.hive.conf.HiveConf;\nimport org.apache.hadoop.hive.metastore.api.FieldSchema;\nimport org.apache.hadoop.hive.ql.Context;\nimport org.apache.hadoop.hive.ql.lib.*;\nimport org.apache.hadoop.hive.ql.parse.*;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.IOException;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Stack;\n\npublic class MyProcessor implements NodeProcessor {\n\n    private static Logger logger = LoggerFactory.getLogger(MyProcessor.class);\n    private static Context context = null;\n    private final static String HDFS_SESSION_PATH_KEY = \"_hive.hdfs.session.path\";\n    private final static String LOCAL_SESSION_PATH_KEY = \"_hive.local.session.path\";\n\n    private static String hdfsTemporaryDirectory(HiveConf hiveConf) {\n        return hiveConf.get(\"hadoop.tmp.dir\", \"/tmp\");\n    }\n\n\n    private static String localTemporaryDirectory() {\n        return System.getProperty(\"java.io.tmpdir\", \"/tmp\");\n    }\n\n    static {\n        HiveConf hiveConf = new HiveConf();\n        if (hiveConf.get(HDFS_SESSION_PATH_KEY) == null) {\n            hiveConf.set(HDFS_SESSION_PATH_KEY, hdfsTemporaryDirectory(hiveConf));\n        }\n        if (hiveConf.get(LOCAL_SESSION_PATH_KEY) == null) {\n            hiveConf.set(LOCAL_SESSION_PATH_KEY, localTemporaryDirectory());\n        }\n        try {\n            context = new Context(hiveConf);\n        } catch (IOException e) {\n            logger.error(\"Init hive context fail, message: \" + e);\n        }\n    }\n\n    String tableName = \"\";\n    List<FieldSchema> fieldSchemas;\n\n    public void parse(String query) throws ParseException, SemanticException {\n        ParseDriver pd = new ParseDriver();\n        ASTNode tree = pd.parse(query, context);\n        while ((tree.getToken() == null) &amp;&amp; (tree.getChildCount() > 0)) {\n            tree = (ASTNode) tree.getChild(0);\n        }\n        logger.info(\"start to analyze query: {}, ASTNode: {}\", query, tree.dump());\n        Map<Rule, NodeProcessor> rules = Maps.newLinkedHashMap();\n        Dispatcher disp = new DefaultRuleDispatcher(this, rules, null);\n        GraphWalker ogw = new DefaultGraphWalker(disp);\n        final List<Node> topNodes = Lists.newArrayList(tree);\n\n        // 遍历\n        ogw.startWalking(topNodes, null);\n        // 打印\n        System.out.println(tableName);\n        System.out.println(fieldSchemas);\n    }\n\n    @Override\n    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {\n        ASTNode pt = (ASTNode) nd;\n        switch (pt.getToken().getType()) {\n            case org.apache.hadoop.hive.ql.parse.HiveParser.TOK_CREATETABLE:\n                for (Node node : pt.getChildren()) {\n                    ASTNode createTableChild = (ASTNode) node;\n                    if (createTableChild.getToken().getType() == HiveParser.TOK_TABNAME) {\n                        tableName = BaseSemanticAnalyzer.getUnescapedName(createTableChild);\n                    } else if (createTableChild.getToken().getType() == HiveParser.TOK_TABCOLLIST) {\n                        fieldSchemas = BaseSemanticAnalyzer.getColumns(createTableChild, true);\n                    }\n                }\n        }\n        return null;\n    }\n}\n\n```\n\n测试用例，解析了hive的建表语句\n\n```\nimport org.junit.Test;\n\npublic class MyProcessorTest {\n\n    @Test\n    public void parse() throws Exception{\n        String query = \"create table my_table(id int,name string)row format delimited fields terminated by '\\\\t'\";\n        MyProcessor processor = new MyProcessor();\n        processor.parse(query);\n    }\n\n}\n\n```\n\n输出\n\n<img src=\"/images/517519-20211224211238664-378901836.png\" width=\"800\" height=\"69\" loading=\"lazy\" />\n\n上面例子中是将hive表名和字段解析出来，其他属性也可以使用类似的方法从语法树中取出\n\n&nbsp;\n","tags":["antlr"]},{"title":"面试题目——leetcode总结","url":"/面试题目——leetcode总结.html","content":"## **知识点**\n\n打印数组使用Arrays.toString(array)\n\n交换list的两个元素可以使用Collections.swap(list, i, j)\n\n子序列subsequence是不连续的，子串substring是连续的\n\n/2使用无符号右移>>>1；\n\nboolean数组初始化默认值是false\n\nchar类型的范围是0-255，所以可以直接char a，然后放到int[] arr的arr[c]\n\nTreeMap按key从小到大排序\n\nMath.ceil向上取整，Math.floor向下取整，Math.round四舍五入，注意把int类型的输入转成double类型\n\nInteger.compare(a,b)，如果a>b，返回1；如果a<b，返回-1；如果a==b，返回0\n\n(\"\" + o1).compareTo(\"\" + o2) 结果是 [1, 10, 11, 12, 13, 14, 15, 2, 3, 4, 5, 6, 7, 8, 9]\n\n(a + b).compareTo(b + a) 结果是 [10, 1, 11, 12, 13, 14, 15, 2, 3, 4, 5, 6, 7, 8, 9]\n\nInteger.bitCount(255) = 8，统计二进制表示中1的个数\n\nInteger.numberOfTrailingZeros(4) = 2，统计二进制表示中结尾中0的个数\n\nInteger.toBinaryString(200) = 11111111，Integer转二进制字符串，长度不定\n\nInteger.lowestOneBit(2) = 2，返回最右边的1的index\n\nInteger.parseInt(str, 2)，二进制字符串转int\n\na.retainAll(b),从a list中移除在b list中不存在的元素\n\nCharacter.isLetter('1') // false ; Character.isLetter('a') // true\n\nCharacter.isDigit('1') // true ; Character.isDigit('a') // false\n\nCharacter.isLowerCase('a') 判断是否是小写; Character.isUpperCase('A') 判断是否是大写\n\nstr.split(d, -1); 当 limit < 0 的时候，str将被尽可能多的分割，否则 \",,\" 会被分割成空数组，参考：[java split(String regex, int limit) 的使用](https://www.cnblogs.com/yxmfighting/p/7383013.html)\n\n字符在字符串中的index，str.indexOf('a') 以及 lastIndexOf('a')\n\n<!--more-->\n&nbsp;\n\n## 数据结构特性\n\n<img src=\"/images/517519-20160327094703104-1322798934.png\" width=\"600\" height=\"163\" />\n\n<img src=\"/images/517519-20160327095123308-1687538523.png\" width=\"600\" height=\"155\" />\n\n<img src=\"/images/517519-20160328140211894-750034714.png\" width=\"600\" height=\"119\" />\n\n&nbsp;\n\n## **排序算法**\n\n<img src=\"/images/517519-20210426141048686-908675939.png\" width=\"500\" height=\"190\" loading=\"lazy\" />\n\n1.[Java排序算法&mdash;&mdash;冒泡排序](https://www.cnblogs.com/tonglin0325/p/5329571.html)\n\n2.[Java排序算法&mdash;&mdash;快速排序](https://www.cnblogs.com/tonglin0325/p/5374408.html)\n\n输入可能是list或者array\n\n3.leetcode [1051. 高度检查器](https://leetcode-cn.com/problems/height-checker/) / HeightChecker\n\n**思路**：排序后对比不同\n\n4. leetcode [215. 数组中的第K个最大元素](https://leetcode-cn.com/problems/kth-largest-element-in-an-array/) / FindKthLargest\n\n**思路**：1.使用PriorityQueue，默认从小到大排序，peek为最大，offer和poll，add和remove\n\n5. leetcode [973. 最接近原点的 K 个点](https://leetcode-cn.com/problems/k-closest-points-to-origin/) / KClosest\n\n**思路**：使用PriorityQueue，并实现Comparator从小到大排序，然后全部添加进去，poll到剩下K个，时间复杂度O(NlogK)，空间复杂度O(K)\n\n6. leetcode [703. 数据流中的第 K 大元素](https://leetcode-cn.com/problems/kth-largest-element-in-a-stream/) / KthLargest\n\n**思路**：使用PriorityQueue，再实现一个add函数，使用add和poll\n\n7. leetcode [179. 最大数](https://leetcode-cn.com/problems/largest-number/) / LargestNumber\n\n**思路**：使用comparator实现字典排序，(a+b).compareTo(b+a); 注意如果第一个是\"0\"，直接返回\"0\"\n\n8. leetcode [347. 前 K 个高频元素](https://leetcode-cn.com/problems/top-k-frequent-elements/) / TopKFrequent\n\n**思路**：先hashmap计数，然后使用PriorityQueue来取前k个\n\n&nbsp;\n\n## **栈和队列**\n\n1. leetcode&nbsp;[394. 字符串解码](https://leetcode-cn.com/problems/decode-string/) / DecodeString\n\n**思路**：利用栈，遇到 ] 出栈，拼接字符串直到 [ ，之后再入栈，注意数字可能是多位的，需要base *= 10来计算count\n\n2. leetcode [150. 逆波兰表达式求值](https://leetcode-cn.com/problems/evaluate-reverse-polish-notation/) / EvalRPN\n\n**思路**：使用栈，遇到+-*/，出栈2个进行计算，计算结果再入栈\n\n2. leetcode [剑指 Offer 59 - II. 队列的最大值](https://leetcode-cn.com/problems/dui-lie-de-zui-da-zhi-lcof/) / MaxQueue\n\n**思路**：使用2个双端队列Deque，queue存储元素，helper从first到last为从大到小，遇到小于first的不进helper，所以长度会小于等于queue\n\n3. leetcode [622. 设计循环队列](https://leetcode-cn.com/problems/design-circular-queue/) / MyCircularQueue\n\n**思路**：\n\nint[] queue;&nbsp;&nbsp;&nbsp;&nbsp;\n\nint count; // 当前队列的大小&nbsp;&nbsp;&nbsp;&nbsp;\n\nint headIndex; // 当前队列添加元素的位置&nbsp;&nbsp;&nbsp;&nbsp;\n\nint capacity; // 队列的总容量\n\n4. leetcode [20. 有效的括号](https://leetcode-cn.com/problems/valid-parentheses/) /\n\n**思路**：使用栈\n\n5. leetcode [1006. 笨阶乘](https://leetcode-cn.com/problems/clumsy-factorial/)\n\n**思路**：使用栈；int op = 0; // 0乘，1除，2加，3减，最后再相加\n\n6. leetcode [155. 最小栈](https://leetcode.cn/problems/min-stack/)\n\n**思路**：能O(1) getMin 使用2个栈，一个栈按照入栈顺序，一个栈按照栈顶最小，栈底最大的顺序。\n\n注意：minStack的size会小于dataStack，因为如果入栈顺序的1，2，3，则minStack只会是1\n\n7. leetcode [224. 基本计算器](https://leetcode-cn.com/problems/basic-calculator/) / Calculate\n\n**思路**：双栈，3道题都是直接使用完美解法\n\n[224. 基本计算器](https://leetcode-cn.com/problems/basic-calculator/) （只有+ - 和 () ）\n\n[772. 基本计算器 III](https://leetcode.cn/problems/basic-calculator-iii/description/) （+ - * / () 都有）\n\n&nbsp;\n\n## **堆**\n\n1. leetcode [295. 数据流的中位数](https://leetcode-cn.com/problems/find-median-from-data-stream/) / MedianFinder\n\n**思路**：使用PriorityQueue（默认是小顶堆）来构建小顶堆（从小到大）和大顶堆（从大到小），优先放大顶堆，可能会比小顶堆多1个，使用一个index来表示当前数据流的大小，先放再调整两个堆\n\n时间复杂度O(logN)，空间复杂度O(N)，堆的插入和删除时间复杂度都是O(N)\n\n2. leetcode [480. 滑动窗口中位数](https://leetcode-cn.com/problems/sliding-window-median/) / MedianSlidingWindow\n\n**思路**：使用PriorityQueue（默认是小顶堆）来构建小顶堆（从小到大）和大顶堆（从大到小）,bigSize和smallSize分别表示大顶堆的大小和小顶堆的大小，大小一样的话优化放大顶堆\n\ni >= k - 1的时候需要计算result[]，i > k - 1需要remove堆中的数，remove的时候需要确定remove的值在大顶堆还是小顶堆，还要注意调整两个堆的大小\n\n&nbsp;\n\n## **链表**\n\n1. leetcode [1389. 按既定顺序创建目标数组](https://leetcode-cn.com/problems/create-target-array-in-the-given-order/)\n\n**思路**：使用链表的性质，然后再放回数组中\n\n2. leetcode [82. 删除排序链表中的重复元素 II](https://leetcode-cn.com/problems/remove-duplicates-from-sorted-list-ii/) / DeleteDuplicates\n\n**思路**：使用递归，head和head.next相等，while(head==head.next)，head=head.next，然后递归；不相等，直接递归head.next\n\n3. leetcode [142. 环形链表 II](https://leetcode-cn.com/problems/linked-list-cycle-ii/) / DetectCycle\n\n**思路**：快慢指针，然后再把一个指针从头开始走，相遇即环的开始节点\n\n4. leetcode [147. 对链表进行插入排序](https://leetcode-cn.com/problems/insertion-sort-list/) / InsertionSortList\n\n**思路**：开头添加min，然后pre,cur,next，当pre > cur的时候说明需要排序，从头开始需要插入的位置\n\n5. leetcode [剑指 Offer 25. 合并两个排序的链表](https://leetcode-cn.com/problems/he-bing-liang-ge-pai-xu-de-lian-biao-lcof/) / MergeTwoSortedLists\n\n**思路**：有递归和非递归两种解法\n\n6. leetcode [23. 合并K个升序链表](https://leetcode-cn.com/problems/merge-k-sorted-lists/) / MergeNSortedLists\n\n**思路**：使用归并思想，分别求\n\nListNode l1 = mergeKLists(Arrays.copyOfRange(lists, 0, mid + 1));\n\nListNode l2 = mergeKLists(Arrays.copyOfRange(lists, mid + 1, length));\n\n最后 return MergeTwoLists(l1, l2);\n\n7.leetcode [86. 分隔链表](https://leetcode-cn.com/problems/partition-list/) / Partition\n\n**思路**：两个虚拟节点，遍历链表，大于的放大链表，小于的放小链表，然后再将两个链表连接\n\n8. leetcode [19. 删除链表的倒数第N个节点](https://leetcode-cn.com/problems/remove-nth-node-from-end-of-list/) / RemoveNthNodeFromEndOfList\n\n**思路**：先把cur移动n位，如果n大于等于链表的长度的话，去掉head，然后cur和temp同时向后移动，cur已经走了N步，当cur到结束的时候，temp位于倒数N的位置\n\n9. leetcode [143. 重排链表](https://leetcode-cn.com/problems/reorder-list/) / ReorderList\n\n**思路**：快慢指针确定中心，拆成2个链表，翻转第2个链表，再合并2个链表\n\n10. leetcode [92. 反转链表 II](https://leetcode-cn.com/problems/reverse-linked-list-ii/) / ReverseBetween\n\n反转从位置 m 到 n 的链表。请使用一趟扫描完成反转\n\n**思路**：链表最开始添加一个虚拟节点，然后pre，cur，next3个节点，加上一个first节点记录记录m位置的前一个节点，然后连接m位置到n位置的下一个节点，连接m位置到n位置\n\n11. leetcode [25. K 个一组翻转链表](https://leetcode-cn.com/problems/reverse-nodes-in-k-group/) / ReverseKGroup\n\n**思路**：需要一个翻转链表的函数，链表最开始添加一个虚拟节点，然后遍历到k个一组断开链表，并翻转链表，需要定义pre，cur用于遍历，start和end用于标记链表的起始（翻转后start和end对调），以及preEnd用于连接end（end为翻转后的start）\n\n12. leetcode [61. 旋转链表](https://leetcode-cn.com/problems/rotate-list/) / RotateRight\n\n**思路**：统计链表长度，并找到最后的链表节点，然后连接到head，找到新head的位置，并断开链表\n\n13. leetcode [24. 两两交换链表中的节点](https://leetcode-cn.com/problems/swap-nodes-in-pairs/) / SwapNodesInPairs\n\n**思路**：pre,cur,next，添加一个fake节点，while(cur != null &amp;&amp; cur.next != null)，把next节点的初始化放在while里面\n\n14. leetcode [148. 排序链表](https://leetcode.cn/problems/sort-list/)\n\n**思路**：找到链表的中间节点，并断开；递归求两个排好序的链表；再合并2个排序好的链表\n\n&nbsp;\n\n## **哈希表**\n\n1. leetcode [535. TinyURL 的加密与解密](https://leetcode-cn.com/problems/encode-and-decode-tinyurl/) / Codec\n\n**思路**：使用 int key = random.nextInt(Integer.MAX_VALUE); 来作为HashMap的key\n\n2. leetcode [187. 重复的DNA序列](https://leetcode-cn.com/problems/repeated-dna-sequences/) / FindRepeatedDnaSequences\n\n**思路**：找到长度为10的重复出现的子串，利用HashSet，不在hash的加入，在的话就加入结果中\n\n3. leetcode [451. 根据字符出现频率排序](https://leetcode-cn.com/problems/sort-characters-by-frequency/) / FrequencySort\n\n**思路**：使用int[255]先统计出现的次数，然后使用TreeMap<Integer, List<Character>>存储，key是出现的次数，value是包含出现key次的字符的list，TreeMap默认是按key从小到大，所以最后要reverse\n\n4. leetcode [49. 字母异位词分组](https://leetcode-cn.com/problems/group-anagrams/) / GroupAnagrams\n\n**思路**：将string变成char[]之后排序，再转string，然后通过hashset来判断是否存在\n\n5. leetcode [1. 两数之和](https://leetcode-cn.com/problems/two-sum/) / TwoSum\n\n**思路**：使用HashMap\n\n6. leetcode [336. 回文对](https://leetcode-cn.com/problems/palindrome-pairs/) / PalindromePairs\n\n**思路**：将word reverse后放入HashMap，然后遍历words，查看每个word的前缀和后缀是否在reverse的HashMap中\n\n7. leetcode [299. 猜数字游戏](https://leetcode-cn.com/problems/bulls-and-cows/)\n\n**思路**：2个变量A和B，A表示猜对位置的个数，B表示猜对，但是位置不对的个数；map的表示不相等的时候，数字key出现的次数为value\n\n扫一遍统计A，再扫一次，统计B\n\n8. leetcode [219. 存在重复元素 II](https://leetcode-cn.com/problems/contains-duplicate-ii/)\n\n**思路**：哈希表\n\n&nbsp;\n\n## **双指针**\n\n1. leetcode [443. 压缩字符串](https://leetcode-cn.com/problems/string-compression/)\n\n**思路**：要求原地，read指针和write指针，再加一个pre记录相同字符开始的位置\n\n2.&nbsp;leetcode [11. 盛最多水的容器](https://leetcode-cn.com/problems/container-with-most-water/)\n\n**思路**：从两边开始向中心收拢，收拢时取两边中高度低的那边\n\n3. leetcode [986. 区间列表的交集](https://leetcode-cn.com/problems/interval-list-intersections/)\n\n**思路**：使用两对双指针\n\n4. leetcode [392. 判断子序列](https://leetcode-cn.com/problems/is-subsequence/)\n\n**思路**：使用双指针，while(sIndex < sLen &amp;&amp; tIndex < tLen)，字符相等两个index都++，不等tIndex++，最后如果sIndex==sLen的话，返回true，否则false\n\n5. leetcode [209. 长度最小的子数组](https://leetcode-cn.com/problems/minimum-size-subarray-sum/)\n\n**思路**：使用双指针left和right，两个while\n\n6. leetcode [503. 下一个更大元素 II](https://leetcode-cn.com/problems/next-greater-element-ii/)\n\n**思路**：使用双指针left和right\n\n7. leetcode [31. 下一个排列](https://leetcode-cn.com/problems/next-permutation/)\n\n**思路**：构建swap和reverse方法，然后 寻找nums[i - 1] < nums[i]，记下index\n\n8. leetcode [面试题 01.05. 一次编辑](https://leetcode-cn.com/problems/one-away-lcci/)\n\n**思路**：保证len1 >= len2，找到第一个不同的字符，后面的一定要相同，不然就是false\n\n9. leetcode [763. 划分字母区间](https://leetcode-cn.com/problems/partition-labels/)\n\n**思路**：遍历一遍字符S，求map[i]表示i字母在S中的最远位置，再遍历一遍S字符，当i和前一段所有的max重合，就可以切分\n\n10. leetcode [80. 删除排序数组中的重复项 II](https://leetcode-cn.com/problems/remove-duplicates-from-sorted-array-ii/)\n\n**思路**：双指针\n\n11. leetcode [26. 删除排序数组中的重复项](https://leetcode-cn.com/problems/remove-duplicates-from-sorted-array/)\n\n**思路**：双指针\n\n12. leetcode [27. 移除元素](https://leetcode-cn.com/problems/remove-element/)\n\n**思路**：双指针\n\n13. leetcode [15. 三数之和](https://leetcode-cn.com/problems/3sum/)\n\n**思路**：排序，然后遍历每一个数，然后从left=i+1到right=len-1找2个数和i相加的和等于target，while(left<right)开始双指针，三种情况sum==0；sum<0，然后left++和sum>0，然后right--\n\n注意在遍历每一个数的时候，如果遇到nums[i]==nums[i-1]相等的可以continue剪枝，遇到大于0也可以剪枝，因为后面的会更大，不可能相加等于0\n\n14. leetcode [16. 最接近的三数之和](https://leetcode-cn.com/problems/3sum-closest/)\n\n**思路**：首先int result = nums[0] + nums[1] + nums[2];，然后如果 if (Math.abs(sum - target) < Math.abs(result - target)) result = sum；不能使用回溯搜索，因为会超时\n\n15. leetcode [457. 环形数组是否存在循环](https://leetcode-cn.com/problems/circular-array-loop/)\n\n**思路**：快慢指针\n\n16. leetcode [165. 比较版本号](https://leetcode-cn.com/problems/compare-version-numbers/)\n\n**思路**：使用双指针比较多轮，第一轮比较每一个.之前的数值的大小，如果第一轮没有比较出结果，再比较下一轮\n\n&nbsp;\n\n## **二分查找**\n\n**二分是while(start<=end) {}, 然后mid<target, start=mid+1，否则mid<target, end=mid-1<br />**\n\n1. leetcode&nbsp;[33. 搜索旋转排序数组](https://leetcode.cn/problems/search-in-rotated-sorted-array/) （没有重复元素，返回index） \n\n**思路**：二分查找，可以写个binarySerach来递归，也可以用while来非递归。时间复杂度O(lgN)，分情况：左边有序和右边有序，结束条件是mid==left，也就是最后剩下1个或者2个元素\n\n2. leetcode [81. 搜索旋转排序数组 II](https://leetcode.cn/problems/search-in-rotated-sorted-array-ii/) （有重复元素，返回存不存在boolean）\n\n3. leetcode [153. 寻找旋转排序数组中的最小值](https://leetcode-cn.com/problems/find-minimum-in-rotated-sorted-array/)\n\n**思路**：mid如果大于right，left=mid+1；否则right=mid\n\n4. leetcode [154. 寻找旋转排序数组中的最小值 II](https://leetcode-cn.com/problems/find-minimum-in-rotated-sorted-array-ii/)\n\n**思路**：mid如果大于right，left=mid+1；小于right=mid；等于right--\n\n5. 剑指offer 38（书） 数字在排序数组中出现的次数 **GetNumberOfK**，统计一个数字在排序数组中出现的次数。比如，1，2，3，3，3，3，4，5和k=3，结果是4次\n\n**思路**：二分查找，找到第一个数和最后一个数\n\n6. leetcode [287. 寻找重复数](https://leetcode-cn.com/problems/find-the-duplicate-number/)\n\n**思路**：比较难，使用二分，start=0，end=len-1，计算小于等于mid的个数，如果count小于等于mid，在右边，反之在左边，注意二分都是start<=end，start = mid+1,end = mid-1\n\n7. leetcode [162. 寻找峰值](https://leetcode-cn.com/problems/find-peak-element/)\n\n**思路**：二分，nums[mid] < nums[mid + 1]递归右边，否则递归左边\n\n8. leetcode [633. 平方数之和](https://leetcode-cn.com/problems/sum-of-square-numbers/)\n\n**思路**：二分，在start和end之间使用二分查找寻找是否有mid*mid=n\n\n9. leetcode [378. 有序矩阵中第K小的元素](https://leetcode-cn.com/problems/kth-smallest-element-in-a-sorted-matrix/)\n\n**思路**：(0,0)到(row-1,col-1)开始二分查找，统计不大于mid的个数，然后 start = mid + 1; 或者 end = mid;\n\n10. leetcode [剑指 Offer 53 - II. 0～n-1中缺失的数字](https://leetcode-cn.com/problems/que-shi-de-shu-zi-lcof/)\n\n**思路**：简单的二分查找\n\n11. leetcode [69. x 的平方根](https://leetcode-cn.com/problems/sqrtx/) / MySqrt\n\n**思路**：如果输入为int，使用二分查找，时间复杂度O(logx)，空间复杂度O(1)：小于等于1，直接返回；然后while(start<=end)，start=1，end=x，如果mid==x/mid，直接返回mid；否则二分，最后返回end\n\n如果输入为float的变种，小于等于0，直接返回，然后while (Math.abs(end - begin) > 0.00001)，如果mid==x/mid，直接返回mid；否则二分，最后返回end\n\n12. leetcode [35. 搜索插入位置](https://leetcode-cn.com/problems/search-insert-position/) / SearchInsert\n\n**思路**：首先是否小于最小值，或者大于最大值；然后二分查找，left==right的时候，可能是mid或者mid+1，left!=right的时候，进行二分查找\n\n13. leetcode [34. 在排序数组中查找元素的第一个和最后一个位置](https://leetcode-cn.com/problems/find-first-and-last-position-of-element-in-sorted-array/) / SearchRange\n\n**思路**：先用searchRange方法进行二分查找，找到等于target的mid，然后再使用seachFirst方法找第一个（mid==0或者nums[mid-1]!=target）和seachLast方法找最后一个（mid==len-1或者nums[mid+1]!==target），也都是使用了二分查找\n\n14. leetcode [540. 有序数组中的单一元素](https://leetcode-cn.com/problems/single-element-in-a-sorted-array/) / SingleNonDuplicate\n\n**思路**：二分查找，while(start<end) 通过end-mid是奇数还是偶数为flag，然后nums[mid - 1] == nums[mid]和nums[mid] == nums[mid + 1]来判断单个是在左边还是右边，如果是都不等于，那直接就返回mid\n\n15. leetcode [4. 寻找两个正序数组的中位数](https://leetcode.cn/problems/median-of-two-sorted-arrays/)\n\n**思路**：二分，如果 arr[mid] > arr[mid+1]，则在左边，right=mid-1；否则在右边，left=mid+1\n\n## **数组**\n\n最大化最小值/最小化最大值\n\n[https://leetcode.cn/problems/find-a-peak-element-ii/solutions/2571587/tu-jie-li-yong-xing-zui-da-zhi-pan-duan-r4e0n/](https://leetcode.cn/problems/find-a-peak-element-ii/solutions/2571587/tu-jie-li-yong-xing-zui-da-zhi-pan-duan-r4e0n/)\n\n## **数组**\n\n1. leetcode [两数之和 II - 输入有序数组](https://leetcode-cn.com/problems/two-sum-ii-input-array-is-sorted/)\n\n给定一个已按照****升序排列**&nbsp;**的有序数组，找到两个数使得它们相加之和等于目标数。\n\n**思路**：第一个数遍历找，第二个数二分找\n\n2.&nbsp;[剑指 Offer 56 - I. 数组中数字出现的次数](https://leetcode-cn.com/problems/shu-zu-zhong-shu-zi-chu-xian-de-ci-shu-lcof/)\n\n一个整型数组 `nums` 里除两个数字之外，其他数字都出现了两次。请写程序找出这**两个**只出现一次的数字。要求时间复杂度是O(n)，空间复杂度是O(1)。\n\n**思路**：如果是只有一个的话，直接异或，如果是2个的话，分组异或，(temp &amp; div) == 0就div *= 2；然后分组异或\n\n3. [剑指 Offer 56 - II. 数组中数字出现的次数 II](https://leetcode-cn.com/problems/shu-zu-zhong-shu-zi-chu-xian-de-ci-shu-ii-lcof/)\n\n在一个数组 `nums` 中除一个数字只出现一次之外，其他数字都出现了三次。请找出那个只出现一次的数字。\n\n**思路**：转换成二进制，用一个int[32]来让每一位的数相加，之后余3，得到结果\n\n4. leetcode [1300. 转变数组后最接近目标值的数组和](https://leetcode-cn.com/problems/sum-of-mutated-array-closest-to-target/)\n\n**思路**：发现当下值大于（目标值减去累加和除以剩下个数的值）时，则返回剩余的平均值，注意<=0.5靠近curAve，>0.5靠近curAve+1\n\n5. leetcode [755. 倒水](https://leetcode-cn.com/problems/pour-water/) / PourWater\n\n**思路**：遍历每一滴水滴\n\n6. leetcode [28. 实现 strStr()](https://leetcode-cn.com/problems/implement-strstr/)\n\n**思路**：常规方法时间复杂度O((N&minus;L)L)，时间复杂度O(N)方法\n\n7. [剑指 Offer 21. 调整数组顺序使奇数位于偶数前面](https://leetcode-cn.com/problems/diao-zheng-shu-zu-shun-xu-shi-qi-shu-wei-yu-ou-shu-qian-mian-lcof/)\n\n**思路**：先找出奇数的个数，然后使用双指针\n\n8. leetcode&nbsp;[1299. 将每个元素替换为右侧最大元素](https://leetcode-cn.com/problems/replace-elements-with-greatest-element-on-right-side/)\n\n**思路**：从最后开始遍历\n\n9. leetcode&nbsp;[1497. 检查数组对是否可以被 k 整除](https://leetcode-cn.com/problems/check-if-array-pairs-are-divisible-by-k/) / CheckIfArrayPairsAreDivisibleByK\n\n**思路**：先求每个数除以k的余数，注意可能有负数，所以需要+k后再余k，然后求余数为i和k-i的是否相等\n\n10. leetcode [238. 除自身以外数组的乘积](https://leetcode-cn.com/problems/product-of-array-except-self/)\n\n**思路**：分情况讨论，有1个以上0的话全部都是0；有1个0，除了i的那个以外其他都是0；没有0，求所有的乘积，除以本身\n\n11. leetcode [277. 搜寻名人](https://leetcode-cn.com/problems/find-the-celebrity/)\n\n**思路**：找到出度为0的人，最后验证这个名人是否别人都认识，以及都不认识别人\n\n12. leetcode [268. 缺失数字](https://leetcode-cn.com/problems/missing-number/)\n\n**思路**：index和nums[i]异或两次，missing ^= i ^ nums[i];\n\n13. leetcode [134. 加油站](https://leetcode-cn.com/problems/gas-station/)\n\n**思路**：先求remain数组的和，如果小于0肯定不行，大于0说明肯定可以，再求遍历一遍求前缀remain数组的和，遇到小于left置为right的下一个\n\n14. leetcode&nbsp;[455. 分发饼干](https://leetcode-cn.com/problems/assign-cookies/)\n\n**思路**：遍历每个孩子，如果不能满足就饼干++\n\n15. leetcode [442. 数组中重复的数据](https://leetcode-cn.com/problems/find-all-duplicates-in-an-array/)\n\n**思路**：int index = Math.abs(nums[i]) - 1，并将其翻转成负数，如果后面遍历遇到负数就是出现2次的数\n\n16. leetcode [674. 最长连续递增序列](https://leetcode-cn.com/problems/longest-continuous-increasing-subsequence/)\n\n**思路**：比较nums[i]和nums[i-1]，count++，否则count=1，比较得出max，序列不是连续的，子串是连续的，题目要求连续序列\n\n17. leetcode [581. 最短无序连续子数组](https://leetcode-cn.com/problems/shortest-unsorted-continuous-subarray/)\n\n**思路**：找到left和right，0到left为递增，right到len-1为递增，之后找到left和right之间的min和max，left向左找到小于min的，right向右找到大于max的\n\n18. leetcode [41. 缺失的第一个正数](https://leetcode-cn.com/problems/first-missing-positive/)\n\n**思路**：原地址哈希法，while (nums[i] > 0 &amp;&amp; nums[i] <= len &amp;&amp; nums[nums[i] - 1] != nums[i]) {&nbsp;swap(nums, i, nums[i] - 1); }\n\n19. leetcode [289. 生命游戏](https://leetcode-cn.com/problems/game-of-life/)\n\n**思路**：统计live的数量，然后判断，使用 {-1, 0, 1} 辅助遍历周围个点\n\n20. leetcode [845. 数组中的最长山脉](https://leetcode-cn.com/problems/longest-mountain-in-array/)\n\n**思路**：计算上升的长度和下降的长度，都大于0的时候upLen+downLen+1，相等i++\n\n21. leetcode [剑指 Offer 39. 数组中出现次数超过一半的数字](https://leetcode-cn.com/problems/shu-zu-zhong-chu-xian-ci-shu-chao-guo-yi-ban-de-shu-zi-lcof/)\n\n**思路**：每找出两个不同的element，则成对删除。最终剩下的一定就是所求的\n\n22. leetcode [229. 求众数 II](https://leetcode-cn.com/problems/majority-element-ii/)\n\n**思路**：使用摩尔投票法确定2个候选，再确定是否真的大于N/3\n\n23. leetcode [152. 乘积最大子数组](https://leetcode-cn.com/problems/maximum-product-subarray/)\n\n**思路**：整数肯定是越乘越多，且至少包含一个数字，如果是负数交换一下再比较自身和乘积，时间复杂度O(N)\n\n24. leetcode [1014. 最佳观光组合](https://leetcode-cn.com/problems/best-sightseeing-pair/)\n\n**思路**：将A[i]+i看成一个整体，将A[j]-j看成一个整体，遍历，记下max为A[i]+i为最大值，result为max+(A[j]-j)的最大值\n\n25. leetcode [217. 存在重复元素](https://leetcode-cn.com/problems/contains-duplicate/)\n\n**思路**：排序然后检查i-1和i是否相同\n\n26. leetcode [945. 使数组唯一的最小增量](https://leetcode-cn.com/problems/minimum-increment-to-make-array-unique/)\n\n**思路**：时间复杂度O(NlogN)，先排序，遍历如果A[i-1]>=A[i]，将A[i] = A[i-1] + 1，并计算result += (A[i - 1] - A[i] + 1);\n\n计数排序，时间复杂度O(L)，L的数量级是数组 A 的长度加上其数据范围内的最大值\n\n27. leetcode [189. 旋转数组](https://leetcode-cn.com/problems/rotate-array/)\n\n**思路**：先k%=len，然后整体翻转一次，0至k-1翻转一次，k到len-1翻转一次\n\n28. leetcode [136. 只出现一次的数字](https://leetcode-cn.com/problems/single-number/)\n\n**思路**：异或 ^=\n\n29. leetcode [75. 颜色分类](https://leetcode-cn.com/problems/sort-colors/)\n\n**思路**：2遍扫描，第一遍统计0 1 2的个数，第二个放置0 1 2\n\n30. leetcode [1534. 统计好三元组](https://leetcode-cn.com/problems/count-good-triplets/)\n\n**思路**：暴力法，3个while循环\n\n31. leetcode [280. 摆动排序](https://leetcode.cn/problems/wiggle-sort/)\n\n给你一个无序的数组 nums, 将该数字 原地 重排后使得 nums[0] <= nums[1] >= nums[2] <= nums[3]...\n\n**思路**：写一个swap函数，然后遍历0到i-1，按flag分情况，不符合就swap，最后flag = !flag\n\n32. leetcode [324. 摆动排序 II](https://leetcode.cn/problems/wiggle-sort-ii/)\n\n&nbsp;\n\n## **二叉树**\n\n1.&nbsp; [剑指 Offer 26. 树的子结构](https://leetcode-cn.com/problems/shu-de-zi-jie-gou-lcof/)\n\n**思路**：使用递归，2个递归函数，时间复杂度O(M*N)，空间复杂度O(M)，M为A树的节点数量，N为B树的节点数量\n\n2. 二叉树深度 coding interview guide\n\n**思路**：使用递归\n\n3. leetcode [94. 二叉树的中序遍历](https://leetcode-cn.com/problems/binary-tree-inorder-traversal/)\n\n**思路**：递归，非递归，Morris遍历\n\n4. leetcode [173. 二叉搜索树迭代器](https://leetcode-cn.com/problems/binary-search-tree-iterator/)\n\n**思路**：使用中序遍历将二叉搜索树放入ArrayList中，然后通过index和size来构造hasNext和next函数\n\n5. leetcode [116. 填充每个节点的下一个右侧节点指针](https://leetcode-cn.com/problems/populating-next-right-pointers-in-each-node/)\n\n**思路**：层次遍历变种，时间复杂度O(N)，空间复杂度O(N)；拉拉链解法，使用递归，时间复杂度O(N)，空间复杂度O(1)\n\n6. leetcode [117. 填充每个节点的下一个右侧节点指针 II](https://leetcode-cn.com/problems/populating-next-right-pointers-in-each-node-ii/)\n\n**思路**：层次遍历变种，时间复杂度O(N)，空间复杂度O(N)；递归，需要利用next指针找到有子节点的上一层节点，时间复杂度O(N)，空间复杂度O(1)\n\n7. leetcode [654. 最大二叉树](https://leetcode-cn.com/problems/maximum-binary-tree/)\n\n**思路**：找到最大的index，找不到index=-1，然后递归(0,index)和(index+1,len)\n\n8. leetcode [538. 把二叉搜索树转换为累加树](https://leetcode-cn.com/problems/convert-bst-to-greater-tree/)\n\n**思路**：逆中序遍历，时间复杂度O(N)，空间复杂度O(N)；Morris中序遍历，时间复杂度O(N)，空间复杂度O(1)\n\n9. leetcode [222. 完全二叉树的节点个数](https://leetcode-cn.com/problems/count-complete-tree-nodes/)\n\n**思路**：递归比较左右子树高度，如果相等，左子树为满树，加上1<<left，继续算右子树；反之右子树为满树，加上1<<right，继续算左子树\n\n10. leetcode [450. 删除二叉搜索树中的节点](https://leetcode-cn.com/problems/delete-node-in-a-bst/)\n\n**思路**：通过大小判断，递归左右子树，相等的时候，分情况，没有左子树返回右子树，没有右子树返回左子树，都有，找到右子树中最左的孩子，然后连接左子树后，返回右子树\n\n11. leetcode [543. 二叉树的直径](https://leetcode-cn.com/problems/diameter-of-binary-tree/)\n\n**思路**：递归求左右子树深度\n\n12. leetcode [513. 找树左下角的值](https://leetcode-cn.com/problems/find-bottom-left-tree-value/)\n\n**思路**：层次遍历\n\n13. leetcode [114. 二叉树展开为链表](https://leetcode-cn.com/problems/flatten-binary-tree-to-linked-list/)\n\n**思路**：将root的右子树接到左子树的最右边，再把root的左子树放到其右子树的位置\n\n14. leetcode [95. 不同的二叉搜索树 II](https://leetcode-cn.com/problems/unique-binary-search-trees-ii/)\n\n**思路**：递归，i从start到end，然后递归start到i-1，递归i+1到end\n\n15. leetcode [285. 二叉搜索树中的顺序后继](https://leetcode-cn.com/problems/inorder-successor-in-bst/)\n\n**思路**：中序遍历\n\n16. leetcode [701. 二叉搜索树中的插入操作](https://leetcode-cn.com/problems/insert-into-a-binary-search-tree/)\n\n**思路**：递归，时间复杂度O(N)，空间复杂度O(1)\n\n17. leetcode [剑指 Offer 55 - II. 平衡二叉树](https://leetcode-cn.com/problems/ping-heng-er-cha-shu-lcof/)\n\n**思路**：递归，diff = Math.abs(leftHeight - rightHeight); diff > 1返回-1，否则 return Math.max(leftHeight, rightHeight) + 1;\n\n18. leetcode [230. 二叉搜索树中第K小的元素](https://leetcode-cn.com/problems/kth-smallest-element-in-a-bst/)\n\n**思路**：使用中序遍历\n\n19. leetcode [515. 在每个树行中找最大值](https://leetcode-cn.com/problems/find-largest-value-in-each-tree-row/)\n\n**思路**：bfs层次遍历\n\n20. [剑指 Offer 32 - I. 从上到下打印二叉树](https://leetcode-cn.com/problems/cong-shang-dao-xia-da-yin-er-cha-shu-lcof/)\n\n**思路**：使用deque，层次遍历，参考 PrintFromTopToBottom\n\n21. [107. 二叉树的层序遍历 II](https://leetcode-cn.com/problems/binary-tree-level-order-traversal-ii/)\n\n思路：从下到上，层次遍历，添加的时候添加到头部，result.add(0, list);\n\n22. leetcode [面试题 04.03. 特定深度节点链表](https://leetcode-cn.com/problems/list-of-depth-lcci/)\n\n**思路**：层次遍历\n\n23. leetcode [235. 二叉搜索树的最近公共祖先](https://leetcode-cn.com/problems/lowest-common-ancestor-of-a-binary-search-tree/)\n\n**思路**：二叉搜索树，递归，如果root的val比p和q都大，递归左子树；如果root的val比p和q都小，递归右子树\n\n24. leetcode [236. 二叉树的最近公共祖先](https://leetcode.cn/problems/lowest-common-ancestor-of-a-binary-tree/)\n\n**思路**：二叉搜索树，递归，递归左边如果是null，在右边；递归右边如果是null，在左边，两边都不为null，返回root\n\n25. leetcode [124. 二叉树中的最大路径和](https://leetcode-cn.com/problems/binary-tree-maximum-path-sum/)\n\n**思路**：递归，初始化一个result=Integer.MIN_VALUE，然后额外写一个函数来计算root节点到叶子节点的最大贡献值，递归左右孩子的时候，需要和0进行比较，因为可能是负数\n\n26. leetcode [145. 二叉树的后序遍历](https://leetcode-cn.com/problems/binary-tree-postorder-traversal/)\n\n**思路**：递归和非递归\n\n27. leetcode [144. 二叉树的前序遍历](https://leetcode-cn.com/problems/binary-tree-preorder-traversal/)\n\n**思路**：递归和非递归\n\n28. leetcode [99. 恢复二叉搜索树](https://leetcode-cn.com/problems/recover-binary-search-tree/)\n\n**思路**：二叉搜索树的中序遍历为有序，找出无序的两个节点，交换val，注意必须找到不符合递增的第一个数和最后一个数，然后进行交换\n\n29. leetcode [199. 二叉树的右视图](https://leetcode-cn.com/problems/binary-tree-right-side-view/)\n\n**思路**：层次遍历\n\n30. leetcode [700. 二叉搜索树中的搜索](https://leetcode-cn.com/problems/search-in-a-binary-search-tree/)\n\n**思路**：递归\n\n31. leetcode [108. 将有序数组转换为二叉搜索树](https://leetcode-cn.com/problems/convert-sorted-array-to-binary-search-tree/)\n\n**思路**：递归\n\n```\n        TreeNode root = new TreeNode(nums[mid]);\n        root.left = helper(nums, left, mid - 1);\n        root.right = helper(nums, mid + 1, right);\n\n```\n\n32. leetcode [109. 有序链表转换二叉搜索树](https://leetcode-cn.com/problems/convert-sorted-list-to-binary-search-tree/)\n\n**思路**：快慢指针找到中间的节点，然后pre表示慢指针的前一个节点，断开和慢指针的连接，然后递归head和slow.next为左孩子和右孩子\n\n33. leetcode [1315. 祖父节点值为偶数的节点和](https://leetcode-cn.com/problems/sum-of-nodes-with-even-valued-grandparent/)\n\n**思路**：使用递归\n\n34. leetcode [129. 求根到叶子节点数字之和](https://leetcode-cn.com/problems/sum-root-to-leaf-numbers/)\n\n**思路**：dfs，需要preSum记录之前的和，dfs函数计算到root节点为止的数字之和\n\n```\n    public int dfs(TreeNode root, int preSum) {\n        if (root == null) {\n            return 0;\n        }\n        int sum = preSum * 10 + root.val;\n        if (root.left == null &amp;&amp; root.right == null) {\n            return sum;\n        } else {\n            return dfs(root.left, sum) + dfs(root.right, sum);\n        }\n    }\n\n```\n\n35. leetcode [1361. 验证二叉树](https://leetcode-cn.com/problems/validate-binary-tree-nodes/)\n\n**思路**：使用一个int[] in，表示每个点的入度，几个条件：1.每个点的入度都不能超过 1；2.入度为 0 的点不能没有；3.入度为 0 的点也不能超过一个\n\n36. leetcode [662. 二叉树最大宽度](https://leetcode-cn.com/problems/maximum-width-of-binary-tree/)\n\n**思路**：层次遍历变种，添加一个含有index下标的IndexNode，然后左孩子index为父节点的index*2，右孩子index为父节点的index*2+1，width = Math.max(width, endIndex - startIndex + 1);\n\n```\n    class IndexNode {\n\n        TreeNode node;\n        int index;\n\n        public IndexNode(TreeNode node, int index) {\n            this.node = node;\n            this.index = index;\n        }\n\n    }\n\n```\n\n37. leetcode [1448. 统计二叉树中好节点的数目](https://leetcode-cn.com/problems/count-good-nodes-in-binary-tree/)\n\n思路：中序遍历，递归\n\n38. [剑指 Offer 55 - II. 平衡二叉树](https://leetcode-cn.com/problems/ping-heng-er-cha-shu-lcof/) / IsBalanced\n\n**思路**：利用后序遍历，时间复杂度O(N)，空间复杂度O(h)，h为树的高度，root为null；左子树的高度等于-1，返回-1；右子树的高度等于-1，返回-1；计算diff，diff大于1，返回-1；返回返回左右子树max高度+1\n\n39. leetcode [101. 对称二叉树](https://leetcode-cn.com/problems/symmetric-tree/) / SymmetricTree\n\n**思路**：递归法或者使用栈的递归法\n\n40. leetcode [1305. 两棵二叉搜索树中的所有元素](https://leetcode-cn.com/problems/all-elements-in-two-binary-search-trees/)\n\n**思路：**中序遍历转成2个list，然后归并排序\n\n41. leetcode [863. 二叉树中所有距离为 K 的结点](https://leetcode-cn.com/problems/all-nodes-distance-k-in-binary-tree/)\n\n**思路**：分情况：从孩子中找，从父亲中找\n\n42. leetcode&nbsp;[1382. 将二叉搜索树变平衡](https://leetcode-cn.com/problems/balance-a-binary-search-tree/)\n\n**思路**：中序遍历加到list，之后使用递归重新构建二叉搜索树\n\n43. leetcode [257. 二叉树的所有路径](https://leetcode-cn.com/problems/binary-tree-paths/)\n\n**思路**：dfs\n\n44. leetcode [814. 二叉树剪枝](https://leetcode-cn.com/problems/binary-tree-pruning/)\n\n**思路**：一个单独的函数判断是否含有1，使用dfs递归判断；\n\n左子树不含1，剪掉；否则递归\n\n右子树不含1，剪掉；否则递归\n\n最后要是左右子树都被剪掉，root也剪掉\n\n45. leetcode [314. 二叉树的垂直遍历](https://leetcode-cn.com/problems/binary-tree-vertical-order-traversal/)\n\n**思路**：使用两个map，分别记录node->index和index-list，外加层次遍历\n\n46. leetcode [545. 二叉树的边界](https://leetcode-cn.com/problems/boundary-of-binary-tree/)\n\n**思路**：分别计算左视图，右视图，最后一层视图，最后将其都加起来\n\n47. leetcode [106. 从中序与后序遍历序列构造二叉树](https://leetcode-cn.com/problems/construct-binary-tree-from-inorder-and-postorder-traversal/)\n\n**思路**：后序遍历的最后一个是根节点，然后在中序遍历中找root的index，index左边都是左子树，index右边都是右子树，递归构建二叉树\n\n48. leetcode [105. 从前序与中序遍历序列构造二叉树](https://leetcode-cn.com/problems/construct-binary-tree-from-preorder-and-inorder-traversal/)\n\n**思路**：前序遍历的第一个是根节点，使用map来标记tree value和inorder index的关系；然后写一个buildCore函数，如果 preStart == preEnd 的话，直接返回初始化的根节点，计算leftLen和rightLen，leftLen>0就递归左子树，rightLen>0就递归右子树\n\n49. leetcode [426. 将二叉搜索树转化为排序的双向链表](https://leetcode-cn.com/problems/convert-binary-search-tree-to-sorted-doubly-linked-list/)\n\n**思路**：\n\n50. leetcode [138. 复制带随机指针的链表](https://leetcode-cn.com/problems/copy-list-with-random-pointer/)\n\n**思路**：第一次遍历，在每个next节点后面复制一个一样的节点 1&mdash;>2->3 => 1->1->2->2->3->3；第二次遍历，连接random节点，第二遍的时候每次跳2个节点，新的节点连到新的节点；第三次遍历，一个一个跳，连接cur.next.next\n\n51. leetcode [98. 验证二叉搜索树](https://leetcode.cn/problems/validate-binary-search-tree/)\n\n52. leetcode [687. 最长同值路径](https://leetcode.cn/problems/longest-univalue-path/)\n\n53. leetcode 298. 二叉树最长连续序列（自顶向下）\n\n**思路**：额外写一个void dfs（TreeNode root, int preLen）函数，如果左孩子不为null，分情况：root+1==left，preLen+1；否则preLen放1；右孩子一样\n\n54. leetcode 549. 二叉树最长连续序列 II（不要求自顶向下）&nbsp;\n\n**思路**：\n\n&nbsp;\n\n## **滑动窗口**\n\n1. leetcode [剑指 Offer 59 - I. 滑动窗口的最大值](https://leetcode-cn.com/problems/hua-dong-chuang-kou-de-zui-da-zhi-lcof/)\n\n**思路**：使用**单调队列**，队列为递减的值的下标，较难\n\n2. leetcode [424. 替换后的最长重复字符](https://leetcode-cn.com/problems/longest-repeating-character-replacement/)\n\n**思路**：left和right表示滑动窗口，right++扩充窗口，记录重复出现最多的字符个数max，当right-left+1的窗口大小-max<=k说明窗口可以该窗口符合条件，否则说明窗口扩大了，需要left++\n\n3. leetcode [567. 字符串的排列](https://leetcode-cn.com/problems/permutation-in-string/)\n\n**思路**：使用int[] map = new int[26]; ，然后计算窗口中的2个map是否相等，\n\n4. leetcode [76. 最小覆盖子串](https://leetcode-cn.com/problems/minimum-window-substring/)\n\n**思路**：使用int[] map = new int[256]; ，然后使用count==0来判断是否覆盖，先右移窗口，count==0后再左移窗口，并记录下最小min时候的窗口index\n\n5. leetcode [438. 找到字符串中所有字母异位词](https://leetcode-cn.com/problems/find-all-anagrams-in-a-string/)\n\n**思路**：按招聘岗位的模板题来做，int[26] needs 岗位计划列表，int[26] hires 实际岗位剩余情况\n\n6. leetcode [3. 无重复字符的最长子串](https://leetcode-cn.com/problems/longest-substring-without-repeating-characters/)\n\n**思路**： 使用map，key是字符，value是字符出现的index，当map包含这个的时候，窗口left取该字符较大的index\n\n7. leetcode [220. 存在重复元素 III](https://leetcode-cn.com/problems/contains-duplicate-iii/)\n\n**思路**：使用TreeSet，set是一个大小为k的窗口，保证set中的元素的index和i之间的差在k之间，之后使用set.ceiling\n\n8. leetcode [239. 滑动窗口最大值](https://leetcode.cn/problems/sliding-window-maximum/?envType=featured-list&amp;envId=2cktkvj)\n\n**思路**：使用**单调队列**，存的是index，first是大的值的index，last是小的值的index，当新来一个nums[index]比last要大，while出列，因为不可能是窗口的最大值了；左边超过窗口也要出列；i+1>=k开始填充结果\n\n&nbsp;\n\n## **前缀和**\n\n1. leetcode [523. 连续的子数组和](https://leetcode-cn.com/problems/continuous-subarray-sum/)\n\n**思路**：连续子数组一定要想到前缀和，使用map来记录，key=前缀和%k，value=index，i-index>1即可\n\n2. leetcode [560. 和为K的子数组](https://leetcode-cn.com/problems/subarray-sum-equals-k/)\n\n给定一个整数数组和一个整数&nbsp;**k，**你需要找到该数组中和为&nbsp;**k&nbsp;**的连续的子数组的个数。\n\n**思路**：较长的前缀和减去较短的前缀和，等于中间的子数组的和\n\n3. leetcode [1248. 统计「优美子数组」](https://leetcode-cn.com/problems/count-number-of-nice-subarrays/)\n\n**思路**：HashMap的key表示前缀子序列的奇数个数，val表示个数\n\n4. leetcode [437. 路径总和 III](https://leetcode-cn.com/problems/path-sum-iii/)\n\n**思路**：前序遍历+前缀和\n\n5. leetcode [974. 和可被 K 整除的子数组](https://leetcode-cn.com/problems/subarray-sums-divisible-by-k/)\n\n**思路**：HashMap record的key表示前缀和除以K剩下余数（正数）,value表示余数等于key的个数\n\n&nbsp;\n\n## **递归和动态规划**\n\ndp全家桶参考：[https://oi-wiki.org/dp/memo/](https://oi-wiki.org/dp/memo/) 和 [https://chengzhaoxi.xyz/42296.html](https://chengzhaoxi.xyz/42296.html)\n\n### 线性dp：单串dp，dp[i]（最长不下降xx）\n\n**思路**：dp，时间复杂度O(N^2)，时间复杂度O(N)；贪心+二分，时间复杂度O(NlogN)，时间复杂度O(N)，需要再看\n\n2. leetcode [32. 最长有效括号](https://leetcode-cn.com/problems/longest-valid-parentheses/)\n\n**思路**：使用dp，遍历当时 ')' 的时候，如果前一个是 '(' ，dp[i] = i - 2 >= 0 ? dp[i - 2] + 2 : 2; 否则 寻找 i - dp[i - 1] - 1 位置\n\n4. leetcode [983. 最低票价](https://leetcode-cn.com/problems/minimum-cost-for-tickets/)\n\n**思路**：使用dp，dp[i]表示第i天的最小花费，初始化dp[366]，dp[0]=set.contains(0) ? costs[0]: 0，遍历1-366天\n\n5. leetcode [413. 等差数列划分](https://leetcode-cn.com/problems/arithmetic-slices/)\n\n**思路：**dp[i]表示以nums[0-i]的等差子数组个数，dp[i] = dp[i-1]+1，sum+=dp[i]\n\n6. leetcode [1024. 视频拼接](https://leetcode-cn.com/problems/video-stitching/)\n\n**思路**：dp[i]表示拼接[0,i)视频的最小数量；先填充 Arrays.fill(dp, Integer.MAX_VALUE - 1);；然后遍历，如果区间重合的话 dp[i] = Math.min(dp[i], dp[clip[0]] + 1);\n\n7. leetcode [91. 解码方法](https://leetcode-cn.com/problems/decode-ways/)\n\n**思路**：dp[i]表示0-i的字符串的解码个数，如果i位置不等于'0'，dp[i]=dp[i-1]，如果i-1到i的字符为10-26，则需要加上1或者dp[i-2]\n\n### 线性dp：单串dp+多维状态，dp[i][k]\n\n1. leetcode [887. 鸡蛋掉落](https://leetcode-cn.com/problems/super-egg-drop/)\n\n**思路**：\n\n### 线性dp：矩阵dp/双串dp（最长公共xx，最短路径，字符串模糊匹配）\n\n1. leetcode&nbsp;[1143. 最长公共子序列](https://leetcode-cn.com/problems/longest-common-subsequence/) 或者 coding interview guide ch4 LongestCommonSubsequence （LCS）\n\n**思路**：使用二维dp，时间复杂度O(M*N)，空间复杂度O(M*N)\n\n一维dp后面再看\n\n2. coding interview guide ch4 LongestSubstring 最长公共子串\n\n**思路**：使用二维dp\n\n3. leetcode [718. 最长重复子数组](https://leetcode-cn.com/problems/maximum-length-of-repeated-subarray/)\n\n**思路**：二维dp，dp[i][j]表示A的0到i和B的0到j的最长重复数\n\n4. leetcode [392. 判断子序列](https://leetcode-cn.com/problems/is-subsequence/)\n\n**思路**：双指针，或者逐个遍历，也可以使用**动态规划dp**，dp[i][j]表示以i结尾的字符串是否是以j结尾的字符串的子序列\n\n5. leetcode&nbsp;[64. 最小路径和](https://leetcode.cn/problems/minimum-path-sum/) 或者 coding interview guide ch4 MinPathSum\n\n给定一个包含非负整数的 **m**&nbsp;x&nbsp;**n**&nbsp;网格，请找出一条从左上角到右下角的路径，使得路径上的数字总和为最小。\n\n**思路**：使用二维dp，时间复杂度O(M*N)，空间复杂度O(M*N)\n\n一维dp后面再看\n\n6. leetcode [120. 三角形最小路径和](https://leetcode-cn.com/problems/triangle/)\n\n**思路**：dp[i][j]表示从顶部走到(i,j)的最小路径和，(i,j)指的是第i行第j列的位置，先计算第一列，因为只能从上往下走；再两层循环，i从0到len，j从0到i，计算dp[i][j]\n\n7. leetcode [221. 最大正方形](https://leetcode-cn.com/problems/maximal-square/)\n\n**思路**：dp[i][j]表示以(i,j)为右下角，只包括1的正方形的最大边长，dp[i][j] = Math.min(Math.min(dp[i - 1][j], dp[i][j - 1]), dp[i - 1][j - 1]) + 1;\n\n8. leetcode&nbsp;[85. 最大矩形](https://leetcode-cn.com/problems/maximal-rectangle/)\n\n**思路**：dp[i][j]表示matrix[i][j]左边（包含本身）连续1的个数，然后从i开始到0，遍历并计算最小宽度时候的面积\n\n9. leetcode [剑指 Offer 47. 礼物的最大价值](https://leetcode-cn.com/problems/li-wu-de-zui-da-jie-zhi-lcof/)\n\n**思路**：dp[i][j]来自dp[i-1][j]或者dp[i][j-1]\n\n10. leetcode [10. 正则表达式匹配](https://leetcode-cn.com/problems/regular-expression-matching/)\n\n**思路**：动态规划，boolean[][] dp = new boolean[sLen + 1][pLen + 1];\n\n11. leetcode [44. 通配符匹配](https://leetcode-cn.com/problems/wildcard-matching/)\n\n**思路**：动态规划，boolean[][] dp = new boolean[sLen + 1][pLen + 1];\n\n12. leetcode [72. 编辑距离](https://leetcode-cn.com/problems/edit-distance/)\n\n**思路**：dp[i][j]表示word1的前i个字符变换到word2的前j个字符，需要的最短操作次数；\n\n分3中情况，从[i-1,j]删一个字符 或者 从[i,j-1]加1个字符的情况；第i和第j个字符相等，不用变换；第i和第j个字符不相等，需要变换\n\n### 线性dp：矩阵dp/双串dp+多维状态，dp[i][j][k]\n\n1. leetcode [1139. 最大的以 1 为边界的正方形](https://leetcode-cn.com/problems/largest-1-bordered-square/)\n\n**思路**：3层dp，dp[i][j][0]表示grid[i-1,j-1]左边连续1的个数(包括自身)；dp[i][j][1]表示grid[i-1,j-1]上边连续1的个数(包括自身)\n\n### 线性dp：其他\n\n1. leetcode [198. 打家劫舍](https://leetcode-cn.com/problems/house-robber/)\n\n**思路**：dp表示偷到nums[i]户的最多钱，dp[i] = Math.max(dp[i - 1], dp[i - 2] + nums[i]); 时间复杂度O(N)，空间复杂度O(N)；空间复杂度可以压缩到O(1)，使用first和second\n\n2. leetcode [213. 打家劫舍 II](https://leetcode-cn.com/problems/house-robber-ii/)\n\n**思路**：使用上一题的答案，加上 return Math.max(rob2(Arrays.copyOfRange(nums, 0, len - 1)), rob2(Arrays.copyOfRange(nums, 1, len))); ，因为第一家要么偷，要么不偷\n\n3. leetcode [剑指 Offer 14- I. 剪绳子](https://leetcode-cn.com/problems/jian-sheng-zi-lcof/) / [343. 整数拆分](https://leetcode-cn.com/problems/integer-break/)\n\n**思路**：dp[i]表示i米绳子的最大乘积/整数i拆分后的最大乘积，dp[1]=1，两层循环，dp[i] = Math.max(dp[i], (Math.max(j, dp[j])) * (Math.max(i - j, dp[i - j])))\n\n4. leetcode [139. 单词拆分](https://leetcode-cn.com/problems/word-break/)\n\n**思路**：dp[i]表示0-i的字符串是否能用list中的单词组成，两层循环，外层判断0到len的单词如果存在，直接 dp[j] = true; 内层再遍历0到i，如果 wordDict.contains(s.substring(j, i)) &amp;&amp; dp[j] ，dp[j] = true\n\n5. leetcode [650. 只有两个键的键盘](https://leetcode-cn.com/problems/2-keys-keyboard/)\n\n**思路**：dp[i]表示i个A的最少操作数\n\n6. leetcode [787. K 站中转内最便宜的航班](https://leetcode-cn.com/problems/cheapest-flights-within-k-stops/)\n\n**思路**：dp[i][j]表示从src开始，经过i个中转站到达目的地j的花费\n\n7. leetcode [978. 最长湍流子数组](https://leetcode-cn.com/problems/longest-turbulent-subarray/)\n\n**思路**：可以使用一维dp；可以优化空间复杂度，使用Integer.compare()来比较两个数，大于返回1，小于返回-1，等于返回0，，pre*cur小于0时候count++\n\n8. leetcde [376. 摆动序列](https://leetcode-cn.com/problems/wiggle-subsequence/)\n\n**思路**：978要求连续，376可以是不连续的，1个diff，1个diff2\n\n```\n        if (nums.length < 2) {\n            return nums.length;\n        }\n        int diff = nums[1] - nums[0];\n        int count = diff == 0 ? 1 : 2;\n        for (int i = 2; i < nums.length; i++) {\n            int diff2 = nums[i] - nums[i - 1];\n            if ((diff <= 0 &amp;&amp; diff2 > 0) || (diff >= 0 &amp;&amp; diff2 < 0)) {\n                count++;\n                diff = diff2;\n            }\n        }\n        return count;\n\n```\n\n### **区间dp**\n\n给定一个序列或字符串要进行一些操作，从最后一步出发，要将序列或字符串去头、去尾，比如最长回文子串。\n\n区间型 dp 一般用 dp[i][j] ，i 代表左端点，j 代表右端点，若有其他维度可再添加，若两个端点之间存在联系，则可再压缩空间。参考：[区间dp合集](https://leetcode-cn.com/problems/scramble-string/solution/miao-dong-de-qu-jian-xing-dpsi-lu-by-sha-yu-la-jia/)\n\n1. [剑指 Offer 42. 连续子数组的最大和](https://leetcode-cn.com/problems/lian-xu-zi-shu-zu-de-zui-da-he-lcof/) 和 leetcode [53. 最大子数组和](https://leetcode.cn/problems/maximum-subarray/)\n\n**思路**：一维dp，dp[i]数组存储以nums[i]为结尾的子数组的最大和，时间复杂度O(N)，空间复杂度O(1)，如果dp[i-1]大于0，则dp[i]=dp[i-1]+nums[i]；否则dp[i] = nums[i]\n\n2. leetcode [5. 最长回文子串](https://leetcode-cn.com/problems/longest-palindromic-substring/)\n\n**思路**：dp[i][j]表示i到j（前后包含，dp[0][0]表示第一个字符）的字符串是否是回文，然后true的时候比较maxLen，并记下start 或者 中心扩展法（更优）\n\n3. leetcode [516. 最长回文子序列](https://leetcode-cn.com/problems/longest-palindromic-subsequence/)\n\n**思路**：使用dp，i从len-1到0，j从i+1到len-1，相等时dp[i][j] = dp[i + 1][j - 1] + 2;，否则dp[i][j] = Math.max(dp[i][j - 1], dp[i + 1][j]);\n\n4. leetcode&nbsp;[647. 回文子串](https://leetcode-cn.com/problems/palindromic-substrings/)\n\n**思路**：可以使用dp，时间复杂度O(N^2)，空间复杂度O(N^2)；使用中心扩展法，时间复杂度O(N^2)，空间复杂度O(1)\n\n5. leetcode [312. 戳气球](https://leetcode-cn.com/problems/burst-balloons/)\n\n**思路**：dp[i][j]表示开区间(i,j)内能拿到多少金币，比较难\n\n6. leetcode [1246. 删除回文子数组](https://leetcode-cn.com/problems/palindrome-removal/)\n\n**思路**：dp[i][j]表示[i,j]删除到空需要的最少次数，对角线都是1，当j=i+1，dp[i][j] = arr[i] == arr[j] ? 1 : 2; 其他情况需要求min，参考代码\n\n7. leetcode [87. 扰乱字符串](https://leetcode-cn.com/problems/scramble-string/)\n\n**思路**：\n\n8. leetcode [1312. 让字符串成为回文串的最少插入次数](https://leetcode-cn.com/problems/minimum-insertion-steps-to-make-a-string-palindrome/)\n\n**思路**：dp[i][j]表示i到j变成回文的最少插入次数\n\n### **递推型/<strong>计数**DP</strong>\n\n该类问题能通过公式进行求解\n\n1. leetcode [70. 爬楼梯](https://leetcode-cn.com/problems/climbing-stairs/)\n\n**思路**：n阶由n-2阶和n-1阶的方法数相加\n\n2. leetcode [403. 青蛙过河](https://leetcode-cn.com/problems/frog-jump/)\n\n**思路**：map表示可以走value中的步数到key位置\n\n3. leetcode [62. 不同路径](https://leetcode-cn.com/problems/unique-paths/)\n\n**思路**：dp[i][j]表示到达i，j有的路径数\n\n4. leetcode [63. 不同路径 II](https://leetcode-cn.com/problems/unique-paths-ii/)\n\n**思路**：如果0,0为1，直接返回0；其他分dp[0][j]，dp[i][0]和dp[i][j]情况讨论\n\n5. leetcode [96. 不同的二叉搜索树](https://leetcode-cn.com/problems/unique-binary-search-trees/)\n\n**思路**：使用 [卡特兰数](https://baike.baidu.com/item/%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0)\n\n```\nG(n)   长度为n的序列构成的二叉搜索树的个数\nF(i,n) 以i为根节点的，长度为n的树的个数\n\nG(n) = F(0,n) + F(1,n) + ... + F(n,n)\nF(i,n) = G(i-1)*G(n-i)\n\nG(0) = 1\nG(1) = 1\n\nG(n) = G(0)*G(n-1)+G(1)*G(n-2)+...+G(n-1)*G(0)\nG(2) = G(0)*G(1) + G(1)*G(0)\n\n```\n\n### **数位dp**\n\n1. leetcode\n\n### **状态压缩dp**\n\n3. leetcode [935. 骑士拨号器](https://leetcode-cn.com/problems/knight-dialer/)\n\n**思路**：dp[i][j]表示第i次调到j的不同号码的组合数，最后需要把i==n-1时候的j从0到9累加\n\n### **树形dp**\n\n1. leetcode [337. 打家劫舍 III](https://leetcode-cn.com/problems/house-robber-iii/)\n\n**思路**：除了使用使用dfs，时间复杂度O(N)，空间复杂度O(N)；还可以使用树形dp\n\n### **01背包**\n\n**背包问题(Knapsack problem)是一种组合优化的NP完全问题。 背包问题可以描述为：<strong>给定一组物品，每种物品都有自己的重量和价格，在限定的总重量内，我们如何选择，才能使得物品的总价格最高。**</strong>\n\n由于每个物体只有两种可能的状态（取与不取），对应二进制中的0和1<img src=\"/images/517519-20230811153709070-613023755.gif\" title=\"1\" />，这类问题便被称为「0-1 背包问题」\n\n2. leetcode [416. 分割等和子集](https://leetcode-cn.com/problems/partition-equal-subset-sum/)\n\n**思路：**先求sum/2，奇数肯定不行，使用dp，dp[i][j]表示，能不能找出和为数组总和一半的子集\n\n3. leetcode [494. 目标和](https://leetcode-cn.com/problems/target-sum/)\n\n**思路**：dp[i]表示nums数组的部分能组成i的方案数，正数x，负数y，x+y=sum，x-y=S，x=(sum+S)/2\n\n4. leetcode [879. 盈利计划](https://leetcode-cn.com/problems/profitable-schemes/)\n\n思路：\n\n### **01背包**+多维状态\n\n1. leetcode&nbsp;[474. 一和零](https://leetcode-cn.com/problems/ones-and-zeroes/)\n\n**思路**：dp[i][j]表示i个0和j个1能包含的最多的字符串的个数，另外需要额外写一个函数countzeroandone，返回int[]来用统计0和1的个数\n\n### **完全背包dp**\n\n完全背包模型与 0-1 背包类似，与 0-1 背包的区别仅在于一个物品可以选取无限次，而非仅能选取一次。\n\n1. leetcode [面试题 08.11. 硬币](https://leetcode-cn.com/problems/coin-lcci/)\n\n**思路**：换硬币的可能性有多少种，一维dp，先遍历coins，然后再遍历1到N；二维dp，dp[x][y]表示使用前coins[x]换钱的方法，注意coin要先遍历，大于时 dp[i][j] = (dp[i - 1][j] + dp[i][j - coins[i]]) % 1000000007; 小于时 dp[i][j] = dp[i - 1][j];\n\n2. leetcode或者cc150 [322. 零钱兑换](https://leetcode-cn.com/problems/coin-change/)\n\n**思路**：一维dp，dp[i]表示能凑成i元的最少硬币数，dp[0]=1，注意先遍历coin，然后再从coin开始遍历到amount\n\n3. leetcode [518. 零钱兑换 II](https://leetcode-cn.com/problems/coin-change-2/)\n\n**思路**：使用一维dp，dp[0]=1，注意coin要先遍历，然后再遍历1到amount，不然会重复，时间复杂度O(N*amount)，空间复杂度O(amount)\n\n4. leetcode [279. 完全平方数](https://leetcode-cn.com/problems/perfect-squares/)\n\n**思路**：322换零钱变种，先构造成长度为&radic;n的coins数组，使用平方和填满1,4,9,16...，然后先遍历coins数组，在从coin开始遍历到n，计算 dp[i] = Math.min(dp[i], dp[i - arr[j]] + 1);\n\n5. leetcode [1449. 数位成本和为目标值的最大数字](https://leetcode-cn.com/problems/form-largest-integer-with-digits-that-add-up-to-target/)\n\n**思路**：\n\n### **股票买卖问题**\n\n1. leetcode [剑指 Offer 63. 股票的最大利润](https://leetcode-cn.com/problems/gu-piao-de-zui-da-li-run-lcof/) / [121. 买卖股票的最佳时机](https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock/) （最多只能买卖一次）\n\n**思路**：就一个循环，min=Integer.MAX_VALUE，遍历的时候更新最小值，同时使用prices[i]-min计算最大的利润，时间复杂度O(N)\n\n2. leetcode [122. 买卖股票的最佳时机 II](https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-ii/) （最多只能持有一只股票，可能买卖多次）\n\n**思路**：3种解法逐渐优化：\n\na.dp[i][0]表示第i天持有的最大收益，dp[i][0]表示第i天不持有的最大收益\n\nb.dp[i]表示第i天的最大收益\n\nc.使用滚动数组进一步优化空间\n\n3. leetcode [123. 买卖股票的最佳时机 III](https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-iii/)（最多可以完成 两笔 交易，买卖2次）\n\n**思路**：2种解法逐渐优化：\n\na.dp[len][4]，dp[i][0]表示第i天买一次，dp[i][1]表示第i天卖一次，dp[i][2]表示第i天买二次，dp[i][3]表示第i天卖二次\n\nb.滚动数组优化空间\n\n4. leetcode [309. 最佳买卖股票时机含冷冻期](https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-with-cooldown/)\n\n**思路**：dp[i][j]表示第i天在j状态下的最大收益，0是不持股（卖掉了），1是持股（买入），2是冷冻期，注意换状态转移图\n\n5. leetcode [714. 买卖股票的最佳时机含手续费](https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-with-transaction-fee/)\n\n**思路**：初始时候，profit1=0，表示不持有股票最大利润，profit2=-prices[0]，表示持有股票最大利润，从1开始遍历\n\n### **博弈问题**\n\n1. leetcode [877. 石子游戏](https://leetcode-cn.com/problems/stone-game/)\n\n思路：动态规划，dp[i][j]表示当数组剩下的部分为 下标i 到 下标j 时，当前玩家与另一个玩家的分数之差的最大值\n\n2. leetcode [486. 预测赢家](https://leetcode-cn.com/problems/predict-the-winner/)\n\n思路：动态规划，dp[i][j]表示当数组剩下的部分为 下标i 到 下标j 时，当前玩家与另一个玩家的分数之差的最大值\n\n## **DFS(深度优先遍历)/<strong>BFS(广度优先遍历)**</strong> \n\n### dfs遍历\n\n很多都是矩阵\n\n1. leetcode [79. 单词搜索](https://leetcode-cn.com/problems/word-search/) (矩阵中找单词)\n\n**思路**：dfs，注意\"\".equal(str)的时候返回true\n\n2. leetcode [200. 岛屿数量](https://leetcode-cn.com/problems/number-of-islands/)\n\n**思路**：使用dfs染成0，遇到1，就从这个点开始dfs，并把这个点置为0，时间复杂度O(M*N)，空间是在矩阵原地\n\n3. leetcode&nbsp;[733. 图像渲染](https://leetcode-cn.com/problems/flood-fill/)\n\n**思路**：使用dfs，时间复杂度O(M*N)，空间复杂度O(M*N)\n\n4. leetcode [463. 岛屿的周长](https://leetcode-cn.com/problems/island-perimeter/)\n\n**思路**：使用dfs，遇到水+1，遇到边+1\n\n5. leetcode [剑指 Offer 13. 机器人的运动范围](https://leetcode-cn.com/problems/ji-qi-ren-de-yun-dong-fan-wei-lcof/)\n\n**思路**：使用dfs，需要写一个isValid验证横坐标x和纵坐标y是否满足k\n\n6. leetcode [365. 水壶问题](https://leetcode-cn.com/problems/water-and-jug-problem/)\n\n**思路**：总共6种情况，x空，y空，x满，y满，（x倒y，x空或y满），（y倒x，y空或x满）\n\n7. leetcode [695. 岛屿的最大面积](https://leetcode-cn.com/problems/max-area-of-island/)\n\n**思路**：dfs，boolean visit[][]标记是否遍历过\n\n8. leetcode [130. 被围绕的区域](https://leetcode-cn.com/problems/surrounded-regions/)\n\n**思路**：从边上开始dfs，遇到是O的改成A，最后再将O改成X，A改回O\n\n9. leetcode [337. 打家劫舍 III](https://leetcode-cn.com/problems/house-robber-iii/)\n\n**思路**：使用dfs，时间复杂度O(N)，空间复杂度O(N)\n\n```\n    public int dfs(TreeNode root) {\n        int result = 0;\n        if (root == null) {\n            return result;\n        }\n        result += root.val;\n        if (root.left != null) {\n            result += (dfs(root.left.left) + dfs(root.left.right));\n        }\n        if (root.right != null) {\n            result += (dfs(root.right.left) + dfs(root.right.right));\n        }\n        return Math.max(result, dfs(root.left) + dfs(root.right));\n    }\n\n```\n\n10. leetcode [547. 朋友圈](https://leetcode-cn.com/problems/friend-circles/)\n\n**思路**：visited[] dfs，时间复杂度O(N^2)，空间复杂度O(N)\n\n### **bfs遍历<br />**\n\n**使用queue**\n\n1. leetcode [127. 单词接龙](https://leetcode-cn.com/problems/word-ladder/)\n\n**思路**：bfs，queue visit[]，每一层count++，一个函数计算两个自字符串是否相差一个字符\n\n2. leetcode [103. 二叉树的锯齿形层次遍历](https://leetcode-cn.com/problems/binary-tree-zigzag-level-order-traversal/)\n\n**思路**：deque，层比遍历变种，加上isReverse\n\n3. leetcode [994. 腐烂的橘子](https://leetcode-cn.com/problems/rotting-oranges/)\n\n**思路**：使用bfs和queue，count表示新鲜橘子的数量\n\n4. leetcode [297. 二叉树的序列化与反序列化](https://leetcode-cn.com/problems/serialize-and-deserialize-binary-tree/)\n\n**思路**：使用deque，层次遍历\n\n5. leetcode [428. 序列化和反序列化 N 叉树](https://leetcode-cn.com/problems/serialize-and-deserialize-n-ary-tree/)\n\n**思路**：使用deque，需要添加new Node(null)\n\n6. leetcode [542. 01 矩阵](https://leetcode-cn.com/problems/01-matrix/)\n\n**思路**：使用bfs，将值为0的点全部加到deque中并标记为visited，然后从0开始bfs，再从deque中取出并开始判断其上下左右，再加入deque中\n\n7. leetcode [785. 判断二分图](https://leetcode-cn.com/problems/is-graph-bipartite/)\n\n**思路**：int[] visit = new int[len]; 0表示未被访问，-1表示一个组，1表示另一个组\n\n8. leetcode [784. 字母大小写全排列](https://leetcode-cn.com/problems/letter-case-permutation/)\n\n**思路**：使用层次遍历，遇到a-z和A-Z的这一层同时放小写的和大写的，当字符的长度等于原始字符串的时候就添加到result\n\n9. leetcode [1162. 地图分析](https://leetcode-cn.com/problems/as-far-from-land-as-possible/)\n\n**思路**：int[][] distance表示每个点距离陆地的距离，所有陆地的点到陆地的距离为0，从这些点开始dfs，同时比较得出max\n\n### **<strong>图遍历/<strong>拓扑排序(Topological sorting)**</strong></strong>\n\n拓扑排序的时间复杂度为O(V+E)，图中节点数量和边数量\n\n图的表示法有2种，一种是《邻接表式》，即HashMap，一种是《邻接矩阵式》，即二维数组\n\n1. leetcode [207. 课程表](https://leetcode-cn.com/problems/course-schedule/) （判断是否可能完成所有课程的学习）\n\n**思路**：参考拓扑排序 [Java排序算法&mdash;&mdash;拓扑排序](https://www.cnblogs.com/tonglin0325/p/5837877.html)\n\n2. leetcode [210. 课程表 II](https://leetcode-cn.com/problems/course-schedule-ii/) (返回为了学完所有课程所安排的学习顺序)\n\n**思路**：可以使用bfs；也可以使用dfs，int[] visit，0表示未搜索，1表示正在搜索，2表示已经搜索\n\n3. leetcode [269. 火星词典](https://leetcode-cn.com/problems/alien-dictionary/)\n\n**思路**：使用拓扑排序\n\n4. leetcode [329. 矩阵中的最长递增路径](https://leetcode-cn.com/problems/longest-increasing-path-in-a-matrix/)\n\n**思路**：bfs，计算每个点的出度，比周围4个点值大的出度为0，然后从这些出度为0的点开始bfs\n\n5. leetcode [332. 重新安排行程](https://leetcode-cn.com/problems/reconstruct-itinerary/)\n\n**思路**：使用Map<String, PriorityQueue<String>> map来构造图，然后使用dfs来遍历，倒序add到list中\n\n6. leetcode [841. 钥匙和房间](https://leetcode-cn.com/problems/keys-and-rooms/)\n\n**思路**：dfs和bfs都可以\n\nbfs解法，使用queue，boolean[] isVisited，也可以使用HashSet记录访问过的房间，时间复杂度O(n+m)，n是房间的数量，m是所有房间中的钥匙数量的总数\n\ndfs解法，使用递归dfs，时间复杂度O(n+m)，n是房间的数量，m是所有房间中的钥匙数量的总数，空间复杂度O(n)，n是房间的数量\n\n7. leetcode&nbsp;[133. 克隆图](https://leetcode-cn.com/problems/clone-graph/)\n\nbfs解法，使用queue，使用HashMap记录节点是否克隆，时间复杂度O(N)，N为节点数量，空间复杂度O(N)，使用hashmap\n\ndfs解法，使用递归，使用HashMap记录节点是否克隆，时间复杂度O(N)，N为节点数量，空间复杂度O(N)，使用hashmap\n\n## **相加相乘相除系列**\n\n1.leetcode [415. 字符串相加](https://leetcode-cn.com/problems/add-strings/) / AddStrings\n\n**思路**：字符串非负数值相加，直接都转换成char[]，while(index1 >=0 || index2 >=0 || two != 0)，two为进位；\n\n2.leetcode [2. 两数相加](https://leetcode-cn.com/problems/add-two-numbers/) / AddTwoNumbers\n\n**思路**：链表相加，最高位在链表尾部，使用递归，时间复杂度O(max(m,n))，空间复杂度O(max(m,n))，链表原地相加/也可以另起一个链表\n\n3.leetcode [445. 两数相加 II](https://leetcode-cn.com/problems/add-two-numbers-ii/) / AddTwoNumbers2\n\n**思路**：链表相加，最高位在链表头部，两个链表翻转，然后按上一题的相加，再翻转\n\n4. leetcode [43. 字符串相乘](https://leetcode-cn.com/problems/multiply-strings/) / Multiply\n\n**思路**：需要结合字符串相加，确定两个字符串中短的一个，然后从最低位开始确定循环相加长的字符串，升一位需要在后面加0，使用len-1-i计算添加0的个数\n\n5. leetcode [67. 二进制求和](https://leetcode-cn.com/problems/add-binary/) / AddBinary\n\n**思路**：carry进位，遍历相加，直到i<0&amp;&amp;j<0&amp;&amp;carry==0\n\n6. leetcode [29. 两数相除](https://leetcode-cn.com/problems/divide-two-integers/) （不使用乘法，除法和取余）\n\n思路：\n\n## **字符串**\n\n1. leetcode&nbsp;[28. 实现 strStr()](https://leetcode-cn.com/problems/implement-strstr/)\n\n**思路**：for循环，同时记下对上的个数index，当对不上的时候，回到i-index重新开始比较\n\n2. leetcode [214. 最短回文串](https://leetcode-cn.com/problems/shortest-palindrome/)\n\n**思路**：\n\n3. leetcode [14. 最长公共前缀](https://leetcode-cn.com/problems/longest-common-prefix/)\n\n**思路**：先找到最短的字符串，然后遍历这个最短的字符串的每一个字符，和整个数组的每个的对应位置是否相等，时间复杂度O(M*N)，空间复杂度O(1)\n\n4. leetcode [242. 有效的字母异位词](https://leetcode-cn.com/problems/valid-anagram/)\n\n**思路**：int[256]，不可能小于0\n\n5. leetcode [443. 压缩字符串](https://leetcode-cn.com/problems/string-compression/)\n\n**思路**：\n\n6. leetcode [383. 赎金信](https://leetcode-cn.com/problems/ransom-note/)\n\n**思路**：使用int[255]来替代map\n\n7. leetcode [38. 外观数列](https://leetcode-cn.com/problems/count-and-say/)\n\n**思路**：for循环1到n求解\n\n8. leetcode [1111. 有效括号的嵌套深度](https://leetcode-cn.com/problems/maximum-nesting-depth-of-two-valid-parentheses-strings/)\n\n**思路**：深度最小就是要将连续的(分配到两个组中，分组的结果根据 depth &amp; 1 而定\n\n9. leetcode [820. 单词的压缩编码](https://leetcode-cn.com/problems/short-encoding-of-words/)\n\n**思路**：先把字符放进hashset中，然后遍历每个字符的每个substring，将其从hashset中remove掉，最后计算长度\n\n10. leetcode [556. 下一个更大元素 III](https://leetcode-cn.com/problems/next-greater-element-iii/)\n\n**思路**：从右往左找nums[i-1]<nums[i]，然后用nums[i-1]右边大于它的最小值交换，然后将nums[i-1]右边的从小到大排序\n\n11. leetcode [722. 删除注释](https://leetcode-cn.com/problems/remove-comments/)\n\n**思路**：有个flag标识是否在/* */中\n\n12. leetcode [767. 重构字符串](https://leetcode-cn.com/problems/reorganize-string/)\n\n**思路**：先使用HashMap来找到最多的字符和个数，如果最多的个数超过一半的话，返回空字符；然后先用最多的字符填充偶数位，再用奇数填充，填充满了再继续填充偶数\n\n13. leetcode [151. 翻转字符串里的单词](https://leetcode-cn.com/problems/reverse-words-in-a-string/)\n\n**思路**：使用 Collections.reverse(list); ,时间复杂度O(N)，空间复杂度O(N)\n\n14. leetcode [186. 翻转字符串里的单词 II](https://leetcode-cn.com/problems/reverse-words-in-a-string-ii/)\n\n**思路**：要求原地，先全局翻转，然后再翻转每个单词，先置start为-1，然后遇到非空字符就start=i；只要遇到非空字符就end=i；遇到空字符或者len-1，就swap，并置start=-1\n\n15. leetcode [557. 反转字符串中的单词 III](https://leetcode-cn.com/problems/reverse-words-in-a-string-iii/)\n\n**思路**：不要求原地，直接使用StringBuilder的reverse()\n\n16. leetcode [剑指 Offer 67. 把字符串转换成整数](https://leetcode-cn.com/problems/ba-zi-fu-chuan-zhuan-huan-cheng-zheng-shu-lcof/)\n\n**思路**：使用double来放置，因为可能超过Long.MAX_VALUE，注意去掉首尾的空格，注意符号，注意越界\n\n17. leetcode [6. Z 字形变换](https://leetcode-cn.com/problems/zigzag-conversion/)\n\n**思路**：第二行到倒数第二行之间还有再加上1个，关系是j + index - 2 * i\n\n&nbsp;\n\n## **数学**\n\n1. leetcode [233. 数字 1 的个数](https://leetcode-cn.com/problems/number-of-digit-one/)\n\n给定一个整数 n，计算所有小于等于 n 的非负整数中数字 1 出现的个数。\n\n**思路**：遍历解法时间复杂度为O(NlogN)，数学解法时间复杂度为O(logN)，3种情况，分析700，711，543\n\n**思路**：要求log(N)，N/5+N/25+N/125...\n\n4. leetcode [338. 比特位计数](https://leetcode-cn.com/problems/counting-bits/)\n\n**思路**：n &amp; (n - 1);//每执行一次(n-1)&amp;n都是去掉最后一位的1\n\n5. leetcode [168. Excel表列名称](https://leetcode-cn.com/problems/excel-sheet-column-title/)\n\n**思路**：输入为26和27做分析，小难\n\n6. leetcode [319. 灯泡开关](https://leetcode-cn.com/problems/bulb-switcher/)\n\n**思路**：规律，开根号\n\n7. leetcode [357. 计算各个位数不同的数字个数](https://leetcode-cn.com/problems/count-numbers-with-unique-digits/)\n\n**思路**：数学方法递归，f(n)=f(n-1)+9x(9x8x...x(10-(n-1)))\n\n8. leetcode [397. 整数替换](https://leetcode-cn.com/problems/integer-replacement/)\n\n**思路**：使用递归 return integerReplacement(n / 2) + 1; 或者非递归，为了减少计算次数要尽可能多的往4的倍数上靠\n\n9. leetcode&nbsp;[12. 整数转罗马数字](https://leetcode-cn.com/problems/integer-to-roman/)\n\n**思路**：使用1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1对应的罗马字符\n\n10. leetcode [13. 罗马数字转整数](https://leetcode-cn.com/problems/roman-to-integer/)\n\n**思路**：右加左减：在一个较大的罗马数字的右边记上一个较小的罗马数字，表示大数字加小数字。在一个较大的数字的左边记上一个较小的罗马数字，表示大数字减小数字。\n\n11. leetcode [1131. 绝对值表达式的最大值](https://leetcode-cn.com/problems/maximum-of-absolute-value-expression/)\n\n**思路**：列举出正负所有情况，然后进行重组成4个\n\n12. leetcode [168. Excel表列名称](https://leetcode-cn.com/problems/excel-sheet-column-title/)\n\n**思路**：%26，注意26->Z的时候，需要处理，余数如果是0，b=26，然后n-=1\n\n13. leetcode [628. 三个数的最大乘积](https://leetcode-cn.com/problems/maximum-product-of-three-numbers/)\n\n**思路**：求出最大的3个数和最小的2个数\n\n14. leetcode&nbsp;[1344. 时钟指针的夹角](https://leetcode-cn.com/problems/angle-between-hands-of-a-clock/)\n\n**思路：**一分钟6度，一小时30度，分别计算时针和分针的度数，然后Math.abs，再求&nbsp;Math.min(diff, 360 - diff)\n\n15. leetcode [剑指 Offer 49. 丑数](https://leetcode-cn.com/problems/chou-shu-lcof/)\n\n**思路**：arr[index] = Math.min(2 * arr[num2], Math.min(3 * arr[num3], 5 * arr[num5]));\n\n16. leetcode [273. 整数转换英文表示](https://leetcode-cn.com/problems/integer-to-english-words/)\n\n**思路**：分情况，one是1-9的情况，twoLessThan20是10-19的情况，ten是20-90的情况，two是其他两位数的情况，tree是三位数的情况，然后分别求billion，million，thousand，hundred的每一位是多少\n\n17. leetcode [470. 用 Rand7() 实现 Rand10()](https://leetcode-cn.com/problems/implement-rand10-using-rand7/)\n\n**思路**：调用两次rand7可以得到0-48，只用到0-39，拒绝大于等于40的\n\n18. leetcode [7. 整数反转](https://leetcode-cn.com/problems/reverse-integer/)\n\n**思路**：取余放置到long的sum中，溢出的话为0\n\n19. leetcode [384. 打乱数组](https://leetcode-cn.com/problems/shuffle-an-array/)\n\n**思路**：2个数组origin和array，swap(array, i, randRange(i, len));&nbsp; randRange是 random.nextInt(max - min) + min;\n\n20. leetcode [面试题 16.01. 交换数字](https://leetcode-cn.com/problems/swap-numbers-lcci/)\n\n**思路**：异或3次\n\n21. leetcode [348. 判定井字棋胜负](https://leetcode-cn.com/problems/design-tic-tac-toe/)\n\n**思路**：初始化3个数组，然后列举出所有的情况\n\n```\nrows = new int[n]; // rows[i]==0表示在i行player1和player2落子一样，rows[i]==n-1表示player1落满，rows[i]==1-n表示player2落满\ncols = new int[n]; // cols[j]==0表示在j列player1和player2落子一样，cols[j]==n-1表示player1落满，cols[j]==1-n表示player2落满\ndig = new int[2]; // dig[0]表示i==j对角线上player1和player2的落子数；dig[1]表示i+j=n-1对角线上player1和player2的落子数\n\n```\n\n22. leetcode [LCP 02. 分式化简](https://leetcode-cn.com/problems/deep-dark-fraction/)\n\n**思路**：求最大公约数，使用欧几里得算法\n\n23. leetcode&nbsp;[204. 计数质数](https://leetcode-cn.com/problems/count-primes/)\n\n**思路**：1.朴素 判断是否是质数，从2到&radic;N是否能整除，时间复杂度O(n&radic;n)，空间复杂度O(1)；\n\n2.&nbsp;埃氏筛，从 i*i 开始到小于n，i 的所有倍数都设置为 false，时间复杂度O(NlogNlogN)，空间复杂度O(N)\n\n24. leetcode [405. 数字转换为十六进制数](https://leetcode-cn.com/problems/convert-a-number-to-hexadecimal/)\n\n**思路**：定义好0到f，然后insert num&amp;15，之后 num >>>= 4\n\n&nbsp;\n\n## **系统设计**\n\n1. leetcode [355. 设计推特](https://leetcode-cn.com/problems/design-twitter/)\n\n**思路**：2个HashMap：\n\nHashMap<Integer, Tweet> tweetMap; // key是用户id，value是推特的链表\n\nHashMap<Integer, HashSet<Integer>> followMap; // key是用户id，value是follower id\n\n2. leetcode [251. 展开二维向量](https://leetcode-cn.com/problems/flatten-2d-vector/)\n\n**思路**：airbnb添加了remove方法，使用一个List<Integer>或者rowId，colId，List<List<Integer>> array\n\n&nbsp;\n\n## **矩阵**\n\n1. leetcode [498. 对角线遍历](https://leetcode-cn.com/problems/diagonal-traverse/)\n\n**思路**：遍历的次数等于row+col-1，0向右上遍历，1向左下遍历，注意确定每一层起点的坐标\n\n2. leetcode [73. 矩阵置零](https://leetcode-cn.com/problems/set-matrix-zeroes/)\n\n**思路**：可以使用2个hashset来保存为0的row和col，然后遍历的时候置0，空间复杂度O(m + n)，时间复杂度O(M&times;N)；也可以直接在矩阵上遇到0，遍历改点的行列置0，O(1)的空间复杂度\n\n3. leetcode [74. 搜索二维矩阵](https://leetcode-cn.com/problems/search-a-2d-matrix/)\n\n**思路**：从右上角开始搜索，时间复杂度O(N)\n\n也可以使用二分查找，时间复杂度O(logM+logN)=O(logMN)，分别实现binarySearchRow和binarySearchCol，先二分确定在哪一行，再二分确定在哪一列\n\n4. leetcode [836. 矩形重叠](https://leetcode-cn.com/problems/rectangle-overlap/)\n\n**思路**：求不重叠，然后取反\n\n5. leetcode [419. 甲板上的战舰](https://leetcode-cn.com/problems/battleships-in-a-board/)\n\n**思路**：遍历，只要上面和左边是\"X\"就跳过\n\n6. leetcode [59. 螺旋矩阵 II](https://leetcode-cn.com/problems/spiral-matrix-ii/)\n\n**思路**：定义left，right，top，bottom，然后val = 1，while(val <= n*n)，然后里面四个for循环\n\n7. leetcode [48. 旋转图像](https://leetcode-cn.com/problems/rotate-image/)\n\n**思路**：先转置（matrix[i][j] = matrix[j][i];），再翻转每一行\n\n8. leetcode [240. 搜索二维矩阵 II](https://leetcode-cn.com/problems/search-a-2d-matrix-ii/)\n\n**思路**：从右上角开始判断，小于target就row++，大于target就col--\n\n&nbsp;\n\n## **位运算**\n\n1. leetcode [191. 位1的个数](https://leetcode-cn.com/problems/number-of-1-bits/)\n\n**思路**：根据n &amp; (n-1)去掉最后的1，一边计数\n\n2. leetcode [338. 比特位计数](https://leetcode-cn.com/problems/counting-bits/)\n\n**思路**：可以根据一个个算，根据n &amp; (n-1)去掉最后的1；更高效的做法是使用公式 P(x)=P(x/2)+(xmod2)\n\n3. leetcode [318. 最大单词长度乘积](https://leetcode-cn.com/problems/maximum-product-of-word-lengths/)\n\n**思路**：当两个字符串没有相同字母的时候，二进制余的结果为0\n\n&nbsp;\n\n## **贪心**\n\n**贪心算法一般用来解决需要 &ldquo;找到要做某事的最小数量&rdquo; 或 &ldquo;找到在某些情况下适合的最大物品数量&rdquo; 的问题，且提供的是无序的输入。**\n\n1. leetcode [452. 用最少数量的箭引爆气球](https://leetcode-cn.com/problems/minimum-number-of-arrows-to-burst-balloons/)\n\n**思路**：贪心法，按气球的右边界排序，时间复杂度O(NlogN)，空间复杂度O(1)\n\n2. leetcode [56. 合并区间](https://leetcode-cn.com/problems/merge-intervals/)\n\n**思路**：先排序，然后贪心法，由于使用了排序，时间复杂度为O(NlogN)，空间复杂度O(N)\n\n3. leetcode [55. 跳跃游戏](https://leetcode-cn.com/problems/jump-game/)\n\n**思路**：可以使用贪心法，last表示能到达结束的index，从后向前判断，时间复杂度O(N)，空间复杂度O(1)\n\n也可以使用动态规划，dp[i]表示从i是否能跳到结尾，时间复杂度O(N^2)，空间复杂度O(N)\n\n4. leetcode [759. 员工空闲时间](https://leetcode-cn.com/problems/employee-free-time/)\n\n**思路**：贪心法，startList和endList，count==0时添加到结果中\n\n5. leetcode [435. 无重叠区间](https://leetcode-cn.com/problems/non-overlapping-intervals/)\n\n**思路**：贪心法，对int[]中使用[1]排序，然后比较firstEnd和start的大小\n\n6. leetcode [57. 插入区间](https://leetcode-cn.com/problems/insert-interval/)\n\n**思路**：贪心法\n\n7. leetcode [45. 跳跃游戏 II](https://leetcode-cn.com/problems/jump-game-ii/)\n\n**思路**：走到当前最远能到达的end就step++\n\n8. leetcode [252. 会议室](https://leetcode-cn.com/problems/meeting-rooms/)（给定一个会议时间安排的数组，每个会议时间都会包括开始和结束的时间 [[s1,e1],[s2,e2],...] (si < ei)，请你判断一个人是否能够参加这里面的全部会议）\n\n思路：按会议的开始时间从小到大排序，初始end为int最小值，然后start=interval[0]，如果start<end，返回false；否则end=interval[1]\n\n9. leetcode [253. 会议室 II](https://leetcode-cn.com/problems/meeting-rooms-ii/) （为避免会议冲突，同时要考虑充分利用会议室资源，请你计算至少需要多少间会议室，才能满足这些会议安排）\n\n**思路**：按会议的开始时间从小到大排序，然后再使用一个PriorityQueue<Integer>，堆顶元素是会议结束时间最早的时间，如果start大于等于堆顶，就poll；循环中offer(end)，最后返回queue.size()\n\n10. leetcode [201. 数字范围按位与](https://leetcode-cn.com/problems/bitwise-and-of-numbers-range/)\n\n**思路**：从32位整数的二进制最高位开始，取前缀，一旦前缀不相等就退出循环\n\n11. leetcode [135. 分发糖果](https://leetcode-cn.com/problems/candy/)\n\n**思路**：贪心法，左到右扫一遍，再右到左扫一遍\n\n&nbsp;\n\n## **回溯搜索**\n\n**在「树」上的「深度优先遍历」就是「回溯算法」；在「图」上的「深度优先遍历」是「flood fill<strong>算法**」 </strong>，参考：[图的广度优先遍历（Java）](https://leetcode-cn.com/problems/water-and-jug-problem/solution/tu-de-yan-du-you-xian-bian-li-by-liweiwei1419/)\n\n1. leetcode [22. 括号生成](https://leetcode-cn.com/problems/generate-parentheses/)\n\n**思路**：backtrick(int left, int right, String temp, int target)，left表示当前左括号个数，right表示当前右括号个数，target表示目标对数，left==target&amp;&amp;right==target，找到结果，right > left剪枝；如果left<target，递归left+1；如果right<target，递归right+1；<br />\n\n2. leetcode [51. N 皇后](https://leetcode-cn.com/problems/n-queens/)\n\n**思路**：回溯，从row==0开始，遍历col，如果无其他皇后在同一列和对角，addLast后，回溯 placeQueens(row + 1, arr, size); ，再removeLast\n\n3. leetcode&nbsp;[52. N皇后 II](https://leetcode-cn.com/problems/n-queens-ii/)\n\n**思路**：回溯，从row==0开始，遍历col，如果无其他皇后在同一列和对角，回溯 placeQueens(row + 1, arr, size);\n\n4. leetcode [17. 电话号码的字母组合](https://leetcode-cn.com/problems/letter-combinations-of-a-phone-number/)\n\n**思路**：使用递归+回溯，先使用一个Map<Character,String>来存储数字到字母的映射，然后for循环i位置value的所有字母，并backtrick\n\n5. leetcode [39. 组合总和](https://leetcode-cn.com/problems/combination-sum/)（数组中的数字可以重复使用）\n\n**思路**：回溯搜索，先排序，在回溯搜素，都是正数可剪枝；因为可重复使用，所有backtrick的时候参数是j，backtrick(j, len, candidates, temp, sum, target)\n\n6. [40. 组合总和 II](https://leetcode-cn.com/problems/combination-sum-ii/) （数组中的数字只能使用一次）\n\n**思路**：回溯搜索，先排序，超过剩余的大剪枝，如果和前一个元素相等，小剪枝；因为不可重复使用，所以backtrick的时候参数是j+1，backtrick(j, len, candidates, temp, sum, target)\n\n7. leetcode [216. 组合总和 III](https://leetcode-cn.com/problems/combination-sum-iii/) （只能使用1到9，数组的元素个数为k，和为n）\n\n**思路**：回溯算法，当size大于k的时候剪枝；判断大于45的直接return，然后初始化一个数组{1,2....,9}，在回溯\n\n8. leetcode [377. 组合总和 Ⅳ](https://leetcode-cn.com/problems/combination-sum-iv/) （给你一个由 不同 整数组成的数组 nums ，和一个目标整数 target 。请你从 nums 中找出并返回总和为 target 的元素组合的个数。）\n\n**思路**：回溯+记忆；或者使用dp，dp[target+1]，初始dp[0]=1；如果i>=num，则dp[i]+=dp[i-num]\n\n9. leetcode [46. 全排列](https://leetcode-cn.com/problems/permutations/) （不含重复数字）\n\n**思路**：使用递归+回溯，swap；注意回溯的时候是i+1，backtrick(j+1, len, nums, temp)\n\n10. leetcode .[47. 全排列 II](https://leetcode-cn.com/problems/permutations-ii/) （含有重复数字）\n\n**思路**：回溯搜索\n\n1.使用HashSet去重 + [46. 全排列](https://leetcode-cn.com/problems/permutations/) 解法；\n\n2.首先要排序，使用visit[]来去重，如下剪枝 if (visit[i] || (i > 0 &amp;&amp; nums[i] == nums[i - 1] &amp;&amp; !visit[i - 1]) {continue;}\n\n11. leetcode [77. 组合](https://leetcode-cn.com/problems/combinations/)\n\n**思路**：递归+回溯，上界可优化\n\n12. leetcode [78. 子集](https://leetcode-cn.com/problems/subsets/)\n\n**思路**：递归+回溯\n\n13. leetcode [90. 子集 II](https://leetcode-cn.com/problems/subsets-ii/)\n\n**思路**：回溯搜索，注意先排序，再&nbsp;j > i &amp;&amp; nums[j] == nums[j - 1] 的时候剪枝\n\n14. leetcode [1079. 活字印刷](https://leetcode-cn.com/problems/letter-tile-possibilities/)\n\n**思路**：递归+回溯，将c字符插入到i位置\n\n15. leetcode [491. 递增子序列](https://leetcode-cn.com/problems/increasing-subsequences/)\n\n**思路**：回溯搜索，使用HashSet来对每一层去重\n\n16. leetcode [18. 四数之和](https://leetcode-cn.com/problems/4sum/)\n\n**思路**：回溯搜索，注意去重和剪枝\n\n17. leetcode [60. 排列序列](https://leetcode-cn.com/problems/permutation-sequence/)\n\n**思路**：使用used[]数组来表示是否使用过\n\n18. leetcode [89. 格雷编码](https://leetcode-cn.com/problems/gray-code/)\n\n**思路**：回溯算法，见图示\n\n19. leetcode [1239. 串联字符串的最大长度](https://leetcode-cn.com/problems/maximum-length-of-a-concatenated-string-with-unique-characters/)\n\n**思路**：回溯搜索，使用hashset判断2个字符串是否相等\n\n20. leetcode [131. 分割回文串](https://leetcode-cn.com/problems/palindrome-partitioning/)\n\n**思路**：在回溯搜索中，检查是否是回文\n\n21. leetcode [剑指 Offer 38. 字符串的排列](https://leetcode-cn.com/problems/zi-fu-chuan-de-pai-lie-lcof/)\n\n**思路**：回溯搜索，使用hashset去重，swap，hashset转数组，String[] result = set.toArray(new String[size]);\n\n22. leetcode [93. 复原IP地址](https://leetcode-cn.com/problems/restore-ip-addresses/)\n\n**思路**：回溯搜索，注意多种情况的剪枝\n\n23. leetcode [842. 将数组拆分成斐波那契序列](https://leetcode-cn.com/problems/split-array-into-fibonacci-sequence/)\n\n**思路**：回溯搜索，注意超过2位数有前导0直接剪枝\n\n24. leetcode [140. 单词拆分 II](https://leetcode-cn.com/problems/word-break-ii/)\n\n**思路**：回溯+dp，dp[i]表示s.subString(0,i)是否能由wordDict组成，不用dp会超时\n\n25. leetcode [797. 所有可能的路径](https://leetcode-cn.com/problems/all-paths-from-source-to-target/)\n\n**思路**：回溯\n\n26. leetcode [306. 累加数](https://leetcode-cn.com/problems/additive-number/)\n\n**思路**：回溯\n\n27. leetcode [472. 连接词](https://leetcode-cn.com/problems/concatenated-words/)\n\n思路：哈希表+回溯\n\n&nbsp;\n\n## **单调栈/单调队列**\n\n**参考：https://leetcode-cn.com/problems/largest-rectangle-in-histogram/solution/bao-li-jie-fa-zhan-by-liweiwei1419/**\n\n1. leetcode [739. 每日温度](https://leetcode-cn.com/problems/daily-temperatures/) / DailyTemperatures\n\n**思路**：使用单调栈，栈顶小，栈底大，栈中是int[]，[0]是index，[1]是value\n\n2. leetcode [42. 接雨水](https://leetcode-cn.com/problems/trapping-rain-water/) / Trap\n\n**思路**：当栈顶元素高度小于后面的元素，说明有积水\n\n3. leetcode [84. 柱状图中最大的矩形](https://leetcode-cn.com/problems/largest-rectangle-in-histogram/) /\n\n**思路**：难，栈保持栈顶index在heights中是最大的，当新来的元素大于等于栈顶元素的时候直接入栈，小的时候就需要计算一下矩形的大小，直到新来的元素大于等于栈顶\n\n3. leetcode [316. 去除重复字母](https://leetcode-cn.com/problems/remove-duplicate-letters/) /\n\n**思路**：使用int[256]数组来表示剩余的字符数量，用deque来存放尽量递增的字符序列，用hashset来标识deque中的字符是否存在\n\n4. leetcode [402. 移掉K位数字](https://leetcode-cn.com/problems/remove-k-digits/) /\n\n**思路**：需要尽量排成递增，如果下一个数字小于栈顶，移除栈顶\n\n5. leetcode [907. 子数组的最小值之和](https://leetcode-cn.com/problems/sum-of-subarray-minimums/) /\n\n**思路**：\n\n&nbsp;\n\n## **字典树**\n\n1.leetcode [208. 实现 Trie (前缀树)](https://leetcode-cn.com/problems/implement-trie-prefix-tree/)\n\n**思路**：TrieNode和Trie\n\n2.leetcode [642. 设计搜索自动补全系统](https://leetcode-cn.com/problems/design-search-autocomplete-system/)\n\n**思路**：前缀数TrieNode存放的是path，end，TrieNode[]，该题TrieNode存放的是sentence，times，children\n\n3.leetcode [212. 单词搜索 II](https://leetcode-cn.com/problems/word-search-ii/)\n\n**思路**：字典树+DFS回溯，board[i][j] = '.'&nbsp; 防止多次访问\n\n4.leetcode&nbsp; [1233. 删除子文件夹](https://leetcode.cn/problems/remove-sub-folders-from-the-filesystem/)\n\n**思路**：字典树，FolderNode 3个成员变量，name，children，isEnd，\n\n&nbsp;\n\n## **有限状态自动机<br />**\n\n1.leetcode [65. 有效数字](https://leetcode-cn.com/problems/valid-number/)\n\n**思路**：https://leetcode-cn.com/problems/valid-number/solution/you-xiao-shu-zi-by-leetcode-solution-298l/\n\n<img src=\"/images/517519-20210327144346766-627184982.png\" width=\"800\" height=\"318\" loading=\"lazy\" />\n\n&nbsp;\n\n## **并查集**\n\n参考：[https://cloud.tencent.com/developer/article/1694144?fromSource=gwzcw.1293314.1293314.1293314&amp;cps_key=ad1dd5b36e1c498308f7302ab4cdabb7](https://cloud.tencent.com/developer/article/1694144?fromSource=gwzcw.1293314.1293314.1293314&amp;cps_key=ad1dd5b36e1c498308f7302ab4cdabb7)\n\n1.leetcode [990. 等式方程的可满足性](https://leetcode-cn.com/problems/satisfiability-of-equality-equations/)\n\n并查集合集查看：https://leetcode-cn.com/problems/satisfiability-of-equality-equations/solution/shi-yong-bing-cha-ji-chu-li-bu-xiang-jiao-ji-he-we/\n\n2.leetcode [721. 账户合并](https://leetcode-cn.com/problems/accounts-merge/)\n\n思路：并查集 \n\n3. leetcode [684. 冗余连接](https://leetcode-cn.com/problems/redundant-connection/)\n\n思路：并查集\n\n4. leetcode [399. 除法求值](https://leetcode-cn.com/problems/evaluate-division/)\n\n思路：带权并查集\n\n&nbsp;\n\n## **线段树**\n\n参考：[https://oi-wiki.org/ds/seg/](https://oi-wiki.org/ds/seg/)\n\n1. leetcode [307. 区域和检索 - 数组可修改](https://leetcode-cn.com/problems/range-sum-query-mutable/)\n\n**思路**：线段树\n\n&nbsp;\n\n## **树状数组**\n\n参考：[https://oi-wiki.org/ds/fenwick/](https://oi-wiki.org/ds/fenwick/)\n\nhttps://leetcode-cn.com/problems/count-of-smaller-numbers-after-self/solution/yi-wen-zhang-wo-shu-zhuang-shu-zu-by-a-fei-8/\n\n多用于高效计算数列的前缀和， 区间和\n\n1. leetcode [315. 计算右侧小于当前元素的个数](https://leetcode-cn.com/problems/count-of-smaller-numbers-after-self/)\n\n**思路**：树状数组\n\n&nbsp;\n\n## **扫描线法**\n\n1. leetcode [218. 天际线问题](https://leetcode-cn.com/problems/the-skyline-problem/)\n\n思路：扫描线法\n\n&nbsp;\n\n## **缓存淘汰算法**\n\n参考：[缓存淘汰算法 LRU 和 LFU](https://www.jianshu.com/p/1f8e36285539)\n\n1. leetcode [146. LRU缓存机制](https://leetcode-cn.com/problems/lru-cache/)\n\n**LRU，最近最少使用**，把数据加入一个链表中，按访问时间排序，发生淘汰的时候，把访问时间最旧的淘汰掉。\n\n比如有数据 1，2，1，3，2<br />\n此时缓存中已有（1，2）<br />\n当3加入的时候，得把后面的2淘汰，变成（3，1）\n\n**思路**：可以使用HashMap加双向链表来实现，链表head是最近访问的节点。hashmap用来快速找到对接Node在链表中的位置，使用2个虚拟节点head和tail。<br /><br />\n\n2.leetcode [460. LFU缓存](https://leetcode-cn.com/problems/lfu-cache/)\n\n**LFU，最近不经常使用**，把数据加入到链表中，按频次排序，一个数据被访问过，把它的频次+1，发生淘汰的时候，把频次低的淘汰掉。<br />\n比如有数据 1，1，1，2，2，3<br />\n缓存中有（1(3次)，2(2次)）<br />\n当3加入的时候，得把后面的2淘汰，变成（1(3次)，3(1次)）<br />\n区别：LRU 是得把 1 淘汰。\n\n显然<br />\nLRU对于循环出现的数据，缓存命中不高<br />\n比如，这样的数据，1，1，1，2，2，2，3，4，1，1，1，2，2，2.....<br />\n当走到3，4的时候，1，2会被淘汰掉，但是后面还有很多1，2\n\nLFU对于交替出现的数据，缓存命中不高<br />\n比如，1，1，1，2，2，3，4，3，4，3，4，3，4，3，4，3，4......<br />\n由于前面被（1(3次)，2(2次)）<br />\n3加入把2淘汰，4加入把3淘汰，3加入把4淘汰，然而3，4才是最需要缓存的，1去到了3次，谁也淘汰不了它了。\n\n**思路**：2个HashMap，一个是HashMap<Integer,Node> cache，一个是HashMap<Integer, LinkedHashSet<Node>> fre，使用双向链表LinkedHashSet，按添加的顺序进行排序\n\n&nbsp;\n\n## **数据结构**\n\n1. leetcode [432. 全 O(1) 的数据结构](https://leetcode-cn.com/problems/all-oone-data-structure/)\n\n**思路：**双向链表+哈希表，参考LRU\n\n&nbsp;\n\n## **Tarjan算法<br />**\n\n1. leetcode [1192. 查找集群内的「关键连接」](https://leetcode-cn.com/problems/critical-connections-in-a-network/)\n\n**思路：**tarjan算法，找强连通分量\n\n&nbsp; \n","tags":["刷题"]},{"title":"Elasticsearch学习笔记——索引模板","url":"/Elasticsearch学习笔记——索引模板.html","content":"在索引模板里面,date类型的字段的format支持多种类型,在es中全部会转换成long类型进行存储,参考\n\n```\nhttps://zhuanlan.zhihu.com/p/34240906\n\n```\n\n一个索引模板范例\n\n```\n{\n  \"order\": 0,\n  \"index_patterns\": [\n    \"ssssssssssssssss_test*\"\n  ],\n  \"settings\": {\n    \"index\": {\n      \"codec\": \"best_compression\",\n      \"routing\": {\n        \"allocation\": {\n          \"require\": {\n            \"box_type\": \"master\"\n          },\n          \"total_shards_per_node\": \"3\"\n        }\n      },\n      \"refresh_interval\": \"60s\",\n      \"unassigned\": {\n        \"node_left\": {\n          \"delayed_timeout\": \"900m\"\n        }\n      },\n      \"number_of_shards\": \"3\",\n      \"number_of_replicas\": \"1\"\n    }\n  },\n  \"mappings\": {\n    \"_default_\": {\n      \"_source\": {\n        \"enabled\": true\n      },\n      \"dynamic_templates\": [\n        {\n          \"string_template\": {\n            \"mapping\": {\n              \"type\": \"keyword\"\n            },\n            \"match_mapping_type\": \"string\",\n            \"match\": \"*\"\n          }\n        }\n      ],\n      \"properties\": {\n        \"a1\": {\n          \"search_analyzer\": \"ik_max_word\",\n          \"analyzer\": \"ik_max_word\",\n          \"type\": \"text\"\n        },\n        \"a2\": {\n          \"type\": \"float\"\n        },\n        \"a3\": {\n          \"type\": \"integer\"\n        },\n        \"untitled\": {\n          \"type\": \"float\"\n        },\n        \"a4\": {\n          \"type\": \"long\"\n        },\n        \"a5\": {\n          \"type\": \"double\"\n        },\n        \"a6\": {\n          \"type\": \"ip\"\n        },\n        \"a8\": {\n          \"type\": \"keyword\"\n        }\n      },\n      \"_all\": {\n        \"enabled\": false\n      }\n    }\n  },\n  \"aliases\": {}\n}\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["ELK"]},{"title":"Elasticsearch6.2.1安装elasticsearch-sq插件","url":"/Elasticsearch6.2.1安装elasticsearch-sq插件.html","content":"参考\n\n```\nhttps://github.com/NLPchina/elasticsearch-sql\n\n```\n\n1.下载插件\n\n```\nwget https://github.com/NLPchina/elasticsearch-sql/releases/download/6.2.1.0/elasticsearch-sql-6.2.1.0.zip\n\n```\n\n2.安装\n\n```\n./bin/elasticsearch-plugin instal file:///home/lintong/下载/elasticsearch-sql-6.2.1.0.zip\n\n```\n\n如果遇到 Exception in thread \"main\" java.lang.IllegalArgumentException: Unknown properties in plugin descriptor: [jvm, site]\n\n解压zip文件,然后修改 plugin-descriptor.properties 文件\n\n去掉下面两行后重新压缩成zip,然后再安装\n\n```\nsite=\njvm=\n\n```\n\n参考\n\n```\nhttps://github.com/NLPchina/elasticsearch-sql/issues/610\n\n```\n\n3.下载前端组件\n\n```\nwget https://github.com/NLPchina/elasticsearch-sql/releases/download/5.4.1.0/es-sql-site-standalone.zip\n\n```\n\n解压\n\n```\ncd site-server\nnpm --registry https://registry.npm.taobao.org install express --save\nnode node-server.js \n\n```\n\n4.执行sql\n\n```\ncurl -X GET \"localhost:9200/_sql\" -H 'Content-Type: application/json' -d'select * from es limit 10'\n{\"took\":7,\"timed_out\":false,\"_shards\":{\"total\":5,\"successful\":5,\"skipped\":0,\"failed\":0},\"hits\":{\"total\":20000,\"max_score\":1.0,\"hits\":[{\"_index\":\"es\",\"_type\":\"people\",\"_id\":\"0\",\"_score\":1.0,\"_source\":{\"playerId\":122,\"logId\":0,\"dateTime\":\"2012-01-02 00:21:00\",\"action\":\"私聊:нужен парень 0965188229\"}},{\"_index\":\"es\",\"_type\":\"people\",\"_id\":\"14\",\"_score\":1.0,\"_source\":{\"playerId\":343,\"logId\":14,\"dateTime\":\"2012-01-02 00:35:00\",\"action\":\"私聊:с новым годом!скучно девченкам.0988939122\"}},{\"_index\":\"es\",\"_type\":\"people\",\"_id\":\"19\",\"_score\":1.0,\"_source\":{\"playerId\":219,\"logId\":19,\"dateTime\":\"2012-01-02 00:42:00\",\"action\":\"私聊:ищу взрослую женщину звоните 0661464552\"}},{\"_index\":\"es\",\"_type\":\"people\",\"_id\":\"22\",\"_score\":1.0,\"_source\":{\"playerId\":822,\"logId\":22,\"dateTime\":\"2012-01-02 00:45:00\",\"action\":\"私聊:Жду смс от солидного мужчины за 47  0952342589\"}},{\"_index\":\"es\",\"_type\":\"people\",\"_id\":\"24\",\"_score\":1.0,\"_source\":{\"playerId\":111,\"logId\":24,\"dateTime\":\"2012-01-02 00:47:00\",\"action\":\"私聊:П27 ищу горячую снегурочку 0505618815\"}},{\"_index\":\"es\",\"_type\":\"people\",\"_id\":\"25\",\"_score\":1.0,\"_source\":{\"playerId\":204,\"logId\":25,\"dateTime\":\"2012-01-02 00:48:00\",\"action\":\"私聊:снегурочька хочет деда мороза.тома.0661796082\"}},{\"_index\":\"es\",\"_type\":\"people\",\"_id\":\"26\",\"_score\":1.0,\"_source\":{\"playerId\":789,\"logId\":26,\"dateTime\":\"2012-01-02 00:49:00\",\"action\":\"私聊:молодой парень 31год из сел.местн. Очень хочет познакомиться с м\"}},{\"_index\":\"es\",\"_type\":\"people\",\"_id\":\"29\",\"_score\":1.0,\"_source\":{\"playerId\":328,\"logId\":29,\"dateTime\":\"2012-01-02 00:54:00\",\"action\":\"私聊:ВСЕХ ПАРНЕЙ С НОВЫМ ГОДОМ ЛЮБВИ СЧАСТЬЯ.С УВ.СВЕТА 0666339477.ЗАДЕРЖ\"}},{\"_index\":\"es\",\"_type\":\"people\",\"_id\":\"40\",\"_score\":1.0,\"_source\":{\"playerId\":35,\"logId\":40,\"dateTime\":\"2012-01-02 01:06:00\",\"action\":\"私聊:ищю парня до21\"}},{\"_index\":\"es\",\"_type\":\"people\",\"_id\":\"41\",\"_score\":1.0,\"_source\":{\"playerId\":750,\"logId\":41,\"dateTime\":\"2012-01-02 01:06:00\",\"action\":\"私聊:ВСЕХ ДЕВУШЕК С НОВЫМ ГОДОМ Я ВАС ВСЕХ ЛЮБЛЮ 0507143632 СЕРЁГА\"}}]}}\n\n```\n\n5.web界面\n\n配置在site_configuration.json\n\n<img src=\"/images/517519-20191028221657265-425319582.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n","tags":["ELK"]},{"title":"MySQL学习笔记——乐观锁和悲观锁","url":"/MySQL学习笔记——乐观锁和悲观锁.html","content":"场景：两个用户同时读取了数据库中的一条记录，此时用户A对其中一个字段的值进行了修改操作并进行了提交，后来用户B也对这个字段进行了修改，用户B的提交将会覆盖用户A提交的值\n\n## **乐观锁**和**悲观锁**\n\n### **悲观锁**：\n\n每次去取数据，很悲观，都觉得会被别人修改，所以在拿数据的时候都会上锁。\n\n简言之，共享资源每次都只给一个线程使用，其他线程阻塞，等第一个线程用完后再把资源转让给其他线程。\n\nselect ... for update，synchronized和ReentranLock等都是悲观锁思想的体现。<!--more-->\n&nbsp;\n\n### **乐观锁**：\n\n每次去取数据，都很乐观，觉得不会被被人修改。\n\n因此每次都不上锁，但是在更新的时候，就会看别人有没有在这期间去更新这个数据，如果有更新就重新获取，再进行判断，一直循环，直到拿到没有被修改过的数据。\n\nCAS（Compare and Swap 比较并交换）就是乐观锁的一种实现方式，比如使用version字段或者修改时间字段来判断数据是否被修改，如果返回的受影响行数为 1，表示更新成功；如果返回的受影响行数为 0，表示更新失败（数据已被其他事务修改），可以选择重试或中断操作。\n\n&nbsp;\n\n参考：\n\n[Mysql 事务及数据的一致性处理](https://segmentfault.com/a/1190000012469586)\n\n[CAS（比较并交换）乐观锁解决并发问题的一次实践](https://www.javazhiyin.com/41189.html)\n\n[乐观锁与悲观锁&mdash;&mdash;解决并发问题](https://www.cnblogs.com/0201zcr/p/4782283.html)\n\n[Spring Boot之乐观锁和悲观锁](https://blog.csdn.net/qq_42914528/article/details/82148056)\n","tags":["MySQL"]},{"title":"Nexus上传npm包","url":"/Nexus上传npm包.html","content":"1.创建npm仓库\n\n私服仓库npm-hosted\n\n<img src=\"/images/517519-20191018135625065-1744848825.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n&nbsp;代理仓库npm-proxy\n\n<img src=\"/images/517519-20191018135819659-1405141848.png\" alt=\"\" />\n\n&nbsp;\n\n&nbsp;npm-group\n\n<img src=\"/images/517519-20191018135947505-142829417.png\" alt=\"\" />\n\n创建成功\n\n<img src=\"/images/517519-20191018140022962-1275733183.png\" alt=\"\" />\n\n&nbsp;在工程的根目录下创建文件 .npmrc\n\n```\nregistry=http://xxx:8081/nexus/repository/npm-group/\n\n```\n\n然后安装react\n\n```\nnpm --loglevel info install react\n\n```\n\n或者\n\n```\nnpm install --registry=http://xxx:8081/nexus/repository/npm-group/\n\n```\n\n仓库上将会下载这个npm包\n\n<img src=\"/images/517519-20191018153707561-1580541485.png\" alt=\"\" />\n\n&nbsp;\n\n&nbsp;\n\n<br /><br />\n","tags":["Nexus"]},{"title":"Nginx使用笔记","url":"/Nginx使用笔记.html","content":"## 1.请求转发\n\n比如说我要将127.0.0.1/topics上的所有请求转发到xxx:xxx/上\n\n修改 sudo vim /etc/nginx/nginx.conf\n\n```\n        server {\n               listen       80;\n                server_name  127.0.0.1;\n                location /topics {\n                    #root   html;\n                    #index  index.html index.htm;\n                proxy_pass http://xxx:xxx;\n                }\n            }\n\n```\n\n## 2.代理前端和后端服务\n\n## 3.Nginx使用本地缓存\n\n## 4.代理\n\n### 1.正向代理和反向代理\n\n**正向代理**：代理服务器位于客户端和目标服务器之间，代表客户端向目标服务器发送请求。客户端需要配置代理服务器。\n\n**用途**: 访问控制、内容过滤、匿名浏览、缓存。我们用来kexue上网的工具就属于正向代理。\n\n**反向代理**：代理服务器位于客户端和目标服务器之间，代表服务器处理客户端请求。客户端不知道反向代理的存在。\n\n**用途**: 负载均衡、安全防护、缓存、SSL加速。比如使用Nginx，HAProxy，Apache HTTP Server等作为反向代理。\n\n### 2.透明代理和非透明代理\n\n透明代理和非透明代理都是正向代理，位于客户端和目标服务器之间，代理客户端向目标服务器发送请求。\n\n**透明代理**：网络管理员在网络边界部署透明代理来过滤不良内容。用户浏览网站时，代理自动拦截和检查内容，而用户并不知道代理的存在。\n\n**非透明代理**：用户配置浏览器使用非透明代理，以便匿名访问互联网。用户在浏览器中手动设置代理服务器地址和端口。\n\n## 5.X-Forwarded-For，X-Real-IP和Remote Address\n\n**X-Forwarded-For**通常用于标识通过 HTTP 代理或负载均衡器的原始客户端 IP 地址。X-Forwarded-For 是一个 HTTP 扩展头部。HTTP/1.1（RFC 2616）协议并没有对它的定义，它最开始是由 Squid 这个缓存代理软件引入，用来表示 HTTP 请求端真实 IP。如今它已经成为事实上的标准，被各大 HTTP 代理、负载均衡等转发服务广泛使用，并被写入 RFC 7239（Forwarded HTTP Extension）标准之中。示例值：<!--more-->\n&nbsp;X-Forwarded-For: client1, proxy1, proxy2\n\n**X-Real-IP **有些反向代理服务器（如 Nginx）会将原始客户端 IP 地址放在这个头字段中。X-Real-IP，这是一个自定义头部字段。X-Real-IP 通常被 HTTP 代理用来表示与它产生 TCP 连接的设备 IP，这个设备可能是其他代理，也可能是真正的请求端。需要注意的是，X-Real-IP 目前并不属于任何标准。示例值：X-Real-IP: 203.0.113.195\n\n**Remote Address** 是指服务器端看到的**客户端的IP地址**，即发起请求的源 IP 地址。在网络通信中，remote address 通常用于识别和追踪请求的来源。在不同的代理和负载均衡场景中，remote address 的值可能会发生变化。\n\n参考：[聊聊HTTP的X-Forwarded-For 和 X-Real-IP](https://alili.tech/archive/izbidk3gu3s/)\n\n#### **1.直接访问**\n\n如果客户端直接访问服务器而没有经过任何代理或负载均衡器，remote address 将是客户端的 IP 地址。例如，如果客户端的 IP 地址是 203.0.113.195，服务器将看到：\n\n```\nRemote Address: 203.0.113.195\n\n```\n\n#### 2.经过正向代理\n\n当请求经过正向代理（包括透明代理和非透明代理）时，remote address 可能会有所不同：\n\n- **透明代理**: 客户端不知道代理的存在，remote address 仍然显示为客户端的 IP 地址。\n- **非透明代理**: 客户端知道代理的存在并进行配置，remote address 显示为代理服务器的 IP 地址，而原始客户端的 IP 地址通常会添加到&nbsp;X-Forwarded-For 头字段中（如果代理服务器配置了添加&nbsp;X-Forwarded-For 头字段的话）。\n\n经过非透明代理的请求：\n\n```\nRemote Address: 198.51.100.1 (代理服务器 IP)\nX-Forwarded-For: 203.0.113.195 (原始客户端 IP)\n\n```\n\n#### 3.经过反向代理（如 Nginx）\n\n当使用反向代理服务器（如 Nginx）时，默认情况下，后端服务器看到的&nbsp;remote address 是反向代理服务器的 IP 地址，而不是原始客户端的 IP 地址。\n\n```\nRemote Address: 198.51.100.2 (Nginx 反向代理服务器 IP)\n\n```\n\n为了使后端服务器能够获取原始客户端的 IP 地址，Nginx 通常会在请求头中添加&nbsp;X-Forwarded-For 和&nbsp;X-Real-IP 字段。\n\n配置 Nginx 传递原始客户端 IP\n\nproxy_set_header X-Real-IP $remote_addr: 将原始客户端 IP 地址设置为 X-Real-IP 头字段。\n\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for: 将 X-Forwarded-For 头字段设置为包含原始客户端 IP 地址和所有经过的代理服务器的 IP 地址。\n\n**$proxy_add_x_forwarded_for** 是一个 Nginx 内置变量：\n\n1.如果客户端请求包含 X-Forwarded-For header字段，**$proxy_add_x_forwarded_for** 变量等于X-Forwarded-For header后面附加$remote_addr变量（反向代理服务器的IP），用逗号分隔。\n\n2.如果客户端请求不存在 X-Forwarded-For header字段，**$proxy_add_x_forwarded_for** 变量就等于$remote_addr变量。\n\n参考：[http://nginx.org/en/docs/http/ngx_http_proxy_module.html](http://nginx.org/en/docs/http/ngx_http_proxy_module.html)\n\n```\nhttp {\n    server {\n        listen 80;\n        server_name example.com;\n\n        location / {\n            proxy_pass http://backend_server;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header Host $host;\n        }\n    }\n}\n\n```\n\n参考：[HTTP 请求头中的 X-Forwarded-For](https://blog.csdn.net/liuxiao723846/article/details/107730736)\n","tags":["nginx"]},{"title":"使用filebeat发送nginx日志到kafka","url":"/使用filebeat发送nginx日志到kafka.html","content":"1.配置filebeat_nginx.yml\n\n```\nfilebeat.modules:\n- module: nginx\n  access:\n    enabled: true\n    var.paths: [\"/var/log/nginx/access.log*\"]\n  error:\n    enabled: true\n    var.paths: [\"/var/log/nginx/error.log*\"]\n\n#----------------------------------Kafka output--------------------------------#\noutput.kafka:\n  version: \"1.0.1\"\n  enabled: true\n  hosts: ['xxx:9092', 'xxx:9092', 'xxx:9092']\n  topic: 'temp'\n  required_acks: 1  #default\n  compression: gzip #default\n  max_message_bytes: 1000000 #default\n  codec.format:\n    string: '%{[message]}'\n\n```\n\n2.启动filebeat\n\n```\n./filebeat -e -c filebeat_nginx.yml\n\n```\n\n3.访问nginx\n\n```\ntail -f /var/log/nginx/access.log\n\n```\n\n日志文件输出\n\n```\n{\"ts\":\"2019-10-14 10:53:22\",\"host\":\"127.0.0.1\",\"clientip\":\"127.0.0.1\",\"size\":0,\"responsetime\":0.000,\"upstreamtime\":\"-\",\"upstreamhost\":\"-\",\"http_host\":\"localhost\",\"url\":\"/index.html\",\"domain\":\"localhost\",\"xff\":\"-\",\"referer\":\"-\",\"status\":\"304\"}\n{\"ts\":\"2019-10-14 10:53:23\",\"host\":\"127.0.0.1\",\"clientip\":\"127.0.0.1\",\"size\":0,\"responsetime\":0.000,\"upstreamtime\":\"-\",\"upstreamhost\":\"-\",\"http_host\":\"localhost\",\"url\":\"/index.html\",\"domain\":\"localhost\",\"xff\":\"-\",\"referer\":\"-\",\"status\":\"304\"}\n{\"ts\":\"2019-10-14 10:53:23\",\"host\":\"127.0.0.1\",\"clientip\":\"127.0.0.1\",\"size\":0,\"responsetime\":0.000,\"upstreamtime\":\"-\",\"upstreamhost\":\"-\",\"http_host\":\"localhost\",\"url\":\"/index.html\",\"domain\":\"localhost\",\"xff\":\"-\",\"referer\":\"-\",\"status\":\"304\"}\n{\"ts\":\"2019-10-14 10:53:30\",\"host\":\"127.0.0.1\",\"clientip\":\"127.0.0.1\",\"size\":0,\"responsetime\":0.000,\"upstreamtime\":\"-\",\"upstreamhost\":\"-\",\"http_host\":\"localhost\",\"url\":\"/index.html\",\"domain\":\"localhost\",\"xff\":\"-\",\"referer\":\"-\",\"status\":\"304\"}\n{\"ts\":\"2019-10-14 10:53:31\",\"host\":\"127.0.0.1\",\"clientip\":\"127.0.0.1\",\"size\":0,\"responsetime\":0.000,\"upstreamtime\":\"-\",\"upstreamhost\":\"-\",\"http_host\":\"localhost\",\"url\":\"/index.html\",\"domain\":\"localhost\",\"xff\":\"-\",\"referer\":\"-\",\"status\":\"304\"}\n\n```\n\nkafka输出\n\n```\n{\"ts\":\"2019-10-14 10:53:23\",\"host\":\"127.0.0.1\",\"clientip\":\"127.0.0.1\",\"size\":0,\"responsetime\":0.000,\"upstreamtime\":\"-\",\"upstreamhost\":\"-\",\"http_host\":\"localhost\",\"url\":\"/index.html\",\"domain\":\"localhost\",\"xff\":\"-\",\"referer\":\"-\",\"status\":\"304\"}\n{\"ts\":\"2019-10-14 10:53:23\",\"host\":\"127.0.0.1\",\"clientip\":\"127.0.0.1\",\"size\":0,\"responsetime\":0.000,\"upstreamtime\":\"-\",\"upstreamhost\":\"-\",\"http_host\":\"localhost\",\"url\":\"/index.html\",\"domain\":\"localhost\",\"xff\":\"-\",\"referer\":\"-\",\"status\":\"304\"}\n{\"ts\":\"2019-10-14 10:53:22\",\"host\":\"127.0.0.1\",\"clientip\":\"127.0.0.1\",\"size\":0,\"responsetime\":0.000,\"upstreamtime\":\"-\",\"upstreamhost\":\"-\",\"http_host\":\"localhost\",\"url\":\"/index.html\",\"domain\":\"localhost\",\"xff\":\"-\",\"referer\":\"-\",\"status\":\"304\"}\n{\"ts\":\"2019-10-14 10:53:30\",\"host\":\"127.0.0.1\",\"clientip\":\"127.0.0.1\",\"size\":0,\"responsetime\":0.000,\"upstreamtime\":\"-\",\"upstreamhost\":\"-\",\"http_host\":\"localhost\",\"url\":\"/index.html\",\"domain\":\"localhost\",\"xff\":\"-\",\"referer\":\"-\",\"status\":\"304\"}\n{\"ts\":\"2019-10-14 10:53:31\",\"host\":\"127.0.0.1\",\"clientip\":\"127.0.0.1\",\"size\":0,\"responsetime\":0.000,\"upstreamtime\":\"-\",\"upstreamhost\":\"-\",\"http_host\":\"localhost\",\"url\":\"/index.html\",\"domain\":\"localhost\",\"xff\":\"-\",\"referer\":\"-\",\"status\":\"304\"}\n\n```\n\n<!--more-->\n&nbsp;如果要采集多个log，并发送到不同的topic的话，参考\n\n```\nhttps://blog.csdn.net/Z_GodGirl/article/details/81328000\n\n```\n\n&nbsp;\n","tags":["filebeat","nginx"]},{"title":"由crt和key文件生成keystore文件","url":"/由crt和key文件生成keystore文件.html","content":"该图转自知乎 [海棠依旧](https://www.zhihu.com/people/pingcai)\n\n<!--more-->\n&nbsp;\n\n1.先生成p12文件,生成的时候需要指定密码\n\n```\nopenssl pkcs12 -export -in your_crt.crt -inkey your_key.key -out your_p12.p12\n\n```\n\n2.再生成keystore文件\n\n```\nkeytool -importkeystore -v  -srckeystore your_p12.p12 -srcstoretype pkcs12 -srcstorepass 你设置的密码 -destkeystore your_keystore.keystore -deststoretype jks -deststorepass 你设置的密码\n\n```\n\n3.然后就能使用这个keystore文件和密码来进行https的请求\n\n&nbsp;\n\n参考\n\n```\nhttps://blog.csdn.net/u013944791/article/details/73551253\n\n```\n\n其他证书转换参考\n\n```\nhttp://www.netkiller.cn/cryptography/openssl/format.html\n\n```\n\n&nbsp;\n","tags":["计算机网络"]},{"title":"Nginx修改时间戳","url":"/Nginx修改时间戳.html","content":"1.安装nginx,注意不要安装nginx-full\n\n```\nsudo apt-get install nginx\nsudo apt-get install nginx-common\nsudo apt-get install nginx-extras\n\n```\n\n确认版本\n\n```\napt list --installed | grep nginx\n\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n\nnginx/xenial-updates,xenial-updates,xenial-security,xenial-security,now 1.10.3-0ubuntu0.16.04.4 all [已安装]\nnginx-common/xenial-updates,xenial-updates,xenial-security,xenial-security,now 1.10.3-0ubuntu0.16.04.4 all [已安装]\nnginx-extras/xenial-updates,xenial-security,now 1.10.3-0ubuntu0.16.04.4 amd64 [已安装]\n\n```\n\n2.修改配置 /etc/nginx/nginx.conf\n\n```\nhttp {\n\n        ##\n        # Basic Settings\n        ##\n\n        sendfile on;\n        tcp_nopush on;\n        tcp_nodelay on;\n        keepalive_timeout 65;\n        types_hash_max_size 2048;\n        # server_tokens off;\n\n        # server_names_hash_bucket_size 64;\n        # server_name_in_redirect off;\n\n        include /etc/nginx/mime.types;\n        default_type application/octet-stream;\n\n        ##\n        # SSL Settings\n        ##\n\n        ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # Dropping SSLv3, ref: POODLE\n        ssl_prefer_server_ciphers on;\n\n        ##\n        # Logging Settings\n        ##\n\n#       access_log /var/log/nginx/access.log;\n        error_log /var/log/nginx/error.log;\n\n        ##\n        # Gzip Settings\n        ##\n\n        gzip on;\n        gzip_disable \"msie6\";\n\n        # gzip_vary on;\n        # gzip_proxied any;\n        # gzip_comp_level 6;\n        # gzip_buffers 16 8k;\n        # gzip_http_version 1.1;\n        # gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;\n\n        ##\n        # Virtual Host Configs\n        ##\n\n        include /etc/nginx/conf.d/*.conf;\n        include /etc/nginx/sites-enabled/*;\n\nserver {\n                listen       80;\n                server_name  localhost;\n                location / {\n                    root   html;\n                    index  index.html index.htm;\n                }\n            }\n\nlog_format access_json '{\"ts\":\"$fmt_localtime\",'\n                           '\"schema\":\"com.xxx.xxx\",'\n                           '\"host\":\"$server_addr\",'\n                           '\"clientip\":\"$remote_addr\",'\n                           '\"size\":$body_bytes_sent,'\n                           '\"responsetime\":$request_time,'\n                           '\"upstreamtime\":\"$upstream_response_time\",'\n                           '\"upstreamhost\":\"$upstream_addr\",'\n                           '\"http_host\":\"$host\",'\n                           '\"url\":\"$uri\",'\n                           '\"domain\":\"$host\",'\n                           '\"xff\":\"$http_x_forwarded_for\",'\n                           '\"referer\":\"$http_referer\",'\n                           '\"status\":\"$status\"}';\n\naccess_log  /var/log/nginx/access.log  access_json;\n\nmap $host $fmt_localtime {\n        default '';\n    }\n\nlog_by_lua_block {\n       ngx.var.fmt_localtime = ngx.localtime();\n    }\n\n}\n\n```\n\n3.输出的日志\n\n```\n{\"ts\":\"2019-10-13 22:59:48\",\"schema\":\"com.xxx.xxx\",\"host\":\"127.0.0.1\",\"clientip\":\"127.0.0.1\",\"size\":0,\"responsetime\":0.000,\"upstreamtime\":\"-\",\"upstreamhost\":\"-\",\"http_host\":\"localhost\",\"url\":\"/index.html\",\"domain\":\"localhost\",\"xff\":\"-\",\"referer\":\"-\",\"status\":\"304\"}\n{\"ts\":\"2019-10-13 22:59:48\",\"schema\":\"com.xxx.xxx\",\"host\":\"127.0.0.1\",\"clientip\":\"127.0.0.1\",\"size\":0,\"responsetime\":0.000,\"upstreamtime\":\"-\",\"upstreamhost\":\"-\",\"http_host\":\"localhost\",\"url\":\"/index.html\",\"domain\":\"localhost\",\"xff\":\"-\",\"referer\":\"-\",\"status\":\"304\"}\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["nginx"]},{"title":"Nginx打印json日志","url":"/Nginx打印json日志.html","content":"1.修改配置，在http{}中添加\n\n```\nlog_format access_json '{\"@timestamp\":\"$time_iso8601\",'\n                           '\"host\":\"$server_addr\",'\n                           '\"clientip\":\"$remote_addr\",'\n                           '\"size\":$body_bytes_sent,'\n                           '\"responsetime\":$request_time,'\n                           '\"upstreamtime\":\"$upstream_response_time\",'\n                           '\"upstreamhost\":\"$upstream_addr\",'\n                           '\"http_host\":\"$host\",'\n                           '\"url\":\"$uri\",'\n                           '\"domain\":\"$host\",'\n                           '\"xff\":\"$http_x_forwarded_for\",'\n                           '\"referer\":\"$http_referer\",'\n                           '\"status\":\"$status\"}';\naccess_log  /var/log/nginx/access.log  access_json;\n\n```\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20191012184314679-707127817.png\" alt=\"\" />\n\n&nbsp;\n\n&nbsp;2.重启\n\n```\nsystemctl restart nginx\n\n```\n\n或者\n\n```\nnginx -s reload\n\n```\n\n3.访问，输出日志\n\n```\n{\"@timestamp\":\"2019-10-12T18:41:48+08:00\",\"host\":\"127.0.0.1\",\"clientip\":\"127.0.0.1\",\"size\":0,\"responsetime\":0.000,\"upstreamtime\":\"-\",\"upstreamhost\":\"-\",\"http_host\":\"localhost\",\"url\":\"/index.html\",\"domain\":\"localhost\",\"xff\":\"-\",\"referer\":\"-\",\"status\":\"304\"}\n\n```\n\n&nbsp;\n\n日志中变量的含义\n\n```\nlog_format access_json '{\"ts\":\"$fmt_localtime\",'\n                           '\"server_addr\":\"$server_addr\",'\n                           '\"request\":\"$request\",'\n                           '\"http_accept_language\":\"$http_accept_language\",'\n                           '\"http_user_agent\":\"$http_user_agent\",'\n                           '\"remote_addr\":\"$remote_addr\",'\n                           '\"body_bytes_sent,\":$body_bytes_sent,'\n                           '\"request_time,\":$request_time,'\n                           '\"request_length\":$request_length,'\n                           '\"http_host\":\"$http_host\",'\n                           '\"url\":\"$uri\",'\n                           '\"host\":\"$host\",'\n                           '\"http_x_forwarded_for\":\"$http_x_forwarded_for\",'\n                           '\"http_referer\":\"$http_referer\",'\n                           '\"status\":\"$status\"}';\n\n```\n\n比如\n\n```\n{\n\"ts\":\"2019-10-14 16:02:19\",\n\"server_addr\":\"127.0.0.1\",\n\"request\":\"GET /index.html HTTP/1.1\",\n\"http_accept_language\":\"zh-CN,en-US;q=0.7,en;q=0.3\",\n\"http_user_agent\":\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:69.0) Gecko/20100101 Firefox/69.0\",\n\"remote_addr\":\"127.0.0.1\",\n\"body_bytes_sent,\":0,\n\"request_time,\":0.000,\n\"request_length\":448,\n\"http_host\":\"localhost\",\n\"url\":\"/index.html\",\n\"host\":\"localhost\",\n\"http_x_forwarded_for\":\"-\",\n\"http_referer\":\"-\",\n\"status\":\"304\"\n}\n\n```\n\n参考\n\n```\nhttps://www.iteye.com/blog/bit1129-2205848\n\n```\n\n参数含义\n\n1.访问时间 ts<br />2.访问端口 server_addr<br />3.请求方式（GET或者POST等）request<br />4.用户浏览器语言。如：上例中的 \"es-ES,es;q=0.8\" http_accept_language<br />5.用户浏览器其他信息，浏览器版本、浏览器类型等 http_user_agent<br />6.客户端（用户）IP地址 remote_addr<br />7.发送给客户端的文件主体内容的大小&nbsp;body_bytes_sent<br />8.整个请求的总时间&nbsp;request_time<br />9.请求的长度（包括请求行，请求头和请求正文）request_length<br />10.请求的url地址（目标url地址）的host&nbsp;http_host<br />11.请求url地址（去除host部分） uri<br />12.host 与 http_host的区别在于当使用非80/443端口的时候,http_host = host:port host<br />13.客户端的真实ip，通常web服务器放在反向代理的后面，这样就不能获取到客户端的IP地址了，通 过$remote_add拿到的IP地址是反向代理服务器的iP地址。反向代理服务器在转发请求的http头信息中，可以增加 x_forwarded_for信息，用以记录原有客户端的IP地址和原来客户端的请求的服务器地址&nbsp; http_x_forwarded_for<br />14.记录从哪个页面链接访问过来的（请求头**Referer**的内容）http_referer<br />15.请求状态（状态码，200表示成功)&nbsp; status\n","tags":["nginx"]},{"title":"Nginx添加静态页面","url":"/Nginx添加静态页面.html","content":"1.编辑配置文件\n\n```\nsudo vim  /etc/nginx/nginx.conf\n\n```\n\n在http {}中添加如下\n\n```\nserver {\n                listen       80;\n                server_name  localhost;\n                location / {\n                    root   html;\n                    index  index.html index.htm;\n                }\n            }\n\n```\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20191012182945791-977968792.png\" alt=\"\" />\n\n&nbsp;\n\n2.重启nginx\n\n```\nsystemctl restart nginx\n\n```\n\n3.访问，成功\n\n```\nlocalhost:80/index.html\n\n```\n\n&nbsp;<img src=\"/images/517519-20191012183104393-1719996949.png\" alt=\"\" />\n\n4.访问日志\n\n```\ntail -n 100 /var/log/nginx/access.log \n127.0.0.1 - - [12/Oct/2019:18:22:21 +0800] \"GET /index.html HTTP/1.1\" 200 396 \"-\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:69.0) Gecko/20100101 Firefox/69.0\"\n127.0.0.1 - - [12/Oct/2019:18:22:21 +0800] \"GET /favicon.ico HTTP/1.1\" 404 152 \"-\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:69.0) Gecko/20100101 Firefox/69.0\"\n127.0.0.1 - - [12/Oct/2019:18:30:37 +0800] \"GET /index.html HTTP/1.1\" 200 396 \"-\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:69.0) Gecko/20100101 Firefox/69.0\"\n127.0.0.1 - - [12/Oct/2019:18:30:37 +0800] \"GET /favicon.ico HTTP/1.1\" 404 152 \"-\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:69.0) Gecko/20100101 Firefox/69.0\"\n\n```\n\n&nbsp;\n","tags":["nginx"]},{"title":"Ubuntu16.04安装nginx","url":"/Ubuntu16.04安装nginx.html","content":"1.安装\n\n```\nsudo apt-get install nginx\n\n```\n\n2.启动\n\n```\nsystemctl start nginx.service\n\n```\n\n如果和apache2的80端口冲突了，修改一下apache2的port\n\n```\nsudo vim /etc/apache2/ports.conf\n\n```\n\n冲突的话，日志/var/log/nginx/error.log中将会报\n\n```\n2019/10/12 14:25:31 [emerg] 23836#23836: listen() to 0.0.0.0:80, backlog 511 failed (98: Address already in use)\n\n```\n\n修改成8080\n\n<img src=\"/images/517519-20191012164526636-1550542863.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n重启apache2\n\n```\nsystemctl restart apache2\n\n```\n\n再启动nginx，成功\n\n<img src=\"/images/517519-20191012164820592-327437748.png\" alt=\"\" />\n\n&nbsp;\n\nNginx的负载均衡策略，参考：[nginx负载均衡的5种策略](https://segmentfault.com/a/1190000014483200)\n\n1、轮询 round robin（默认）\n\n2、weight：指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况\n\n3、ip_hash（上面2种会有session丢失的问题，ip_hash解决了这个问题）\n\n4、fair（第三方），按后端服务器的响应时间来分配请求，响应时间短的优先分配\n\n5、url_hash（第三方），按访问url的hash结果来分配请求，使每个url定向到同一个（对应的）后端服务器，后端服务器为缓存时比较有效\n\n&nbsp;\n","tags":["nginx"]},{"title":"Nexus上传python包","url":"/Nexus上传python包.html","content":"参考\n\n```\nhttps://blog.csdn.net/m0_37607365/article/details/79998955\n\n```\n\n1.首先创建pypi仓库\n\n<img src=\"/images/517519-20190930191830954-860937756.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n<img src=\"/images/517519-20190930191900803-32629551.png\" alt=\"\" />\n\n&nbsp;\n\n其中，PyPI类的服务，支持三种：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n\nproxy，提供代理服务\n\nhosted，提供私有包的发布服务\n\ngroup，组合以上两类的多个服务到一起，通过同一个URL对外提供\n\n首先创建pypi-proxy,指定remote storage为阿里云\n\n```\nhttp://mirrors.aliyun.com/pypi\n\n```\n\n&nbsp;<img src=\"/images/517519-20190930192306221-531789336.png\" alt=\"\" />\n\n&nbsp;\n\n创建pypi-hosted\n\n<img src=\"/images/517519-20190930192416843-1265625023.png\" alt=\"\" />\n\n创建pypi-group\n\n<img src=\"/images/517519-20190930202245705-685999648.png\" alt=\"\" />\n\n&nbsp;\n\n&nbsp;在电脑上配置 .pypirc\n\n```\n[distutils]\nindex-servers =\n    nexus\n    nexustest\n\n# 要选择所建仓库中的hosted仓库\n[nexus]\nrepository=http://xxx.com:8081/nexus/repository/pypi-hosted/\nusername=xxx\npassword=xxx\n\n[nexustest]\nrepository=http://xxx.com:8081/nexus/repository/pypi-hosted/\nusername=xxx\npassword=xxx\n\n```\n\n安装twine\n\n```\npip install twine\n\n```\n\n在你的工程中创建一个setup.py文件,比如这样\n\n<img src=\"/images/517519-20190930193925992-2026909709.png\" alt=\"\" />\n\n&nbsp;\n\nsetup.py文件\n\n```\nimport sys\n\nif sys.version_info < (2, 6):\n    print(sys.stderr, \"{}: need Python 2.6 or later.\".format(sys.argv[0]))\n    print(sys.stderr, \"Your Python is {}\".format(sys.version))\n    sys.exit(1)\n\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"xxxxxxxx\",\n    version=\"1.0\",\n    license=\"BSD\",\n    description=\"A python library adding a json log formatter\",\n    package_dir={'': 'src'},\n    packages=find_packages(\"src\", exclude=\"tests\"),\n    install_requires=[\"setuptools\", \"thrift==0.10.0\", \"requests >= 2.13.0\", \"urllib3 >= 1.25.3\"],\n    classifiers=[\n        'Development Status :: 3 - Alpha',\n        'Intended Audience :: Developers',\n        'License :: OSI Approved :: BSD License',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 2',\n        'Programming Language :: Python :: 2.6',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.1',\n        'Programming Language :: Python :: 3.2',\n        'Programming Language :: Python :: 3.3',\n        'Programming Language :: Python :: 3.4',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n        'Topic :: System :: Logging',\n    ]\n)\n\n```\n\n安装\n\n```\npython setup.py install\n\n```\n\n生成压缩包\n\n```\npython setup.py sdist\n\n```\n\n上传nexus,其中nexus就是在.pypirc文件中配置\n\n```\ntwine upload -r nexus dist/*\n\n```\n\n使用\n\n```\npip install -i http://ip:8081/nexus/repository/pypi-group/simple --trusted-host=ip xxxx==1.0.0\n\n```\n\n&nbsp;\n","tags":["Nexus"]},{"title":"Hive学习笔记——parser","url":"/Hive学习笔记——parser.html","content":"Hive是如何解析SQL的呢,首先拿hive的建表语句来举例,比如下面的建表语句\n\n```\ncreate table test(id int,name string)row format delimited fields terminated by '\\t';\n\n```\n\n然后使用hive的show create table语句来查看创建的表结构，这是一张text表\n\n```\nCREATE TABLE `test`(\n  `id` int, \n  `name` string)\nROW FORMAT SERDE \n  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' \nWITH SERDEPROPERTIES ( \n  'field.delim'='\\t', \n  'serialization.format'='\\t') \nSTORED AS INPUTFORMAT \n  'org.apache.hadoop.mapred.TextInputFormat' \nOUTPUTFORMAT \n  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\nLOCATION\n  'hdfs://master:8020/user/hive/warehouse/test'\nTBLPROPERTIES (\n  'transient_lastDdlTime'='1568561230')\n\n```\n\n当然还有其他各种建表语句，比如\n\ncsv表\n\n```\nCREATE EXTERNAL TABLE `default.test_1`(\n\t  `key` string COMMENT 'from deserializer', \n\t  `value` string COMMENT 'from deserializer')\n\tROW FORMAT SERDE \n\t  'org.apache.hadoop.hive.serde2.OpenCSVSerde' \n\tWITH SERDEPROPERTIES ( \n\t  'escapeChar'='\\\\', \n\t  'quoteChar'='\\'', \n\t  'separatorChar'='\\t') \n\tSTORED AS INPUTFORMAT \n\t  'org.apache.hadoop.mapred.TextInputFormat' \n\tOUTPUTFORMAT \n\t  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n\tLOCATION\n\t  'hdfs://master:8020/user/hive/warehouse/test'\n\tTBLPROPERTIES (\n\t  'COLUMN_STATS_ACCURATE'='false', \n\t  'numFiles'='0', \n\t  'numRows'='-1', \n\t  'rawDataSize'='-1', \n\t  'totalSize'='0', \n\t  'transient_lastDdlTime'='xxxx')\n\n```\n\n<!--more-->\n&nbsp;parquet表\n\n```\nCREATE TABLE `default.test`(\n\t  `time` string, \n\t  `server` int, \n\t  `id` bigint)\n\tPARTITIONED BY ( \n\t  `ds` string)\n\tROW FORMAT SERDE \n\t  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' \n\tWITH SERDEPROPERTIES ( \n\t  'field.delim'='\\t', \n\t  'serialization.format'='\\t') \n\tSTORED AS INPUTFORMAT \n\t  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' \n\tOUTPUTFORMAT \n\t  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n\tLOCATION\n\t  'hdfs://master:8020/user/hive/warehouse/test'\n\tTBLPROPERTIES (\n  'transient_lastDdlTime'='xxxx')\n\n```\n\n&nbsp;json表\n\n```\nCREATE EXTERNAL TABLE `default.test`(\n\t  `titleid` string COMMENT 'from deserializer', \n\t  `timestamp` string COMMENT 'from deserializer')\n\tROW FORMAT SERDE \n\t  'org.openx.data.jsonserde.JsonSerDe' \n\tSTORED AS INPUTFORMAT \n\t  'org.apache.hadoop.mapred.TextInputFormat' \n\tOUTPUTFORMAT \n\t  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n\tLOCATION\n\t  'hdfs://master:8020/user/hive/warehouse/test'\n\tTBLPROPERTIES (\n\t  'COLUMN_STATS_ACCURATE'='false', \n\t  'numFiles'='0', \n\t  'numRows'='-1', \n\t  'rawDataSize'='-1', \n\t  'totalSize'='0', \n\n```\n\n&nbsp;es表\n\n```\nCREATE EXTERNAL TABLE `default.test`(\n\t  `id` string COMMENT 'from deserializer', \n\t  `ts` string COMMENT 'from deserializer', ')\n\tPARTITIONED BY ( \n\t  `ds` string)\n\tROW FORMAT SERDE \n\t  'org.elasticsearch.hadoop.hive.EsSerDe' \n\tSTORED BY \n\t  'org.elasticsearch.hadoop.hive.EsStorageHandler' \n\tWITH SERDEPROPERTIES ( \n\t  'serialization.format'='1')\n\tLOCATION\n\t  'hdfs://master:8020/user/hive/warehouse/test'\n\tTBLPROPERTIES (\n\t  'es.index.auto.create'='yes', \n\t  'es.index.read.missing.as.empty'='yes', \n\t  'es.nodes'='host1,host2', \n\t  'es.port'='9200', \n\t  'es.resource'='index1/type1', \n\n```\n\n使用thrift的binary表\n\n```\nCREATE EXTERNAL TABLE `default.test`(\n\t  `bbb` string COMMENT 'from deserializer', \n\t  `aaa` string COMMENT 'from deserializer')\n\tCOMMENT 'aas'\n\tPARTITIONED BY ( \n\t  `ds` string COMMENT '日期分区')\n\tROW FORMAT SERDE \n\t  'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer' \n\tWITH SERDEPROPERTIES ( \n\t  'serialization.class'='com.xxx.xxx.xxx.tables.v1.XXXX', \n\t  'serialization.format'='org.apache.thrift.protocol.TCompactProtocol') \n\tSTORED AS INPUTFORMAT \n\t  'org.apache.hadoop.mapred.SequenceFileInputFormat' \n\tOUTPUTFORMAT \n\t  'org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'\n\tLOCATION\n\t  'hdfs://master:8020/user/hive/warehouse/test'\n\tTBLPROPERTIES (\n\t  'transient_lastDdlTime'='xxxxxx')\n\n```\n\nhbase表\n\n```\nCREATE EXTERNAL TABLE default.hbase_table(key string, k1 string,k2 string,k3 string,k4 string,ts string)\nROW FORMAT SERDE\n'org.apache.hadoop.hive.hbase.HBaseSerDe'\nSTORED BY\n'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\nWITH SERDEPROPERTIES (\"hbase.columns.mapping\"=\":key,c:k1,c:k2,c:k3,c:k4,c:ts\")\nTBLPROPERTIES(\"hbase.table.name\" = \"xxxxx\");\n\n```\n\nmongo表\n\n```\nCREATE EXTERNAL TABLE IF NOT EXISTS xx.xx (\nk1 string,\nk2 int\n)\nSTORED BY 'com.mongodb.hadoop.hive.MongoStorageHandler'\nWITH SERDEPROPERTIES('mongo.columns.mapping'='{\"k1\":\"k1\",\"k2\":\"k2\"}','mongo.input.split_size'='16384')\nTBLPROPERTIES('mongo.uri'='mongodb://xxx:xxx/table1.collection1');\n\n```\n\n等等\n\n可以查看show create table的hive源码\n\n```\nhttps://github.com/apache/hive/blob/68ae4a5cd1b916098dc1deb2bcede5f862afd80e/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/creation/ShowCreateTableOperation.java\n\n```\n\n其中可以看出hive表的一些基本信息\n\n```\nprivate static final String CREATE_TABLE_TEMPLATE =\n      \"CREATE <\" + TEMPORARY + \"><\" + EXTERNAL + \">TABLE `<\" + NAME + \">`(\\n\" +\n      \"<\" + LIST_COLUMNS + \">)\\n\" +\n      \"<\" + COMMENT + \">\\n\" +\n      \"<\" + PARTITIONS + \">\\n\" +\n      \"<\" + BUCKETS + \">\\n\" +\n      \"<\" + SKEWED + \">\\n\" +\n      \"<\" + ROW_FORMAT + \">\\n\" +\n      \"<\" + LOCATION_BLOCK + \">\" +\n      \"TBLPROPERTIES (\\n\" +\n      \"<\" + PROPERTIES + \">)\\n\";\n\n  private String getCreateTableCommand(Table table) {\n    ST command = new ST(CREATE_TABLE_TEMPLATE);\n\n    command.add(NAME, desc.getTableName());\n    command.add(TEMPORARY, getTemporary(table));\n    command.add(EXTERNAL, getExternal(table));\n    command.add(LIST_COLUMNS, getColumns(table));\n    command.add(COMMENT, getComment(table));\n    command.add(PARTITIONS, getPartitions(table));\n    command.add(BUCKETS, getBuckets(table));\n    command.add(SKEWED, getSkewed(table));\n    command.add(ROW_FORMAT, getRowFormat(table));\n    command.add(LOCATION_BLOCK, getLocationBlock(table));\n    command.add(PROPERTIES, getProperties(table));\n\n    return command.render();\n  }\n\n```\n\n当用户输入一行create table语句的时候,可查看源码\n\n```\nhttps://github.com/apache/hive/blob/ff98efa7c6f2b241d8fddd0ac8dc55e817ecb234/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java\n\n```\n\n美团点评 [Hive SQL的编译过程](https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html)\n\n```\nhttps://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html\n\n```\n\n其中可以看到,建表语句首先会使用antlr4将其转换成一颗语法树\n\n```\npublic static ASTNode parse(String command) throws ParseException {\n    return parse(command, null);\n  }\n\n```\n\n然后可以使用getTable抽取其中的库名和表名\n\n```\nhttps://github.com/apache/hive/blob/f37c5de6c32b9395d1b34fa3c02ed06d1bfbf6eb/ql/src/java/org/apache/hadoop/hive/ql/parse/AnalyzeCommandUtils.java\n\n```\n\n源码\n\n```\npublic static Table getTable(ASTNode tree, BaseSemanticAnalyzer sa) throws SemanticException {\n    String tableName = ColumnStatsSemanticAnalyzer.getUnescapedName((ASTNode) tree.getChild(0).getChild(0));\n    String currentDb = SessionState.get().getCurrentDatabase();\n    String [] names = Utilities.getDbTableName(currentDb, tableName);\n    return sa.getTable(names[0], names[1], true);\n  }\n\n```\n\n&nbsp;\n\n```\nhttps://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseDriver.java\n\n```\n\n&nbsp;\n\n```\npublic ASTNode parse(String command) throws ParseException {\n    return parse(command, null);\n  }\n\n```\n\n然后比如要提取inputformat，outpurformat，serde和storageHandler\n\n```\nhttps://github.com/apache/hive/blob/f37c5de6c32b9395d1b34fa3c02ed06d1bfbf6eb/ql/src/java/org/apache/hadoop/hive/ql/parse/StorageFormat.java\n\n```\n\n源码\n\n&nbsp;\n\n要提取字段信息，SkewedValue，表名以及row format\n\n```\nhttps://github.com/apache/hive/blob/f37c5de6c32b9395d1b34fa3c02ed06d1bfbf6eb/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java\n\n```\n\n源码\n\n```\npublic static List<FieldSchema> getColumns(\n      ASTNode ast, boolean lowerCase, TokenRewriteStream tokenRewriteStream,\n      List<SQLPrimaryKey> primaryKeys, List<SQLForeignKey> foreignKeys,\n      List<SQLUniqueConstraint> uniqueConstraints, List<SQLNotNullConstraint> notNullConstraints,\n      List<SQLDefaultConstraint> defaultConstraints, List<SQLCheckConstraint> checkConstraints,\n      Configuration conf) throws SemanticException {\n我是源码\n}\n\n```\n\n源码\n\n```\n /**\n   * Get the unqualified name from a table node.\n   *\n   * This method works for table names qualified with their schema (e.g., \"db.table\")\n   * and table names without schema qualification. In both cases, it returns\n   * the table name without the schema.\n   *\n   * @param node the table node\n   * @return the table name without schema qualification\n   *         (i.e., if name is \"db.table\" or \"table\", returns \"table\")\n   */\n  public static String getUnescapedUnqualifiedTableName(ASTNode node) {\n    assert node.getChildCount() <= 2;\n\n    if (node.getChildCount() == 2) {\n      node = (ASTNode) node.getChild(1);\n    }\n\n    return getUnescapedName(node);\n  }\n\n```\n\n源码\n\n```\n  protected void analyzeRowFormat(ASTNode child) throws SemanticException {\n      child = (ASTNode) child.getChild(0);\n      int numChildRowFormat = child.getChildCount();\n      for (int numC = 0; numC < numChildRowFormat; numC++) {\n        ASTNode rowChild = (ASTNode) child.getChild(numC);\n        switch (rowChild.getToken().getType()) {\n        case HiveParser.TOK_TABLEROWFORMATFIELD:\n          fieldDelim = unescapeSQLString(rowChild.getChild(0)\n              .getText());\n          if (rowChild.getChildCount() >= 2) {\n            fieldEscape = unescapeSQLString(rowChild\n                .getChild(1).getText());\n          }\n          break;\n        case HiveParser.TOK_TABLEROWFORMATCOLLITEMS:\n          collItemDelim = unescapeSQLString(rowChild\n              .getChild(0).getText());\n          break;\n        case HiveParser.TOK_TABLEROWFORMATMAPKEYS:\n          mapKeyDelim = unescapeSQLString(rowChild.getChild(0)\n              .getText());\n          break;\n        case HiveParser.TOK_TABLEROWFORMATLINES:\n          lineDelim = unescapeSQLString(rowChild.getChild(0)\n              .getText());\n          if (!lineDelim.equals(\"\\n\")\n              &amp;&amp; !lineDelim.equals(\"10\")) {\n            throw new SemanticException(SemanticAnalyzer.generateErrorMessage(rowChild,\n                ErrorMsg.LINES_TERMINATED_BY_NON_NEWLINE.getMsg()));\n          }\n          break;\n        case HiveParser.TOK_TABLEROWFORMATNULL:\n          nullFormat = unescapeSQLString(rowChild.getChild(0)\n                    .getText());\n          break;\n        default:\n          throw new AssertionError(\"Unkown Token: \" + rowChild);\n        }\n      }\n    }\n  }\n\n```\n\n分区信息，首先通过取得Map对象，\n\n```\nhttps://github.com/apache/hive/blob/6f18bbbc2e030ce7d446b2475037203cbd4f860d/ql/src/java/org/apache/hadoop/hive/ql/parse/AnalyzeCommandUtils.java\n\n```\n\n源码\n\n```\n  public static Map<String,String> getPartKeyValuePairsFromAST(Table tbl, ASTNode tree,\n      HiveConf hiveConf) throws SemanticException {\n    ASTNode child = ((ASTNode) tree.getChild(0).getChild(1));\n    Map<String,String> partSpec = new HashMap<String, String>();\n    if (child != null) {\n      partSpec = DDLSemanticAnalyzer.getValidatedPartSpec(tbl, child, hiveConf, false);\n    } //otherwise, it is the case of analyze table T compute statistics for columns;\n    return partSpec;\n  }\n\n```\n\n再转换成List<Partition>对象\n\n```\nhttps://github.com/apache/hive/blob/556531182dc989e12fd491d951b353b4df13fd47/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java\n\n```\n\n源码\n\n```\npublic Map<String, String> partSpec; // has to use LinkedHashMap to enforce order<br />public List<Partition> partitions; // involved partitions in TableScanOperator/FileSinkOperator\npartitions = db.getPartitions(table, partSpec);\n\n```\n\nlocation信息，parsedLocation\n\n```\nhttps://github.com/apache/hive/blob/0213afb8a31af1f48d009edd41cec9e6c8942354/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java\n\n```\n\n&nbsp;\n","tags":["Hive"]},{"title":"Ubuntu16.04安装Supervisor","url":"/Ubuntu16.04安装Supervisor.html","content":"安装\n\n```\nsudo apt-get install supervisor \n\n```\n\n启动,否则会报 unix:///tmp/supervisor.sock no such file\n\n```\nservice supervisor start\n\n```\n\n或者\n\n```\nsupervisord -c /etc/supervisor/supervisord.conf\n\n```\n\n<!--more-->\n&nbsp;生成配置文件\n\n```\necho_supervisord_conf > /etc/supervisor/supervisord.conf\n\n```\n\n注意里面的注释去掉\n\n```\n[inet_http_server]         ; inet (TCP) server disabled by default\nport=127.0.0.1:9001        ; (ip_address:port specifier, *:port for all iface)\nusername=user              ; (default is no username (open server))\npassword=123               ; (default is no password (open server))\n\n```\n\n配置文件路径\n\n```\n[include]\nfiles = /etc/supervisor/conf.d/*.conf\n\n```\n\n查看状态\n\n```\nsupervisorctl status #查看supervisorctl状态\nsupervisorctl start openfalcon #启动子进程\nsupervisorctl stop openfalcon  #关闭子进程\nsupervisorctl restart openfalcon #重启子进程\n\n```\n\n比如\n\n```\nlintong@master:~$ supervisorctl status\nopenfalcon                       RUNNING   pid 10759, uptime 0:18:46\n\n```\n\n配置文件在 /etc/supervisor 目录下\n\n```\nlintong@master:/etc/supervisor$ ls\nconf.d  start_openfalcon.conf  supervisord.conf\n\n```\n\n内容\n\n```\n[program:openfalcon]\ncommand = cd ~/software/open-falcon-v0.2.1 &amp; open-falcon start\nuser = lintong\nautostart = true\nautoresart = true\nstderr_logfile = /var/log/supervisor/openfalcon.stderr.log\nstdout_logfile = /var/log/supervisor/openfalcon.stdout.log\n\n```\n\n重启supervisor\n\n```\nsupervisorctl reload\n\n```\n\n重新加载配置\n\n```\nsupervisorctl update\n\n```\n\nweb界面,账号密码参考 /etc/supervisord.conf\n\n```\nlocalhost:9001\n\n```\n\n<img src=\"/images/517519-20190908001037394-1739828796.png\" alt=\"\" />\n\n下面是几个例子\n\n```\nlintong@master:/etc/supervisor/conf.d$ ls\nes.conf  hadoop.conf  hive.conf  redis.conf\n\n```\n\nhadoop\n\n```\n[program:hadoop]\ndirectory=/home/lintong/software/apache/hadoop-2.9.1\ncommand = bash ./sbin/start-all.sh\nuser = lintong\nautostart = true\nautoresart = false\nstderr_logfile = /var/log/supervisor/hadoop.stderr.log\nstdout_logfile = /var/log/supervisor/hadoop.stdout.log\n\n```\n\nhive\n\n```\n[program:hive]\ndirectory=/home/lintong/software/apache/apache-hive-2.3.3-bin\ncommand = bash ./bin/hiveserver2 start\nuser = lintong\nautostart = true\nautoresart = true\nstderr_logfile = /var/log/supervisor/hive.stderr.log\nstdout_logfile = /var/log/supervisor/hive.stdout.log\n\n```\n\nes\n\n```\n[program:es]\ndirectory=/home/lintong/software/apache/elasticsearch-6.2.4\ncommand = bash ./bin/elasticsearch\nuser = lintong\nautostart = true\nautoresart = true\nstderr_logfile = /var/log/supervisor/es.stderr.log\nstdout_logfile = /var/log/supervisor/es.stdout.log\n\n```\n\nkafka_manager\n\n其中的/bin/bash -c 'source \"$0\" &amp;&amp; exec \"$@\"'是为了解决cant find command问题\n\n```\n[program:kafka-manager]\ndirectory=/home/lintong/software/apache/kafka-manager-1.3.3.17/bin\ncommand = /bin/bash -c 'source \"$0\" &amp;&amp; exec \"$@\"' kafka-manager -Dconfig.file=/home/lintong/software/apache/kafka-manager-1.3.3.17/conf/application.conf -Dhttp.port=7778\nuser = lintong\nautostart = true\nautoresart = false\nstderr_logfile = /var/log/supervisor/kafka_manager.stderr.log\nstdout_logfile = /var/log/supervisor/kafka_manager.stdout.log\n\n```\n\nkibana\n\n```\n[program:kibana]\ndirectory=/home/lintong/software/apache/kibana-6.2.4-linux-x86_64\ncommand = bash ./bin/kibana\nuser = lintong\nautostart = false\nautoresart = true\nstderr_logfile = /var/log/supervisor/kibana.stderr.log\nstdout_logfile = /var/log/supervisor/kibana.stdout.log\n\n```\n\nzkui\n\n```\n[program:zkui]\ndirectory=/home/lintong/software/zkui/target\ncommand = java -jar zkui-2.0-SNAPSHOT-jar-with-dependencies.jar\nuser = lintong\nautostart = false\nautorestart = true\nstderr_logfile = /var/log/supervisor/zkui.stderr.log\nstdout_logfile = /var/log/supervisor/zkui.stdout.log\n\n```\n\nkafka-manager\n\n```\n[program:kafka-manager]\ndirectory=/home/lintong/software/kafka-manager-1.3.3.17\ncommand = bash ./bin/kafka-manager -Dconfig.file=/home/lintong/software/kafka-manager-1.3.3.17/conf/application.conf -Dhttp.port=7778\nuser = lintong\nautostart = false\nautorestart = true\nstderr_logfile = /var/log/supervisor/kafka-manager.stderr.log\nstdout_logfile = /var/log/supervisor/kafka-manager.stdout.log\n\n```\n\n如果supervisor启动的进程使用了虚拟环境，则可以使用environment来配置虚拟环境，如下\n\n```\nenvironment=PATH=\"/home/xxx/envs/airflow/bin\"\n\n```\n\n如果遇到xxxx: ERROR (no such file)的报错，需要把airflow的完整启动路径写到command的配置当中\n\nsupervisor的默认配置，比如日志文件最大为50MB，日志文件backup为10个，参考：[supervisor配置文件详解](https://cloud.tencent.com/developer/article/1598696)\n\n&nbsp;\n","tags":["Linux"]},{"title":"Hive学习笔记——metadata","url":"/Hive学习笔记——metadata.html","content":"**Hive结构体系**\n\n```\nhttps://blog.csdn.net/zhoudaxia/article/details/8855937\n\n```\n\n依赖\n\n```\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-client</artifactId>\n            <version>2.6.0</version>\n            <scope>provided</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hive</groupId>\n            <artifactId>hive-exec</artifactId>\n            <version>1.1.0</version>\n            <scope>provided</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hive</groupId>\n            <artifactId>hive-metastore</artifactId>\n            <version>1.1.0</version>\n            <scope>provided</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hive</groupId>\n            <artifactId>hive-jdbc</artifactId>\n            <version>1.1.0</version>\n        </dependency>\n\n```\n\n有2种方法可以取得hive的元数据\n\n**1.****使用hive的****jdbc接口**的getMetaData方法来获取hive表的相关元信息\n\n```\nimport java.sql.*;\n\nClass.forName(\"org.apache.hive.jdbc.HiveDriver\");\nConnection connection = DriverManager.getConnection(\"jdbc:hive2://xxxxx:10000\", \"xxxx\", \"xxxx\");\nStatement statement = connection.createStatement();\nDatabaseMetaData meta = connection.getMetaData();\n\n```\n\n参考\n\n```\nhttps://blog.csdn.net/u010368839/article/details/76358831\n\n```\n\nhive metadata源码解析可以参考\n\n```\nhttps://cloud.tencent.com/developer/article/1330250\n\n```\n\nhive thrift接口可以参考\n\n<!--more-->\n&nbsp;\n\n注意代码中的hive-site.xml和集群上面的保持一致\n\n否则会报错，例如\n\n```\nset_ugi() not successful, Likely cause: new client talking to old server. Continuing without it.\n\n```\n\n获得表的信息接口，指定tableNamePattern为hive表名\n\n```\nResultSet tableRet = meta.getTables(null, \"%\", \"ads_nsh_trade\", new String[]{\"TABLE\"});\nwhile (tableRet.next()) {\n    System.out.println(\"TABLE_CAT:\" + tableRet.getString(\"TABLE_CAT\"));\n    System.out.println(\"TABLE_SCHEM:\" + tableRet.getString(\"TABLE_SCHEM\"));\n    System.out.println(\"TABLE_NAME => \" + tableRet.getString(\"TABLE_NAME\"));\n    System.out.println(\"table_type => \" + tableRet.getString(\"table_type\"));\n    System.out.println(\"remarks => \" + tableRet.getString(\"remarks\"));\n    System.out.println(\"type_cat => \" + tableRet.getString(\"type_cat\"));\n    System.out.println(\"type_schem => \" + tableRet.getString(\"type_schem\"));\n    System.out.println(\"type_name => \" + tableRet.getString(\"type_name\"));\n    System.out.println(\"self_referencing_col_name => \" + tableRet.getString(\"self_referencing_col_name\"));\n    System.out.println(\"ref_generation => \" + tableRet.getString(\"ref_generation\"));\n}\n\n```\n\n其中的参数可以是\n\n```\ntable_cat, table_schem, table_name, table_type, remarks, type_cat, type_schem, type_name, self_referencing_col_name, ref_generation\n\n```\n\n如果填写不正确将会抛出异常\n\n```\njava.sql.SQLException: Could not find COLUMN_NAME in [table_cat, table_schem, table_name, table_type, remarks, type_cat, type_schem, type_name, self_referencing_col_name, ref_generation]\n\tat org.apache.hive.jdbc.HiveBaseResultSet.findColumn(HiveBaseResultSet.java:100)\n\tat org.apache.hive.jdbc.HiveBaseResultSet.getString(HiveBaseResultSet.java:541)\n\n```\n\n输出的结果\n\n```\nTABLE_CAT:\nTABLE_SCHEM:tmp\nTABLE_NAME => ads_nsh_trade\ntable_type => TABLE\nremarks => ???????????\ntype_cat => null\ntype_schem => null\ntype_name => null\nself_referencing_col_name => null\nref_generation => null\n\nTABLE_CAT:\nTABLE_SCHEM:default\nTABLE_NAME => ads_nsh_trade\ntable_type => TABLE\nremarks => null\ntype_cat => null\ntype_schem => null\ntype_name => null\nself_referencing_col_name => null\nref_generation => null\n\n```\n\n如果再指定schemaPattern为hive库名\n\n```\nResultSet tableRet = meta.getTables(null, \"default\", \"ads_nsh_trade\", new String[]{\"TABLE\"});\nwhile (tableRet.next()) {\n    System.out.println(\"TABLE_CAT:\" + tableRet.getString(\"TABLE_CAT\"));\n    System.out.println(\"TABLE_SCHEM:\" + tableRet.getString(\"TABLE_SCHEM\"));\n    System.out.println(\"TABLE_NAME => \" + tableRet.getString(\"TABLE_NAME\"));\n    System.out.println(\"table_type => \" + tableRet.getString(\"table_type\"));\n    System.out.println(\"remarks => \" + tableRet.getString(\"remarks\"));\n    System.out.println(\"type_cat => \" + tableRet.getString(\"type_cat\"));\n    System.out.println(\"type_schem => \" + tableRet.getString(\"type_schem\"));\n    System.out.println(\"type_name => \" + tableRet.getString(\"type_name\"));\n    System.out.println(\"self_referencing_col_name => \" + tableRet.getString(\"self_referencing_col_name\"));\n    System.out.println(\"ref_generation => \" + tableRet.getString(\"ref_generation\"));\n}\n\n```\n\n输出结果\n\n```\nTABLE_CAT:\nTABLE_SCHEM:default\nTABLE_NAME => ads_nsh_trade\ntable_type => TABLE\nremarks => null\ntype_cat => null\ntype_schem => null\ntype_name => null\nself_referencing_col_name => null\nref_generation => null\n\n```\n\n在hive的元数据表中，表的信息主要在TBLS和TABLE_PARAMS这两张表中\n\n参考\n\n```\nhttps://blog.csdn.net/haozhugogo/article/details/73274832\n\n```\n\n比如TBLS表\n\n<img src=\"/images/517519-20190903142819455-638790.png\" alt=\"\" />\n\n和TABLE_PARAMS表\n\n<img src=\"/images/517519-20190903142930797-1181591239.png\" alt=\"\" />\n\n获得表的字段信息的接口\n\n```\nResultSet rs1 = meta.getColumns(\"%\", \"default\", \"my_table_xxx\", \"%\"); while (rs1.next()) { String tableCat = rs1.getString(\"table_cat\"); String tableSchem = rs1.getString(\"table_schem\"); String tableName = rs1.getString(\"table_name\"); String columnName = rs1.getString(\"COLUMN_NAME\"); String columnType = rs1.getString(\"TYPE_NAME\"); String remarks = rs1.getString(\"REMARKS\"); int datasize = rs1.getInt(\"COLUMN_SIZE\"); int digits = rs1.getInt(\"DECIMAL_DIGITS\"); int nullable = rs1.getInt(\"NULLABLE\"); System.out.println(tableCat + \" \" + tableSchem + \" \" + tableName + \" \" + columnName + \" \" + columnType + \" \" + datasize + \" \" + digits + \" \" + nullable + \" \" + remarks); }\n```\n\n其中的参数可以是\n\n```\ntable_cat, table_schem, table_name, column_name, data_type, type_name, column_size, buffer_length, decimal_digits, num_prec_radix, nullable, \n\nremarks, column_def, sql_data_type, sql_datetime_sub, char_octet_length, ordinal_position, is_nullable, scope_catalog, scope_schema, scope_table, source_data_type, is_auto_increment\n\n```\n\n输出的结果\n\n```\nnull default ads_nsh_trade test_string STRING 2147483647 0 1 string??????\nnull default ads_nsh_trade test_boolean BOOLEAN 0 0 1 boolean??????\nnull default ads_nsh_trade test_short SMALLINT 5 0 1 short??????\nnull default ads_nsh_trade test_double DOUBLE 15 15 1 double??????\nnull default ads_nsh_trade test_byte TINYINT 3 0 1 byte??????\nnull default ads_nsh_trade test_list array<string> 0 0 1 list<String>????\nnull default ads_nsh_trade test_map map<string,int> 0 0 1 map<String,Int>????\nnull default ads_nsh_trade test_int INT 10 0 1 int??????\nnull default ads_nsh_trade test_set array<bigint> 0 0 1 set<Long>??????\nnull default ads_nsh_trade col_name DECIMAL 10 2 1 null\nnull default ads_nsh_trade col_name2 DECIMAL 10 2 1 null\nnull default ads_nsh_trade test_long BIGINT 19 0 1 null\nnull tmp ads_nsh_trade test_boolean BOOLEAN 0 0 1 boolean??????\nnull tmp ads_nsh_trade test_short SMALLINT 5 0 1 short??????\nnull tmp ads_nsh_trade test_double DOUBLE 15 15 1 double??????\nnull tmp ads_nsh_trade test_byte TINYINT 3 0 1 byte??????\nnull tmp ads_nsh_trade test_list array<string> 0 0 1 list<String>????\nnull tmp ads_nsh_trade test_map map<string,int> 0 0 1 map<String,Int>????\nnull tmp ads_nsh_trade test_int INT 10 0 1 int??????\nnull tmp ads_nsh_trade test_set array<bigint> 0 0 1 set<Long>??????\nnull tmp ads_nsh_trade test_long BIGINT 19 0 1 null\nnull tmp ads_nsh_trade test_string STRING 2147483647 0 1 null\n\n```\n\n如果读取hive元数据的时候遇到下面报错，hive的schema在外部系统，且使用hive2的版本\n\n```\norg.apache.hive.service.cli.HiveSQLException: MetaException(message:java.lang.UnsupportedOperationException: Storage schema reading not supported)\n\tat org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:267)\n\tat org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:258)\n\tat org.apache.hive.jdbc.HiveDatabaseMetaData.getColumns(HiveDatabaseMetaData.java:226)\n\tat com.xxx.data.udf.ShowColumnsUDF.evaluate(ShowColumnsUDF.java:25)\n\tat com.xxx.data.udf.ShowColumnsUDFTest.evaluate(ShowColumnsUDFTest.java:13)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:363)\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:137)\n\tat com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)\n\tat com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)\n\tat com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:221)\n\tat com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)\nCaused by: org.apache.hive.service.cli.HiveSQLException: MetaException(message:java.lang.UnsupportedOperationException: Storage schema reading not supported)\n\tat org.apache.hive.service.cli.operation.GetColumnsOperation.runInternal(GetColumnsOperation.java:213)\n\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:247)\n\n```\n\n解决的方法是在 hive-site.xml 中添加\n\n```\nmetastore.storage.schema.reader.impl=org.apache.hadoop.hive.metastore.SerDeStorageSchemaReader\n\n```\n\n　　\n\n2.使用**hive的thrift接口**来操作hive元数据\n\n```\nConfiguration config = new Configuration();\nconfig.set(\"hive.metastore.uris\", \"thrift://xxxx:9083\");\nconfig.set(\"javax.security.auth.useSubjectCredsOnly\", \"false\");\nHiveConf hConf = new HiveConf(config, HiveConf.class);\nHiveMetaStoreClient hiveMetaStoreClient = new HiveMetaStoreClient(hConf);\n\nList<FieldSchema> list = hiveMetaStoreClient.getSchema(\"default\", \"avro_test_dwd2\");\nfor (FieldSchema schema: list) {\n    System.out.println(schema);\n}\n\n```\n\n输出\n\n```\nFieldSchema(name:string1, type:string, comment:)\nFieldSchema(name:int1, type:int, comment:)\nFieldSchema(name:tinyint1, type:int, comment:)\nFieldSchema(name:smallint1, type:int, comment:)\nFieldSchema(name:bigint1, type:bigint, comment:)\nFieldSchema(name:boolean1, type:boolean, comment:)\nFieldSchema(name:float1, type:float, comment:)\nFieldSchema(name:double1, type:double, comment:)\nFieldSchema(name:list1, type:array<string>, comment:)\nFieldSchema(name:map1, type:map<string,int>, comment:)\nFieldSchema(name:struct1, type:struct<sint:int,sboolean:boolean,sstring:string>, comment:)\nFieldSchema(name:enum1, type:string, comment:)\nFieldSchema(name:nullableint, type:int, comment:)\nFieldSchema(name:ds, type:string, comment:null)\n\n```\n\n　　\n\n当查看hive表分区的参数的时候，有时numRows和rawDataSize会显示-1，这是hive metastore中没有相应的数据，\n\n<img src=\"/images/517519-20201019170922651-610399289.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n需要执行如下命令重新计算\n\n```\nANALYZE TABLE xxx.xxx PARTITION(ds='2019-02-28') COMPUTE STATISTICS;\n\n```\n\n&nbsp;<img src=\"/images/517519-20201019171037285-491673913.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;如果要刷新整个分区表的话\n\n```\nANALYZE TABLE xxxx.xxxx PARTITION(ds) COMPUTE STATISTICS;\n\n```\n\n&nbsp;就能刷新整个表的metadata\n\n<img src=\"/images/517519-20201023164821040-815416799.png\" alt=\"\" loading=\"lazy\" />&nbsp;&nbsp;&nbsp;<img src=\"/images/517519-20201023165012790-843617423.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;使用NOSCAN将会直接查询元数据，不会重新计算\n\n```\nANALYZE TABLE xxx.xxx PARTITION(ds) COMPUTE STATISTICS NOSCAN;\n\n```\n\n&nbsp;输出\n\n```\nINFO  : Starting task [Stage-1:STATS] in serial mode\nINFO  : Partition xxx.xxx{ds=2018-08-31} stats: [numFiles=1, numRows=86, totalSize=15358, rawDataSize=1032]\nINFO  : Partition xxx.xxx{ds=2018-09-01} stats: [numFiles=1, numRows=728, totalSize=114974, rawDataSize=8736]\nINFO  : Partition xxx.xxx{ds=2018-09-02} stats: [numFiles=1, numRows=787, totalSize=124251, rawDataSize=9444]\nINFO  : Partition xxx.xxx{ds=2018-09-03} stats: [numFiles=1, numRows=670, totalSize=106113, rawDataSize=8040]\nINFO  : Partition xxx.xxx{ds=2018-09-04} stats: [numFiles=1, numRows=594, totalSize=93643, rawDataSize=7128]\nINFO  : Partition xxx.xxx{ds=2018-09-05} stats: [numFiles=1, numRows=627, totalSize=98089, rawDataSize=7524]\nINFO  : Partition xxx.xxx{ds=2018-09-06} stats: [numFiles=1, numRows=558, totalSize=88352, rawDataSize=6696]\n\n```\n\n&nbsp;\n\n对于hiveMetaStoreClient的dropPartition方法，有4个参数，databaseName，tableName，partitionName，deleteData\n\n对于hive的**内部表**，如果deleteData为false，只会删除hive表的分区，不会删除hdfs上的数据\n\ndeleteData为true，两个都会删除\n\n如果找不到该分区，会抛出\n\n```\nNoSuchObjectException(message:partition values=[2020-07-14]\n\n```\n\n对于hive的**外部表**，无论deleteData为false还是true，都不会删除hdfs上的数据\n","tags":["Hive"]},{"title":"Ubuntu16.04安装Filebeat","url":"/Ubuntu16.04安装Filebeat.html","content":"Filebeat官方文档地址\n\n```\nhttps://www.elastic.co/guide/en/beats/filebeat/current/filebeat-installation.html\n\n```\n\n下载和安装\n\n```\ncurl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.3.1-linux-x86_64.tar.gz\ntar xzvf filebeat-7.3.1-linux-x86_64.tar.gz\n\n```\n\n编写filebeat.yml\n\n<!--more-->\n&nbsp;\n\n启动\n\n```\nchmod go-w /home/lintong/software/apache/filebeat-7.3.1-linux-x86_64/filebeat.yml\n./filebeat -e -c filebeat.yml\n\n```\n\ncodec.format\n\n```\ncodec.format:\n    string: '%{[@timestamp]} %{[message]}'\n\n```\n\n输出\n\n```\n2019-09-13T17:06:51.797Z 123123123123\n\n```\n\ncodec.json\n\n```\ncodec.json:\n    pretty: true\n    escape_html: false\n\n```\n\n输出\n\n```\n{\n  \"@timestamp\": \"2019-09-13T09:08:49.590Z\",\n  \"@metadata\": {\n    \"beat\": \"filebeat\",\n    \"type\": \"_doc\",\n    \"version\": \"7.3.1\",\n    \"topic\": \"thrift_json_source\"\n  },\n  \"host\": {\n    \"name\": \"master\"\n  },\n  \"agent\": {\n    \"version\": \"7.3.1\",\n    \"type\": \"filebeat\",\n    \"ephemeral_id\": \"60b93a10-dcce-499b-ae81-0755bfc8bf5c\",\n    \"hostname\": \"master\",\n    \"id\": \"6ebb0912-ffce-4ddd-9cc8-7bf624e62c78\"\n  },\n  \"ecs\": {\n    \"version\": \"1.0.1\"\n  },\n  \"message\": \"123123123123\",\n  \"log\": {\n    \"file\": {\n      \"path\": \"/home/lintong/下载/test.log\"\n    },\n    \"offset\": 0\n  },\n  \"input\": {\n    \"type\": \"log\"\n  }\n}\n\n```\n\n如果pretty是false将输出\n\n```\n{\"@timestamp\":\"2019-09-13T09:10:50.164Z\",\"@metadata\":{\"beat\":\"filebeat\",\"type\":\"_doc\",\"version\":\"7.3.1\",\"topic\":\"thrift_json_source\"},\"log\":{\"file\":{\"path\":\"/home/lintong/下载/test.log\"},\"offset\":0},\"message\":\"123123123123\",\"input\":{\"type\":\"log\"},\"ecs\":{\"version\":\"1.0.1\"},\"host\":{\"name\":\"master\"},\"agent\":{\"type\":\"filebeat\",\"ephemeral_id\":\"b26075f9-37f8-4d95-9341-fccc4504c1b5\",\"hostname\":\"master\",\"id\":\"6ebb0912-ffce-4ddd-9cc8-7bf624e62c78\",\"version\":\"7.3.1\"}}\n\n```\n\n如果需要添加字段\n\n```\n  paths:\n    - /home/lintong/下载/test.log\n  fields:\n    add_field: lintong\n\n```\n\n输出\n\n```\n{\n\"@timestamp\": \"2019-09-16T08:16:06.169Z\",\n\"@metadata\": {\n\"beat\": \"filebeat\",\n\"type\": \"_doc\",\n\"version\": \"7.3.1\",\n\"topic\": \"thrift_json_source\"\n},\n\"host\": {\n\"name\": \"master\"\n},\n\"log\": {\n\"offset\": 31,\n\"file\": {\n\"path\": \"/home/lintong/下载/test.log\"\n}\n},\n\"message\": \"33333333\",\n\"input\": {\n\"type\": \"log\"\n},\n\"fields\": {\n\"add_field\": \"lintong\"\n},\n\"agent\": {\n\"type\": \"filebeat\",\n\"ephemeral_id\": \"c16102da-421d-4ff3-90ad-1737451a909d\",\n\"hostname\": \"master\",\n\"id\": \"6ebb0912-ffce-4ddd-9cc8-7bf624e62c78\",\n\"version\": \"7.3.1\"\n},\n\"ecs\": {\n\"version\": \"1.0.1\"\n}\n}\n\n```\n\n在codec.format中添加字段\n\n```\n  codec.format:\n     string: '%{[@timestamp]} %{[fields.add_field]} %{[message]}'\n\n```\n\n输出\n\n```\n2019-09-16T16:18:34.048Z lintong 55555555\n\n```\n\n如果想添加的字段在json的顶层，就是不在fields字段下层\n\n```\n  paths:\n    - /home/lintong/下载/test.log\n  fields:\n    add_field: lintong\n  fields_under_root: true\n\n```\n\n输出\n\n```\n{\n  \"@timestamp\": \"2019-09-16T08:22:43.997Z\",\n  \"@metadata\": {\n    \"beat\": \"filebeat\",\n    \"type\": \"_doc\",\n    \"version\": \"7.3.1\",\n    \"topic\": \"thrift_json_source\"\n  },\n  \"agent\": {\n    \"ephemeral_id\": \"d8e45d90-6434-4e0d-a6fc-74611b87cbd4\",\n    \"hostname\": \"master\",\n    \"id\": \"6ebb0912-ffce-4ddd-9cc8-7bf624e62c78\",\n    \"version\": \"7.3.1\",\n    \"type\": \"filebeat\"\n  },\n  \"log\": {\n    \"offset\": 58,\n    \"file\": {\n      \"path\": \"/home/lintong/下载/test.log\"\n    }\n  },\n  \"message\": \"66666666\",\n  \"add_field\": \"lintong\",\n  \"input\": {\n    \"type\": \"log\"\n  },\n  \"ecs\": {\n    \"version\": \"1.0.1\"\n  },\n  \"host\": {\n    \"name\": \"master\"\n  }\n}\n\n```\n\n如果要去掉不要的字段\n\n参考：\n\n```\nhttps://studygolang.com/articles/10935\n\n```\n\n和\n\n```\nhttps://www.elastic.co/guide/en/beats/filebeat/current/drop-fields.html\n\n```\n\n比如\n\n```\nprocessors:\n- drop_fields:\n     fields: [\"host\", \"log\", \"input\",\"ecs\",\"agent\"]\n\n```\n\n输出\n\n```\n{\n  \"@timestamp\": \"2019-09-16T08:55:55.934Z\",\n  \"@metadata\": {\n    \"beat\": \"filebeat\",\n    \"type\": \"_doc\",\n    \"version\": \"7.3.1\",\n    \"topic\": \"thrift_json_source\"\n  },\n  \"message\": \"33333333\",\n  \"add_field\": \"lintong\"\n}\n\n```\n\n其中@metadata和@timestamp不能在filebeat中去掉\n\nfilebeat会将自己处理日志文件的进度信息写入到registry文件中，以保证filebeat在重启之后能够接着处理未处理过的数据，而无需从头开始。\n\n如果没有单独配置那么文件路径为 /var/lib/filebeat/registry\n","tags":["filebeat"]},{"title":"Ubuntu16.04安装Consul","url":"/Ubuntu16.04安装Consul.html","content":"1.下载安装包\n\n```\nhttps://www.consul.io/downloads.html\nwget https://releases.hashicorp.com/consul/1.5.3/consul_1.5.3_linux_amd64.zip\n\n```\n\n2.解压\n\n```\nunzip consul_1.5.3_linux_amd64.zip\n\n```\n\n3.mv\n\n```\nsudo mv consul /usr/local/bin/consul\n\n```\n\n4.启动\n\n参考：https://blog.csdn.net/u010046908/article/details/61916389\n\n**-dev 开发模式**启动的时候，数据是存储在内存中，重启之后数据将丢失\n\n```\nconsul agent -dev\n\n```\n\n-server 生成模式启动的时候，如果是server的话需要指定-server，如果是client的话，需要指定-client，比如\n\n```\nconsul agent -ui -server -bootstrap-expect 1 -data-dir /tmp/consul -node=consul-server -bind=192.168.1.100 -client=192.168.1.100\n\n```\n\n-bootstrap-expect 1 通知consul server我们现在准备加入的server节点个数，该参数是为了延迟日志复制的启动直到我们指定数量的server节点成功的加入后启动\n\n-data-dir /tmp/consul 数据持久的路径\n\n-node=consul-server 指定节点在集群中的名称\n\n-bind=192.168.1.100 该地址用来在集群内部的通讯，集群内的所有节点到地址都必须是可达的，**默认是0.0.0.0**，这意味着Consulo会使用第一个可用的私有IP地址，Consul可以使用TCP和UDP并且可以使用共同的端口，如果存在防火墙，这两者协议必须是允许的\n\n-client 指定节点为client，指定客户端接口的绑定地址，包括：HTTP、DNS、RPC，**默认是127.0.0.1**，**只允许回环接口访问**，也就是本机访问，如果要想同一局域网内的其他机器访问，需要修改成自己的内网ip\n\nserver节点，指定client等于内网ip，统一局域网的机器可以访问，指定client=0.0.0.0，外网机器可以访问\n\n```\nnohup ./consul agent -ui -server -bootstrap-expect 1 -data-dir /home/worker/projects/consul-1.5.3/consul-data -node=xxx -client=xxx >> ./logs/consul.log 2>&amp;1 &amp;\n\n```\n\nclient节点，不指定client的话，只能本机访问client节点，指定client=0.0.0.0，外网机器可以访问\n\n```\nnohup ./consul agent -ui -data-dir /home/worker/projects/consul-1.5.3/consul-data -node=xxx -bind=xxxx -client=xxx >> ./logs/consul.log 2>&amp;1 &amp;\n\n```\n\njoin\n\n```\n./consul join server的ip\n\n```\n\n当3台分布式部署的时候，需要如下部署\n\n参考：https://blog.csdn.net/chenchong08/article/details/77885989\n\n```\nnohup ./consul agent -ui -server -bootstrap-expect 3 -data-dir /home/worker/projects/consul-1.5.3/consul-data -node=host1 -client=0.0.0.0 -bind=ip1 >> ./logs/consul.log 2>&amp;1 &amp;\nnohup ./consul agent -ui -server -bootstrap-expect 3 -data-dir /home/worker/projects/consul-1.5.3/consul-data -node=host2 -client=0.0.0.0 -bind=ip2 >> ./logs/consul.log 2>&amp;1 &amp;\n./consul join host1\nnohup ./consul agent -ui -server -bootstrap-expect 3 -data-dir /home/worker/projects/consul-1.5.3/consul-data -node=host3 -client=0.0.0.0 -bind=ip3 >> ./logs/consul.log 2>&amp;1 &amp;\n./consul join host2\n\n```\n\n<!--more-->\n&nbsp;查看集群状态\n\n```\n./consul operator raft list-peers \nNode           ID                                    Address           State     Voter  RaftProtocol\nhost1  f6ec724d-95ec-00da-97a2-xxxxxxxxxxxx  10.90.1.xx1:8300  follower  true   3\nhost2  a79165a3-7ffe-0bcf-9222-xxxxxxxxxxxx  10.90.1.xx2:8300  leader    true   3\nhost3  54fed87b-b76a-432b-9ede-xxxxxxxxxxxx  10.90.1.xx3:8300  follower  true   3\n\n```\n\n这时候host2是leader，当host2挂掉的时候，会有短暂不可用，之后leader选出，host1成为新的leader\n\n```\n./consul operator raft list-peers \nError getting peers: Failed to retrieve raft configuration: Unexpected response code: 500 (rpc error getting client: failed to get conn: dial tcp 10.90.1.xx3:0->10.90.1.xx2:8300: connect: connection refused)\n\n./consul operator raft list-peers \nNode           ID                                    Address           State     Voter  RaftProtocol\nhost1  f6ec724d-95ec-00da-97a2-xxxxxxxxxxxx  10.90.1.xx1:8300  leader    true   3\nhost2  a79165a3-7ffe-0bcf-9222-xxxxxxxxxxxx  10.90.1.xx2:8300  follower  true   3\nhost3  54fed87b-b76a-432b-9ede-xxxxxxxxxxxx  10.90.1.xx3:8300  follower  true   3\n\n```\n\n&nbsp;<img src=\"/images/517519-20191230162202068-1563463677.png\" alt=\"\" />\n\n新的leader的日志不断刷说有个follow挂了\n\n```\n2019/12/30 16:42:17 [ERROR] raft: Failed to heartbeat to 10.90.1.xx1:8300: dial tcp 10.90.1.xx3:0->10.90.1.xx1:8300: connect: connection refused\n    2019/12/30 16:42:18 [WARN] consul: error getting server health from \"host1\": rpc error getting client: failed to get conn: dial tcp 10.90.1.xx3:0->10.90.1.xx1:8300: connect: connection refused\n\n```\n\n启动host2的进程，自动加入集群，不用再join，集群状态恢复\n\n&nbsp;\n\n5.使用\n\n访问8500端口\n\n<img src=\"/images/517519-20190810232812288-876354144.png\" alt=\"\" />\n\n5.kv存储\n\nconsul支持通过**HTTP API**和通过**CLI API**将配置进行存储\n\n**put命令**\n\n```\nconsul kv put app1/config/parameter1 1\nSuccess! Data written to: app1/config/parameter1\n\n```\n\nput成功之后，就可以在页面中看到该配置\n\n<img src=\"/images/517519-20190813154703512-1978557909.png\" alt=\"\" />\n\n具体数值\n\n<img src=\"/images/517519-20190813155001178-1220977422.png\" alt=\"\" />\n\n&nbsp;\n\nput之后可以使用**get命令**来获取这个配置\n\n```\nconsul kv get app1/config/parameter1\n1\n\n```\n\n递归获取\n\n```\nconsul kv get -recurse app1\napp1/config/parameter1:1\napp1/config/parameter2:2\napp1/config/parameter3:3\n\n```\n\n删除\n\n```\nconsul kv delete app1/config/parameter2 2\n\n```\n\n&nbsp;\n\n**HTTP api**\n\n**get kv**\n\n```\ncurl http://xxxx:8500/v1/kv/app1?recurse\n[{\"LockIndex\":0,\"Key\":\"app1/config/parameter1\",\"Flags\":0,\"Value\":\"MQ==\",\"CreateIndex\":5986,\"ModifyIndex\":5986},{\"LockIndex\":0,\"Key\":\"app1/config/parameter2\",\"Flags\":0,\"Value\":\"Mg==\",\"CreateIndex\":5987,\"ModifyIndex\":5987}]\n\n```\n\n**get**单个\n\n```\ncurl http://xxx:8500/v1/kv/app1/config/parameter1\n[{\"LockIndex\":0,\"Key\":\"app1/config/parameter1\",\"Flags\":0,\"Value\":\"MQ==\",\"CreateIndex\":5986,\"ModifyIndex\":5986}]\n\n```\n\n其中kv存储的v的值是经过base64编码过的，需要进行解码\n\n```\necho 'MQ==' | base64 -d\n1\n\n```\n\n**put kv**\n\n```\ncurl -X PUT -d '1111111' http://xxxx:8500/v1/kv/app1/config/parameter1\n\n```\n\n&nbsp;<img src=\"/images/517519-20190814103913567-1266437052.png\" alt=\"\" />\n\n**delete kv**\n\n```\ncurl -X DELETE -d http://xxx:8500/v1/kv/app1/config/parameter2\n\n```\n\n&nbsp;\n\n**consul配置导入和导出**\n\n**export命令**\n\n```\n./consul kv export app1/config\n[\n\t{\n\t\t\"key\": \"app1/config/parameter1\",\n\t\t\"flags\": 0,\n\t\t\"value\": \"MTExMTExMWZmcmZmZg==\"\n\t},\n\t{\n\t\t\"key\": \"app1/config/parameter2\",\n\t\t\"flags\": 0,\n\t\t\"value\": \"Mg==\"\n\t},\n\t{\n\t\t\"key\": \"app1/config/parameter3\",\n\t\t\"flags\": 0,\n\t\t\"value\": \"Mw==\"\n\t}\n]\n\n```\n\n**import命令**\n\n```\n./consul kv import @test.json\nImported: app1/config/parameter4\nImported: app1/config/parameter5\nImported: app1/config/parameter6\n\n```\n\n&nbsp;<img src=\"/images/517519-20190814140318228-817981938.png\" alt=\"\" />\n\n&nbsp;\n\n**watch**\n\n```\nhttps://www.consul.io/docs/agent/watches.html\n\n```\n\n当配置更新能自动回调\n\n需要写一个配置文件，比如 consul-watch.json\n\n```\n{\n  \"watches\": [\n    {\n      \"type\": \"key\",\n      \"key\": \"app1/conf/parameter1\",\n      \"handler_type\": \"http\",\n      \"http_handler_config\": {\n         \"path\":\"http://localhost:8000\",\n         \"method\": \"GET\",\n         \"header\": {\"x-foo\":[\"bar\", \"baz\"]},\n         \"timeout\": \"10s\",\n         \"tls_skip_verify\": false\n      }\n    }\n  ]\n}\n\n```\n\n上面的配置是监听单个key的变化，如果想监听多个\n\n```\n\"type\": \"keyprefix\",\n\"prefix\": \"app1/\",\n\n```\n\n然后在启动命令中加上配置文件夹的地址，比如\n\n```\n-config-dir /home/lintong/software/consul-1.5.3/consul-conf\n\n```\n\n然后当配置发生变动的时候，会自动调用这个回调接口\n\n如果修改了配置，比如把GET接口换成POST接口，需要reload这个配置，执行下面命令\n\n```\nconsul reload\n\n```\n\n如果是post的回调接口的话，consul回调的时候会往post请求的request中放入更新的这一条配置，headers和body如下\n\n```\nHost: localhost:8000\nUser-Agent: Go-http-client/1.1\nContent-Length: 124\nContent-Type: application/json\nX-Consul-Index: 5131\nX-Foo: bar\nX-Foo: baz\nAccept-Encoding: gzip\nConnection: close\n\n{\"Key\":\"app1/config/parameter1\",\"CreateIndex\":7,\"ModifyIndex\":5131,\"LockIndex\":0,\"Flags\":0,\"Value\":\"IjEyMyI=\",\"Session\":\"\"}\n\n```\n\n&nbsp;\n","tags":["Linux"]},{"title":"kerberos相关","url":"/kerberos相关.html","content":"## **1.Kerberos介绍**\n\n**Kerberos**是一种计算机网络授权协议，用来在非安全网络中，对个人通信以安全的手段进行身份认证。这个词又指麻省理工学院为这个协议开发的一套计算机软件。软件设计上采用客户端/服务器结构，并且能够进行相互认证，即客户端和服务器端均可对对方进行身份认证。可以用于防止窃听、防止重放攻击、保护数据完整性等场合，是一种应用对称密钥体制进行密钥管理的系统。\n\n<!--more-->\n&nbsp;\n\n**Kerberos**中的一些概念：\n\n1）KDC：密钥分发中心，负责管理发放票据，记录授权。\n\n2）Realm：Kerberos管理领域的标识。\n\n3）principal：当每添加一个用户或服务的时候都需要向kdc添加一条principal，principl的形式为：主名称／实例名@领域名。\n\n4）主名称：主名称可以是用户名或服务名，表示是用于提供各种网络服务（如hdfs，yarn，hive）的主体。\n\n5）实例名：实例名简单理解为主机名。\n\n## 2.安装及配置KDC服务\n\n卸载老的krb\n\n```\nsudo apt remove --purge krb*\nsudo rm -rf /etc/krb5kdc\nsudo rm -rf /var/lib/krb5kdc\n\n```\n\n安装KDC服务和管理员服务\n\n```\nsudo apt-get install krb5-kdc krb5-admin-server\n\n```\n\n配置Realm域名\n\n<img src=\"/images/517519-20240120160529944-1596173744.png\" width=\"700\" height=\"193\" loading=\"lazy\" />\n\n&nbsp;\n\n输入kerberos服务器的hostname\n\n<img src=\"/images/517519-20200318234705184-88200583.png\" alt=\"\" width=\"600\" height=\"177\" />\n\n&nbsp;\n\n<img src=\"/images/517519-20200318234821509-984092827.png\" alt=\"\" width=\"600\" height=\"203\" />\n\n&nbsp;\n\n创建新的realm，耐心等，Loading random data会很慢\n\n```\nlintong@master:~$ sudo krb5_newrealm\nThis script should be run on the master KDC/admin server to initialize\na Kerberos realm.  It will ask you to type in a master key password.\nThis password will be used to generate a key that is stored in\n/etc/krb5kdc/stash.  You should try to remember this password, but it\nis much more important that it be a strong password than that it be\nremembered.  However, if you lose the password and /etc/krb5kdc/stash,\nyou cannot decrypt your Kerberos database.\nLoading random data\nInitializing database '/var/lib/krb5kdc/principal' for realm 'EXAMPLE.COM',\nmaster key name 'K/M@HADOOP.COM'\nYou will be prompted for the database Master Password.\nIt is important that you NOT FORGET this password.\nEnter KDC database master key:\nRe-enter KDC database master key to verify:\n此处需要输入Kerberos数据库的密码: 自己定\n\nNow that your realm is set up you may wish to create an administrative\nprincipal using the addprinc subcommand of the kadmin.local program.\nThen, this principal can be added to /etc/krb5kdc/kadm5.acl so that\nyou can use the kadmin program on other computers.  Kerberos admin\nprincipals usually belong to a single user and end in /admin.  For\n, if jruser is a Kerberos administrator, then in addition to\nthe normal jruser principal, a jruser/admin principal should be\ncreated.\n\nDon't forget to set up DNS information so your clients can find your\nKDC and admin servers.  Doing so is documented in the administration\nguide.\n\n```\n\n参考：\n\n```\nhttps://docs.cloudera.com/documentation/enterprise/5-15-x/topics/cm_sg_s2_jce_policy.html\nhttp://techpubs.spinlocksolutions.com/dklar/kerberos.html\n\n```\n\n此时，krb的相关配置就配好了，可以查看/etc/krb5.conf和/etc/krb5kdc下面的kadm5.acl&nbsp; kdc.conf&nbsp; stash文件\n\n再创建一个管理员账户，需要设置密码，这里addprinc root或者addprinc admin\n\n```\nlintong@master:~$ sudo kadmin.local\nAuthenticating as principal root/admin@MASTER with password.\nkadmin.local:  addprinc admin\nWARNING: no policy specified for admin@HADOOP.COM; defaulting to no policy\nEnter password for principal \"admin@HADOOP.COM\":\nRe-enter password for principal \"admin@HADOOP.COM\":\nPrincipal \"admin@HADOOP.COM\" created.\n\n```\n\n再添加cloudera-scm/admin\n\n```\nkadmin.local:  addprinc cloudera-scm/admin@HADOOP.COM\nWARNING: no policy specified for cloudera-scm/admin@HADOOP.COM; defaulting to no policy\nEnter password for principal \"cloudera-scm/admin@HADOOP.COM\":\nRe-enter password for principal \"cloudera-scm/admin@HADOOP.COM\":\nPrincipal \"cloudera-scm/admin@HADOOP.COM\" created\n\n```\n\n给admin赋权，添加内容 `*/admin@HADOOP.COM *`\n\n```\nroot@master:/etc/krb5kdc# vim kadm5.acl\n\n```\n\n&nbsp;参考：[CDH配置Kerberos和Sentry （超详细）](https://blog.csdn.net/summer089089/article/details/107369994)\n\n然后重启kbr相关服务\n\n```\nsystemctl restart krb*\n\n```\n\n查看kerberos主体\n\n```\nroot@master:/etc/krb5kdc# sudo kadmin.local\nAuthenticating as principal admin/admin@EXAMPLE.COM with password.\nkadmin.local:  listprincs\nK/M@HADOOP.COM\nadmin@HADOOP.COM\ncloudera-scm/admin@HADOOP.COM\nkadmin/admin@HADOOP.COM\nkadmin/changepw@HADOOP.COM\nkadmin/master@HADOOP.COM\nkiprop/master@HADOOP.COM\nkrbtgt/HADOOP.COM@HADOOP.COM\n\n```\n\n使用kinit认证，并查看票据过期时间\n\n```\nkinit admin\nPassword for admin@EXAMPLE.COM:\n\nklist\nTicket cache: FILE:/tmp/krb5cc_1000\nDefault principal: admin@EXAMPLE.COM\n\nValid starting       Expires              Service principal\n2020-09-16T23:49:57  2020-09-17T09:49:57  krbtgt/EXAMPLE.COM@EXAMPLE.COM\n\trenew until 2020-09-17T23:49:54\n\n```\n\n如果要删除kerberos主体\n\n```\ndelete_principal root/admin\n\n```\n\n安装之后查看是否正常运行\n\n```\nsystemctl status krb*\n\n```\n\n都正常\n\n```\n● krb5-admin-server.service - Kerberos 5 Admin Server\n   Loaded: loaded (/lib/systemd/system/krb5-admin-server.service; enabled; vendor preset: enabled)\n   Active: active (running) since 日 2021-10-17 13:51:12 CST; 1 day 10h ago\n Main PID: 822 (kadmind)\n    Tasks: 1\n   Memory: 1.4M\n      CPU: 12ms\n   CGroup: /system.slice/krb5-admin-server.service\n           └─822 /usr/sbin/kadmind -nofork\n\n10月 17 13:51:12 master kadmind[822]: setsockopt(11,IPV6_V6ONLY,1) worked\n10月 17 13:51:12 master kadmind[822]: listening on fd 12: tcp 0.0.0.0.464\n10月 17 13:51:12 master kadmind[822]: listening on fd 11: tcp ::.464\n10月 17 13:51:12 master kadmind[822]: listening on fd 13: rpc 0.0.0.0.749\n10月 17 13:51:12 master kadmind[822]: setsockopt(14,IPV6_V6ONLY,1) worked\n10月 17 13:51:12 master kadmind[822]: listening on fd 14: rpc ::.749\n10月 17 13:51:12 master kadmind[822]: set up 6 sockets\n10月 17 13:51:12 master kadmind[822]: Seeding random number generator\n10月 17 13:51:22 master kadmind[822]: starting\n10月 17 13:51:22 master kadmind[822]: kadmind: starting...\n\n● krb5-kdc.service - Kerberos 5 Key Distribution Center\n   Loaded: loaded (/lib/systemd/system/krb5-kdc.service; enabled; vendor preset: enabled)\n   Active: active (running) since 日 2021-10-17 13:51:12 CST; 1 day 10h ago\n  Process: 806 ExecStart=/usr/sbin/krb5kdc -P /var/run/krb5-kdc.pid $DAEMON_ARGS (code=exited, status=0/SUCCESS)\n Main PID: 858 (krb5kdc)\n    Tasks: 1\n\n```\n\n&nbsp;\n\ncdh 5.15中HUE集成kerberos，参考\n\n```\nhttps://docs.cloudera.com/documentation/enterprise/5-15-x/topics/cdh_sg_hue_kerberos_config.html\n\n```\n\ncdh 5.15启动kerberos认证向导，参考\n\n```\nhttps://docs.cloudera.com/documentation/enterprise/5-15-x/topics/cm_sg_intro_kerb.html\n\n```\n\n&nbsp;\n\n## 3.安装jce，java1.8_161以上可以不加\n\n```\nhttps://docs.cloudera.com/documentation/enterprise/latest/topics/cm_sg_s2_jce_policy.html\n\n```\n\n需要去oracle的网站下载jce_policy-8.zip文件\n\n```\nhttps://www.oracle.com/de/java/technologies/javase-jce8-downloads.html\n\n```\n\n<img src=\"/images/517519-20240120155616091-93565121.png\" width=\"800\" height=\"233\" loading=\"lazy\" />\n\n按照README.txt中的指示将US_export_policy.jar和local_policy.jar文件复制到$JAVA_HOME的jre/lib/security目录下\n\n```\nlintong@master:/usr/java/jdk1.8/jre/lib/security$ ls\nblacklist          cacerts      java.security  local_policy.jar   US_export_policy.jar\nblacklisted.certs  java.policy  javaws.policy  trusted.libraries\n\n```\n\n如果不加的话，hdfs启动会报错\n\n```\nSocket Reader #1 for port 8022: readAndProcess from client 192.168.8.103 threw exception [javax.security.sasl.SaslException: \nGSS initiate failed [Caused by GSSException: Failure unspecified at GSS-API level (Mechanism level: Encryption type AES256 CTS mode with HMAC SHA1-96 is not supported/enabled)]]\n\n```\n\n&nbsp;\n\n## 4.CDH集群启动kerberos\n\n<img src=\"/images/517519-20200317235238382-1834054994.png\" width=\"600\" height=\"479\" />\n\n&nbsp;\n\n需要执行的步骤，参考：[如何在CDH集群启用Kerberos](https://cloud.tencent.com/developer/article/1077884)\n\n以及[CDH禁用kerberos](https://www.itread01.com/content/1543578548.html) 的相反步骤\n\n全部勾选\n\n<img src=\"/images/517519-20211020012051581-1087584833.png\" width=\"800\" height=\"405\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;<img src=\"/images/517519-20211020012129726-1245352142.png\" width=\"800\" height=\"499\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;不必勾选\n\n<img src=\"/images/517519-20211020012206448-557182665.png\" width=\"800\" height=\"168\" loading=\"lazy\" />\n\n&nbsp;\n\n填写管理员账号密码\n\n<img src=\"/images/517519-20211020012318250-1893180587.png\" width=\"600\" height=\"206\" loading=\"lazy\" />\n\n&nbsp;\n\n下一步\n\n<img src=\"/images/517519-20211020012356242-1787804644.png\" width=\"600\" height=\"371\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;重启集群\n\n<img src=\"/images/517519-20211020012430866-1624625985.png\" width=\"600\" height=\"232\" loading=\"lazy\" />\n\n等待集群重启\n\n<img src=\"/images/517519-20240120162034144-847520758.png\" width=\"800\" height=\"404\" loading=\"lazy\" />\n\n使用-norandkey来不重新生成凭证拿到keytab文件\n\n```\nkadmin.local:  ktadd -k /var/lib/hadoop-hdfs/hdfs.keytab -norandkey hdfs/master@HADOOP.COM\nkadmin.local:  ktadd -k /var/lib/hive/hive.keytab -norandkey hive/master@HADOOP.COM\nkadmin.local:  ktadd -k /etc/hue/hue@HADOOP.COM -norandkey hue/master@HADOOP.COM\n\n```\n\n&nbsp;\n\n## 5.其他问题\n\n1.kinit\n\n```\nkinit -kt ./hdfs.keytab hdfs/master@HADOOP.COM\n\n```\n\n&nbsp;\n\n2.kerberos认证覆盖问题\n\n先显示指定KRB5CCNAME存储的路径\n\n```\nexport KRB5CCNAME=/tmp/krb5cc_xxx\nkinit -kt /home/xxx.keytab xxx\n\n```\n\n&nbsp;\n\n3.生成keytab，重新生成之后，之前的失效\n\n```\nsudo kadmin.local -q \"xst -k hdfs.keytab hdfs\"\n\n```\n\n参考\n\n```\nhttps://wiki.shileizcc.com/confluence/display/zd/Kerberos\n\n```\n\n　　\n\n4.CDH keytab\n\n可以在/run/cloudera-scm-agent/process目录下找到各个组件的keytab，比如\n\n```\n/run/cloudera-scm-agent/process/53582-hive-HIVESERVER2# ls\ncloudera-monitor.properties\t   hive-log4j.properties\t\tsentry-site.xml\ncloudera-stack-monitor.properties  hive-site.xml\t\t\tservice-metrics.properties\nconfig.zip\t\t\t   logs\t\t\t\t\tspark-defaults.conf\ncore-site.xml\t\t\t   navigator.client.properties\t\tsupervisor.conf\ncreds.localjceks\t\t   navigator.lineage.client.properties\tyarn-conf\nfair-scheduler.xml\t\t   proc.json\nhive.keytab\t\t\t   redaction-rules.json\n\n```\n\n　　\n\n5.查看票据期限，默认的过期时间为10小时，10点开始，20点过期\n\n```\nklist -f\nTicket cache: FILE:/tmp/krb5cc_5035\nDefault principal: xxx@XXXXX\n\nValid starting       Expires              Service principal\n05/06/2021 10:53:45  05/06/2021 20:53:45  krbtgt/XXXXX@XXXXX\n\trenew until 05/13/2021 10:53:45, Flags: FRIA\n\n```\n\n&nbsp;\n\n6.开启kerberos之后，浏览器访问HDFS，YARN的http页面会报\n\n```\nHTTP ERROR 401  Problem accessing /cluster/app/application\n\n```\n\n在mac下需要先使用对应的keytab进行kinit，然后再进入firefox的浏览器高级选项 about:config 中配置2个参数\n\n注意mac需要能访问KDC认证服务器的端口，默认为88端口\n\n```\nnetwork.negotiate-auth.trusted-uris: your KDC host\nnetwork.auth.use-sspi:false\n\n```\n\n然后就可以访问了\n\n参考：[解决Ambari启用Kerberos后HDFS/YARN/SPARK等页面无法打开问题](https://blog.csdn.net/OMars/article/details/119871075)\n\n&nbsp;\n","tags":["kerberos"]},{"title":"Linux终端复制粘贴后前后会多出0~和~1","url":"/Linux终端复制粘贴后前后会多出0~和~1.html","content":"在终端中执行即可\n\n```\nprintf \"\\e[?2004l\"\n\n```\n\n在终端无法复制问题\n\n```\nset mouse=r\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["Linux"]},{"title":"Airflow使用指南","url":"/Airflow使用指南.html","content":"## 1.只执行单个任务\n\n将downstream和recursive按钮的点击状态取消，然后点击clear，最后选择**Ignore All Deps**，然后点击run\n\n<img src=\"/images/517519-20190717100704146-2064725384.png\" width=\"500\" height=\"281\" />\n\n## **2.从一个任务开始，执行它以及它的下游任务**\n\n将downstream和recursive按钮的点击状态取消，然后点击clear，最后选择**Ignore Task Deps**，然后点击run\n\n其他：[调度工具airflow的介绍和使用示例](https://blog.csdn.net/zzq900503/article/details/104537121)\n\n## **3.airflow命令行**\n\n```\nhttps://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html#dags\n\n```\n\n### 1.第一次登录创建airflow用户\n\n```\nairflow users create --username airflow --role Admin --password airflow --email airflow@xxx.com --lastname airflow --firstname airflow　\n```\n\n### 2.根据dag id删除一个dag\n\n```\nairflow dags delete {dag_id}\n\n```\n\n### 3.触发一个airflow dag\n\n```\nairflow dags trigger --help\nusage: airflow dags trigger [-h] [-c CONF] [-e EXEC_DATE] [-r RUN_ID]\n                            [-S SUBDIR]\n                            dag_id\n\nTrigger a DAG run\n\npositional arguments:\n  dag_id                The id of the dag\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -c CONF, --conf CONF  JSON string that gets pickled into the DagRun's conf attribute\n  -e EXEC_DATE, --exec-date EXEC_DATE\n                        The execution date of the DAG\n  -r RUN_ID, --run-id RUN_ID\n                        Helps to identify this run\n  -S SUBDIR, --subdir SUBDIR\n                        File location or directory from which to look for the dag. Defaults to '[AIRFLOW_HOME]/dags' where [AIRFLOW_HOME] is the value you set for 'AIRFLOW_HOME' config you set in 'airflow.cfg'\n\nairflow dags trigger -e '2022-07-19T08:00:00' your_dag_id\n\n```\n\n注意execution_time要在start_date和end_date之间，否则会报\n\n```\nValueError: The execution_date [2022-07-19T08:00:00+00:00] should be >= start_date [2022-07-20T00:00:00+00:00] from DAG's default_args\n\n```\n\n### 4.airflow按start_date和end_date触发backfill任务\n\n```\nairflow dags backfill -s 2022-01-01 -e 2022-01-10 DAG_ID\n\n```\n\n执行的任务是从2022-01-01到2022-01-09，包含startr_date，不包含end_date　　\n\n### 5.测试airflow task\n\n## 4.airflow会的connection配置参数\n\n```\nfrom airflow.hooks.base import BaseHook\n\nconnection = BaseHook.get_connection(\"username_connection\")\npassword = connection.password\n```\n\n## 5.在airflow界面上触发特定execution date的任务\n\n点击DAG Runs\n\n<img src=\"/images/517519-20220926143150827-1733552506.png\" alt=\"\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\nadd a new record\n\n<img src=\"/images/517519-20220926143229960-209466707.png\" width=\"500\" height=\"194\" loading=\"lazy\" />\n\nadd dag run\n\nstate注意填写running\n\nrun id格式参考其他的dag：scheduled__2022-09-18T23:00:00+00:00，最后点击save\n\n<img src=\"/images/517519-20220926143320858-226228567.png\" width=\"700\" height=\"434\" loading=\"lazy\" />\n\n成功触发一个特定execution date的dag\n\n<img src=\"/images/517519-20220926143642249-20063997.png\" alt=\"\" loading=\"lazy\" />\n\n如果需要回溯的dag特别多，也可以将dag的start_date设置好，并将catchup=True，这样如果是新建的dag的会，就会从start_date开始运行\n\n## 6.sensor的reschedule mode\n\n对于长时间检测的sensor，如果sensor的条件不满足，reschedule mode的sensor会释放worker给其他的task\n\nIn sensor mode='reschedule' means that if the criteria of the sensor isn't True then the sensor will release the worker to other tasks.\n\nThis is very useful for cases when sensor may wait for a long time.&nbsp;参考：\n\n```\nhttps://stackoverflow.com/questions/66715894/how-does-the-mode-reschedule-in-airflow-sensors-work\n\n```\n\n　　\n\n## 7.sensor的retry\n\nsensor中可以指定重试的次数，比如\n\n```\ntimeout=60 * 60,\nretries=2,\nretry_delay=timedelta(minutes=5),\n\n```\n\nretries=2表示总共会尝试运行3轮，超时之后运行下一轮，所以将会在airflow日志中看到\n\n```\n[2022-09-06 00:02:19,350] {taskinstance.py:1068} INFO - Starting attempt 1 of 3\n\n[2022-09-06 01:09:58,153] {taskinstance.py:1068} INFO - Starting attempt 2 of 3\n\n[2022-09-06 02:16:38,812] {taskinstance.py:1068} INFO - Starting attempt 3 of 3\n\n```\n\ntimeout=60 * 60 表示每一轮尝试的超时时间会1个小时，但是在同一轮中会尝试多次，timeout默认值是60*60*24*7（1周）\n\nretry_delay=timedelta(minutes=5) 表示超时后retry下一轮的间隔是5分钟，所以可以看到上面3轮重试的间隔是1小时5分钟\n\npoke_interval 表示sensor check的间隔，默认值是60（1分钟）\n\n参考：[https://help.aliyun.com/document_detail/336856.html](https://help.aliyun.com/document_detail/336856.html)\n\n&nbsp;\n\n## 8.出现报错：The scheduler does not appear to be running. Last heartbeat was received X minutes ago\n\n<img src=\"/images/517519-20230313105916252-610103050.png\" alt=\"\" loading=\"lazy\" />\n\n查看/health接口之后scheduler的状态是unhealthy\n\n文章介绍主要原因是配置中parsing_processes的数量过大，大于cpu的核数，将scheduler上报心跳的线程的cpu占用了导致\n\n参考：[记一次 Airflow 性能调优](https://old-panda.com/2021/06/30/airflow-performance-tuning/)\n\n&nbsp;\n\n## 9.airflow运行hive operator任务出现报错：airflow.exceptions.AirflowException: /usr/bin/env: &lsquo;bash&rsquo;: No such file or directory\n\n<img src=\"/images/517519-20230314103832275-978378493.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n这可能是由于使用的是supervisor来启动的airflow，所以需要额外的airflow中配置environment，如下\n\n```\nenvironment=PATH=\"/home/xxx/envs/your_env/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin\"\n\n```\n\n参考：[supervisor环境变量](https://blog.csdn.net/hunter_yang_tuziki/article/details/86675121)\n\n&nbsp;\n\n## 10.airflow配置parsing_processes，parallelism，dag_concurrencyd参数的调整\n\n参考：[记一次 Airflow 性能调优](https://old-panda.com/2021/06/30/airflow-performance-tuning/)\n\n&nbsp;\n","tags":["Airbnb"]},{"title":"Superset配置impala数据源","url":"/Superset配置impala数据源.html","content":"1.安装impyla\n\n```\npip install impyla \n\n```\n\n<!--more-->\n&nbsp;2.在superset页面配置如下,此时impala是有kerberos认证的\n\n```\nimpala://xxxx:xx/default?auth_mechanism=GSSAPI&amp;kerberos_service_name=impala\n\n```\n\n如果遇到下面的问题,是thrift-sasl版本过高\n\n```\nThe error message returned was:\\n'TSocket' object has no attribute 'isOpen'\n\n```\n\n&nbsp;降级为0.2.1版本就可以\n\n```\npip list | grep thrift-sasl\nthrift-sasl            0.3.0      \npip install thrift-sasl==0.2.1\n\n```\n\n测试,ok\n\n<img src=\"/images/517519-20190717143626518-1580552449.png\" alt=\"\" />\n\n查询\n\n<img src=\"/images/517519-20190717143912681-2147357456.png\" alt=\"\" />\n\n&nbsp;\n\n当将thrift-sasl从0.3.0降级为0.2.1之后,连接hive会报\n\n```\nfrom pyhive import hive ImportError: cannot import name 'constants'\n\n```\n\n<img src=\"/images/517519-20190717154710996-751089213.png\" alt=\"\" />\n\n解决方法是将原来hive的uri从\n\n```\nhive://xxx:xxx/default?auth=KERBEROS&amp;kerberos_service_name=hive\n\n```\n\n&nbsp;修改为\n\n```\nimpala://xxx:xxx/default?auth_mechanism=GSSAPI&amp;kerberos_service_name=hive\n\n```\n\n<img src=\"/images/517519-20190717154736638-737250925.png\" alt=\"\" />\n\n同时附上impala和hive查询的时间对比,impala要快很多\n\nhive\n\n<img src=\"/images/517519-20190717155548148-2033007788.png\" alt=\"\" />\n\nimpala\n\n<img src=\"/images/517519-20190717155637530-1824052500.png\" alt=\"\" />\n\n其中city为array,在impala中不能使用select语句来查询,因为impala中select语句只支持标量\n\n<img src=\"/images/517519-20190717160521629-988144003.png\" alt=\"\" />\n\n如果要在impala中查询array中的数据，需要这么查\n\n```\nselect * from default.arraydemo, default.arraydemo.city\n\n```\n\n结果，这是将array中的数据进行了展平\n\n<img src=\"/images/517519-20191024213925857-81252943.png\" alt=\"\" />\n\n&nbsp;\n","tags":["Airbnb","impala"]},{"title":"Superset配置hive数据源","url":"/Superset配置hive数据源.html","content":"1.在uri中配置 hive://localhost:10000/default\n\n<img src=\"/images/517519-20190716141858363-1051059112.png\" alt=\"\" />\n\n2.查询\n\n<img src=\"/images/517519-20190716152012205-1456417108.png\" alt=\"\" />\n\n3.如果你的hive集群是带有kerberos认证的,hive数据源需要这样配置\n\n```\nhive://xxx:xxx/default?auth=KERBEROS&amp;kerberos_service_name=hive\n\n```\n\n如果在连接的时候报了如下的错\n\n```\nCould not start SASL: b'Error in sasl_client_start (-1) SASL(-1): generic failure: GSSAPI Error: Unspecified GSS failure\n\n```\n\n那就就是你没有用keytab进行认证\n\n```\nkinit -kt xxx.keytab xxx@XXXX\n\n```\n\n<img src=\"/images/517519-20190716172747111-993865591.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;superset也支持模拟用户,配置方法如下\n\n<img src=\"/images/517519-20190716214224454-570215086.png\" alt=\"\" />\n\n&nbsp;\n\n然后就能支持使用hive,impala或者hdfs用户来对普通用户进行模拟,如果模拟出来的用户没有权限的话,就会抛出异常\n\n<img src=\"/images/517519-20190716214147842-459598961.png\" alt=\"\" />\n\n&nbsp;\n","tags":["Airbnb","Hive"]},{"title":"Superset配置mysql数据源","url":"/Superset配置mysql数据源.html","content":"1.添加mysql数据源\n\n<img src=\"/images/517519-20190716114754877-1692056408.png\" alt=\"\" />\n\n测试连接的时候遇到\n\n```\nNo module named 'MySQLdb'\"\n\n```\n\n安装mysqlclient\n\n```\npip install mysqlclient\n\n```\n\n如果遇到\n\n```\nERROR: /bin/sh: 1: mysql_config: not found\n\n```\n\n安装\n\n```\nsudo apt-get install libmysqlclient-dev python3-dev\n\n```\n\n添加mysql的url\n\n```\nmysql://root:xxxx@localhost/mysql?charset=utf8\n\n```\n\n测试ok\n\n<img src=\"/images/517519-20190716124318205-152081659.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n","tags":["Airbnb"]},{"title":"Hadoop学习笔记——HDFS","url":"/Hadoop学习笔记——HDFS.html","content":"**1.查看hdfs文件的block信息**\n\n不正常的文件\n\n```\nhdfs fsck /logs/xxx/xxxx.gz.gz -files -blocks -locations\nConnecting to namenode via http://xxx-01:50070/fsck?ugi=xxx&amp;files=1&amp;blocks=1&amp;locations=1&amp;path=%2Flogs%2Fxxx%2Fxxx%2F401294%2Fds%3Dxxxx-07-14%2Fxxx.gz.gz\nFSCK started by xxxx (auth:KERBEROS_SSL) from /10.90.1.91 for path xxxxx.gz.gz at Mon Jul 15 11:44:13 CST 2019\nStatus: HEALTHY\n Total size:\t0 B (Total open files size: 194 B)\n Total dirs:\t0\n Total files:\t0\n Total symlinks:\t\t0 (Files currently being written: 1)\n Total blocks (validated):\t0 (Total open file blocks (not validated): 1)\n Minimally replicated blocks:\t0\n Over-replicated blocks:\t0\n Under-replicated blocks:\t0\n Mis-replicated blocks:\t\t0\n Default replication factor:\t3\n Average block replication:\t0.0\n Corrupt blocks:\t\t0\n Missing replicas:\t\t0\n Number of data-nodes:\t\t99\n Number of racks:\t\t3\nFSCK ended at Mon Jul 15 11:44:13 CST 2019 in 0 milliseconds\n\n```\n\n<!--more-->\n&nbsp;正常的文件\n\n```\nConnecting to namenode via http://xxx:50070/fsck?ugi=xxx&amp;files=1&amp;blocks=1&amp;locations=1&amp;path=%2Fxxx%2Fxxx%2Fxxx%2F401294%2Fds%3Dxxx-07-14%2Fxx.gz\nFSCK started by xxxx (auth:KERBEROS_SSL) from /10.90.1.91 for path /logs/xxxx.gz at Mon Jul 15 11:46:12 CST 2019\n/logs/xxxx.gz 74745 bytes, 1 block(s):  OK\n0. BP-1760298736-10.90.1.6-1536234810107:blk_1392467116_318836510 len=74745 Live_repl=3 [DatanodeInfoWithStorage[10.90.1.99:1004,DS-9d465b1f-943f-4716-bce0-8b36e5631b4a,DISK], DatanodeInfoWithStorage[10.90.1.216:1004,DS-160924c6-4cd7-4822-93c0-9ac9cf9c5784,DISK], DatanodeInfoWithStorage[10.90.1.191:1004,DS-d0a2e418-610f-4bef-8f1d-4ce045533656,DISK]]\n\nStatus: HEALTHY\n Total size:\t74745 B\n Total dirs:\t0\n Total files:\t1\n Total symlinks:\t\t0\n Total blocks (validated):\t1 (avg. block size 74745 B)\n Minimally replicated blocks:\t1 (100.0 %)\n Over-replicated blocks:\t0 (0.0 %)\n Under-replicated blocks:\t0 (0.0 %)\n Mis-replicated blocks:\t\t0 (0.0 %)\n Default replication factor:\t3\n Average block replication:\t3.0\n Corrupt blocks:\t\t0\n Missing replicas:\t\t0 (0.0 %)\n Number of data-nodes:\t\t99\n Number of racks:\t\t3\nFSCK ended at Mon Jul 15 11:46:12 CST 2019 in 1 milliseconds\n\n```\n\n&nbsp;\n\n**2.修复hdfs文件命令**\n\n```\nhdfs debug recoverLease -path /logs/xxxx.gz.gz -retries 3\n\n```\n\n修复之后\n\n```\nhdfs fsck /logs/xxx.gz.gz -files -blocks -locations\nConnecting to namenode via http://xxx-01:50070/fsck?ugi=xxx&amp;files=1&amp;blocks=1&amp;locations=1&amp;path=%2Flogs%2Fnsh%2Fjson%2F401294%2Fds%3D2019-07-14%2Fxxx.gz.gz\nFSCK started by xxx (auth:KERBEROS_SSL) from /10.90.1.91 for path /logs/xxxx.gz.gz at Mon Jul 15 11:48:01 CST 2019\n/logs/xxxx.gz.gz 67157 bytes, 1 block(s):  OK\n0. BP-1760298736-10.90.1.6-1536234810107:blk_1392594522_319757834 len=67157 Live_repl=3 [DatanodeInfoWithStorage[10.90.1.213:1004,DS-6aee5c90-c834-475e-8f20-7a0f8bd8d315,DISK], DatanodeInfoWithStorage[10.90.1.207:1004,DS-cd79bacc-89ff-4fb3-82b5-79341391ae8d,DISK], DatanodeInfoWithStorage[10.90.1.97:1004,DS-ba5953f8-c0c3-444a-8996-3bcfa1bcf851,DISK]]\n\nStatus: HEALTHY\n Total size:\t67157 B\n Total dirs:\t0\n Total files:\t1\n Total symlinks:\t\t0\n Total blocks (validated):\t1 (avg. block size 67157 B)\n Minimally replicated blocks:\t1 (100.0 %)\n Over-replicated blocks:\t0 (0.0 %)\n Under-replicated blocks:\t0 (0.0 %)\n Mis-replicated blocks:\t\t0 (0.0 %)\n Default replication factor:\t3\n Average block replication:\t3.0\n Corrupt blocks:\t\t0\n Missing replicas:\t\t0 (0.0 %)\n Number of data-nodes:\t\t99\n Number of racks:\t\t3\nFSCK ended at Mon Jul 15 11:48:01 CST 2019 in 1 milliseconds\n\n```\n\n&nbsp;\n\n3. 其他博客：[Hadoop学习笔记&mdash;HDFS](https://www.cnblogs.com/niceshot/p/14614436.html)\n\n&nbsp;\n\n4. 一些脚本\n\n遍历hdfs路径，并截取**最后一层**子路径，其中&nbsp;##*/ 是截取 / 后面的字符，参考：[Shell当中的字符串切割](https://blog.csdn.net/qq_26768741/article/details/66974713)\n\n```\n#!/usr/bin/env bash\n\nfunction grep_dir(){\n   kinit -kt xxx.keytab xxx\n   # 遍历hdfs路径\n   for file in `hadoop fs -ls /logs/xxxx | awk '{print $8}'`\n        do\n            echo $file\"   \"${file##*/}\n        done\n}\n\ngrep_dir\n\n```\n\n输出\n\n```\n/logs/xxxx/ds=2019-04-01   ds=2019-04-01\n/logs/xxxx/ds=2019-04-02   ds=2019-04-02\n/logs/xxxx/ds=2019-04-03   ds=2019-04-03\n/logs/xxxx/ds=2019-04-04   ds=2019-04-04\n/logs/xxxx/ds=2019-04-05   ds=2019-04-05\n/logs/xxxx/ds=2019-04-06   ds=2019-04-06\n/logs/xxxx/ds=2019-04-07   ds=2019-04-07\n\n```\n\n&nbsp;\n\n遍历hdfs路径，并截取**中间层**路径，其中\n\n```\nlogid=`echo $file | awk -F \"/\" '{print $5}'`\n\n```\n\n是使用 / 切分后，取对应数组第5个位置的字符串\n\n```\n#!/usr/bin/env bash\n\nfunction grep_dir(){\n   kinit -kt xxxx.keytab xxxx\n   # 遍历hdfs路径\n   for file in `hadoop fs -ls /logs/xxx/json/status-1 | awk '{print $8}'`\n        do\n            echo $file\"   \"${file##*/}\n            logid=`echo $file | awk -F \"/\" '{print $5}'`\n            echo $logid\n        done\n}\n\ngrep_dir\n\n```\n\n输出\n\n```\n/logs/xxx/json/status-1/ds=2021-04-18   ds=2021-04-18\nstatus-1\n/logs/xxx/json/status-1/ds=2021-04-19   ds=2021-04-19\nstatus-1\n/logs/xxx/json/status-1/ds=2021-04-20   ds=2021-04-20\nstatus-1\n\n```\n\n　　\n\n**5.webhdfs**\n\n```\nhttp://master:14000/webhdfs/v1/?op=liststatus&amp;user.name=hadoop\n\n```\n\n<img src=\"/images/517519-20220326125015981-2027408281.png\" width=\"500\" height=\"696\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;\n\n参考：[WebHDFS与HttpFS的使用&nbsp;](https://www.jianshu.com/p/9b46aebcb963%20)\n\n&nbsp;\n","tags":["Hadoop"]},{"title":"Ubuntu16.04安装Superset","url":"/Ubuntu16.04安装Superset.html","content":"**Superset **是Airbnb 开源的大数据可视化平台\n\n其支持的datasource\n\n```\nhttps://superset.incubator.apache.org/index.html?highlight=datasource\n\n```\n\n类似的开源项目Zeppelin所支持的datasource\n\n```\nhttps://zeppelin.apache.org/docs/0.8.0/quickstart/sql_with_zeppelin.html\n\n```\n\n<!--more-->\n&nbsp;\n\n1.升级python3.5到python3.6,否则会报 ERROR: Sorry, Python < 3.6 is not supported\n\n```\nsudo add-apt-repository ppa:jonathonf/python-3.6\nsudo apt update\nsudo apt install python3.6 \n\n```\n\n2.官方的安装文档\n\n```\nhttps://superset.incubator.apache.org/installation.html\n\n```\n\n3.安装虚拟环境\n\n```\nsudo apt-get install python3.6-venv\npython3.6 -m venv venv\n. venv/bin/activate\n\n```\n\n4.安装superset\n\n```\npip install --upgrade setuptools pip\npip install superset\n\n```\n\n&nbsp;之后按照官方文档的安装方法来,遇到\n\n```\nRunning setup.py install for python-geohash ... error\n\n```\n\n解决方法\n\n```\nsudo apt-get install python3.6-dev libsasl2-dev gcc\n\n```\n\n执行superset db upgrade时,如果遇到\n\n```\nImportError: cannot import name '_maybe_box_datetimelike'\n\n```\n\n是pandas版本过高导致的,进行降级\n\n```\npip list | grep pandas\npandas                 0.24.2\npip install pandas==0.23.4\n\n```\n\n如果遇到\n\n```\nImportError: cannot import name '_maybe_box_datetimelike'\n\n```\n\n是SQLAlchemy版本过高导致的\n\n```\npip install SQLAlchemy==1.2\n\n```\n\n之后参照官方文档进行安装和启动就可以了\n\n<img src=\"/images/517519-20190716114557299-1425483581.png\" alt=\"\" />\n\n&nbsp;\n\nsuperset默认使用的数据库是sqlite,将其修改成mysql的方式为编辑 vim config.py,改为\n\n```\nSQLALCHEMY_DATABASE_URI = 'mysql://superset:superset@localhost/superset?charset=UTF8'  \n\n```\n\n&nbsp;创建mysql用户\n\n```\nmysql> CREATE DATABASE superset\n    ->   DEFAULT CHARACTER SET utf8\n    ->   DEFAULT COLLATE utf8_general_ci;\nQuery OK, 1 row affected (0.00 sec)\n\nmysql> CREATE USER 'superset';\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql> SET PASSWORD FOR 'superset' = PASSWORD('superset');\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> GRANT ALL ON superset.* TO 'superset';\nQuery OK, 0 rows affected (0.00 sec)\n\n```\n\n&nbsp;再执行\n\n```\nsuperset db upgrade\n\n```\n\n&nbsp;\n\n如果pip3找不到了\n\n```\nsudo python3 -m pip install --upgrade --force-reinstall pip\n\n```\n\n安装Python3.6\n\n```\nsudo add-apt-repository ppa:deadsnakes/ppa\nsudo apt-get update\nsudo apt-get install python3.6\n\n```\n\n安装pip3.6\n\n```\nwget https://bootstrap.pypa.io/get-pip.py\nsudo python3.6 get-pip.py\n\n```\n\n安装python-dev\n\n```\nsudo apt-get install python3.6-dev\n\n```\n\n&nbsp;\n","tags":["Airbnb"]},{"title":"Ubuntu16.04安装CDH 5.16.2","url":"/Ubuntu16.04安装CDH 5.16.2.html","content":"CDH安装官方参考文档:\n\n```\nhttps://www.cloudera.com/documentation/enterprise/5-16-x/topics/configure_cm_repo.html\n\n```\n\n如果是在生产环境进行安装，建议查看cloudera官方提供的机型建议\n\n```\nhttps://docs.cloudera.com/documentation/enterprise/release-notes/topics/hardware_requirements_guide.html\n\n```\n\n同时还在CDH的角色分布建议，参考：[如何给Hadoop集群划分角色](https://cloud.tencent.com/developer/article/1035458)<!--more-->\n&nbsp;\n\n```\nhttps://docs.cloudera.com/documentation/enterprise/latest/topics/cm_ig_host_allocations.html\n\n```\n\n　　\n\n安装cdh前如果安装过其他版本的,记得删除各种目录残留文件，比如/run下面的\n\n如果误删了log4j/properties文件,文件内容在这\n\n```\ncmf.root.logger=INFO,CONSOLE\ncmf.log.dir=.\ncmf.log.file=cmf-server.log\n\n# Define the root logger to the system property \"cmf.root.logger\".\nlog4j.rootLogger=${cmf.root.logger}\n\n# Logging Threshold\nlog4j.threshhold=ALL\n\n# Disable most JDBC tracing by default.\nlog4j.logger.org.jdbcdslog=FATAL\n\n# Disable overly loud Avro IPC logging\nlog4j.logger.org.apache.avro.ipc.NettyTransceiver=FATAL\n\n# Disable overly loud Flume config validation logging\nlog4j.logger.org.apache.flume.conf.FlumeConfiguration=ERROR\n\nlog4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\nlog4j.appender.CONSOLE.target=System.err\nlog4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\nlog4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %5p [%t:%c{2}@%L] %m%n\n\nlog4j.appender.LOGFILE=org.apache.log4j.RollingFileAppender\nlog4j.appender.LOGFILE.MaxFileSize=10MB\nlog4j.appender.LOGFILE.MaxBackupIndex=10\nlog4j.appender.LOGFILE.File=${cmf.log.dir}/${cmf.log.file}\nlog4j.appender.LOGFILE.layout=org.apache.log4j.PatternLayout\nlog4j.appender.LOGFILE.layout.ConversionPattern=%d{ISO8601} %5p [%t:%c{2}@%L] %m%n\n\n```\n\n&nbsp;\n\n正式开始安装\n\n1.配置源\n\n```\nwget https://archive.cloudera.com/cm5/ubuntu/xenial/amd64/cm/archive.key\nsudo apt-key add archive.key\n\ncd /etc/apt/sources.list.d/\nwget https://archive.cloudera.com/cm5/ubuntu/xenial/amd64/cm/cloudera.list\ncurl -s https://archive.cloudera.com/cm5/ubuntu/trusty/amd64/cm/archive.key| sudo apt-key add -\nsudo apt-get update\n\n```\n\n2.安装Java\n\n3.关防火墙\n\n参考\n\n```\nhttps://wenku.baidu.com/view/5462a2132f3f5727a5e9856a561252d380eb20e5.html\n\n```\n\n&nbsp;\n\n```\nsudo service ufw stop\n\n```\n\n4.安装CDH server\n\n```\nsudo apt-get install cloudera-manager-daemons cloudera-manager-server\n\n```\n\n或者通过下载的deb包来进行安装\n\n```\nsudo dpkg -i ./cloudera-manager-daemons_5.16.2-1.cm5162.p0.7~xenial-cm5_all.deb\nsudo dpkg -i ./cloudera-manager-server_5.16.2-1.cm5162.p0.7~xenial-cm5_all.deb\nsudo dpkg -i ./cloudera-manager-agent_5.16.2-1.cm5162.p0.7~xenial-cm5_amd64.deb \n\n```\n\n期间如果依赖不满足的话\n\n```\nsudo apt-get install -f\n\n```\n\n4.安装MySQL JDBC Driver\n\n```\nsudo apt-get install libmysql-java\n\n```\n\n5.按照官方教程配置数据库等，注意`IDENTIFIED BY`后面跟着的是密码\n\n```\nCREATE DATABASE scm DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;\nCREATE DATABASE amon DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;\nCREATE DATABASE rman DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;\nCREATE DATABASE hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;\nCREATE DATABASE metastore DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;\nCREATE DATABASE sentry DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;\nCREATE DATABASE nav DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;\nCREATE DATABASE navms DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;\nCREATE DATABASE oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;\nCREATE USER 'scm';\nCREATE USER 'amon';\nCREATE USER 'rman';\nCREATE USER 'hue';\nCREATE USER 'hive';\nCREATE USER 'sentry';\nCREATE USER 'nav';\nCREATE USER 'navms';\nCREATE USER 'oozie';\nGRANT ALL ON scm.* TO 'scm'@'%' IDENTIFIED BY 'scm';\nGRANT ALL ON amon.* TO 'amon'@'%' IDENTIFIED BY 'amon';\nGRANT ALL ON rman.* TO 'rman'@'%' IDENTIFIED BY 'rman';\nGRANT ALL ON hue.* TO 'hue'@'%' IDENTIFIED BY 'hue';\nGRANT ALL ON metastore.* TO 'hive'@'%' IDENTIFIED BY 'hive';\nGRANT ALL ON sentry.* TO 'sentry'@'%' IDENTIFIED BY 'sentry';\nGRANT ALL ON nav.* TO 'nav'@'%' IDENTIFIED BY 'nav';\nGRANT ALL ON navms.* TO 'navms'@'%' IDENTIFIED BY 'navms';\nGRANT ALL ON oozie.* TO 'oozie'@'%' IDENTIFIED BY 'oozie';\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20191210224006520-731305261.png\" alt=\"\" />\n\n7.初始化数据库,每执行完要输入密码\n\n```\nsudo /usr/share/cmf/schema/scm_prepare_database.sh mysql scm scm\n\n```\n\n如果配置的是外部数据库，参考\n\n```\nhttps://docs.cloudera.com/documentation/enterprise/5-13-x/topics/cm_ig_installing_configuring_dbs.html#concept_i2r_m3m_hn\n\n```\n\n8.启动\n\n```\nsystemctl start cloudera-scm-server\nsystemctl start cloudera-scm-agent\n\n```\n\n1.如果启动的时候,/var/log/cloudera-scm-server/cloudera-scm-server.out出现\n\n```\nCaused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactoryBean': FactoryBean threw exception on object creation; nested exception is javax.persistence.PersistenceException: org.hibernate.exception.GenericJDBCException: Could not open connection\n\n```\n\n可以参考\n\n```\nhttps://blog.csdn.net/qq_41623990/article/details/83008860\n\n```\n\n&nbsp;在/usr/share/cmf/schema执行\n\n```\nbash scm_prepare_database.sh mysql  -uroot -p --scm-host localhost scm scm scm\n\n```\n\n2.如果启动的时候,/var/log/cloudera-scm-server/cloudera-scm-server.out出现,但是cloudera-scm-server.log和cmf-server-perf.log日志都没有\n\n```\nCaused by: org.springframework.beans.BeanInstantiationException: Could not instantiate bean class [com.cloudera.server.cmf.log.components.ServerLogFetcherImpl]: Constructor threw exception; nested exception is java.io.FileNotFoundException: Unable to locate the Cloudera Manager log file in the log4j settings\n\nCaused by: java.io.FileNotFoundException: Unable to locate the Cloudera Manager log file in the log4j settings\n\n```\n\n查看/etc/cloudera-scm-server/log4j.properties是否为空\n\n为空的话,加入内容,启动cloudera-scm-server,然后cloudera-scm-server.log和cmf-server-perf.log日志都成功出现\n\n```\n# Copyright (c) 2012 Cloudera, Inc. All rights reserved.\n#\n# !!!!! IMPORTANT !!!!!\n# The Cloudera Manager server finds its log file by querying log4j. It\n# assumes that the first file appender in this file is the server log.\n# See LogUtil.getServerLogFile() for more details.\n#\n# Define some default values that can be overridden by system properties\ncmf.root.logger=INFO,CONSOLE\ncmf.log.dir=.\ncmf.log.file=cmf-server.log\ncmf.perf.log.file=cmf-server-perf.log\n\n# Define the root logger to the system property \"cmf.root.logger\".\nlog4j.rootLogger=${cmf.root.logger}\n\n# Logging Threshold\nlog4j.threshhold=ALL\n\n# Disable most JDBC tracing by default.\nlog4j.logger.org.jdbcdslog=FATAL\n\n# Disable overly loud Avro IPC logging\nlog4j.logger.org.apache.avro.ipc.NettyTransceiver=FATAL\n\n# Disable overly loud Flume config validation logging\nlog4j.logger.org.apache.flume.conf.FlumeConfiguration=ERROR\n\n# Disable overly loud CXF logging\nlog4j.logger.org.apache.cxf.phase.PhaseInterceptorChain=ERROR\n\n# Disable \"Mapped URL path\" messages from Spring\nlog4j.logger.org.springframework.web.servlet.mvc.annotation.DefaultAnnotationHandlerMapping=WARN\n\nlog4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\nlog4j.appender.CONSOLE.target=System.err\nlog4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\nlog4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %t:%c: %m%n\n\nlog4j.appender.LOGFILE=org.apache.log4j.RollingFileAppender\nlog4j.appender.LOGFILE.MaxFileSize=10MB\nlog4j.appender.LOGFILE.MaxBackupIndex=10\nlog4j.appender.LOGFILE.File=${cmf.log.dir}/${cmf.log.file}\nlog4j.appender.LOGFILE.layout=org.apache.log4j.PatternLayout\nlog4j.appender.LOGFILE.layout.ConversionPattern=%d{ISO8601} %p %t:%c: %m%n\n\nlog4j.appender.LOGFILE=org.apache.log4j.RollingFileAppender\nlog4j.appender.LOGFILE.MaxFileSize=10MB\nlog4j.appender.LOGFILE.MaxBackupIndex=10\nlog4j.appender.LOGFILE.File=${cmf.log.dir}/${cmf.log.file}\nlog4j.appender.LOGFILE.layout=org.apache.log4j.PatternLayout\nlog4j.appender.LOGFILE.layout.ConversionPattern=%d{ISO8601} %p %t:%c: %m%n\n\nlog4j.additivity.com.cloudera.server.cmf.debug.components.PerfLogger=false\nlog4j.logger.com.cloudera.server.cmf.debug.components.PerfLogger=INFO,PERFLOGFILE\nlog4j.appender.PERFLOGFILE=org.apache.log4j.RollingFileAppender\nlog4j.appender.PERFLOGFILE.MaxFileSize=10MB\nlog4j.appender.PERFLOGFILE.MaxBackupIndex=10\nlog4j.appender.PERFLOGFILE.File=${cmf.log.dir}/${cmf.perf.log.file}\nlog4j.appender.PERFLOGFILE.layout=org.apache.log4j.PatternLayout\nlog4j.appender.PERFLOGFILE.layout.ConversionPattern=%d{ISO8601} %p %t:%c: %m%n\n\n```\n\n&nbsp;\n\n&nbsp;\n\n之后访问 http://localhost:7180/cmf/login\n\n向集群添加主机\n\n<img src=\"/images/517519-20200308001503890-1406048201.png\" alt=\"\" />\n\n&nbsp;\n\n&nbsp;填master的host\n\n<img src=\"/images/517519-20200308001535950-1620863545.png\" alt=\"\" />\n\n&nbsp;\n\n&nbsp;继续\n\n<img src=\"/images/517519-20200308001600668-1723937046.png\" alt=\"\" />\n\n&nbsp;\n\n&nbsp;继续\n\n<img src=\"/images/517519-20200308001655185-2138136335.png\" alt=\"\" />\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n\n9.安装的时候提供 SSH 登录凭据使用root和密码\n\n<img src=\"/images/517519-20191221235831053-896397644.png\" alt=\"\" />\n\n或者使用私钥,要保证root用户可以用私钥ssh过去\n\n<img src=\"/images/517519-20200308001740626-1622416960.png\" alt=\"\" />\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n\n如果安装的时候报\n\n```\nsed: can't read /etc/cloudera-scm-agent/config.ini: No such file or directory \n\n```\n\n<img src=\"/images/517519-20200308000711348-1462712841.png\" alt=\"\" />\n\n&nbsp;拷贝一个正确的文件,把文件的内容补上\n\n&nbsp;\n\n如果遇到 安装失败。 无法接收 Agent 发出的检测信号。\n\n<img src=\"/images/517519-20200308002114341-711378718.png\" alt=\"\" />\n\n&nbsp;将/etc/cloudera-scm-agent/config.ini中security的注释掉,并将设置use_tls=0,就能安装成功<br /><img src=\"/images/517519-20200308110307805-1421550711.png\" alt=\"\" />\n\n&nbsp;安装parcel,等待即可\n\n<img src=\"/images/517519-20200308110417572-920007176.png\" alt=\"\" />\n\n&nbsp;完成\n\n<img src=\"/images/517519-20200308110632071-539604080.png\" alt=\"\" />\n\n&nbsp;继续安装组件\n\n<img src=\"/images/517519-20200308110751169-531390182.png\" alt=\"\" />\n\n分配角色\n\n<img src=\"/images/517519-20200308110839382-365931310.png\" alt=\"\" />\n\n&nbsp;配置集群数据库\n\n<img src=\"/images/517519-20200308111204984-385454080.png\" alt=\"\" />\n\n&nbsp;继续\n\n<img src=\"/images/517519-20200308111310759-1174022311.png\" alt=\"\" />\n\n&nbsp;全部启动成功\n\n&nbsp;<img src=\"/images/517519-20200308112336408-1347967589.png\" alt=\"\" />\n\n&nbsp;\n\n&nbsp;\n\n9.安装Manager Service,否则会包host monitor连不上\n\n需要在这一步输入report manager的数据库账号密码,如果连不上可能是防火墙没关,\n\nmysql的host填localhost\n\n10.查看parcels的下载情况,如果要离线安装的,将parcels下载后拷贝到/opt/cloudera/parcel-repo目录,注意修改权限一起重启server和agent\n\n```\n/opt/cloudera/parcel-repo$ ls -alh\n总用量 2.4G\ndrwxr-xr-x 2 cloudera-scm cloudera-scm 4.0K 12月 15 11:31 .\ndrwxr-xr-x 6 cloudera-scm cloudera-scm 4.0K 12月 15 00:03 ..\n-rwxr-xr-x 1 cloudera-scm cloudera-scm 1.9G 12月 15 11:12 CDH-5.16.2-1.cdh5.16.2.p0.8-xenial.parcel\n-rwxr-xr-x 1 cloudera-scm cloudera-scm   41 12月 15 11:12 CDH-5.16.2-1.cdh5.16.2.p0.8-xenial.parcel.sha\n-rw-r----- 1 cloudera-scm cloudera-scm  75K 12月 15 11:31 CDH-5.16.2-1.cdh5.16.2.p0.8-xenial.parcel.torrent\n-rw-r----- 1 cloudera-scm cloudera-scm  84M 12月 15 02:55 KAFKA-4.1.0-1.4.1.0.p0.4-xenial.parcel\n-rw-r----- 1 cloudera-scm cloudera-scm   41 12月 15 02:55 KAFKA-4.1.0-1.4.1.0.p0.4-xenial.parcel.sha\n-rw-r----- 1 cloudera-scm cloudera-scm 3.5K 12月 15 02:55 KAFKA-4.1.0-1.4.1.0.p0.4-xenial.parcel.torrent\n-rw-r----- 1 cloudera-scm cloudera-scm 453M 12月 15 04:29 KUDU-1.4.0-1.cdh5.12.2.p0.8-xenial.parcel\n-rw-r----- 1 cloudera-scm cloudera-scm   41 12月 15 04:29 KUDU-1.4.0-1.cdh5.12.2.p0.8-xenial.parcel.sha\n-rw-r----- 1 cloudera-scm cloudera-scm  18K 12月 15 04:29 KUDU-1.4.0-1.cdh5.12.2.p0.8-xenial.parcel.torrent\n-rw-r--r-- 1 cloudera-scm cloudera-scm  66K 6月  18 21:21 manifest.json\n\n```\n\n&nbsp;\n\n&nbsp;\n\n<img src=\"/images/517519-20190721173156302-820376115.png\" alt=\"\" />\n\n安装kafka，分配并激活\n\n<img src=\"/images/517519-20200308140439962-413081067.png\" alt=\"\" width=\"1202\" height=\"400\" />\n\n然后点击添加服务，选择kafka进行安装\n\n失败，查看stderr日志\n\n<img src=\"/images/517519-20200308143828616-2017701461.png\" alt=\"\" width=\"910\" height=\"450\" />\n\n发现\n\n```\n+ exec /opt/cloudera/parcels/KAFKA-4.1.0-1.4.1.0.p0.4/lib/kafka/bin/kafka-server-start.sh /run/cloudera-scm-agent/process/106-kafka-KAFKA_BROKER/kafka.properties\n三月 08, 2020 2:29:36 下午 org.glassfish.jersey.internal.inject.Providers checkProviderRuntime\n警告: A provider nl.techop.kafka.KafkaTopicsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider nl.techop.kafka.KafkaTopicsResource will be ignored. \n三月 08, 2020 2:29:36 下午 org.glassfish.jersey.internal.inject.Providers checkProviderRuntime\n警告: A provider nl.techop.kafka.TopicMetricNameResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider nl.techop.kafka.TopicMetricNameResource will be ignored. \nRedaction rules file doesn't exist, not redacting logs. file: redaction-rules.json, directory: /run/cloudera-scm-agent/process/106-kafka-KAFKA_BROKER\n\n```\n\n所以执行命令\n\n```\n/opt/cloudera/parcels/KAFKA-4.1.0-1.4.1.0.p0.4/lib/kafka/bin/kafka-server-start.sh /run/cloudera-scm-agent/process/106-kafka-KAFKA_BROKER/kafka.properties\n\n```\n\n发现是没有log4j.peoperties文件\n\n```\n/opt/cloudera/parcels/KAFKA-4.1.0-1.4.1.0.p0.4/lib/kafka/bin/kafka-server-start.sh /run/cloudera-scm-agent/process/106-kafka-KAFKA_BROKER/kafka.properties\nlog4j:ERROR Could not read configuration file from URL [file:/opt/cloudera/parcels/KAFKA-4.1.0-1.4.1.0.p0.4/lib/kafka/bin/../config/log4j.properties].\njava.io.FileNotFoundException: /opt/cloudera/parcels/KAFKA-4.1.0-1.4.1.0.p0.4/lib/kafka/bin/../config/log4j.properties (没有那个文件或目录)\n\tat java.io.FileInputStream.open0(Native Method)\n\tat java.io.FileInputStream.open(FileInputStream.java:195)\n\tat java.io.FileInputStream.<init>(FileInputStream.java:138)\n\tat java.io.FileInputStream.<init>(FileInputStream.java:93)\n\tat sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)\n\tat sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)\n\tat org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:557)\n\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n\tat org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\n\tat org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:66)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:358)\n\tat com.typesafe.scalalogging.Logger$.apply(Logger.scala:48)\n\tat kafka.utils.Log4jControllerRegistration$.<init>(Logging.scala:25)\n\tat kafka.utils.Log4jControllerRegistration$.<clinit>(Logging.scala)\n\tat kafka.utils.Logging$class.$init$(Logging.scala:47)\n# Change the two lines below to adjust the general broker logging level (output to server.log and stdout)\n\tat com.cloudera.kafka.wrap.Kafka$.<init>(Kafka.scala:30)\n\tat com.cloudera.kafka.wrap.Kafka$.<clinit>(Kafka.scala)\n\tat com.cloudera.kafka.wrap.Kafka.main(Kafka.scala)\nlog4j:ERROR Ignoring configuration file [file:/opt/cloudera/parcels/KAFKA-4.1.0-1.4.1.0.p0.4/lib/kafka/bin/../config/log4j.properties].\n\n```\n\n添加该文件\n\n```\nhttps://github.com/apache/kafka/blob/trunk/config/log4j.properties&nbsp;\n```\n\n再执行，发现\n\n```\nkafka.common.InconsistentBrokerIdException: Configured broker.id 72 doesn't match stored broker.id 94 in meta.properties. If you moved your data, make sure your configured broker.id matches. If you intend to create a new broker, you should remove all data in your data directories (log.dirs).\n\n```\n\n去kafka的配置里面将broke.id从72修改成94\n\n<img src=\"/images/517519-20200308144504539-1683765055.png\" alt=\"\" width=\"1225\" height=\"450\" />\n\n&nbsp;启动成功\n\n<img src=\"/images/517519-20200308144626036-1928569994.png\" alt=\"\" width=\"1000\" height=\"260\" />\n\n&nbsp;\n\n如果在过程中报错\n\n```\nError getting health report from Service Monitor.\norg.apache.avro.AvroRemoteException: java.net.ConnectException: 拒绝连接 (Connection refused)\n\n```\n\n那可能是cloudera-scm-agent没有启动好，应该先启动cloudera-scm-server，再启动cloudera-scm-agent\n\n&nbsp;\n\nGzipCodec设置默认压缩方式，参考\n\n```\nhttps://docs.cloudera.com/cdp-private-cloud-base/7.1.3/scaling-namespaces/topics/hdfs-enablegzipcodec-as-the-default-compression-codec.html\n\n```\n\n&nbsp;\n","tags":["CDH"]},{"title":"docker安装和使用","url":"/docker安装和使用.html","content":"1.安装的docker版本\n\n```\ndocker -v\nDocker version 17.03.2-ce\n\n```\n\n2.查看本地的镜像\n\n```\ndocker images\n\n```\n\n3.拉取镜像\n\n```\ndocker pull centos:7\n\n```\n\n4.编写Dockerfile\n\n```\nFROM nginx\nRUN echo '<h1>Hello, Docker!</h1>' > /usr/share/nginx/html/index.html\n\n```\n\n5.build Dockerfile\n\n```\ndocker build -t . xxx # 镜像的名字\n\n```\n\n或者\n\n```\ndocker build -f ./Dockerfile . -t xxx:xxxx\n\n```\n\nbash\n\n```\ndocker run -i -t ubuntu:15.10 /bin/bash\n\n```\n\n将容器中的8080端口映射到本机的18080端口\n\n```\ndocker run -p 18080:8080 -it xxxx:v1.0.0dev /bin/bash\n\n```\n\n如果是多个端口，使用多个-p\n\n```\n-p 18080:8080 -p 18081:8081\n\n```\n\n参数\n\n```\n-it：这是两个参数，一个是 -i：交互式操作，一个是 -t 终端。我们这里打算进入 bash 执行一些命令并查看返回结果，因此我们需要交互式终端。\n--rm：这个参数是说容器退出后随之将其删除。默认情况下，为了排障需求，退出的容器并不会立即删除，除非手动 docker rm。我们这里只是随便执行个命令，看看结果，不需要排障和保留结果，因此使用 --rm 可以避免浪费空间\n\n```\n\n6.运行\n\n```\ndocker run xxx # 镜像的名字\n\n```\n\n7.查看自己镜像的id\n\n```\ndocker ps\n\n```\n\n8.列出所有的容器，包括exit状态的容器\n\n```\ndocker ps -a\n\n```\n\n9.查看内网ip地址等运行情况\n\n```\ndocker inspect id | grep IP \n            \"LinkLocalIPv6Address\": \"\",\n            \"LinkLocalIPv6PrefixLen\": 0,\n            \"SecondaryIPAddresses\": null,\n            \"SecondaryIPv6Addresses\": null,\n            \"GlobalIPv6Address\": \"\",\n            \"GlobalIPv6PrefixLen\": 0,\n            \"IPAddress\": \"172.17.0.5\",\n            \"IPPrefixLen\": 16,\n            \"IPv6Gateway\": \"\",\n                    \"IPAMConfig\": null,\n                    \"IPAddress\": \"172.17.0.5\",\n                    \"IPPrefixLen\": 16,\n                    \"IPv6Gateway\": \"\",\n                    \"GlobalIPv6Address\": \"\",\n                    \"GlobalIPv6PrefixLen\": 0,\n\n```\n\n9.请求nginx\n\n```\ncurl 172.17.0.5:80\n<h1>Hello, Docker!</h1>\n\n```\n\n10.如果想要镜像不退出,然后进行镜像中\n\n```\n# run\nENTRYPOINT [\"/bin/bash\",\"-c\",\"cat /hosts.txt >> /etc/hosts &amp;&amp; bash /sleep.sh\"]\n\n```\n\nsleep.sh内容\n\n```\n#!/usr/bin/env bash\n\nwhile true;\ndo\n    sleep 10000;\ndone\n\n```\n\n或者\n\n```\nENTRYPOINT [\"/bin/bash\",\"-c\",\"while true; do sleep 10000; done\"]\n\n```\n\n然后执行\n\n```\ndocker run  --network=host xxxx\n\n```\n\n查看id\n\n```\ndocker ps\n\n```\n\n进入镜像中\n\n```\ndocker exec -it xxxx bash\n\n```\n\n11.删除container，先停再删\n\n```\ndocker stop 3f6822d8f262\ndocker rm 3f6822d8f262\n\n```\n\n12.使用docker命令从pod向宿主机拷贝文件\n\n```\ndocker cp pod_id:/tmp/abc /tmp/\n\n```\n\n使用docker命令从宿主机向pod中拷贝文件\n\n```\ndocker cp ./xx-xx pod_id:/home/xxx　　\n```\n\n13.**mac使用docker的时候配置仓库**\n\n```\nhttps://registry.docker-cn.com\nhttp://docker.mirrors.ustc.edu.cn\nhttp://hub-mirror.c.163.com\n\n```\n\n<img src=\"/images/517519-20200322162611354-629263110.png\" width=\"500\" height=\"573\" />\n\n注意：mac版本docker升级到高版本后Daemon选项没了，需要在docker engine中填写json配置\n\n<img src=\"/images/517519-20220606165651191-975979512.png\" width=\"500\" height=\"269\" loading=\"lazy\" />\n\n14.**ubuntu配置docker仓库**，在/etc/docker目录下新建daemon.json，内容\n\n```\n{\n  \"registry-mirrors\": [\"https://registry.docker-cn.com\",\"http://docker.mirrors.ustc.edu.cn\",\"http://hub-mirror.c.163.com\",\"https://3laho3y3.mirror.aliyuncs.com\",\"http://f1361db2.m.daocloud.io\"]\n}\n\n```\n\n重启docker服务\n\n```\nsudo systemctl restart docker\n\n```\n\n15.查看docker的磁盘使用情况\n\n```\nlintong@master:~$ docker system df\nTYPE            TOTAL     ACTIVE    SIZE      RECLAIMABLE\nImages          31        1         19.17GB   19.11GB (99%)\nContainers      1         1         0B        0B\nLocal Volumes   48        0         0B        0B\nBuild Cache     0         0         0B        0B\n\n```\n\n清理磁盘，删除关闭的容器、无用的数据卷和网络，以及dangling镜像(即无tag的镜像)\n\n```\ndocker system prune\nWARNING! This will remove:\n        - all stopped containers\n        - all networks not used by at least one container\n        - all dangling images\n        - all build cache\nAre you sure you want to continue? [y/N] y\nTotal reclaimed space: 0B\n\n```\n\n如果有2个镜像的id相同，则删除的时候会报\n\n```\ndocker rmi 85f602b72c3c\nError response from daemon: conflict: unable to delete 85f602b72c3c (must be forced) - image is referenced in multiple repositories\n\n```\n\n此时应该找到2个id相同的镜像，然后一个个进行删除\n\n```\ndocker images | grep 85f602b72c3c\nxxxxx                                   v0.9.1          85f602b72c3c   6 weeks ago     2.65GB\nxxxxx   <none>             85f602b72c3c   6 weeks ago     2.65GB\n\n```\n\n删除第一个\n\n```\ndocker rmi xxxxx:v0.9.1\n\n```\n\n再删除另一个\n\n```\ndocker rmi 85f602b72c3c\n\n```\n\n15.[使用 Docker 容器应该避免的 10 个事情](https://www.oschina.net/translate/10-things-to-avoid-in-docker-containers) \n\n```\n1.不要在容器中存储数据 &ndash;  容器可能被停止，销毁，或替换。\n2.不要将你的应用发布两份 &ndash;  一些人将容器视为虚拟机。\n3.不要创建超大镜像 &ndash; 一个超大镜像只会难以分发。\n4.不要使用单层镜像\n5.不要为运行中的容器创建镜像\n6.不要只使用&ldquo;最新&rdquo;标签\n7.不要在单一容器中运行超过一个进程\n8.不要在镜像中存储凭据\n9.使用非root用户运行进程\n10.不要依赖IP地址\n\n```\n\n参考：[10 things to avoid in docker containers](https://developers.redhat.com/blog/2016/02/24/10-things-to-avoid-in-docker-containers/)\n\n16.docker设置时区\n\n```\nENV TZ=Asia/Shanghai\n# ENV TZ=America/Los_Angeles\n\nRUN echo ${TZ} > /etc/timezone \\\n    &amp;&amp; ln -fs /usr/share/zoneinfo/${TZ} /etc/localtime \\\n    &amp;&amp; apt update \\\n    &amp;&amp; apt install -y tzdata\n\n```\n\n17.ubuntu镜像换源\n\n```\nRUN sed -i s@/archive.ubuntu.com/@/mirrors.aliyun.com/@g /etc/apt/sources.list &amp;&amp; \\\n\tsed -i s@/security.ubuntu.com/@/mirrors.aliyun.com/@g /etc/apt/sources.list &amp;&amp; \\\n\tapt-get clean &amp;&amp; \\\n\tapt-get update\n\n```\n","tags":["docker"]},{"title":"Thrift关键字","url":"/Thrift关键字.html","content":"在编译thrift文件的时候发现报了如下的错误\n\n```\nCannot use reserved language keyword: \"class\"\n\n```\n\n后来查了一下,发现class是thrift的关键字之一,变量起名的时候不能和关键字重复\n\nthrift的全部关键字可以查看thrift的源码\n\n```\nhttps://github.com/apache/thrift/blob/master/compiler/cpp/src/thrift/generate/t_py_generator.cc\n\n```\n\n搜索keywords,下面这些都是thrift关键字,在起名的时候需要注意\n\n<img src=\"/images/517519-20190327100917790-236207791.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;以及\n\n```\nhttps://github.com/apache/thrift/tree/master/compiler/cpp/src/thrift/generate/t_generator.cc\n\n```\n\n关键字\n\n<img src=\"/images/517519-20190918173127840-1933290753.png\" alt=\"\" />\n\n&nbsp;\n","tags":["Thrift"]},{"title":"Parquet格式解析","url":"/Parquet格式解析.html","content":"parquet是列式存储格式，官方文档\n\n```\nhttps://parquet.apache.org/documentation/latest/\n\n```\n\n<!--more-->\n&nbsp;一个Parquet文件是**由一个header以及一个或多个block块组成，以一个footer结尾**。\n\n**header**中只包含一个4个字节的数字PAR1用来识别整个Parquet文件格式，PAR1的ASCII码是[80, 65, 82, 49]，所以如果使用读取parquet文件的时候，如果magic number不对的话，会报如下错误\n\n```\nexpected magic number at tail [80, 65, 82, 49] but found [xx, xx, xx, xx]\n\n```\n\n&nbsp;\n\n文件中**所有的metadata都存在于footer中**。**footer**中的metadata包含了格式的版本信息，schema信息、key-value paris以及所有block中的metadata信息。footer中最后两个字段为一个以4个字节长度的footer的metadata,以及同header中包含的一样的PAR1。\n\n在Parquet文件中，每一个block都具有一组Row group,它们是由一组Column chunk组成的列数据。继续往下，每一个column chunk中又包含了它具有的pages。每个page就包含了来自于相同列的值\n\n<img src=\"/images/517519-20190109170718876-1163668207.jpg\" alt=\"\" />\n\nParquet 文件在磁盘上的分布情况如下图所示：\n\n<img src=\"/images/517519-20210407164507257-1677080847.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n所有的数据被水平切分成 Row group，一个 Row group 包含这个 Row group 对应的区间内的所有列的 column chunk。\n\n一个 column chunk 负责存储某一列的数据，这些数据是这一列的 Repetition levels, Definition levels 和 values。\n\n一个 column chunk 是由 Page 组成的，Page 是压缩和编码的单元，对数据模型来说是透明的。\n\n一个 Parquet 文件最后是 Footer，存储了文件的元数据信息和统计信息。Row group 是数据读写时候的缓存单元，所以推荐设置较大的 Row group 从而带来较大的并行度，当然也需要较大的内存空间作为代价。一般情况下推荐配置一个 Row group 大小 1G，一个 HDFS 块大小 1G，一个 HDFS 文件只含有一个块。\n\n&nbsp;\n\n**列式存储的优势**：\n\n1.可以跳过不符合条件的数据，只读取需要的数据，降低 IO 数据量。\n\n2.压缩编码可以降低磁盘存储空间。由于同一列的数据类型是一样的，可以使用更高效的压缩编码（例如 Run Length Encoding 和 Delta Encoding）进一步节约存储空间。\n\n3.只读取需要的列，支持向量运算，能够获取更好的扫描性能。\n\n参考：[深入分析 Parquet 列式存储格式](https://www.infoq.cn/article/in-depth-analysis-of-parquet-column-storage-format)\n\n&nbsp;\n\n[**游程编码（Run Length Encoding）**](https://zh.wikipedia.org/wiki/%E6%B8%B8%E7%A8%8B%E7%BC%96%E7%A0%81)\n\n场景：重复数据\n\n举例来说，一组资料串\"AAAABBBCCDEEEE\"，由4个A、3个B、2个C、1个D、4个E组成，经过**变动长度编码法**可将资料压缩为4A3B2C1D4E（由14个单位转成10个单位）。\n\n[**差分编码（Delta encoding）**](https://zh.wikipedia.org/wiki/%E5%B7%AE%E5%88%86%E7%B7%A8%E7%A2%BC)\n\n场景：有序数据集，例如 timestamp，自动生成的 ID，以及监控的各种 metrics\n\n差异存储在称为&ldquo;delta&rdquo;或&ldquo;diff&rdquo;的不连续文件中。由于改变通常很小（平均占全部大小的2%），差分编码能大幅减少资料的重复。一连串独特的delta文件在空间上要比未编码的相等文件有效率多了。\n\n差分编码的简单例子是存储序列式资料之间的差异（而不是存储资料本身）：不存&ldquo;2, 4, 6, 9, 7&rdquo;，而是存&ldquo;2, 2, 2, 3, -2&rdquo;。单独使用用处不大，但是在序列式数值常出现时可以帮助压缩资料\n\n**[字典编码器 (Dictionary coder)](https://baike.baidu.com/item/%E8%AF%8D%E5%85%B8%E7%BC%96%E7%A0%81/4097538)**\n\n场景：小规模的数据集合，例如 IP 地址\n\n词典编码是指用符号代替一串字符，在编码中仅仅把字符串看成是一个号码，而不去管它来表示什么意义，1977年由两位以色列教授发明，1985年美国Wekch对该算法进行了改进。\n\n&nbsp;\n\n其他资料：[获得parquet文件的schema 合并parquet小文件](https://blog.csdn.net/woloqun/article/details/80776233)\n\n&nbsp;\n","tags":["parquet"]},{"title":"ubuntu安装thrift","url":"/ubuntu安装thrift.html","content":"ubuntu环境下安装thrift-0.10.0\n\n1.解压\n\n2.编译安装\n\n```\n./configure -with-cpp -with-boost -without-python -without-csharp -with-java -without-erlang -without-perl -without-php -without-php_extension -without-ruby -without-haskell -without-go\nmake\nsudo make install\n\n```\n\n3.是否安装成功\n\n```\nthrift -version\nThrift version 0.10.0\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["Thrift"]},{"title":"xxl-job安装教程","url":"/xxl-job安装教程.html","content":"**xxl-job**是一个开源的分布式调度框架,其他类似的框架还有airflow,oozie等等,需要进行对比\n\n```\nhttps://github.com/xuxueli/xxl-job\n\n```\n\n**1.首先git clone工程**\n\n```\ngit clone git@github.com:xuxueli/xxl-job.git\n\n```\n\n打包工程,打包的过程中会下载所需要的jar包\n\n```\nmvn package\n\n```\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20181222154948560-1702427234.png\" alt=\"\" />\n\n**2.在idea中打开工程**\n\n<img src=\"/images/517519-20181222155010490-467987189.png\" alt=\"\" />\n\n需要修改一下logback.xml和properties中日志路径,然后运行工程\n\n**3.初始化数据库**\n\n```\nsource /home/lintong/coding/java/xxl-job/doc/db/tables_xxl_job.sql\n\n```\n\n并修改application.properties中的数据库的用户密码\n\n**4.运行web工程**\n\n<img src=\"/images/517519-20181222155727269-1426298240.png\" alt=\"\" />\n\n**5.访问地址**\n\n```\nhttp://localhost:8080/xxl-job-admin\n\n```\n\n&nbsp;<img src=\"/images/517519-20181222155856671-1704783516.png\" alt=\"\" />\n\n默认的账号密码admin 123456\n\n<img src=\"/images/517519-20181222155946225-1017920283.png\" alt=\"\" />\n\n**5.运行执行器工程**\n\n<img src=\"/images/517519-20181222162958600-1883236164.png\" alt=\"\" />\n\n在启动了执行器之后,你就会发现上面的图中的执行器数量从0变成1\n\n<img src=\"/images/517519-20181222163035359-702584605.png\" alt=\"\" />\n\n&nbsp;\n","tags":["Linux"]},{"title":"Elasticsearch学习笔记——常用命令","url":"/Elasticsearch学习笔记——常用命令.html","content":"1.创建索引，名字为index\n\n```\ncurl -XPUT http://localhost:9200/index\n\n```\n\n2.创建一个mapping\n\n```\ncurl -XPOST http://localhost:9200/index/fulltext/_mapping -H 'Content-Type:application/json' -d'\n{\n        \"properties\": {\n            \"content\": {\n                \"type\": \"text\",\n                \"analyzer\": \"ik_max_word\",\n                \"search_analyzer\": \"ik_max_word\"\n            }\n        }\n\n}'\n\n```\n\n3.查看mapping\n\n```\ncurl -XPUT http://localhost:9200/xxx/yyy/_mapping\n\n```\n\n4.删除一个文档,按照id来删除\n\n```\ncurl -XDELETE 'http://localhost:9200/index3/fulltext3/272'\n\n```\n\n5.通过query来删除文档\n\n不同版本之间的es不太一样，6.2的参考\n\n```\nhttps://www.elastic.co/guide/en/elasticsearch/reference/6.2/docs-delete-by-query.html\n\n```\n\n比如使用kibana里面的dev tool，就可以删掉所有schema字段是&ldquo;xxxx&rdquo;的数据\n\n```\nPOST xxxxx_2019-12-09/_delete_by_query\n{\n  \"query\": { \n    \"match\": {\n      \"schema\": \"xxxx\"\n    }\n  }\n}\n\n```\n\n6.es的task api，参考\n\n```\nhttp://xiaorui.cc/archives/3089\n\n```\n\n7.scroll查看数据，from+size查询最多只能查10000\n\n参考：https://www.elastic.co/guide/en/elasticsearch/reference/6.8/search-request-scroll.html\n\n```\ncurl -XPOST -H 'Content-Type: application/json' http://localhost:9200/_search/scroll -d@data.json\n\n```\n\ndata.json\n\n```\n{\n    \"scroll\" : \"1m\",\n    \"scroll_id\" : \"xxxxxxxx\"\n}\n```\n\n8.删除一个索引\n\n```\ncurl -XDELETE http://ip:port/xxxx\n\n```\n\n　　\n\n<!--more-->\n&nbsp;\n","tags":["ELK"]},{"title":"Elasticsearch学习笔记——分词","url":"/Elasticsearch学习笔记——分词.html","content":"**1.测试Elasticsearch的分词**\n\nElasticsearch有多种分词器（参考:https://www.jianshu.com/p/d57935ba514b）\n\nSet the shape to semi-transparent by calling set_trans(5)\n\n（1）standard analyzer：标准分词器（默认是这种）<br />\nset,the,shape,to,semi,transparent by,calling,set_trans,5\n\n（2）simple analyzer：简单分词器<br />\nset, the, shape, to, semi, transparent, by, calling, set, trans\n\n（3）whitespace analyzer：空白分词器。大小写，下划线等都不会转换<br />\nSet, the, shape, to, semi-transparent, by, calling, set_trans(5)\n\n（4）language analyzer：（特定语言分词器，比如说English英语分词器）<br />\nset, shape, semi, transpar, call, set_tran, 5\n\n<!--more-->\n&nbsp;\n\n**2.为Elasticsearch的index设置分词**\n\n这样就将这个index里面的所有type的分词设置成了simple\n\n```\nPUT my_index\n{\n\"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\"default\":{\"type\":\"simple\"}}\n    }\n  }\n}\n\n```\n\n&nbsp;\n\n**标准分词器 : standard analyzer**\n\nes5进行分词测试\n\n```\nhttp://localhost:9200/_analyze?analyzer=standard&amp;pretty=true&amp;text=test测试\n\n```\n\nes6进行分词测试\n\n```\ncurl -H 'Content-Type: application/json' http://localhost:9200/_analyze?pretty=true -d@data.json\n\n```\n\ndata.json\n\n```\n{\n        \"analyzer\":\"standard\",\n        \"text\": \"test测试\"\n}\n\n```\n\n分词结果都是\n\n```\n{\n  \"tokens\" : [\n    {\n      \"token\" : \"test\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 4,\n      \"type\" : \"<ALPHANUM>\",\n      \"position\" : 0\n    },\n    {\n      \"token\" : \"测\",\n      \"start_offset\" : 4,\n      \"end_offset\" : 5,\n      \"type\" : \"<IDEOGRAPHIC>\",\n      \"position\" : 1\n    },\n    {\n      \"token\" : \"试\",\n      \"start_offset\" : 5,\n      \"end_offset\" : 6,\n      \"type\" : \"<IDEOGRAPHIC>\",\n      \"position\" : 2\n    }\n  ]\n}\n\n```\n\n**简单分词器 : simple analyzer**\n\n```\nhttp://localhost:9200/_analyze?analyzer=simple&amp;pretty=true&amp;text=test_测试\n\n```\n\n&nbsp;结果\n\n```\n{\n  \"tokens\" : [\n    {\n      \"token\" : \"test\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 4,\n      \"type\" : \"word\",\n      \"position\" : 0\n    },\n    {\n      \"token\" : \"测试\",\n      \"start_offset\" : 5,\n      \"end_offset\" : 7,\n      \"type\" : \"word\",\n      \"position\" : 1\n    }\n  ]\n}\n\n```\n\n**IK分词器 : `ik_max_word <strong>analyzer **和 <code>ik_smart&nbsp;**analyzer**`</code><br /></strong>\n\n首先需要安装\n\n```\nhttps://github.com/medcl/elasticsearch-analysis-ik\n\n```\n\n下zip包,然后使用install plugin进行安装,我机器上的es版本是5.6.10,所以安装的就是5.6.10\n\n```\n./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v5.6.10/elasticsearch-analysis-ik-5.6.10.zip\n\n```\n\n然后重新启动Elasticsearch就可以了\n\n进行测试，es5\n\n```\nhttp://localhost:9200/_analyze?analyzer=ik_max_word&amp;pretty=true&amp;text=test_tes_te测试\n\n```\n\nes6\n\n```\ncurl -H 'Content-Type: application/json' http://localhost:9200/_analyze?pretty=true -d@data.json\n\n```\n\ndata.json\n\n```\n{\n        \"analyzer\":\"ik_max_word\",\n        \"text\": \"test测试\"\n}\n\n```\n\n结果\n\n```\n{\n  \"tokens\" : [\n    {\n      \"token\" : \"test_tes_te\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 11,\n      \"type\" : \"LETTER\",\n      \"position\" : 0\n    },\n    {\n      \"token\" : \"test\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 4,\n      \"type\" : \"ENGLISH\",\n      \"position\" : 1\n    },\n    {\n      \"token\" : \"tes\",\n      \"start_offset\" : 5,\n      \"end_offset\" : 8,\n      \"type\" : \"ENGLISH\",\n      \"position\" : 2\n    },\n    {\n      \"token\" : \"te\",\n      \"start_offset\" : 9,\n      \"end_offset\" : 11,\n      \"type\" : \"ENGLISH\",\n      \"position\" : 3\n    },\n    {\n      \"token\" : \"测试\",\n      \"start_offset\" : 11,\n      \"end_offset\" : 13,\n      \"type\" : \"CN_WORD\",\n      \"position\" : 4\n    }\n  ]\n}\n\n```\n\n&nbsp;\n","tags":["ELK"]},{"title":"Ubuntu下安装antlr-4.7.1","url":"/Ubuntu下安装antlr-4.7.1.html","content":"简介:antlr工具将语法文件转换成可以识别该语法文件所描述的语言的程序.\n\n例如:给定一个识别json的语法,antlr工具将会根据该语法生成一个程序,该程序可以通过antlr运行库来识别输入的json.\n\n<!--more-->\n&nbsp;\n\n1.下载jar包，antlr-4.7.1-complete.jar\n\n```\nhttp://www.antlr.org/download/\n\n```\n\n&nbsp;<img src=\"/images/517519-20181102151552944-627948375.png\" alt=\"\" />\n\n2.将这个jar包移动到 /usr/local/lib 目录下\n\n<img src=\"/images/517519-20181102151704451-461111899.png\" alt=\"\" />\n\n3.修改 ~.bashrc 文件\n\n```\n#Java\nexport JAVA_HOME=/usr/local/jdk1.8.0_121\nexport JRE_HOME=${JAVA_HOME}/jre\nexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib:/usr/local/lib/antlr-4.7.1-complete.jar\nexport PATH=${JAVA_HOME}/bin:$PATH\nalias antlr4='java -Xmx500M -cp \"/usr/local/lib/antlr-4.7.1-complete.jar:$CLASSPATH\" org.antlr.v4.Tool'\nalias grun='java org.antlr.v4.runtime.misc.TestRig'\n\n```\n\n其中的TestRig是一个antlr在运行库中提供的一个调试工具,它可以详细列出一个语言类应用程序在匹配输入文本过程中的信息,这些输入文本可以来自文件或者标准输入.TestRig使用java的反射机制来调用编译后的识别程序,这里使用grun作为别名.\n\n4. source ~/.bashrc\n\n5. 在idea中安装antlr插件\n\n<img src=\"/images/517519-20181102152056996-654138298.png\" alt=\"\" />\n\n6. 建立一个 Hello.g4 文件进行测试\n\n```\n//Define a grammar called Hello\ngrammar Hello;\nr : 'hello' ID; // match keyword hello followed by an identifier\nID : [a-z]+; // match lower-case identifiers\nWS : [ \\t\\r\\n]+ -> skip; // skip spaces, tabs, newlines\n\n```\n\n7. 生成java文件和编译java文件\n\n```\nantlr4 Hello.g4\njavac Hello*.java\n\n```\n\n&nbsp;<img src=\"/images/517519-20181102152000526-1578077527.png\" alt=\"\" />\n\n**xxxParser.java** 该文件包含一个语法分析器类的定义,这个语法分析器专门用来识别语法xxx的.\n\n在该类中,每条规则都有对应的方法,此外还有一些辅助代码\n\n**xxxLexer.java** 该文件包含的是词法分析器的类定义,它是由antlr通过分析词法规则,以及语法中的字面值'{' ',' '}'等生成的.词法分析器的作用是将输入字符序列分解成词汇符号\n\n**xxx.tokens** antlr会给每个我们定义的词法符号指定一个数字形式的类型,然后将它们的对应关系存储于该文件中.\n\n**xxxListener.java xxxBaseListener.java** 在遍历语法分析树的时候,遍历器能够触发一系列事件(回调),并通知我们提供的监听器对象.xxxListener接口给出了这些回调方法的定义,我们可以实现它来完成自定义的功能.\n\n&nbsp;\n\n<img src=\"/images/517519-20181104224543884-539639076.png\" alt=\"\" />\n\n&nbsp;\n\n&nbsp;8.查看结果，注意输入hello world之后要ctrl+D\n\n其他参数详情请参考ANTLR4权威指南28页\n\n```\n - tokens #打印出token流\n - tree   #用LISP表单打印出解析树\n - gui    #在对话框中可视化地展示解析树\n\n```\n\ntokens 打印出词法符号流\n\n```\ngrun  Hello r -tokens\nWarning: TestRig moved to org.antlr.v4.gui.TestRig; calling automatically\nhello world\n[@0,0:4='hello',<'hello'>,1:0]\n[@1,6:10='world',<ID>,1:6]\n[@2,12:11='<EOF>',<EOF>,2:0]\n\n```\n\ntree 以LISP格式打印出词法分析树\n\n```\ngrun Hello r -tree\nWarning: TestRig moved to org.antlr.v4.gui.TestRig; calling automatically\nhello world\n(r hello world)\n\n```\n\ngui 显示语法分析树\n\n```\ngrun Hello r -gui\nWarning: TestRig moved to org.antlr.v4.gui.TestRig; calling automatically\nhello world\n\n```\n\n&nbsp;<img src=\"/images/517519-20181102153932666-339938059.png\" alt=\"\" />\n\n&nbsp;antlr提供两种遍历语法分析树的方式:**1.监听器** 和 **2.访问者模式**\n\n&nbsp;\n\n在**antlr的jar包**中,有两个关键部分:**1.antlr工具** 和 **2.antlr运行库(运行时语法分析)api**\n\n**antlr工具**:使用org.antlr.v3.Tool类来生成一些代码(语法分析器和词法分析器)\n\n**antlr运行库**:是一个由若干类和方法组成的库,这些类和方法是自动生成的代码(如parse,lexer和token)运行所必须的<br />&nbsp;\n","tags":["antlr"]},{"title":"Flink学习笔记——读写hudi","url":"/Flink学习笔记——读写hudi.html","content":"使用flink来读写hudi有2种API，一个是Flink SQL API，另一个是DataStream API，参考\n\n```\nhttps://hudi.apache.org/cn/docs/flink-quick-start-guide\n```\n\n## 1.Flink SQL API\n\n首先启动yarn session\n\n```\n/usr/lib/flink/bin/yarn-session.sh -n 3 -s 5 -jm 1024 -tm 4096 -d\n\n```\n\n使用SQL API提交任务到YARN上的方式有以下几种：\n\n### 1.使用交互式的sql client\n\n不过由于sql client目前处于beta版本，所以建议用于原型验证，不建议在生产环境中使用，参考：[Apache Flink 零基础入门（四）：客户端操作的 5 种模式](https://developer.aliyun.com/article/709475)\n\n```\nhttps://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/sqlclient/\n\n```\n\n启动\n\n```\n/usr/lib/flink/bin/sql-client.sh\n\n```\n\n<img src=\"/images/517519-20230308215908824-235391100.png\" width=\"500\" height=\"193\" loading=\"lazy\" />\n\n首先使用Flink SQL创建hudi表\n\n```\nFlink SQL> CREATE TABLE hudi_test_table(\n            _id STRING,\n            xxx STRING,\n            primary key(_id) not enforced\n        ) WITH (\n            'connector' = 'hudi',\n            'path' = 's3a://xxxx/hudi_test_table',\n            'table.type' = 'MERGE_ON_READ',\n            'changelog.enabled'= 'true',\n            'compaction.async.enabled'='true',\n            'compaction.tasks'= '4',\n            'compaction.trigger.strategy'= 'time_elapsed',\n            'compaction.delta_seconds'= '600',\n            'compaction.max_memory'= '1024',\n            'write.option' = 'upsert',\n            'read.streaming.check-interval'= '3',\n            'hive_sync.enable' = 'true',\n            'hive_sync.mode' = 'hms',\n            'hive_sync.metastore.uris' = 'thrift://xxx:9083',\n            'hive_sync.table'='hudi_test_table',\n            'hive_sync.db'='default',\n            'write.tasks'='4'\n        );\n\n```\n\n查询该hudi表\n\n```\nFlink SQL> select * from hudi_test_table limit 10;\n\n```\n\n查询结果\n\n<img src=\"/images/517519-20230418144035519-1571562662.png\" alt=\"\" loading=\"lazy\" />\n\n### 2.编写flink job，使用StreamExecutionEnvironment\n\n在生产环境中如果使用FlinkSQL，建议使用<!--more-->\n&nbsp;StreamExecutionEnvironment，参考\n\n```\nhttps://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/queries/overview/\n```\n\n&nbsp;\n\n## 2.Flink DataStream API\n\n首先同样是启动yarn session\n\n&nbsp;\n","tags":["Flink","Hudi"]},{"title":"wherehows踩坑记录","url":"/wherehows踩坑记录.html","content":"wherehows是Linkedin开源的大数据治理框架,提供了元数据管理,数据血缘,数据预览,集成多种数据源的功能,最近在进行调研工作\n\n类似的框架有Netflix的metacat,这个两个开源项目都是坑不少,目前还在踩坑阶段中...由于网上关于这两个项目的文章有价值,本文希望能对你有帮助\n\n其他公司也有很多类似的数据发现系统，参考：[Data Discovery Platforms and Their Open Source Solutions](https://eugeneyan.com/writing/data-discovery-platforms/)\n\n<!--more-->\n&nbsp;\n\n首先的坑是项目文档不统一的问题,项目首页的文档和get-start的文档不一致,get-start的文档比较首页的文档步骤要多不少\n\n```\nhttps://github.com/linkedin/WhereHows\n\n```\n\n&nbsp;\n\n```\nhttps://github.com/linkedin/WhereHows/blob/master/wherehows-docs/getting-started.md\n\n```\n\n&nbsp;wherehows提供了VM和quick start版本,我根本就没有去看这个版本,直接尝试了从源码编译\n\n1.官方的guide第一步是让你创建mysql的用户和密码\n\n```\nCREATE DATABASE wherehows\n  DEFAULT CHARACTER SET utf8\n  DEFAULT COLLATE utf8_general_ci;\n\nCREATE USER 'wherehows';\nSET PASSWORD FOR 'wherehows' = PASSWORD('wherehows');\nGRANT ALL ON wherehows.* TO 'wherehows';\n\n```\n\n<img src=\"/images/517519-20180918160141962-999616780.png\" alt=\"\" />\n\n去workbench中查看一下 users and privileges , 多了wherehows用户\n\n<img src=\"/images/517519-20180918160359689-263684474.png\" alt=\"\" width=\"548\" height=\"277\" />\n\n2.第二步,从github上git clone源代码,我git的是master分支\n\n```\ngit@github.com:linkedin/WhereHows.git\n\n```\n\n3.在刚刚创建的wherehows database中做一些初始化的操作\n\nwherehows这个database还是空的,官方的教程是 Execute the [DDL files](https://github.com/linkedin/WhereHows/tree/master/wherehows-data-model/DDL) to create the required repository tables in **wherehows** database.\n\n就是让你执行 wherehows-data-model/DDL 目录下的脚本来创建表以及插入一些数据\n\n到这个子项目中查看一下 READMR文档 ,执行下面的代码来批量创建表和插入数据\n\n```\ncd WhereHows/wherehows-data-model/DDL\nmysql -hlocalhost -uwherehows -pwherehows -Dwherehows < create_all_tables_wrapper.sql\n\n```\n\n成功的输出\n\n```\nlintong@master:~/coding/java/WhereHows/wherehows-data-model/DDL$ mysql -hlocalhost -uwherehows -pwherehows -Dwherehows < create_all_tables_wrapper.sql\nmysql: [Warning] Using a password on the command line interface can be insecure.\nTables_in_wherehows\ncfg_application\ncfg_cluster\ncfg_data_center\ncfg_database\ncfg_deployment_tier\ncfg_job_type\ncfg_job_type_reverse_map\ncfg_object_name_map\ncfg_search_score_boost\ncomments\ndataset_capacity\ndataset_case_sensitivity\ndataset_compliance\ndataset_constraint\ndataset_deployment\ndataset_index\ndataset_inventory\ndataset_owner\ndataset_partition\ndataset_partition_layout_pattern\ndataset_reference\ndataset_schema_info\ndataset_tag\ndict_business_metric\ndict_dataset\ndict_dataset_field_comment\ndict_dataset_instance\ndict_dataset_sample\ndict_dataset_schema_history\ndict_field_detail\ndir_external_group_user_map\ndir_external_group_user_map_flatten\ndir_external_user_info\nfavorites\nfield_comments\nfilename_pattern\nflow\nflow_dag\nflow_execution\nflow_execution_id_map\nflow_job\nflow_owner_permission\nflow_schedule\nflow_source_id_map\njob_attempt_source_code\njob_execution\njob_execution_data_lineage\njob_execution_ext_reference\njob_execution_id_map\njob_execution_script\njob_source_id_map\nlog_dataset_instance_load_status\nlog_lineage_pattern\nlog_reference_job_id_pattern\nsource_code_commit_info\nstg_cfg_object_name_map\nstg_database_scm_map\nstg_dataset_owner\nstg_dataset_owner_unmatched\nstg_dict_business_metric\nstg_dict_dataset\nstg_dict_dataset_field_comment\nstg_dict_dataset_instance\nstg_dict_dataset_sample\nstg_dict_field_detail\nstg_dir_external_group_user_map\nstg_dir_external_group_user_map_flatten\nstg_dir_external_user_info\nstg_flow\nstg_flow_dag\nstg_flow_dag_edge\nstg_flow_execution\nstg_flow_job\nstg_flow_owner_permission\nstg_flow_schedule\nstg_git_project\nstg_job_execution\nstg_job_execution_data_lineage\nstg_job_execution_ext_reference\nstg_product_repo\nstg_repo_owner\nstg_source_code_commit_info\ntrack_object_access_log\nuser_login_history\nuser_settings\nusers\nwatch\nwh_etl_job_history\nwh_etl_job_schedule\n\n```\n\n4.由于这个项目是gradle构建的,所以需要使用gradle来编译\n\n```\nlintong@master:~/coding/java/WhereHows$ ./gradlew build\n\n```\n\n在编译的过程中,在编译子项目wherehows-etl的时候报错了\n\n<img src=\"/images/517519-20180918162741142-796651181.png\" alt=\"\" />\n\n原来是少了jar包,再次去看README文档,官方说\n\n&nbsp;<img src=\"/images/517519-20180918162850289-1296854716.png\" alt=\"\" />\n\n需要在 wherehows-etl/extralibs 目录下添加一些第三方的jar包,这些jar是在maven仓库下载不到的\n\n我首先看文档,下载了 gsp.jar 这个jar包, 下载地址,解压了之后将 gsp.jar 放在 extralibs 目录下\n\n```\nhttp://www.sqlparser.com/dl/gsp_java_trial_1_9_4_4.zip\n\n```\n\n&nbsp;<img src=\"/images/517519-20180918163330514-1229039973.png\" alt=\"\" />\n\n然后再次编译试试,如果编译之中下载jar包比较慢的话,我会直接wget下载这个jar包,然后丢到~/.m2/repository下面各个不同jar包的本地仓库中\n\n最终编译成功\n\n<img src=\"/images/517519-20180918164041498-646716574.png\" alt=\"\" />\n\n5.然后按照官方的文档,打开两个终端,分别启动 wherehows 的后端和前端组件\n\n```\nlintong@master:~/coding/java/WhereHows$ ./gradlew wherehows-backend:runPlayBinary\nlintong@master:~/coding/java/WhereHows$ ./gradlew wherehows-frontend:runPlayBinary\n\n```\n\n然后访问 localhost:9001看web界面,第一次当然是不可能成功的\n\n<img src=\"/images/517519-20180918164806018-679579753.png\" alt=\"\" />\n\n&nbsp;至此,wherehows首页的readme已经走不通了,继续参考get-start的文档\n\n```\nhttps://github.com/linkedin/WhereHows/blob/master/wherehows-docs/getting-started.md\n\n```\n\n文档中说需要继续添加第3放的jar,需要添加的jar包地址,我只加了这两个,另外两个Oracle的jar我没有添加\n\n```\nhttps://github.com/linkedin/WhereHows/tree/master/wherehows-etl/extralibs\n\n```\n\n&nbsp;<img src=\"/images/517519-20180918170011544-131092010.png\" alt=\"\" />\n\n按照文档还需要创建一些mysql用户\n\n<img src=\"/images/517519-20180918171214495-1531452962.png\" alt=\"\" width=\"786\" height=\"237\" />\n\n注意这里官方文档写错了,不是 GRANT ALL ON wherehows.* TO 'wherehows'@'wherehows'; 而是 GRANT ALL ON wherehows.* TO 'wherehows'@'localhost';\n\n```\nCREATE USER 'wherehows'@'localhost' IDENTIFIED BY 'wherehows';\nGRANT ALL ON wherehows.* TO 'wherehows'@'localhost';\n\nCREATE USER 'wherehows_ro'@'localhost' IDENTIFIED BY 'readmetadata';\nCREATE USER 'wherehows_ro'@'%' IDENTIFIED BY 'readmetadata';\nGRANT SELECT ON wherehows.* TO 'wherehows_ro'@'localhost';\nGRANT SELECT ON wherehows.* TO 'wherehows_ro'@'%';\n\n```\n\n&nbsp;结果成功\n\n```\nQuery OK, 0 rows affected (0.00 sec)\n\n```\n\n&nbsp;文档中说还需要初始化一些表,不太清楚和刚开始的有没有区别,再初始化一遍\n\n```\ncd ~/coding/java/WhereHows/wherehows-data-model/DDL\n进入mysql\nmysql> use wherehows\nReading table information for completion of table and column names\nYou can turn off this feature to get a quicker startup with -A\n\nDatabase changed\nmysql> source /home/lintong/coding/java/WhereHows/wherehows-data-model/DDL/create_all_tables_wrapper.sql\n\n```\n\n6.初始化es的索引\n\nwherehows中数据集的搜索需要用到es,暂时先跳过\n\n7.接着来安装wherehows的前端\n\n上面报的错的原因,原来是由于需要修改application.conf配置文件去 match your environment\n\n<img src=\"/images/517519-20180918172257917-1704816639.png\" alt=\"\" width=\"739\" height=\"364\" />\n\n但是怎么做修改,在getting-start文档中缺没有说明白,所以只能去看子项目 wherehows-frontend 的README文档\n\n&nbsp;\n\n界面\n\n<img src=\"/images/517519-20190730203400328-1038617657.png\" alt=\"\" />\n\n搜索\n\n<img src=\"/images/517519-20190730203451448-812052909.png\" alt=\"\" />\n\n浏览功能\n\n<img src=\"/images/517519-20190730203734559-1762271685.png\" alt=\"\" />\n\n&nbsp;\n\n&nbsp;\n\n数据集页面\n\n<img src=\"/images/517519-20190730203540079-1210217112.png\" alt=\"\" />\n\n&nbsp;Schema\n\n<img src=\"/images/517519-20190730203842570-2056626420.png\" alt=\"\" />\n\n属性页面\n\nhive的数据集详情页\n\n<img src=\"/images/517519-20190730203906454-1941653099.png\" alt=\"\" />\n\n&nbsp;hdfs的数据集详情页\n\n<img src=\"/images/517519-20190731163634404-1617461368.png\" alt=\"\" />\n","tags":["Linkedin"]},{"title":"elephant-bird学习笔记","url":"/elephant-bird学习笔记.html","content":"elephant-bird是Twitter的开源项目，项目的地址为 [https://github.com/twitter/elephant-bird](https://github.com/twitter/elephant-bird)\n\n该项目是Twitter为LZO,thrift,protocol buffer相关的hadoop InputFormats, OutputFormats, Writables, Pig加载函数, Hive SerDe, HBase二级索引等编写的库\n\n```\nmvn clean install -U -Dprotobuf.version=2.5.0 -DskipTests=true\n\n```\n\nmvn package的时候需要签名\n\n```\ngpg --gen-key\n\n```\n\n以及需要安装apache Thrift和Protocol Buffers\n\nthrift安装参考\n\n```\nhttps://www.cnblogs.com/tonglin0325/p/10190050.html\n\n```\n\nPB安装参考\n\n```\nhttps://www.cnblogs.com/tonglin0325/p/13685527.html\n\n```\n\n<!--more-->\n&nbsp;\n\n使用elephant-bird来建hive表的类型对应关系\n\n该表的表结构不用显示定义，将会自动从'serialization.class'='com.xxx.xxx.xxx'中自动反序列化出来\n\n参考：https://github.com/twitter/elephant-bird/wiki/How-to-use-Elephant-Bird-with-Hive\n\n<img src=\"/images/517519-20200929101124871-1025250140.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;建表之后show create table的结果\n\n```\nCREATE EXTERNAL TABLE `xxxx`(\n\t  `ts` string COMMENT 'from deserializer', \n\t  `schema` string COMMENT 'from deserializer', \n\t  `test_string` string COMMENT 'from deserializer', \n\t  `test_long` bigint COMMENT 'from deserializer', \n\t  `test_int` int COMMENT 'from deserializer', \n\t  `test_short` smallint COMMENT 'from deserializer', \n\t  `test_double` double COMMENT 'from deserializer', \n\t  `test_byte` tinyint COMMENT 'from deserializer', \n\t  `test_bool` boolean COMMENT 'from deserializer', \n\t  `test_list` array<string> COMMENT 'from deserializer', \n\t  `test_set` array<bigint> COMMENT 'from deserializer', \n\t  `test_map` map<string,int> COMMENT 'from deserializer')\n\tCOMMENT 'test_all_type'\n\tPARTITIONED BY ( \n\t  `ds` string COMMENT '日期分区')\n\tROW FORMAT SERDE \n\t  'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer' \n\tWITH SERDEPROPERTIES ( \n\t  'serialization.class'='com.xxx.xxx.xxx', \n\t  'serialization.format'='org.apache.thrift.protocol.TCompactProtocol') \n\tSTORED AS INPUTFORMAT \n\t  'org.apache.hadoop.mapred.SequenceFileInputFormat' \n\tOUTPUTFORMAT \n\t  'org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'\n\tLOCATION\n\t  'hdfs://xxxxxxx'\n\tTBLPROPERTIES (\n\n```\n\n&nbsp;\n","tags":["twitter"]},{"title":"Glide和Govendor安装和使用","url":"/Glide和Govendor安装和使用.html","content":"**下面介绍几种go的包管理工具，推荐使用go mod**\n\n## **1.go mod**\n\n**参考：**[go学习笔记&mdash;&mdash;引入依赖](https://www.cnblogs.com/tonglin0325/p/5321328.html)\n\n## **2.Glide**\n\n**参考**：[golang 依赖管理](https://segmentfault.com/a/1190000013016957)\n\n/etc/profile\n\n```\n#Go\nexport GOROOT=/home/lintong/software/go\nexport PATH=$PATH:$GOROOT/bin:$GOPATH/bin\nexport GOPATH=/home/lintong/software/gopath\nexport GOBIN=$GOROOT/bin\n\n```\n\nLinux下安装\n\n```\ncurl https://glide.sh/get | sh\n\n```\n\nMac下安装\n\n```\nbrew install glide\n\n```\n\n初始化\n\n```\nglide init\n\n```\n\n依赖下载\n\n```\nglide update\n\n```\n\n然后就能编译原来不能编译的Go工程了\n\n<!--more-->\n&nbsp;\n\n如果遇到不能拉下依赖的情况,比如\n\n```\n[WARN]  Unable to checkout golang.org/x/sys/unix\n[ERROR] Error looking for golang.org/x/sys/unix: Cannot detect VCS\n\n```\n\n是因为墙的问题,需要在/etc/profile中设置一下代理，然后source一下\n\n```\n#Proxy\nexport http_proxy=socks5://127.0.0.1:xxxx\nexport https_proxy=$http_proxy\nexport ftp_proxy=$http_proxy\nexport rsync_proxy=$http_proxy\nexport no_proxy=\"localhost,127.0.0.1,localaddress,.localdomain.com\"\n\n```\n\n## **3.Govendor**\n\n```\nhttps://github.com/kardianos/govendor\n\n```\n\n注意在使用 `go get -u github.com/kardianos/govendor` 的时候要加上 sudo，不然会找不到 govendor\n\n使用参考：[Go 包依赖管理工具 &mdash;&mdash; govendor](https://shockerli.net/post/go-package-manage-tool-govendor/)\n","tags":["golang"]},{"title":"Ubuntu下Ansible安装和使用","url":"/Ubuntu下Ansible安装和使用.html","content":"Ansible是一个批量部署的工具\n\n参考：[Ansible中文权威指南](http://ansible-tran.readthedocs.io/en/latest/index.html)\n\n1.安装\n\n```\nsudo apt-get install software-properties-common\nsudo apt-add-repository ppa:ansible/ansible\nsudo apt-get update\nsudo apt-get install ansible\n\n```\n\n2.在/etc/ansible/hosts文件中添加服务器的ip\n\n```\nvim /etc/ansible/hosts\n\n```\n\n3.需要将自己机器的~/.ssh/目录下公钥，即pub文件，添加到服务器的<!--more-->\n&nbsp;`~/.``ssh``/authorized_keys&nbsp;`文件中\n\n```\nvim ~/.ssh/authorized_keys\n\n```\n\n例如\n\n```\n[XXXX]\nxx.xx.xx.[12:35] ansible_ssh_port=xxx\nxx.xx.xx.[66:115] ansible_ssh_port=xxx\n\n```\n\n测试ssh是否可以免密登录\n\n4.测试能否ping通，以root用户登录\n\n```\nansible all -m ping -u root\n\n```\n\n5.测试建立一个test文件夹\n\n```\nansible all -u root -a \"/bin/mkdir test\"\n\n```\n\n6.也可以把ansible命令写到yaml文件中\n\n&nbsp;\n\n然后\n","tags":["Linux"]},{"title":"Logstash安装和使用","url":"/Logstash安装和使用.html","content":"**Logstash** 是开源的服务器端数据处理管道，能够同时 从多个来源采集数据、转换数据，然后将数据发送到您最喜欢的 &ldquo;存储库&rdquo; 中。（我们的存储库当然是 Elasticsearch。）\n\n**作用：集中、转换和存储数据**\n\n### **官方网站：**\n\n```\nhttps://www.elastic.co/cn/products/logstash\n\n```\n\n### **一个民间的中文Logstash最佳实践：**\n\n```\nhttps://doc.yonyoucloud.com/doc/logstash-best-practice-cn/index.html\n\n```\n\n<!--more-->\n&nbsp;\n\n### **1.下载Logstash**，版本为6.2.4，下载地址\n\n```\nhttps://artifacts.elastic.co/downloads/logstash/logstash-6.2.4.tar.gz\n\n```\n\n### **2.解压到目录**\n\n**<img src=\"/images/517519-20180516111235275-1127859893.png\" alt=\"\" />**\n\n### **3.启动Logstash进程**，Hello World Demo\n\n```\nbin/logstash -e 'input { stdin { } } output { stdout {} }'\n\n```\n\n<img src=\"/images/517519-20180516113218039-1502717353.png\" alt=\"\" />\n\n```\nbin/logstash -e 'input{stdin{}}output{stdout{codec=>rubydebug}}'\n\n```\n\n输入：Hello World\n\n输出：\n\n<img src=\"/images/517519-20180516103254465-1624051284.png\" alt=\"\" />\n\n在这个Demo中，Hello World作为数据，在线程之间以 **事件** 的形式流传。不要叫**行**，因为 logstash 可以处理[多行](https://doc.yonyoucloud.com/doc/logstash-best-practice-cn/codec/multiline.html)事件。\n\nLogstash 会给事件添加一些额外信息。最重要的就是 **@timestamp**，用来标记事件的发生时间。因为这个字段涉及到 Logstash 的内部流转，所以必须是一个 joda 对象，如果你尝试自己给一个字符串字段重命名为 `@timestamp` 的话，Logstash 会直接报错。所以，**请使用 [filters/date 插件](https://doc.yonyoucloud.com/doc/logstash-best-practice-cn/filter/date.html) 来管理这个特殊字段**。\n\n此外，大多数时候，还可以见到另外几个：\n\n1. **host** 标记事件发生在哪里。\n1. **type** 标记事件的唯一类型。\n1. **tags** 标记事件的某方面属性。这是一个数组，一个事件可以有多个标签。\n\n### **4.语法**\n\nLogstash 设计了自己的 DSL &mdash;&mdash; 有点像 Puppet 的 DSL，或许因为都是用 Ruby 语言写的吧 &mdash;&mdash; 包括有区域，注释，数据类型(布尔值，字符串，数值，数组，哈希)，条件判断，字段引用等。\n\n### 区段(section)\n\nLogstash 用 `{}` 来定义区域。区域内可以包括插件区域定义，你可以在一个区域内定义多个插件。插件区域内则可以定义键值对设置。示例如下：\n\n```\ninput {\n    stdin {}\n    syslog {}\n}\n\n```\n\n### 数据类型\n\nLogstash 支持少量的数据值类型：\n\n```\nbool　　　debug => true \nstring　　host => \"hostname\" \nnumber    port => 514\narray    match => [\"datetime\", \"UNIX\", \"ISO8601\"]\nhash\noptions => {\n    key1 => \"value1\",\n    key2 => \"value2\"\n}\n\n```\n\n### 条件判断(condition)\n\n```\n表达式支持下面这些操作符：\n    equality, etc: ==, !=, <, >, <=, >=\n    regexp: =~, !~\n    inclusion: in, not in\n    boolean: and, or, nand, xor\n    unary: !()\n\n比如：\nif \"_grokparsefailure\" not in [tags] {\n} else if [status] !~ /^2\\d\\d/ and [url] == \"/noc.gif\" {\n} else {\n}\n\n```\n\n### **命令行参数：****`logstash`命令**\n\n```\n参数：\n执行    -e    　　　　　　　　　　bin/logstash -e ''\n文件    --config 或 -f    　　　bin/logstash -f agent.conf\n测试    --configtest 或 -t     用来测试 Logstash 读取到的配置文件语法是否能正常解析。\n日志    --log 或 -l    　　　　 Logstash 默认输出日志到标准错误。生产环境下你可以通过 bin/logstash -l logs/logstash.log 命令来统一存储日志。\n\n```\n\n&nbsp;\n\n**使用Logstash的Kafka插件 <br />**\n\n```\nhttps://www.elastic.co/guide/en/logstash/current/plugins-inputs-kafka.html\n\n```\n\n启动一个kafka作为输入，并输入1231212\n\n```\n~/software/apache/kafka_2.11-0.10.0.0$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test\n\n```\n\n&nbsp;stdin.conf文件\n\n```\ninput{\n        kafka{\n                bootstrap_servers => [\"127.0.0.1:9092\"]\n                topics => [ 'test' ]\n        }\n}\noutput {\n        stdout {\n                codec => rubydebug\n        }\n}\n\n```\n\n启动logstash\n\n```\nbin/logstash -f stdin.conf\n\n```\n\n输出\n\n<img src=\"/images/517519-20180516153850182-899595603.png\" alt=\"\" />\n\n关于**auto_offset_reset参数**：\n\n由于Kafka是消息队列，消费过的就不会再消费\n\n**<i>可以在stdin.conf中设置auto_offset_reset=\"earliest\"，比如**\n\n```\ninput{\n        kafka{\n                bootstrap_servers => [\"127.0.0.1:9092\"]\n                topics => [ 'test' ]\n                auto_offset_reset => \"earliest\"\n        }\n}\noutput {\n        stdout {\n                codec => rubydebug\n        }\n}\n\n```\n\n在kafka中依次输入\n\n```\n1111\n2222\n3333\n\n```\n\n输出为，注意这里timestamp的时间是1111 -> 2222 -> 3333，logstash会从头开始消费没有消费的消息\n\n<img src=\"/images/517519-20180516161609612-784544679.png\" alt=\"\" />\n\n&nbsp;\n\n**<ii>当auto_offset_reset=\"latest\"**\n\nlogstash会从进程启动的时候开始消费消息，之前的消息会丢弃\n\n在kafka中依次输入\n\n```\n1111\n2222\n3333\n\n```\n\n输出为\n\n&nbsp;\n\n**Kafka -> logstash -> Es的conf文件**\n\n```\ninput{\n\tkafka{\n\t\tbootstrap_servers => [\"127.0.0.1:9092\"]\n\t\ttopics => [ 'topicB' ]\n\t\tauto_offset_reset => \"earliest\"\n\t\tconsumer_threads => 1\n\t\tcodec => json\n\t}\n}\noutput {\n\telasticsearch{\n\t\thosts => [\"127.0.0.1:9200\"]\n\t\tindex => \"XXX\"\n\t}\n}\n\n```\n\n**Kafka -> logstash -> File的conf文件**\n\n**参考**\n\n```\nhttps://www.elastic.co/guide/en/logstash/current/plugins-outputs-file.html\n\n```\n\n**注意：如果是kafka输入是line格式的，使用codec => line { format => \"custom format: %{message}\"}**\n\n**<strong>关于codec的说明**</strong>\n\n```\nhttps://www.elastic.co/guide/en/logstash/6.2/codec-plugins.html\n\n```\n\n**如果kafka输入是json格式的，使用codec => json**\n\n```\ninput{\n\tkafka{\n\t\tbootstrap_servers => [\"127.0.0.1:9092\"]\n\t\ttopics => [ 'topicB' ]\n\t\tauto_offset_reset => \"earliest\"\n\t\tconsumer_threads => 1\n\t\tcodec => json\n\t}\n}\noutput {\n\tstdout {\n\t\tcodec => rubydebug {}\n    \t}\n\tfile {\n\t\tpath => \"/home/lintong/桌面/logs/path/to/1.txt\"\n\t\t#codec => line { format => \"custom format: %{message}\"}\t\t\n\t\tcodec => json\n\t}\n}\n\n```\n\n&nbsp;\n\n**使用Logstash的HDFS插件 **\n\n```\nhttps://www.elastic.co/guide/en/logstash/current/plugins-outputs-webhdfs.html\n\n```\n\n配置文件\n\n```\ninput{\n\tkafka{\n\t\tbootstrap_servers => [\"127.0.0.1:9092\"]\n\t\ttopics => [ 'topicB' ]\n\t\tauto_offset_reset => \"earliest\"\n\t\tconsumer_threads => 1\n\t\tcodec => json\n\t}\n}\noutput {\n\tstdout {\n\t\tcodec => rubydebug {}\n    \t}\n\twebhdfs {\n\t\thost => \"127.0.0.1\"                 # (required)\n\t\tport => 50070                       # (optional, default: 50070)\n\t\tpath => \"/user/lintong/xxx/logstash/dt=%{+YYYY-MM-dd}/logstash-%{+HH}.log\"  # (required)\n\t\tuser => \"lintong\"                       # (required)\n\t\tcodec => json\n\t}\n}\n\n```\n\n到 http://localhost:50070 下看文件内容\n\n<img src=\"/images/517519-20180517112951671-1316000501.png\" alt=\"\" />\n\n&nbsp;\n","tags":["ELK"]},{"title":"Centos7.0下Nexus私服搭建","url":"/Centos7.0下Nexus私服搭建.html","content":"1.下载nexus\n\n```\nwget https://sonatype-download.global.ssl.fastly.net/nexus/oss/nexus-2.11.2-03-bundle.tar.gz\n\n```\n\n2.解压，会出现两个目录，nexus-2.11.2-03是服务<!--more-->\n&nbsp; sonatype-work是私有库目录\n\n```\nmkdir nexus\ntar -zxvf nexus-2.11.2-03-bundle.tar.gz -C ./nexus\n\n```\n\n3.修改配置，在nexus-2.11.2-03/bin目录下修改nexus文件\n\n```\nNEXUS_HOME=&ldquo;安装目录&rdquo;\nRUN_AS_USER=xxx\n\n```\n\n4.修改端口，nexus-2.11.2-03/conf修改nexus.properties文件\n\n```\napplication-port=XXX\n\n```\n\n5.启动\n\n```\n./bin/nexus start\nStarting Nexus OSS...\nStarted Nexus OSS.\n\n```\n\n6.修改密码\n\n<img src=\"/images/517519-20180513101849640-615222982.png\" alt=\"\" />\n\n```\nadmin：该用户拥有Nexus的全部权限，默认密码为admin123。\ndeployment：该用户能够访问Nexus，浏览仓库内容、搜索、上传部署构件，但是不能对Nexus进行任何配置，默认密码为deployment123。\nanonymous：该用户对应了所有未登录的匿名用户，它们可以浏览仓库并进行搜索。\n\n```\n\n7.上传Jar包\n\n参考：[上传jar包到nexus私服](https://my.oschina.net/lujianing/blog/297128)\n\n登录之后可以看到如下仓库\n\n<img src=\"/images/517519-20180513105207865-876119826.png\" alt=\"\" />\n\n&nbsp;\n\n<1>如果上传的是第三方的jar包\n\n在图中填入相应的jar包的信息就可以了\n\n<img src=\"/images/517519-20180515004000163-132156256.png\" alt=\"\" />\n\n同时在maven的setting.xml文件中配置\n\n```\n  <servers>\n\t<server>    \n\t\t<id>nexus-releases</id>    \n\t\t<username>admin</username>    \n\t\t<password>xxxx</password>    \n\t</server>    \n\t<server>    \n\t\t<id>nexus-snapshots</id>    \n\t\t<username>admin</username>    \n\t\t<password>xxxx</password>    \n\t</server> \n  </servers>\n\n```\n\n&nbsp;以及\n\n```\n<mirror>\n    <id>mynexus</id>\n    <mirrorOf>central</mirrorOf>\n    <name>My Nexus</name>\n    <url>http://XXX/nexus/content/repositories/thirdparty/</url>\n</mirror>\n\n```\n\n&nbsp;<2>上传snapshot包\n\n在工程的pom文件中配置\n\n```\n    <distributionManagement>\n        <repository>\n            <id>nexus-releases</id>\n            <name>Nexus Release Repository</name>\n            <url>http://XXX/nexus/content/repositories/releases/</url>\n        </repository>\n        <snapshotRepository>\n            <id>nexus-snapshots</id>\n            <name>Nexus Snapshot Repository</name>\n            <url>http://XXX/nexus/content/repositories/snapshots/</url>\n        </snapshotRepository>\n    </distributionManagement>\n\n```\n\n&nbsp;\n\n&nbsp;同时在maven的setting.xml文件中配置\n\n```\n    </profiles>\n\t\t<profile>    \n\t\t\t<id>nexus</id>    \n\t\t\t<repositories>    \n\t\t\t\t<repository>    \n\t\t\t\t\t<id>nexus-releases</id>    \n\t\t\t\t\t<url>http://nexus-releases</url>    \n\t\t\t\t\t<releases><enabled>true</enabled></releases>    \n\t\t\t\t\t<snapshots><enabled>true</enabled></snapshots>    \n\t\t\t\t</repository>    \n\t\t\t\t<repository>    \n\t\t\t\t\t<id>nexus-snapshots</id>    \n\t\t\t\t\t<url>http://nexus-snapshots</url>    \n\t\t\t\t\t<releases><enabled>true</enabled></releases>    \n\t\t\t\t\t<snapshots><enabled>true</enabled></snapshots>    \n\t\t\t\t</repository>    \n\t\t\t</repositories>    \n\t\t\t<pluginRepositories>    \n\t\t\t\t<pluginRepository>    \n\t\t\t\t\t<id>nexus-releases</id>    \n\t\t\t\t\t<url>http://nexus-releases</url>    \n\t\t\t\t\t<releases><enabled>true</enabled></releases>    \n\t\t\t\t\t<snapshots><enabled>true</enabled></snapshots>    \n\t\t\t\t</pluginRepository>    \n\t\t\t\t<pluginRepository>    \n\t\t\t\t\t<id>nexus-snapshots</id>    \n\t\t\t\t\t<url>http://nexus-snapshots</url>    \n\t\t\t\t\t<releases><enabled>true</enabled></releases>    \n\t\t\t\t\t<snapshots><enabled>true</enabled></snapshots>    \n\t\t\t\t</pluginRepository>    \n\t\t\t</pluginRepositories>    \n\t\t</profile>    \n\t</profiles>\n\n\t<activeProfiles>    \n\t\t<activeProfile>nexus</activeProfile>    \n\t</activeProfiles> \n\n```\n\n&nbsp;然后在项目下运行，就会生成snapshot包\n\n```\nmvn deploy\n\n```\n\n&nbsp;\n","tags":["Nexus"]},{"title":"使用MegaCli监控Linux硬盘","url":"/使用MegaCli监控Linux硬盘.html","content":"**1.**首先查看机器是否使用的是MegaRAID卡\n\n```\ndmesg | grep RAID\n[    6.932741] scsi host0: Avago SAS based MegaRAID driver\n\n```\n\n**2.**添加 megaraid 源：\n\n修改 /etc/apt/sources.list 在末尾添加\n\n```\ndeb http://hwraid.le-vert.net/ubuntu precise main\n\n```\n\n然后执行：\n\n```\napt-get update\napt-get install megacli megactl megaraid-status\n\n```\n\n如果执行提示 GPG 错误，需要执行如下命令添加证书：\n\n```\nwget -O - http://hwraid.le-vert.net/debian/hwraid.le-vert.net.gpg.key | sudo apt-key add -\n\n```\n\n然后再次执行：\n\n```\napt-get install megacli megactl megaraid-status\n\n```\n\n**3.**使用megacli命令进行一些简单的查询\n\n<i>显示所有RAID级别、设置及逻辑盘信息\n\n```\nsudo megacli -LDInfo -Lall -aALL \n\n```\n\n输出\n\n```\n                                     \nAdapter 0 -- Virtual Drive Information:\nVirtual Drive: 0 (Target Id: 0)\nName                :\nRAID Level          : Primary-1, Secondary-0, RAID Level Qualifier-0\nSize                : 1.090 TB\nSector Size         : 512\nIs VD emulated      : No\nMirror Data         : 1.090 TB\nState               : Optimal\nStrip Size          : 64 KB\nNumber Of Drives    : 2\nSpan Depth          : 1\nDefault Cache Policy: WriteBack, ReadAhead, Direct, No Write Cache if Bad BBU\nCurrent Cache Policy: WriteBack, ReadAhead, Direct, No Write Cache if Bad BBU\nDefault Access Policy: Read/Write\nCurrent Access Policy: Read/Write\nDisk Cache Policy   : Disk's Default\nEncryption Type     : None\nDefault Power Savings Policy: Controller Defined\nCurrent Power Savings Policy: None\nCan spin up in 1 minute: Yes\nLD has drives that support T10 power conditions: Yes\nLD's IO profile supports MAX power savings with cached writes: No\nBad Blocks Exist: No\nIs VD Cached: No\n\n\n\nExit Code: 0x00\n\n```\n\n<ii>显示所有的物理信息\n\n```\nsudo megacli -PDList -aAll\n\n```\n\n过滤输出\n\n```\nsudo megacli -PDList -aAll  | grep \"Firmware state\\|Slot Number\\|Error\"\n\n```\n\n<!--more-->\n&nbsp;参考输出\n\n```\nEnclosure Device ID: 32\nSlot Number: 11\nEnclosure position: 1\nDevice Id: 11\nWWN: 50000397eb88257a\nSequence Number: 2\nMedia Error Count: 0\nOther Error Count: 4\nPredictive Failure Count: 0\nLast Predictive Failure Event Seq Number: 0\nPD Type: SATA\n\nRaw Size: 7.277 TB [0x3a3812ab0 Sectors]\nNon Coerced Size: 7.276 TB [0x3a3712ab0 Sectors]\nCoerced Size: 7.276 TB [0x3a3700000 Sectors]\nSector Size:  512\nLogical Sector Size:  512\nPhysical Sector Size:  4096\nFirmware state: JBOD\nDevice Firmware Level: GX4D\nShield Counter: 0\nSuccessful diagnostics completion on :  N/A\nSAS Address(0): 0x500056b3295a37cb\nConnected Port Number: 0(path0) \nInquiry Data:         77N4K3BIFYRDTOSHIBA MG05ACA800E                         GX4D\nFDE Capable: Not Capable\nFDE Enable: Disable\nSecured: Unsecured\nLocked: Unlocked\nNeeds EKM Attention: No\nForeign State: None \nDevice Speed: 6.0Gb/s \nLink Speed: 6.0Gb/s \nMedia Type: Hard Disk Device\nDrive Temperature :28C (82.40 F)\nPI Eligibility:  No \nDrive is formatted for PI information:  No\nPI: No PI\nDrive's NCQ setting : N/A\nPort-0 :\nPort status: Active\nPort's Linkspeed: 6.0Gb/s \nDrive has flagged a S.M.A.R.T alert : No\n\n\n\nEnclosure Device ID: 32\nSlot Number: 12\nDrive's position: DiskGroup: 0, Span: 0, Arm: 0\nEnclosure position: 1\nDevice Id: 12\nWWN: 5000CCA02D78FCDB\nSequence Number: 2\nMedia Error Count: 0\nOther Error Count: 0\nPredictive Failure Count: 0\nLast Predictive Failure Event Seq Number: 0\nPD Type: SAS\n\nRaw Size: 1.090 TB [0x8bba0cb0 Sectors]\nNon Coerced Size: 1.090 TB [0x8baa0cb0 Sectors]\nCoerced Size: 1.090 TB [0x8ba80000 Sectors]\nSector Size:  512\nLogical Sector Size:  512\nPhysical Sector Size:  512\nFirmware state: Online, Spun Up\nDevice Firmware Level: FU29\nShield Counter: 0\nSuccessful diagnostics completion on :  N/A\nSAS Address(0): 0x5000cca02d78fcd9\nSAS Address(1): 0x0\nConnected Port Number: 0(path0) \nInquiry Data: HGST    HUC101812CSS200 FU2906J4JPYZ            \nFDE Capable: Not Capable\nFDE Enable: Disable\nSecured: Unsecured\nLocked: Unlocked\nNeeds EKM Attention: No\nForeign State: None \nDevice Speed: 12.0Gb/s \nLink Speed: 12.0Gb/s \nMedia Type: Hard Disk Device\nDrive Temperature :55C (131.00 F)\nPI Eligibility:  No \nDrive is formatted for PI information:  Yes \nPI: PI with type 2\nPort-0 :\nPort status: Active\nPort's Linkspeed: 12.0Gb/s \nPort-1 :\nPort status: Active\nPort's Linkspeed: 12.0Gb/s \nDrive has flagged a S.M.A.R.T alert : No\n\n```\n\n&nbsp;\n\n参考：[Linux系统下安装MegaCli64工具查看和管理raid卡](https://blog.csdn.net/lufeisan/article/details/53398756)\n\n[MegaCli 监控raid状态&nbsp;](http://blog.51cto.com/rolandqu/1421866)\n","tags":["Linux"]},{"title":"使用SMART监控Ubuntu","url":"/使用SMART监控Ubuntu.html","content":"参考：[完全用 GNU/Linux 工作 - 29. 檢測硬碟 S.M.A.R.T. 健康狀態](https://chusiang.gitbooks.io/working-on-gnu-linux/29.checking-hd-smart.html)\n\n1.安装\n\n```\nsudo apt-get install smartmontools\n\n```\n\n2.查看硬盘的参数，需要获得Root权限\n\n```\nsudo smartctl -i /dev/sda\n\nsmartctl 6.5 2016-01-24 r4214 [x86_64-linux-4.4.0-122-generic] (local build)\nCopyright (C) 2002-16, Bruce Allen, Christian Franke, www.smartmontools.org\n\n=== START OF INFORMATION SECTION ===\nModel Family:     Samsung based SSDs\nDevice Model:     Samsung SSD 850 EVO 250GB\nSerial Number:    S3LCNF0J807262K\nLU WWN Device Id: 5 002538 d42253d35\nFirmware Version: EMT03B6Q\nUser Capacity:    250,059,350,016 bytes [250 GB]\nSector Size:      512 bytes logical/physical\nRotation Rate:    Solid State Device\nForm Factor:      2.5 inches\nDevice is:        In smartctl database [for details use: -P show]\nATA Version is:   ACS-2, ATA8-ACS T13/1699-D revision 4c\nSATA Version is:  SATA 3.1, 6.0 Gb/s (current: 6.0 Gb/s)\nLocal Time is:    Mon May  7 10:14:29 2018 CST\nSMART support is: Available - device has SMART capability.\nSMART support is: Enabled\n\n```\n\n3.如果使用下面的Python脚本来调用该命令的时候，是需要获得Root权限的\n\n```\n#!/usr/bin/python\n# !-*- coding:utf8 -*-\n\nimport os\n\n\ndef main():\n    dir = \"~/桌面/test.sh\"\n    result = os.system(\"sh ~/桌面/test.sh\")\n    # print result\n\n\nif __name__ == '__main__':\n    main()\n\n```\n\n可以使用下面的方法来实现**免输入密码**，使用 **sudo visudo** 命令对/etc/sudoers文件增加下面设置\n\n```\nlintong  ALL=(ALL) NOPASSWD: /usr/sbin/smartctl\n\n```\n\n<!--more-->\n&nbsp;注意不要使用vim等工具来进行修改，无法修改错的话可能会出现下面的错误\n\n```\nsudo: /etc/sudoers 中第 33 行附近有解析错误sudo: 没有找到有效的 sudoers 资源，退出sudo: 无法初始化策略插件\n\n```\n\n&nbsp;如果出现了的话，可以使用Ubuntu启动盘进入系统，在root用户下将/etc/sudoers文件改回来就可以了\n","tags":["Linux"]},{"title":"Kibana学习笔记——安装和使用","url":"/Kibana学习笔记——安装和使用.html","content":"1.首先下载Kibana\n\n```\nhttps://www.elastic.co/downloads\n\n```\n\n2.解压\n\n```\ntar -zxvf kibana-6.2.1-linux-x86_64.tar.gz -C ~/software/\n\n```\n\n3.修改配置，在config文件夹下面修改kibana.yml\n\n```\n#配置本机ip\nserver.host: \"127.0.0.1\"\n#配置es集群url\nelasticsearch.url: \"http://127.0.0.1:9200\"\n\n```\n\n4.启动\n\n```\n./bin/kibana\n\n```\n\n5.访问web\n\n```\nhttp://localhost:5601/app/kibana\n\n```\n\n6.在**Manager**里面添加index，demo中的index名称叫做es\n\n<img src=\"/images/517519-20180506224702887-81039618.png\" alt=\"\" width=\"615\" height=\"399\" />\n\n添加之后\n\n<img src=\"/images/517519-20180506224750150-132294548.png\" alt=\"\" width=\"1145\" height=\"413\" />\n\n7.在**discover**中可以查看数据，里面还会对top数据进行统计\n\n<img src=\"/images/517519-20180506225140553-88255147.png\" alt=\"\" width=\"864\" height=\"633\" />\n\n8.在**Dev tool**中可以进行查询\n\n<img src=\"/images/517519-20180506230132956-504444958.png\" alt=\"\" width=\"953\" height=\"435\" />\n\n<!--more-->\n&nbsp;\n\nkibana新建index pattern的时候报403解决方法\n\n```\ncurl -XPUT -H 'Content-Type: application/json' http://localhost:9200/_settings -d '{\"index\": {\"blocks\": {\"read_only_allow_delete\": \"false\"}}}'\n\n```\n\n&nbsp;\n","tags":["ELK"]},{"title":"open-falcon实现邮件报警","url":"/open-falcon实现邮件报警.html","content":"1.请安装好Go的环境，参考上一篇open-falcon的安装博文\n\n2.安装** mail-provider**\n\n```\nhttps://github.com/open-falcon/mail-provider\n\n```\n\n安装方法\n\n```\ncd $GOPATH/src\nmkdir github.com/open-falcon/ -p\ncd github.com/open-falcon/\ngit clone https://github.com/open-falcon/mail-provider.git\ncd mail-provider\ngo get ./...\n./control build\n\n```\n\n<!--more-->\n&nbsp;编译成功之后，修改cfg.json文件相关信息，使用\n\n```\n./control start\n\n```\n\n&nbsp;在cfg.json里面使用的163邮箱的smtp服务，需要开启客户端授权码，如果提示：发送数量超过配额，请过24小时再请求手机验证码\n\n<img src=\"/images/517519-20180503105158286-1079167835.png\" alt=\"\" />\n\n&nbsp;\n\n使用curl命令，验证是否能发邮件，返回success的话就是成功了\n\n```\ncurl http://127.0.0.1:4000/sender/mail -d \"tos=你的邮箱&amp;subject=报警测试&amp;content=这是一封测试邮件\"\nsuccess\n\n```\n\n&nbsp;\n\n3.安装** sender**\n\n```\nhttps://github.com/open-falcon-archive/sender\n\n```\n\n&nbsp;编译和安装\n\n```\n# set $GOPATH and $GOROOT\nmkdir -p $GOPATH/src/github.com/open-falcon\ncd $GOPATH/src/github.com/open-falcon\ngit clone https://github.com/open-falcon/sender.git\ncd sender\ngo get ./...\n./control build\n# vi cfg.json modify configuration\n./control start\n\n```\n\n将cfg.json中的配置改成\n\n```\n \"mail\": \"http://127.0.0.1:4000/sender/mail\"\n\n```\n\n&nbsp;4.在open-falcon的portal的**expression**中配置报警表达式，如下图\n\n记住配置报警接受人的组，这决定了谁的邮箱将会收到报警\n\n<img src=\"/images/517519-20180503151919658-818386057.png\" alt=\"\" width=\"1342\" height=\"508\" />\n\n5.模拟报警环境，然后将会收到邮件，同时在alarm-dashboard中也能看到报警\n\n<img src=\"/images/517519-20180503152154729-1781647042.png\" alt=\"\" />\n\n&nbsp;\n","tags":["open-falcon"]},{"title":"Ubuntu下安装open-falcon-v0.2.1","url":"/Ubuntu下安装open-falcon-v0.2.1.html","content":"在Ubuntu下安装open-falcon和Centos下安装的方法有点区别，因为Ubuntu使用的包管理器是apt-get，而Centos下使用的是Yum，建议不要再Ubuntu下使用yum\n\n建议自己下载源码打包二进制包来安装，因为官方给出的二进制包应该是再centos下打包的，再Ubuntu下运行可能会出现问题\n\n1.安装Go，首先去官网下载，需要fq\n\n```\nhttps://golang.org\n\n```\n\n顺便安装Goland，注册服务器 [http://idea.youbbs.org](http://idea.youbbs.org/)\n\n2.在/etc/profile中添加，后source /etc/profile\n\n```\n#Go\nexport GOROOT=/home/lintong/software/go\nexport PATH=$PATH:$GOROOT/bin:$GOPATH/bin\nexport GOPATH=/home/lintong/software/gopath\n\n```\n\n验证是否安装成功\n\n```\ngo version\ngo version go1.10.2 linux/amd64\n\n```\n\n接下来参考\n\n[Ubuntu 16.04 部署 open-falcon](http://www.sysctl.me/2017/09/07/open-falcon/)\n\n[运维监控系统之Open-Falcon](http://www.cnblogs.com/nulige/p/7741580.html)\n\n[Open-falcon的官方README](https://github.com/open-falcon/falcon-plus/blob/master/README.md)\n\n<!--more-->\n&nbsp;3.安装redis和mysql\n\n```\nsudo apt-get install redis-server\nsudo apt-get install mysql-server\n\n```\n\n&nbsp;确保两个服务已经启动\n\n```\nsystemctl status mysql redis-server\n\n```\n\n&nbsp; <img src=\"/images/517519-20180502094620967-353137590.png\" alt=\"\" />\n\n4.下载open-falcon源码，地址\n\n```\nhttps://github.com/open-falcon/falcon-plus\n\n```\n\n&nbsp;初始化MySQL表结构\n\n```\nmkdir -p $GOPATH/src/github.com/open-falcon\ncd $GOPATH/src/github.com/open-falcon/falcon-plus/scripts/mysql/db_schema/\nmysql -h 127.0.0.1 -u root -p < 1_uic-db-schema.sql\nmysql -h 127.0.0.1 -u root -p < 2_portal-db-schema.sql\nmysql -h 127.0.0.1 -u root -p < 3_dashboard-db-schema.sql\nmysql -h 127.0.0.1 -u root -p < 4_graph-db-schema.sql\nmysql -h 127.0.0.1 -u root -p < 5_alarms-db-schema.sql\n\n```\n\n5. 编译二进制包\n\n```\ncd $GOPATH/src/github.com/open-falcon/falcon-plus/\n# make all modules\nmake all\n# make specified module\nmake agent\n# pack all modules\nmake pack\n\n```\n\n&nbsp;打包成功，在目录下多了文件open-falcon-v0.2.1.tar.gz\n\n```\nlintong@master:~/software/go/src/github.com/open-falcon/falcon-plus$ make all\ngo build -o bin/agent/falcon-agent ./modules/agent\ngo build -o bin/aggregator/falcon-aggregator ./modules/aggregator\ngo build -o bin/graph/falcon-graph ./modules/graph\ngo build -o bin/hbs/falcon-hbs ./modules/hbs\ngo build -o bin/judge/falcon-judge ./modules/judge\ngo build -o bin/nodata/falcon-nodata ./modules/nodata\ngo build -o bin/transfer/falcon-transfer ./modules/transfer\ngo build -o bin/gateway/falcon-gateway ./modules/gateway\ngo build -o bin/api/falcon-api ./modules/api\ngo build -o bin/alarm/falcon-alarm ./modules/alarm\ngo build -ldflags \"-X main.GitCommit=`git rev-parse --short HEAD` -X main.Version=0.2.1\" -o open-falcon\nlintong@master:~/software/go/src/github.com/open-falcon/falcon-plus$ make agent\ngo build -o bin/agent/falcon-agent ./modules/agent\nlintong@master:~/software/go/src/github.com/open-falcon/falcon-plus$ make pack\ngo build -ldflags \"-X main.GitCommit=`git rev-parse --short HEAD` -X main.Version=0.2.1\" -o open-falcon\ntar -C out -zcf open-falcon-v0.2.1.tar.gz .\n\n```\n\n&nbsp;6.解压到安装目录\n\n```\nmkdir ~/software/open-falcon-v0.2.1\ntar -zxvf open-falcon-v0.2.1.tar.gz -C ~/software/open-falcon-v0.2.1/\n\n```\n\n&nbsp;7.安装open-falcon的前端框架dashboard\n\n```\ncd ~/software/open-falcon-v0.2.1\ngit clone https://github.com/open-falcon/dashboard.git\n\n```\n\n8.修改数据库密码，因为open-falcon需要操作数据库，这一步是官方教程中没有提到的\n\n如果没有进行修改的话，在open-falcon-v0.2.1目录下使用./open-falcon start命令进行启动之后，使用./open-falcon check检查组件的状态，有些使用mysql的组件的状态是down\n\n下面列出了使用mysql的组件，总共有6个：**aggregator，graph、hbs、nodata、api、alarm模块**\n\n**需要对这些组件的配置文件进行修改：**\n\n```\nvim ~/software/open-falcon-v0.2.1/aggregator/config/cfg.json\n\n```\n\n<img src=\"/images/517519-20180502101314257-240920095.png\" alt=\"\" />\n\n修改成\n\n&nbsp;<img src=\"/images/517519-20180502101358631-69672682.png\" alt=\"\" />\n\n其他配置文件的修改方式类似\n\n```\nvim graph/config/cfg.json\n\n```\n\n<img src=\"/images/517519-20180502101555459-1613391214.png\" alt=\"\" />\n\n```\nvim hbs/config/cfg.json\n\n```\n\n<img src=\"/images/517519-20180502101744759-813570782.png\" alt=\"\" />\n\n```\nvim nodata/config/cfg.json\n\n```\n\n&nbsp;<img src=\"/images/517519-20180502101847187-1897663701.png\" alt=\"\" />\n\n```\nvim api/config/cfg.json # 5处全部修改\n\n```\n\n<img src=\"/images/517519-20180502101941050-2054676409.png\" alt=\"\" />\n\n```\nvim alarm/config/cfg.json\n\n```\n\n<img src=\"/images/517519-20180502102107297-31765138.png\" alt=\"\" />\n\n接下来需要在前端组件中，配置数据库密码\n\n```\ncd ~/software/open-falcon-v0.2.1/dashboard/rrd\nvim config.py\n\n```\n\n&nbsp;<img src=\"/images/517519-20180502102355827-873482344.png\" alt=\"\" />\n\n9.安装Python的依赖，在Ubuntu下不要使用Yum\n\n```\nsudo apt-get install python-pip python-virtualenv\nsudo apt-get install python-dev\nsudo apt-get install ldap-utils\nsudo apt-get install libmysqld-dev\n\n```\n\n10.安装virtualenv的运行环境，在服务器上是不建议直接pip安装的，因为Python包的冲突可能会影响到别人的服务\n\n```\n~/software/open-falcon-v0.2.1/dashboard\nvirtualenv ./env\n./env/bin/pip install -r pip_requirements.txt -i https://pypi.douban.com/simple\n\n```\n\n&nbsp;如果出现 Failed building wheel for python-ldap 的问题<br />请参考 &nbsp;&nbsp; &nbsp; Installing python-ldap in Ubuntu&nbsp;\n\n```\nsudo apt-get install python-dev\nsudo apt-get install libldap2-dev\nsudo apt-get install libsasl2-dev\n# 然后再\n./env/bin/pip install python-ldap\n\n```\n\n请确保每个Python包都安装到位，成功如下\n\n```\n./env/bin/pip install -r pip_requirements.txt -i https://pypi.douban.com/simple\nLooking in indexes: https://pypi.douban.com/simple\nRequirement already satisfied: Flask==0.10.1 in ./env/lib/python2.7/site-packages (from -r pip_requirements.txt (line 1)) (0.10.1)\nRequirement already satisfied: Flask-Babel==0.9 in ./env/lib/python2.7/site-packages (from -r pip_requirements.txt (line 2)) (0.9)\nRequirement already satisfied: Jinja2==2.7.2 in ./env/lib/python2.7/site-packages (from -r pip_requirements.txt (line 3)) (2.7.2)\nRequirement already satisfied: Werkzeug==0.9.4 in ./env/lib/python2.7/site-packages (from -r pip_requirements.txt (line 4)) (0.9.4)\nRequirement already satisfied: gunicorn==19.1.1 in ./env/lib/python2.7/site-packages (from -r pip_requirements.txt (line 5)) (19.1.1)\nRequirement already satisfied: python-dateutil==2.2 in ./env/lib/python2.7/site-packages (from -r pip_requirements.txt (line 6)) (2.2)\nRequirement already satisfied: requests==2.3.0 in ./env/lib/python2.7/site-packages (from -r pip_requirements.txt (line 7)) (2.3.0)\nRequirement already satisfied: mysql-python in ./env/lib/python2.7/site-packages (from -r pip_requirements.txt (line 8)) (1.2.5)\nRequirement already satisfied: python-ldap in ./env/lib/python2.7/site-packages (from -r pip_requirements.txt (line 9)) (3.0.0)\nRequirement already satisfied: itsdangerous>=0.21 in ./env/lib/python2.7/site-packages (from Flask==0.10.1->-r pip_requirements.txt (line 1)) (0.24)\nRequirement already satisfied: speaklater>=1.2 in ./env/lib/python2.7/site-packages (from Flask-Babel==0.9->-r pip_requirements.txt (line 2)) (1.3)\nRequirement already satisfied: Babel>=1.0 in ./env/lib/python2.7/site-packages (from Flask-Babel==0.9->-r pip_requirements.txt (line 2)) (2.5.3)\nRequirement already satisfied: markupsafe in ./env/lib/python2.7/site-packages (from Jinja2==2.7.2->-r pip_requirements.txt (line 3)) (1.0)\nRequirement already satisfied: six in ./env/lib/python2.7/site-packages (from python-dateutil==2.2->-r pip_requirements.txt (line 6)) (1.11.0)\nRequirement already satisfied: pyasn1-modules>=0.1.5 in ./env/lib/python2.7/site-packages (from python-ldap->-r pip_requirements.txt (line 9)) (0.2.1)\nRequirement already satisfied: pyasn1>=0.3.7 in ./env/lib/python2.7/site-packages (from python-ldap->-r pip_requirements.txt (line 9)) (0.4.2)\nRequirement already satisfied: pytz>=0a in ./env/lib/python2.7/site-packages (from Babel>=1.0->Flask-Babel==0.9->-r pip_requirements.txt (line 2)) (2018.4)\n\n```\n\n11.启动open-falcon的后端，注意check的时候，每个组件需要都是up状态，down状态说明出了问题\n\n```\ncd ~/software/open-falcon-v0.2.1\n./open-falcon start\n\n[falcon-graph] 16736\n[falcon-hbs] 16749\n[falcon-judge] 16763\n[falcon-transfer] 16774\n[falcon-nodata] 16786\n[falcon-aggregator] 16797\n[falcon-agent] 16809\n[falcon-gateway] 16819\n[falcon-api] 16829\n[falcon-alarm] 16844\n\n./open-falcon check\n\n        falcon-graph         UP           16736 \n          falcon-hbs         UP           16749 \n        falcon-judge         UP           16763 \n     falcon-transfer         UP           16774 \n       falcon-nodata         UP           16786 \n   falcon-aggregator         UP           16797 \n        falcon-agent         UP           16809 \n      falcon-gateway         UP           16819 \n          falcon-api         UP           16829 \n        falcon-alarm         UP           16844\n\n```\n\n&nbsp;以开发者模式启动前端组件\n\n```\n./env/bin/python wsgi.py\n\n```\n\n12.访问 127.0.0.1:8081，注册一个自己的账号和密码，然后登录，成功\n\n&nbsp;<img src=\"/images/517519-20180502104358071-1718571620.png\" alt=\"\" />\n\n&nbsp;\n","tags":["open-falcon"]},{"title":"open-falcon监控Flume","url":"/open-falcon监控Flume.html","content":"1.首先你需要知道flume的http监控端口是否启动\n\n请参考博文 [Flume的监控参数](http://www.cnblogs.com/tonglin0325/p/8963395.html)\n\n即在 http://localhost:3000/metrics 可以访问到如下内容\n\n<img src=\"/images/517519-20180502105657695-1413771658.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n2.在open-falcon中安装flume监控插件，参考官方文档 http://book.open-falcon.org/zh_0_2/usage/flume.html\n\n官方文档写的很不清楚，请参考本文接下来给出的步骤\n\n首先修改agent的配置文件，agent负责的是采集数据，同时有调度脚本插件的功能\n\n```\n~/software/open-falcon-v0.2.1/agent/config\nvim cfg.json\n\n```\n\n&nbsp;修改如下，即写入了flume监控脚本的git地址，在此感谢插件作者在学习过程中的指导\n\n```\nhttps://github.com/mdh67899/openfalcon-monitor-scripts\n\n```\n\n&nbsp;<img src=\"/images/517519-20180502110913963-1706647185.png\" alt=\"\" />\n\n在内存中更新cfg.json配置，如果不更新配置是没有加载的，访问下面网址就可以\n\n```\nhttp://127.0.0.1:1988/config/reload\n\n```\n\n&nbsp;<img src=\"/images/517519-20180502111252323-1821073085.png\" alt=\"\" />\n\n&nbsp;\n\n下载插件，访问 http://localhost:1988/plugin/update\n\n<img src=\"/images/517519-20180502111339772-967148150.png\" alt=\"\" />\n\n之后查看目录 ~/software/open-falcon-v0.2.1/plugin，会发现多了一个flume文件夹，这个就是刚刚下载下来的插件\n\n```\n~/software/open-falcon-v0.2.1/plugin$ ls\nflume\n\n```\n\n查看下载的插件有没有执行权限\n\n```\nlintong@master:~/software/open-falcon-v0.2.1/plugin/flume$ ls\n60_flume-monitor.py  README.md\nlintong@master:~/software/open-falcon-v0.2.1/plugin/flume$ ls -al\n总用量 16\ndrwxrwxr-x 2 lintong lintong 4096 5月   2 11:13 .\ndrwxrwxr-x 4 lintong lintong 4096 5月   2 11:13 ..\n-rw-rw-r-- 1 lintong lintong 2813 5月   2 11:13 60_flume-monitor.py\n-rw-rw-r-- 1 lintong lintong 3981 5月   2 11:13 README.md\n\n```\n\n如果没有执行权限的话，添加执行权限\n\n```\nchmod +x 60_flume-monitor.py\n\n```\n\n&nbsp;3.在open-falcon中绑定插件\n\n首先在host group中添加一个新的组，起名就叫flume吧\n\n<img src=\"/images/517519-20180502112113375-774579427.png\" alt=\"\" width=\"1348\" height=\"169\" />\n\n添加一个host，就添加本机，我的本机叫lintong-XXXX\n\n<img src=\"/images/517519-20180502112408697-1884392905.png\" alt=\"\" width=\"1364\" height=\"172\" />\n\n&nbsp;添加一个plugins，就添加flume插件\n\n<img src=\"/images/517519-20180502112538900-1216451973.png\" alt=\"\" width=\"1036\" height=\"211\" />\n\n注意：这里的**plugin dir**和 ~/software/open-falcon-v0.2.1/agent/config/cfg.json中的**dir**有关\n\n比如我们的**cfg.json**中写的是./plugin，这里的根目录指的是 ~/software/open-falcon-v0.2.1，然后**插件的安装目录**就是 ~/software/open-falcon-v0.2.1/plugin\n\n然后我们的**plugin dir**需要和**插件安装目录**组成一个完整的插件地址\n\n因为我们的插件地址在 ~/software/open-falcon-v0.2.1/plugin/flume/60_flume-monitor.py，所以这里的**plugin dir**就要写flume\n\n4.对flume监控脚本进行修改\n\n```\n#r = requests.post(\"http://127.0.0.1:1988/v1/push\", data=json.dumps(payload)) # 去掉注释\n\n```\n\n5.重新启动open-falcon，这一步待寻找有没有不用重启的方式\n\n```\n./open-falcon stop\n./open-falcon start\n\n```\n\n6.在agent/logs下查看插件运行日志，已经出现了，说明插件已经成功run起来了\n\n```\n2018/05/02 12:41:23 plugin.go:78: <Plugins:[flume], Timestamp:1525236083>\n\n```\n\n&nbsp;\n","tags":["open-falcon"]},{"title":"Python学习笔记——发邮件","url":"/Python学习笔记——发邮件.html","content":"参考：[Python3实现163邮箱SMTP发送邮件](https://blog.csdn.net/weixin_40475396/article/details/78693408)\n\n1.首先需要注册一个网易的邮箱，开启smtp服务，并使用其授权码\n\n2.发送邮件的Python脚本\n\n```\n#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport smtplib\nfrom email.header import Header\nfrom email.mime.text import MIMEText\n\n# 第三方 SMTP 服务  \nmail_host = \"smtp.163.com\"  # SMTP服务器\nmail_user = \"XXX\"  # 用户名\nmail_pass = \"XXX\"  # 授权密码，非登录密码\n\nsender = \"XXX@163.com\"  # 发件人邮箱(最好写全, 不然会失败)\nreceivers = [\"XXX@126.com\"]  # 接收邮件，可设置为你的QQ邮箱或者其他邮箱\n\ncontent = '我用Python'\ntitle = '人生苦短'  # 邮件主题  \n\n\ndef sendEmail():\n    message = MIMEText(content, 'plain', 'utf-8')  # 内容, 格式, 编码\n    message['From'] = \"{}\".format(sender)\n    message['To'] = \",\".join(receivers)\n    message['Subject'] = title\n\n    try:\n        smtpObj = smtplib.SMTP_SSL(mail_host, 465)  # 启用SSL发信, 端口一般是465  \n        smtpObj.login(mail_user, mail_pass)  # 登录验证  \n        smtpObj.sendmail(sender, receivers, message.as_string())  # 发送  \n        print(\"mail has been send successfully.\")\n    except smtplib.SMTPException as e:\n        print(e)\n\n\ndef send_email2(SMTP_host, from_account, from_passwd, to_account, subject, content):\n    email_client = smtplib.SMTP(SMTP_host)\n    email_client.login(from_account, from_passwd)\n    # create msg  \n    msg = MIMEText(content, 'plain', 'utf-8')\n    msg['Subject'] = Header(subject, 'utf-8')  # subject  \n    msg['From'] = from_account\n    msg['To'] = to_account\n    email_client.sendmail(from_account, to_account, msg.as_string())\n\n    email_client.quit()\n\n\nif __name__ == '__main__':\n    sendEmail()\n    # receiver = '***'  \n    # send_email2(mail_host, mail_user, mail_pass, receiver, title, content)  \n\n```\n\n<!--more-->\n&nbsp;\n","tags":["Python"]},{"title":"Flume的监控参数","url":"/Flume的监控参数.html","content":"参考 [flume的http监控参数说明](http://flume.cn/2016/05/18/flume%E7%9A%84http%E7%9B%91%E6%8E%A7%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E/)\n\n<!--more-->\n&nbsp;\n\n普通的flume启动命令\n\n```\nbin/flume-ng agent -c conf -f conf/flume-conf.properties -n agent -Dflume.root.logger=INFO,console\n\n```\n\n日志信息在终端输出，只有去掉这个参数，日志才能在log4j和logback中输出\n\n```\n-Dflume.root.logger=INFO,console\n\n```\n\n如果要加上**http监控**的话\n\n```\nbin/flume-ng agent -c conf -f conf/flume-conf.properties -n agent -Dflume.root.logger=INFO,console -Dflume.monitoring.type=http -Dflume.monitoring.port=34545\n\n```\n\n&nbsp;即加上参数，flume.monitoring.type=http 指定了Reporting的方式为http，flume.monitoring.port 指定了http服务的端口号\n\n```\n-Dflume.monitoring.type=http -Dflume.monitoring.port=34545\n\n```\n\n&nbsp;访问\n\n```\nhttp://localhost:34545/metrics\n\n```\n\n<img src=\"/images/517519-20180428170053870-1849949372.png\" alt=\"\" />\n\n&nbsp;\n\n**参数说明：**\n\n### （1）、SOURCE\n\nSOURCE作为flume的数据源组件，所有收集日志的第一个到达的地方，它的监控信息非常重要。通过监控我们能够得到的监控数据有这些：\n\nKafkaEventGetTimer\n\nAppendBatchAcceptedCount（追加到channel中的批数量） 速率\n\nEventAcceptedCount（成功放入channel的event数量） 速率\n\nAppendReceivedCount（source追加目前收到的数量） 速率\n\nStartTime（组件开始时间）\n\nAppendBatchReceivedCount（source端刚刚追加的批数量） 速率\n\nKafkaCommitTimer\n\nEventReceivedCount（source端成功收到的event数量） 速率\n\nType（组件类型）\n\nAppendAcceptedCount（放入channel的event数量） 速率\n\nOpenConnectionCount（打开的连接数）\n\nKafkaEmptyCount\n\nStopTime（组件停止时间）\n\n当然这些只是flume监控源码中已经自带的监控元素，如果你需要其他的监控信息，例如ip、端口号等，有两种方法，第一个，修改监控源码，添加你需要的监控元素，这种方法只是在原有代码基础上，添加一些满足自己需求的监控元素，比较简单，但灵活性不足；第二个就是自定义监控组件，这种方法是在原有监控框架中，自己实现自己的监控组件，这样可以达到完全满足自己需求，且灵活性很高。至于这两种方法如何操作，在后面Flume监控如何实现有讨论到。\n\n同理CHANNEL、SINK这两个组件的监控也可以使用这两种方法来添加自己想要的监控元素。\n\n### （2）、CHANNEL\n\nCHANNEL是flume的一个通道组件，对数据有一个缓存的作用。能够得到的数据：\n\nChannelCapacity（通道容量）\n\nChannelFillPercentage（通道使用比例）\n\nType（组件类型）\n\nChannelSize（目前在channel中的event数量）\n\nEventTakeSuccessCount（从channel中成功取走的event数量） 速率\n\nEventTakeAttemptCount（尝试从channel中取走event的次数） 速率\n\nStartTime（组件开始时间）\n\nEventPutAttemptCount（尝试放入将event放入channel的次数） 速率\n\nEventPutSuccessCount（成功放入channel的event数量） 速率\n\nStopTime（组件停止时间）\n\n### （3）、SINK\n\nSINK是数据即将离开flume的最后一个组件，它从channel中取走数据，然后发送到缓存系统或者持久化数据库。能得到数据：\n\nConnectionCreatedCount（创建连接数） 速率\n\nBatchCompleteCount(完成的批数量)&nbsp; 速率\n\nBatchEmptyCount（批量取空的数量，`空的批量的数量，如果数量很大表示souce写数据比sink清理数据慢速度慢很多`） 速率\n\nEventDrainSuccessCount（成功发送event的数量） 速率\n\nStartTime（组件开始时间）\n\nBatchUnderflowCount（正处于批量处理的batch数）等。&nbsp; 速率\n\nConnectionFailedCount（连接失败数） 速率\n\nConnectionClosedCount（关闭连接数量） 速率\n\nType（组件类型）\n\nRollbackCount\n\nEventDrainAttemptCount（尝试提交的event数量） 速率\n\nKafkaEventSendTimer\n\nStopTime（组件停止时间）\n\n&nbsp;\n\n在实际生产环境中，由于**数据量比较大（Kafka中导入200M左右的数据）**，Flume有时候会遇到下面**oom问题**：\n\n问题1\n\n```\nException in thread \"PollableSourceRunner-KafkaSource-r1\" java.lang.OutOfMemoryError: GC overhead limit exceeded\n\n```\n\n或者\n\n```\nException in thread \"PollableSourceRunner-KafkaSource-r1\" java.lang.OutOfMemoryError: Java heap space\n\n```\n\n&nbsp;这是由于**flume启动时的默认最大的堆内存大小是20M**\n\n**解决方法：在flume的基础配置文件conf下的flume-env.sh中添加**\n\n```\nexport JAVA_OPTS=\"-Xms2048m -Xmx2048m -Xss256k -Xmn1g -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:-UseGCOverheadLimit\"\n\n```\n\n问题2\n\n```\n13:54:27.213 ERROR org.apache.flume.source.kafka.KafkaSource:317 - KafkaSource EXCEPTION, {}\norg.apache.flume.ChannelFullException: Space for commit to queue couldn't be acquired. Sinks are likely not keeping up with sources, or the buffer size is too tight\n\n```\n\n&nbsp;flume的properties文件中添加\n\n```\nagent.channels.c1.capacity = 1000000 #改大一点\nagent.channels.c1.keep-alive = 60\n\n```\n\n&nbsp;\n","tags":["flume"]},{"title":"Ubuntu下安装Kafka Manager","url":"/Ubuntu下安装Kafka Manager.html","content":"参考 ： [kafka管理器kafka-manager部署安装](https://blog.csdn.net/lsshlsw/article/details/47300145)\n\n<!--more-->\n&nbsp;\n\n下载Kafka Manager，并进行打包，由于Kafka manager是由scala写的，所以需要由sbt的支持\n\n```\ngit clone https://github.com/yahoo/kafka-manager\ncd kafka-manager\n./sbt clean dist\n\n```\n\n&nbsp;关于sbt的的安装，请移步\n\n```\nhttp://www.cnblogs.com/tonglin0325/p/8884470.html\n\n```\n\n&nbsp;配置zk的地址\n\n```\n在conf/application.conf中将kafka-manager.zkhosts的值设置为localhost\n\n```\n\n&nbsp;打包完成后，对kafka-manager-1.3.3.17.zip包进行解压，位置在\n\n```\nkafka-manager/target/universal\n\n```\n\n&nbsp;修改配置\n\n```\nvim kafka-manager-1.3.3.17/conf/application.conf\n\n```\n\n&nbsp;启动Kafka manager\n\n```\nbin/kafka-manager -Dconfig.file=/home/lintong/software/kafka-manager-1.3.3.17/conf/application.conf -Dhttp.port=7778\n\n```\n\n访问\n\n```\nhttp://localhost:9000\n\n```\n\n然后在cluster中add cluster，比如\n\n<img src=\"/images/517519-20180427203627212-731388856.png\" alt=\"\" />\n\n&nbsp;\n\n启动后如果遇到\n\n```\n[warn] o.a.k.c.p.Errors - Unexpected error code: 38.\n[warn] o.a.k.c.NetworkClient - Error while fetching metadata with correlation id 98 : {__consumer_offsets=UNKNOWN}\n\n```\n\n&nbsp;就手动创建一个叫__consumer_offsets的topic\n\n如果你的kafka只有一台机器,然后遇到kafka-console-consumer无法消费的情况,可以查看一下offsets.topic.replication.factor这个参数是否为1,不是1的话消费者的offset是无法提交上去的\n\n<img src=\"/images/517519-20190918235137834-230371540.png\" alt=\"\" />\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;启动Kafka的JMX，只需要修改kafka的启动命令就行，注意给kafka添加jmx需要在kafka manager之前\n\n&nbsp;启动kafka服务时指定JMX_PORT值:\n\n```\nJMX_PORT=9999 bin/kafka-server-start.sh -daemon config/server.properties   //一台机器部署多个server采用此方法\n\n```\n\n&nbsp;或者修改kafka-server-start.sh，在前面加上：\n\n```\nexport JMX_PORT=9999  //如果一台机器部署一个server 建议采用此方法\n\n```\n\n&nbsp;<img src=\"/images/517519-20180427203508544-1720696408.png\" alt=\"\" width=\"1320\" height=\"530\" />\n\n点击please enable consumer plooing here，配置查看消费者信息\n\n<img src=\"/images/517519-20200317234509701-402686842.png\" alt=\"\" width=\"1000\" height=\"781\" />\n\n&nbsp;\n\nKafka Manager API 参考\n\n```\nhttps://github.com/yahoo/CMAK/blob/master/conf/routes\n\n```\n\n　　\n\n关于Kafka Manager中的一些参数说明：\n\n## Topic 指标\n\nReplication （副本数）\n\nNumber of Partitions (分区数)\n\nSum of partition offsets (offset大小，需要开启JMX支持）\n\nTotal number of Brokers （Broker总数）\n\nNumber of Brokers for Topic （Topic所占Broker数）\n\nPreferred Replicas % （）\n\nBrokers Skewed % （Broker 均衡率）\n\nBrokers Spread % （Broker 扩散率）\n\nUnder-replicated % （处于同步状态的比率）\n\n&nbsp;\n\n## Metrics 指标\n\nMessage in /sec　　消息数据流量\n\nBytes in /sec　　kafka输入数据流量\n\nBytes out /sec　　kafka输出数据流量\n\nBytes rejected /sec　　拒绝的流量\n\nFailed fetch request /sec　　失败的获取请求\n\nFailed produce request /sec　　失败的生产请求\n","tags":["kafka"]},{"title":"Ubuntu系统监控indicator-sysmonitor","url":"/Ubuntu系统监控indicator-sysmonitor.html","content":"参考： http://www.cnblogs.com/EasonJim/p/7130171.html\n\n安装**indicator-sysmonitor**\n\n```\nsudo add-apt-repository ppa:fossfreedom/indicator-sysmonitor\nsudo apt-get update\nsudo apt-get install indicator-sysmonitor\n\n```\n\n或者直接下载deb包\n\n```\nhttps://launchpad.net/indicator-sysmonitor/+download\n\n```\n\n启动：\n\nindicator-sysmonitor &amp;\n\n添加\n\ncpu: {cpu} 内存: {mem} 网络: {net}保存\n\n<!--more-->\n&nbsp;\n\n还有另一款**Indicator-Multiload**\n\n```\nsudo add-apt-repository ppa:indicator-multiload/stable-daily\nsudo apt-get update\nsudo apt-get install indicator-multiload\n\n```\n\n&nbsp;\n","tags":["Linux"]},{"title":"kafka知识点","url":"/kafka知识点.html","content":"1. 搭建kafka的时候需要根据数据流量预估kafka集群的规模，aws为其MSK服务（aws上的托管kafka）提供了一个excel表格，可以输入参数来评估集群需要的硬件参数\n\n```\nhttps://amazonmsk.s3.amazonaws.com/MSK_Sizing_Pricing.xlsx\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["kafka"]},{"title":"Ubuntu下安装sbt","url":"/Ubuntu下安装sbt.html","content":"参考 <!--more-->\n&nbsp;[ubuntu14 手动安装sbt](http://www.cnblogs.com/wrencai/p/3867898.html)\n\n&nbsp;\n\n1、下载sbt通用平台压缩包：sbt-0.13.5.tgz\n\n```\nhttp://www.scala-sbt.org/download.html\n```\n\n&nbsp;\n\n　2、建立目录，解压文件到所建立目录\n\n```\n$ sudo tar zxvf sbt-0.13.5.tgz -C /opt/scala/\n```\n\n&nbsp;\n\n　3、建立启动sbt的脚本文件\n\n```\n/*选定一个位置，建立启动sbt的脚本文本文件，如/opt/scala/sbt/ 目录下面新建文件名为sbt的文本文件*/\n$ cd /opt/scala/sbt/\n$ vim sbt\n/*在sbt文本文件中添加 \nSBT_OPTS=\"-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M\"\njava $SBT_OPTS -jar /opt/scala/sbt/bin/sbt-launch.jar \"$@\" \n然后按esc键 输入 :wq 保存退出，注意红色字体中的路径是定位到解压的sbt文件包中的sbt-launch.jar文件的绝对路径*/\n\n/&times;修改sbt文件权限&times;/\n$ chmod u+x sbt \n\n```\n\n&nbsp;\n\n　4、配置PATH环境变量，保证在控制台中可以使用sbt命令\n\n```\n$ vim ~/.bashrc\n/*在文件尾部添加如下代码后，保存退出*/\nexport PATH=/opt/scala/sbt/:$PATH\n\n/*使配置文件立刻生效*/\n$ source ~/.bashrc\n\n```\n\n&nbsp;\n\n　5、测试sbt是否安装成功\n\n```\n/*第一次执行时，会下载一些文件包，然后才能正常使用，要确保联网了，安装成功后显示如下*/\n$ sbt sbt-version\n[info] Set current project to sbt (in build file:/opt/scala/sbt/)\n[info] 0.13.5\n\n```\n\n&nbsp;\n\n6. 设置源，vim ~/.sbt/repositories\n\n```\n[repositories]\nlocal\nhuaweicloud-maven: https://repo.huaweicloud.com/repository/maven/\nmaven-central: https://repo1.maven.org/maven2/\nhuaweicloud-ivy: https://repo.huaweicloud.com/repository/ivy/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext]\n\n```\n\n&nbsp;或者\n\n```\n[repositories]\nlocal\naliyun: https://maven.aliyun.com/repository/public\ntypesafe: https://repo.typesafe.com/typesafe/ivy-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext], bootOnly\nivy-sbt-plugin:https://dl.bintray.com/sbt/sbt-plugin-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext]\nsonatype-oss-releases\nmaven-central\nsonatype-oss-snapshots\n\n```\n\n&nbsp;\n","tags":["Linux"]},{"title":"Ubuntu安装shadow$ocks-libev","url":"/Ubuntu安装shadow$ocks-libev.html","content":"参考 [ubuntu16.04 配置shadowsocks及使用教程，支持chacha20-ietf-poly1305加密方式 ](https://blog.csdn.net/yasinsun/article/details/79217657)\n\n### 安装shadow$ocks-libev\n\n```\nsudo apt-get install software-properties-common -y\nsudo add-apt-repository ppa:max-c-lv/shadowsocks-libev -y\nsudo apt-get update\nsudo apt install shadowsocks-libev \n\n```\n\n### **<!--more-->\n&nbsp;配置文件**\n\n```\nsudo vi /etc/shadowsocks-libev.json\n\n```\n\n内容\n\n```\n{\n\"server\":\"XXXX服务器地址\",\n\"server_port\":XXXX端口,\n\"local_address\":\"127.0.0.1\",\n\"local_port\":1080,\n\"password\":\"XXXX密码\",\n\"timeout\":60,\n\"method\":\"chacha20-ietf-poly1305\",\n\"fast_open\":false,\n\"workers\":1\n}\n\n```\n\n### &nbsp;运行shadow$ocks\n\n```\nss-local -c /etc/shadowsocks-libev.json &amp;\n\n```\n\n&nbsp;\n\n`Centos下安装`\n\n`参考`\n\n```\nhttps://gist.github.com/aa65535/ea090063496b0d3a1748\nhttps://roxhaiy.wordpress.com/2017/08/04/430/\n\n```\n\n步骤\n\n```\ncd /tmp\n# 编译环境准备&amp;安装依赖包\nyum install -y gcc make libtool build-essential git\nyum install -y yum install gettext gcc autoconf libtool automake make asciidoc xmlto c-ares-devel libev-devel\n# 克隆源码\ngit clone --recursive https://github.com/shadowsocks/shadowsocks-libev.git\n# 开始编译\ncd shadowsocks-libev\n./autogen.sh\n./configure --prefix=/usr &amp;&amp; make\nmake install\n# 准备必须的文件\nmkdir -p /etc/shadowsocks-libev\ncp ./rpm/SOURCES/etc/init.d/shadowsocks-libev /etc/init.d/shadowsocks-libev\ncp ./debian/config.json /etc/shadowsocks-libev/config.json\nchmod +x /etc/init.d/shadowsocks-libev\n# 编辑配置文件\nvim /etc/shadowsocks-libev/config.json\n# 添加开机自启动服务\nchkconfig --add shadowsocks-libev\nchkconfig shadowsocks-libev on\n# 启动服务\nservice shadowsocks-libev start\n\n```\n\n期间遇到\n\nconfigure: error: mbed TLS libraries not found.\n\n```\nyum install -y mbedtls-devel\n\n```\n\nconfigure: error: The Sodium crypto library libraries not found.\n\n```\ncd /tmp\nwget https://download.libsodium.org/libsodium/releases/libsodium-1.0.13.tar.gz\ntar -zxvf libsodium-1.0.13.tar.gz\ncd libsodium-1.0.13\n./configure\nmake &amp;&amp; make check\nsudo make install\n\n```\n\n配置\n\n```\n{\n    \"server\":\"0.0.0.0\",\n    \"server_port\":xxxx,\n    \"local_port\":xxxx,\n    \"password\":\"xxxx\",\n    \"timeout\":60,\n    \"method\":\"chacha20-ietf-poly1305\"\n}\n\n```\n\n&nbsp;\n\n`&nbsp;`\n","tags":["Linux"]},{"title":"ubuntu下GRUB使用","url":"/ubuntu下GRUB使用.html","content":"GRUB 是一个用于加载和管理系统启动的完整程序。它是Linux 发行版中最常见的 引导程序 bootloader 。引导程序是计算机启动时运行的第一个软件。\n\n## 1.grub命令行模式\n\n如果进入的grub命令行模式的话，则说明GNU grub找不到正确的引导文件，这时候可以通过命令手动来进行选择，如下\n\n<img src=\"/images/517519-20230907114650884-1677157538.png\" width=\"700\" height=\"525\" loading=\"lazy\" />\n\n查看当前路径\n\n```\ngrub> ls\n\n```\n\n输出如下\n\n```\n(proc) (hd0)、(hd0, msdos1) (hd1) (hd1,gpt4) (hd1,gpt3) (hd1,gpt2) (hd1,gpt1) (hd2) (hd2,gpt3) (hd2,gpt2) (hd2,gpt1) (hd3) (hd3,gpt1)\n\n```\n\n我这边机器挂了4块硬盘，所以就会有hd0，hd1，hd2，hd4\n\n这时候就需要找到linux系统安装所在的分区，只能每个盘的每个分区一个个试过去，比如hd0只有1个分区，我们就可以使用如下命令查看1分区的文件目录\n\n```\ngrub> ls (hd0,1) #再按tab键\n\n```\n\n出现的是lost+found，说明这是一个数据盘，并不是linux系统分区，当试到hd1,4的时候\n\n```\ngrub> ls (hd1,4) #再按tab键\n\n```\n\n出现，说明找到了linux系统安装的盘\n\n```\nlost+found/ etc/ media/ bin/ boot/ dev/ home/ lib/ lib64/ mnt/ opt/ proc/ root/ run/ sbin/ snap/ srv/ sys/ tmp/ usr/ var/ initrd.img/ vmlinux cdrom/ lib32/\n\n```\n\n接下来使用如下命令选择这个引导\n\n```\nset root=(hd1,4)\nset prefix=(hd1,4)/boot/grub\ninsmod normal\nnormal\n\n```\n\n会正常进到grub系统选择页面，然后就可以正常进入linux系统，如下\n\n<img src=\"/images/517519-20230906220114516-1292972033.png\" width=\"700\" height=\"466\" />\n\n参考：[Ubuntu开机出现grub指令，无法正常开机](https://juejin.cn/post/6844903934255955976)\n\n## 2.修复GRUB引导\n\n安装boot-repair来对引导进行修复\n\n```\nsudo add-apt-repository ppa:yannubuntu/boot-repair &amp;&amp; sudo apt-get update\nsudo apt-get install -y boot-repair\n\n```\n\n然后运行boot-repair按布置操作即可\n\n参考：[ubuntu启动盘修复grub引导](https://blog.csdn.net/kevin_1996/article/details/124086483)\n\n对于ubuntu18.04以下的版本，可能无法通过apt-get来安装boot-reapir，会报源找不到的错误\n\n可以直接下载deb包来进行安装boot-repair\n\n```\n# Download debian packages\nwget https://launchpad.net/~yannubuntu/+archive/ubuntu/boot-repair/+files/glade2script_3.2.3~ppa4_all.deb\nwget https://launchpad.net/~yannubuntu/+archive/ubuntu/boot-repair/+files/boot-sav_4ppa65_all.deb\nwget https://launchpad.net/~yannubuntu/+archive/ubuntu/boot-repair/+files/boot-repair_4ppa65_all.deb\n\n# Attempt to install them\nsudo dpkg -i ./glade2script_3.2.3~ppa4_all.deb\nsudo dpkg -i ./boot-sav_4ppa65_all.deb\nsudo dpkg -i ./boot-repair_4ppa65_all.deb\n\n# Now that dpkg knows we need to install dependancies for these packages\n# use apt-get to auto-install said dependancies\nsudo apt-get -f install\n\n# Install the packages FOR REAL this time\nsudo dpkg -i ./glade2script_3.2.3~ppa4_all.deb\nsudo dpkg -i ./boot-sav_4ppa65_all.deb\nsudo dpkg -i ./boot-repair_4ppa65_all.deb\n\n```\n\n参考：\n\n```\nhttps://gist.github.com/wilm0x42/6f11a58d3ef1ccb1238045e29834af40\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["Linux"]},{"title":"Kafka中bootstrap-server、broker-list和zookeeper的区别","url":"/Kafka中bootstrap-server、broker-list和zookeeper的区别.html","content":"参考 [Kafka bootstrap-servers vs zookeeper in kafka-console-consumer](https://stackoverflow.com/questions/41774446/kafka-bootstrap-servers-vs-zookeeper-in-kafka-console-consumer)<!--more-->\n&nbsp; 中说建议使用新版(新版本指的是kafka 0.8.0之后的版本)的 --bootstrap-server\n\n&nbsp;\n\nKafka专业术语，参考 [Apache kafka 工作原理介绍](https://www.ibm.com/developerworks/cn/opensource/os-cn-kafka/index.html)\n\n**Broker**：Kafka 集群包含一个或多个服务器，这种服务器被称为 broker。\n\n**Topic**：每条发布到 Kafka 集群的消息都有一个类别，这个类别被称为 Topic。（物理上不同 Topic 的消息分开存储，逻辑上一个 Topic 的消息虽然保存于一个或多个 broker 上，但用户只需指定消息的 Topic 即可生产或消费数据而不必关心数据存于何处）。\n\n**Partition**：Partition 是物理上的概念，每个 Topic 包含一个或多个 Partition。\n\n**Producer**：负责发布消息到 Kafka broker。\n\n**Consumer**：消息消费者，向 Kafka broker 读取消息的客户端。\n\n**Consumer Group**：每个 Consumer 属于一个特定的 Consumer Group（可为每个 Consumer 指定 group name，若不指定 group name 则属于默认的 group）。\n\n&nbsp;\n\n在**《Kafka权威指南》**中是这样描述的\n\n<img src=\"/images/517519-20180425152328157-954538404.png\" alt=\"\" />\n\n&nbsp;\n\n对于**消费者**，kafka中有两个设置的地方：对于老的消费者，由**--zookeeper参数**设置；对于新的消费者，由**--bootstrap-server参数**设置\n\n如果使用了--zookeeper参数,那么consumer的信息将会存放在zk之中\n\n查看的方法是使用./zookeeper-client,然后 ls /consumers/[group_id]/offsets/[topic]/[broker_id-part_id],这个是查看某个group_id的某个topic的offset\n\n如果使用了--bootstrap-server参数,那么consumer的信息将会存放在kafka之中\n\n<img src=\"/images/517519-20180425153405277-265739183.png\" alt=\"\" />\n\n&nbsp;\n\n对于**console生产者**，**--broker-list参数**指定了所使用的broker\n\n&nbsp;\n","tags":["kafka"]},{"title":"idea中git颜色不显示或者文件右键没有git按钮解决方法","url":"/idea中git颜色不显示或者文件右键没有git按钮解决方法.html","content":"**VCS--->Enable Version Control Integration**，然后选择git就可以了\n","tags":["开发工具"]},{"title":"flume学习笔记——安装和使用","url":"/flume学习笔记——安装和使用.html","content":"**Flume**是一个分布式、可靠、和高可用的**海量日志聚合的系统**，支持在系统中定制各类数据发送方，用于收集数据；<br />同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。\n\n**Flume**是一个专门设计用来从大量的源，推送数据到Hadoop生态系统中各种各样存储系统中去的，例如HDFS和HBase。\n\n**Guide**: http://flume.apache.org/FlumeUserGuide.html\n\n<!--more-->\n&nbsp;\n\n## 体系架构 \n\nFlume的数据流由**事件(Event)**贯穿始终。**事件**是Flume的基本数据单位，它携带日志数据(字节数组形式)并且携带有头信息，这些Event由Agent外部的Source生成，当**Source捕获事件**后会进行特定的格式化，然后Source会把事件推入(单个或多个)**Channel**中。你可以把Channel看作是一个缓冲区，它将保存事件直到**Sink处理**完该事件。Sink负责持久化日志或者把事件推向另一个Source。\n\n&nbsp;Flume以**Flume Agent**为**最小的独立运行单位**。一个Agent就是一个JVM。**单agent由Source、Sink和Channel三大组件构成。**一个Flume Agent可以连接一个或者多个其他的Flume Agent；一个Flume Agent也可以从一个或者多个Flume Agent接收数据。\n\n**注意**：在Flume管道中如果有意想不到的错误、超时并进行了重试，Flume会产生重复的数据最终被被写入，后续需要处理这些冗余的数据。\n\n**<img src=\"/images/517519-20180411211152242-1287192651.png\" alt=\"\" />**\n\n具体可以参考文章：[Flume教程(一) Flume入门教程 ](https://blog.csdn.net/yuan_xw/article/details/51143698)\n\n&nbsp;\n\n## 组件 \n\n**Source**：source是从一些其他产生数据的应用中接收数据的活跃组件。Source可以监听一个或者多个网络端口，用于接收数据或者可以从本地文件系统读取数据。每个Source必须至少连接一个Channel。基于一些标准，一个Source可以写入几个Channel，复制事件到所有或者某些Channel。\n\nSource可以通过处理器 - 拦截器 - 选择器路由写入多个Channel。\n\n**Channel：**channel的行为像队列，Source写入到channel，Sink从Channel中读取。多个Source可以安全地写入到相同的Channel，并且多个Sink可以从相同的Channel进行读取。\n\n可是一个Sink只能从一个Channel读取。如果多个Sink从相同的Channel读取，它可以保证只有一个Sink将会从Channel读取一个指定特定的事件。\n\nFlume自带两类Channel：Memory Channel和File Channel。Memory Channel的数据会在JVM或者机器重启后丢失；File Channel不会。\n\n**Sink: **sink连续轮询各自的Channel来读取和删除事件。\n\n**拦截器：**每次Source将数据写入Channel，它是通过委派该任务到其Channel处理器来完成，然后Channel处理器将这些事件传到一个或者多个Source配置的**拦截器**中。\n\n拦截器是一段代码，基于某些标准，如正则表达式，拦截器可以用来删除事件，为事件添加新报头或者移除现有的报头等。每个Source可以配置成使用多个拦截器，按照配置中定义的顺序被调用，将拦截器的结果传递给链的下一个单元。一旦拦截器处理完事件，拦截器链返回的事件列表传递到Channel列表，即通过Channel选择器为每个事件选择的Channel。\n\n<img src=\"/images/517519-20180418170946454-1536572889.png\" alt=\"\" />\n\n&nbsp;\n<td valign=\"top\" width=\"76\">组件</td><td valign=\"top\" width=\"680\">功能</td>\n\n功能\n<td valign=\"top\" width=\"76\">Agent</td><td valign=\"top\" width=\"680\">使用JVM运行Flume。每台机器运行一个agent，但是可以在一个agent中包含多个sources和sinks。</td>\n\n使用JVM运行Flume。每台机器运行一个agent，但是可以在一个agent中包含多个sources和sinks。\n<td valign=\"top\" width=\"76\">Client</td><td valign=\"top\" width=\"680\">生产数据，运行在一个独立的线程。</td>\n\n生产数据，运行在一个独立的线程。\n<td valign=\"top\" width=\"76\">Source</td><td valign=\"top\" width=\"680\">从Client收集数据，传递给Channel。</td>\n\n从Client收集数据，传递给Channel。\n<td valign=\"top\" width=\"76\">Sink</td><td valign=\"top\" width=\"680\">从Channel收集数据，运行在一个独立线程。</td>\n\n从Channel收集数据，运行在一个独立线程。\n<td valign=\"top\" width=\"76\">Channel</td><td valign=\"top\" width=\"680\">连接sources和sinks，这个有点像一个队列。</td>\n\n连接sources和sinks，这个有点像一个队列。\n<td valign=\"top\" width=\"76\">Events</td><td valign=\"top\" width=\"680\">可以是日志记录、avro对象等。</td>\n\n可以是日志记录、avro对象等。\n\n&nbsp;\n\n## 配置文件\n\nFlume Agent使用纯文本配置文件来配置。Flume配置使用属性文件格式，仅仅是用换行符分隔的键值对的纯文本文件，如：key1 = value1；当有多个的时候：agent.sources = r1 r2\n\n参考 [flume配置介绍](https://www.jianshu.com/p/535d4bb230cd)\n\n&nbsp;\n\n1. 从file source 到 file sink的配置文件\n\n```\n# ========= Name the components on this agent =========\nagent.sources = r1\nagent.channels = c1\nagent.sinks = s1\nagent.sources.r1.interceptors = i1\n\nagent.sources.r1.interceptors.i1.type = Inteceptor.DemoInterceptor$Builder\n\n# ========= Describe the source =============\nagent.sources.r1.type = spooldir\nagent.sources.r1.spoolDir = /home/lintong/桌面/data/input\n\n# ========= Describe the channel =============\n# Use a channel which buffers events in memory\nagent.channels.c1.type = memory\nagent.channels.c1.capacity = 100000\nagent.channels.c1.transactionCapacity = 1000\n\n# ========= Describe the sink =============\nagent.sinks.s1.type = file_roll\nagent.sinks.s1.sink.directory = /home/lintong/桌面/data/output\nagent.sinks.s1.sink.rollInterval = 0\n\n# ========= Bind the source and sink to the channel =============\nagent.sources.r1.channels = c1\nagent.sinks.s1.channel = c1\n\n```\n\n&nbsp;\n\n2. 从kafka source 到 file sink的配置文件，kafka使用zookeeper，但是建议使用bootstrap-server\n\n```\n# ========= Name the components on this agent =========\nagent.sources = r1\nagent.channels = c1\nagent.sinks = s1\nagent.sources.r1.interceptors = i1\n\nagent.sources.r1.interceptors.i1.type = Inteceptor.DemoInterceptor$Builder\n\n# ========= Describe the source =============\nagent.sources.r1.type=org.apache.flume.source.kafka.KafkaSource  \nagent.sources.r1.zookeeperConnect=127.0.0.1:2181  \nagent.sources.r1.topic=test #不能写成topics\n#agent.sources.kafkaSource.groupId=flume  \nagent.sources.kafkaSource.kafka.consumer.timeout.ms=100  \n\n# ========= Describe the channel =============\n# Use a channel which buffers events in memory\nagent.channels.c1.type = memory\nagent.channels.c1.capacity = 100000\nagent.channels.c1.transactionCapacity = 1000\n\n# ========= Describe the sink =============\nagent.sinks.s1.type = file_roll\nagent.sinks.s1.sink.directory = /home/lintong/桌面/data/output\nagent.sinks.s1.sink.rollInterval = 0\n\n# ========= Bind the source and sink to the channel =============\nagent.sources.r1.channels = c1\nagent.sinks.s1.channel = c1\n\n```\n\n&nbsp;\n\n3.kafka source到kafka sink的配置文件\n\n```\n# ========= Name the components on this agent =========\nagent.sources = r1\nagent.channels = c1\nagent.sinks = s1 s2\nagent.sources.r1.interceptors = i1\n\nagent.sources.r1.interceptors.i1.type = com.XXX.interceptor.XXXInterceptor$Builder\n\n# ========= Describe the source =============\nagent.sources.r1.type = org.apache.flume.source.kafka.KafkaSource\nagent.sources.r1.channels = c1\nagent.sources.r1.zookeeperConnect = localhost:2181\nagent.sources.r1.topic = input\n\n# ========= Describe the channel =============\n# Use a channel which buffers events in memory\nagent.channels.c1.type = memory\nagent.channels.c1.capacity = 100000\nagent.channels.c1.transactionCapacity = 1000\n\n# ========= Describe the sink =============\nagent.sinks.s1.type = org.apache.flume.sink.kafka.KafkaSink\nagent.sinks.s1.topic = test\nagent.sinks.s1.brokerList = localhost:9092\n# 避免死循环\nagent.sinks.s1.allowTopicOverride = false\n\nagent.sinks.s2.type = file_roll\nagent.sinks.s2.sink.directory = /home/lintong/桌面/data/output\nagent.sinks.s2.sink.rollInterval = 0\n\n# ========= Bind the source and sink to the channel =============\nagent.sources.r1.channels = c1\nagent.sinks.s1.channel = c1\n#agent.sinks.s2.channel = c1\n\n```\n\n&nbsp;\n\n4.file source到hbase sink的配置文件\n\n从文件读取实时消息，不做处理直接存储到Hbase\n\n```\n# ========= Name the components on this agent =========\nagent.sources = r1\nagent.channels = c1\nagent.sinks = s1\n\n# ========= Describe the source =============\nagent.sources.r1.type = exec\nagent.sources.r1.command = tail -f /home/lintong/桌面/test.log\nagent.sources.r1.checkperiodic = 50\n\n\n# ========= Describe the sink =============\nagent.channels.c1.type = memory\nagent.channels.c1.capacity = 100000\nagent.channels.c1.transactionCapacity = 1000\n\n# agent.channels.file-channel.type = file \n# agent.channels.file-channel.checkpointDir = /data/flume-hbase-test/checkpoint \n# agent.channels.file-channel.dataDirs = /data/flume-hbase-test/data\n\n# ========= Describe the sink =============\nagent.sinks.s1.type = org.apache.flume.sink.hbase.HBaseSink\nagent.sinks.s1.zookeeperQuorum=master:2183\n#HBase表名\nagent.sinks.s1.table=mikeal-hbase-table\n#HBase表的列族名称\nagent.sinks.s1.columnFamily=familyclom1\nagent.sinks.s1.serializer = org.apache.flume.sink.hbase.SimpleHbaseEventSerializer\n#HBase表的列族下的某个列名称\nagent.sinks.s1.serializer.payloadColumn=cloumn-1\n\n\n# ========= Bind the source and sink to the channel =============\nagent.sources.r1.channels = c1\nagent.sinks.s1.channel=c1\n\n```\n\n&nbsp;\n\n5.source是http，sink是kafka\n\n```\n# ========= Name the components on this agent =========\nagent.sources = r1\nagent.channels = c1\nagent.sinks = s1 s2\n\n# ========= Describe the source =============\nagent.sources.r1.type=http\nagent.sources.r1.bind=localhost\nagent.sources.r1.port=50000\nagent.sources.r1.channels=c1\n\n# ========= Describe the channel =============\n# Use a channel which buffers events in memory\nagent.channels.c1.type = memory\nagent.channels.c1.capacity = 100000\nagent.channels.c1.transactionCapacity = 1000\n\n# ========= Describe the sink =============\nagent.sinks.s1.type = org.apache.flume.sink.kafka.KafkaSink\nagent.sinks.s1.topic = test_topic\nagent.sinks.s1.brokerList = master:9092\n# 避免死循环\nagent.sinks.s1.allowTopicOverride = false\n\nagent.sinks.s2.type = file_roll\nagent.sinks.s2.sink.directory = /home/lintong/桌面/data/output\nagent.sinks.s2.sink.rollInterval = 0\n\n# ========= Bind the source and sink to the channel =============\nagent.sources.r1.channels = c1\nagent.sinks.s1.channel = c1\n#agent.sinks.s2.channel = c1\n\n```\n\n&nbsp;\n\n如果在启动flume的时候遇到\n\n```\njava.lang.NoClassDefFoundError: org/apache/hadoop/hbase/***\n\n```\n\n解决方案，在 ~/software/apache/hadoop-2.9.1/etc/hadoop/hadoop-env.sh 中添加\n\n```\nHADOOP_CLASSPATH=/home/lintong/software/apache/hbase-1.2.6/lib/*\n\n```\n\n&nbsp;\n\n5.kafka source到hdfs sink的配置文件\n\n```\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\n# The configuration file needs to define the sources,\n# the channels and the sinks.\n# Sources, channels and sinks are defined per agent,\n# in this case called 'agent'\n\n# ========= Name the components on this agent =========\nagent.sources = r1\nagent.channels = c1\nagent.sinks = s1\nagent.sources.r1.interceptors = i1\n\nagent.sources.r1.interceptors.i1.type = Util.HdfsInterceptor$Builder\n\n# ========= Describe the source =============\nagent.sources.r1.type = org.apache.flume.source.kafka.KafkaSource\nagent.sources.r1.channels = c1\nagent.sources.r1.zookeeperConnect = localhost:2181\nagent.sources.r1.topic = topicB\n#agent.sources.r1.kafka.consumer.max.partition.fetch.bytes = 409600000\n\n# ========= Describe the channel =============\n# Use a channel which buffers events in memory\nagent.channels.c1.type = memory\nagent.channels.c1.capacity = 100000\nagent.channels.c1.transactionCapacity = 1000\n#agent.channels.c1.keep-alive = 60\n\n# ========= Describe the sink =============\nagent.sinks.s1.type = hdfs\nagent.sinks.s1.hdfs.path = /user/lintong/logs/nsh/json/%{filepath}/ds=%{ds}\nagent.sinks.s1.hdfs.filePrefix = test\nagent.sinks.s1.hdfs.fileSuffix = .log\nagent.sinks.s1.hdfs.fileType = DataStream\nagent.sinks.s1.hdfs.useLocalTimeStamp = true\nagent.sinks.s1.hdfs.writeFormat = Text\nagent.sinks.s1.hdfs.rollCount = 0\nagent.sinks.s1.hdfs.rollSize = 10240\nagent.sinks.s1.hdfs.rollInterval = 600\nagent.sinks.s1.hdfs.batchSize = 500\nagent.sinks.s1.hdfs.threadsPoolSize = 10\nagent.sinks.s1.hdfs.idleTimeout = 0\nagent.sinks.s1.hdfs.minBlockReplicas = 1\nagent.sinks.s1.channel = fileChannel\n\n\n# ========= Bind the source and sink to the channel =============\nagent.sources.r1.channels = c1\nagent.sinks.s1.channel = c1\n\n```\n\n&nbsp;hdfs sink的配置参数参考：[Flume中的HDFS Sink配置参数说明](http://lxw1234.com/archives/2015/10/527.htm)\n\n因为写HDFS的速度很慢，当数据量大的时候会出现一下问题\n\n```\norg.apache.flume.ChannelException: Take list for MemoryTransaction, capacity 1000 full, consider committing more frequently, increasing capacity, or increasing thread count\n\n```\n\n可以将内存channel改成file channel或者改成kafka channel\n\n当换成kafka channel的时候，数据量大的时候，依然会问题\n\n```\n16:07:48.615 ERROR org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:550 - Error ILLEGAL_GENERATION occurred while committing offsets for group flume\n16:07:48.617 ERROR org.apache.flume.source.kafka.KafkaSource:317 - KafkaSource EXCEPTION, {}\norg.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed due to group rebalance\n\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:552)\n\n```\n\n或者\n\n```\nERROR org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:550 - Error UNKNOWN_MEMBER_ID occurred while committing offsets for group flume\n\n```\n\n参考：[flume1.7使用KafkaSource采集大量数据](http://flume.cn/2017/03/23/flume1-7%E4%BD%BF%E7%94%A8KafkaSource%E9%87%87%E9%9B%86%E5%A4%A7%E9%87%8F%E6%95%B0%E6%8D%AE/)\n\n[Flume官方使用kafka channel的Demo](https://www.cloudera.com/documentation/kafka/2-2-x/topics/kafka_flume.html)\n\n修改增大以下两个参数\n\n```\nagent.sources.r1.kafka.consumer.max.partition.fetch.bytes = 409600000\nagent.sources.r1.kafka.consumer.timeout.ms = 100\n\n```\n\nkafka channel 爆了\n\n```\nERROR org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:550 - Error UNKNOWN_MEMBER_ID occurred while committing offsets for group flume\n\n```\n\n添加参数\n\n```\nagent.channels.c1.kafka.consumer.session.timeout.ms=100000\nagent.channels.c1.kafka.consumer.request.timeout.ms=110000\nagent.channels.c1.kafka.consumer.fetch.max.wait.ms=1000\n\n```\n\n&nbsp;\n\n## 命令\n\n启动\n\n```\nbin/flume-ng agent -c conf -f conf/flume-conf.properties -n agent -Dflume.root.logger=INFO,console\n\n```\n\n&nbsp;\n","tags":["flume"]},{"title":"Elasticsearch学习笔记——安装、数据导入和查询","url":"/Elasticsearch学习笔记——安装、数据导入和查询.html","content":"到elasticsearch网站下载最新版本的elasticsearch 6.2.1\n\n```\nhttps://www.elastic.co/downloads/elasticsearch\n\n```\n\n**其他版本**\n\n```\nhttps://www.elastic.co/cn/downloads/past-releases/elasticsearch-6-4-2\n\n```\n\n**嫌弃官方下载速度慢的可以去华为的镜像站去**\n\n```\nhttps://mirrors.huaweicloud.com/elasticsearch/6.4.2/\n\n```\n\n**中文文档**请参考\n\n```\nhttps://www.elastic.co/guide/cn/elasticsearch/guide/current/index.html\n\n```\n\n**英文文档**及其**Java API使用方法**请参考，官方文档比任何博客都可信\n\n```\nhttps://www.elastic.co/guide/en/elasticsearch/client/java-api/current/index.html\n\n```\n\n**Python API**使用方法\n\n```\nhttp://elasticsearch-py.readthedocs.io/en/master/\n\n```\n\n下载tar包，然后解压到/usr/local目录下，修改一下用户和组之后可以使用非root用户启动，启动命令\n\n```\n./bin/elasticsearch\n\n```\n\n然后访问http://127.0.0.1:9200/\n\n<img src=\"/images/517519-20180213160735734-1038531418.png\" alt=\"\" width=\"460\" height=\"295\" />\n\n如果需要让外网访问Elasticsearch的9200端口的话，需要将es的host绑定到外网\n\n修改 /configs/elasticsearch.yml文件，添加如下\n\n```\nnetwork.host: 0.0.0.0\nhttp.port: 9200\n\n```\n\n然后重启，如果遇到下面问题的话\n\n```\n[2018-01-28T23:51:35,204][INFO ][o.e.b.BootstrapChecks    ] [qR5cyzh] bound or publishing to a non-loopback address, enforcing bootstrap checks\nERROR: [2] bootstrap checks failed\n[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]\n[2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]\n\n```\n\n解决方法\n\n第一个ERROR,\n\n在文件中添加 sudo vim /etc/security/limits.conf,然后重新登录\n\n```\n* soft nofile 65536\n* hard nofile 131072\n* soft nproc 2048\n* hard nproc 4096\n\n```\n\n如果你是用supervisor启动es的话,需要修改文件 vim /etc/supervisor/supervisord.conf,然后重启supervisor\n\n```\n[supervisord]\n\nminfds=65536\n\n```\n\n<!--more-->\n&nbsp;\n\n第二个ERROR,在root用户下执行\n\n临时解决\n\n```\nsysctl -w vm.max_map_count=262144\n\n```\n\n永久解决\n\n```\ncat /proc/sys/vm/max_map_count\nsudo vim /etc/sysctl.conf\n\n```\n\n添加\n\n```\nvm.max_map_count=262144\n\n```\n\n然后使其生效\n\n```\nsysctl -p\n\n```\n\n&nbsp;\n\n接下来导入json格式的数据，数据内容如下\n\n```\n{\"index\":{\"_id\":\"1\"}}\n{\"title\":\"许宝江\",\"url\":\"7254863\",\"chineseName\":\"许宝江\",\"sex\":\"男\",\"occupation\":\" 滦县农业局局长\",\"nationality\":\"中国\"}\n{\"index\":{\"_id\":\"2\"}}\n{\"title\":\"鲍志成\",\"url\":\"2074015\",\"chineseName\":\"鲍志成\",\"occupation\":\"医师\",\"nationality\":\"中国\",\"birthDate\":\"1901年\",\"deathDate\":\"1973年\",\"graduatedFrom\":\"香港大学\"}\n\n```\n\n&nbsp;需要注意的是{\"index\":{\"_id\":\"1\"}}和文件末尾另起一行换行是不可少的\n\n其中的id可以从0开始，甚至是abc等等\n\n否则会出现400状态，错误提示分别为\n\n```\nMalformed action/metadata line [1], expected START_OBJECT or END_OBJECT but found [VALUE_STRING]\n\n```\n\n```\nThe bulk request must be terminated by a newline [\\n]\"\n\n```\n\n使用下面命令来导入json文件\n\n其中的people.json为文件的路径，可以是/home/common/下载/xxx.json\n\n其中的es是index，people是type，在elasticsearch中的index和type可以理解成关系数据库中的database和table，两者都是必不可少的\n\n```\ncurl -H \"Content-Type: application/json\" -XPOST 'localhost:9200/es/people/_bulk?pretty&amp;refresh' --data-binary \"@people.json\"\n\n```\n\n&nbsp;成功后的返回值是200，比如\n\n```\n{\n  \"took\" : 233,\n  \"errors\" : false,\n  \"items\" : [\n    {\n      \"index\" : {\n        \"_index\" : \"es\",\n        \"_type\" : \"people\",\n        \"_id\" : \"1\",\n        \"_version\" : 1,\n        \"result\" : \"created\",\n        \"forced_refresh\" : true,\n        \"_shards\" : {\n          \"total\" : 2,\n          \"successful\" : 1,\n          \"failed\" : 0\n        },\n        \"_seq_no\" : 0,\n        \"_primary_term\" : 1,\n        \"status\" : 201\n      }\n    },\n    {\n      \"index\" : {\n        \"_index\" : \"es\",\n        \"_type\" : \"people\",\n        \"_id\" : \"2\",\n        \"_version\" : 1,\n        \"result\" : \"created\",\n        \"forced_refresh\" : true,\n        \"_shards\" : {\n          \"total\" : 2,\n          \"successful\" : 1,\n          \"failed\" : 0\n        },\n        \"_seq_no\" : 0,\n        \"_primary_term\" : 1,\n        \"status\" : 201\n      }\n    }\n  ]\n}\n\n```\n\n**<0>查看字段的mapping**\n\n```\nhttp://localhost:9200/es/people/_mapping\n\n```\n\n&nbsp;<img src=\"/images/517519-20180327112033221-14493376.png\" alt=\"\" />\n\n&nbsp;\n\n&nbsp;接下来可以使用对应的查询语句对数据进行查询\n\n&nbsp;<1>按**id**来查询\n\n```\nhttp://localhost:9200/es/people/1\n\n```\n\n&nbsp;<img src=\"/images/517519-20180213173140234-383781592.png\" alt=\"\" width=\"310\" height=\"263\" />\n\n<2>简单的匹配查询，查询**某个字段**中包含**某个关键字**的数据（GET）\n\n```\nhttp://localhost:9200/es/people/_search?q=_id:1\n\n```\n\n```\nhttp://localhost:9200/es/people/_search?q=title:许\n\n```\n\n&nbsp;<img src=\"/images/517519-20180213174923062-1734972255.png\" alt=\"\" width=\"296\" height=\"460\" />\n\n<3>**多字段**查询，在**多个字段**中查询包含**某个关键字**的数据（POST）\n\n可以使用Firefox中的**RESTer插件**来构造一个POST请求，在升级到Firefox quantum之后，原来使用的Poster插件挂了\n\n在title和sex字段中查询包含 许 字的数据\n\n```\n{\n    \"query\": {\n        \"multi_match\" : {\n            \"query\" : \"许\",\n            \"fields\": [\"title\", \"sex\"]\n        }\n    }\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20180214145007906-1237146861.png\" alt=\"\" width=\"661\" height=\"376\" />\n\n<img src=\"/images/517519-20180214145054249-2025180644.png\" alt=\"\" width=\"692\" height=\"151\" />\n\n还可以额外指定返回值\n\nsize指定返回的数量\n\nfrom指定返回的id起始值\n\n_source指定返回的字段\n\nhighlight指定语法高亮\n\n```\n{\n    \"query\": {\n        \"multi_match\" : {\n            \"query\" : \"中国\",\n            \"fields\": [\"nationality\", \"sex\"]\n        }\n    },\n    \"size\": 2,\n    \"from\": 0,\n    \"_source\": [ \"title\", \"sex\", \"nationality\" ],\n    \"highlight\": {\n        \"fields\" : {\n            \"title\" : {}\n        }\n    }\n}\n\n```\n\n<4>**Boosting**\n\n用于提升字段的权重，可以将max_score的分数乘以一个系数\n\n```\n{\n    \"query\": {\n        \"multi_match\" : {\n            \"query\" : \"中国\",\n            \"fields\": [\"nationality^3\", \"sex\"]\n        }\n    },\n    \"size\": 2,\n    \"from\": 0,\n    \"_source\": [ \"title\", \"sex\", \"nationality\" ],\n    \"highlight\": {\n        \"fields\" : {\n            \"title\" : {}\n        }\n    }\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20180214183807952-372020081.png\" alt=\"\" width=\"384\" height=\"442\" />\n\n<5>**组合查询，可以实现一些比较复杂的查询**\n\n**AND -> must**\n\n**<strong>NOT -> must not<br />**</strong>\n\n**<strong>OR -> should**</strong>\n\n```\n{\n    \"query\": {\n        \"bool\": {\n            \"must\": {\n                \"bool\" : { \n                    \"should\": [\n                      { \"match\": { \"title\": \"鲍\" }},\n                      { \"match\": { \"title\": \"许\" }} ],\n                    \"must\": { \"match\": {\"nationality\": \"中国\" }}\n                }\n            },\n            \"must_not\": { \"match\": {\"sex\": \"女\" }}\n        }\n    }\n}\n\n```\n\n&nbsp;<6>**模糊（Fuzzy）查询（POST）**\n\n```\n{\n    \"query\": {\n        \"multi_match\" : {\n            \"query\" : \"厂长\",\n            \"fields\": [\"title\", \"sex\",\"occupation\"],\n            \"fuzziness\": \"AUTO\"\n        }\n    },\n    \"_source\": [\"title\", \"sex\", \"occupation\"],\n    \"size\": 1\n}\n\n```\n\n&nbsp;通过模糊匹配将 厂长 和 局长 匹配上\n\nAUTO的时候，当query的长度大于5的时候，模糊值指定为2\n\n<img src=\"/images/517519-20180218185911280-1052380582.png\" alt=\"\" width=\"364\" height=\"410\" />\n\n<7>**通配符（Wildcard）查询（POST）**\n\n**`？`&nbsp;匹配任何字符**\n\n**`*`&nbsp;匹配零个或多个字**\n\n```\n{\n    \"query\": {\n        \"wildcard\" : {\n            \"title\" : \"*宝\"\n        }\n    },\n    \"_source\": [\"title\", \"sex\", \"occupation\"],\n    \"size\": 1\n}\n\n```\n\n&nbsp;<8>**正则（Regexp）查询（POST）**\n\n```\n{\n    \"query\": {\n        \"regexp\" : {\n            \"authors\" : \"t[a-z]*y\"\n        }\n    },\n    \"_source\": [\"title\", \"sex\", \"occupation\"],\n    \"size\": 3\n}\n\n```\n\n<9>**短语匹配（Match Phrase）查询（POST）**\n\n**<strong>短语匹配查询**&nbsp;要求在请求字符串中的所有查询项必须都在文档中存在，文中顺序也得和请求字符串一致，且彼此相连。</strong>\n\n**默认情况下，查询项之间必须紧密相连，但可以设置&nbsp;`slop`&nbsp;值来指定查询项之间可以分隔多远的距离，结果仍将被当作一次成功的匹配。**\n\n```\n{\n    \"query\": {\n        \"multi_match\" : {\n            \"query\" : \"许长江\",\n            \"fields\": [\"title\", \"sex\",\"occupation\"],\n            \"type\": \"phrase\"\n        }\n    },\n    \"_source\": [\"title\", \"sex\", \"occupation\"],\n    \"size\": 3\n}\n\n```\n\n&nbsp;注意使用slop的时候距离是累加的，滦农局 和&nbsp;滦县农业局 差了2个距离\n\n```\n{\n    \"query\": {\n        \"multi_match\" : {\n            \"query\" : \"滦农局\",\n            \"fields\": [\"title\", \"sex\",\"occupation\"],\n            \"type\": \"phrase\",\n            \"slop\":2\n        }\n    },\n    \"_source\": [\"title\", \"sex\", \"occupation\"],\n    \"size\": 3\n}\n\n```\n\n<10>**短语前缀（Match Phrase Prefix）查询**\n\n```\nhttps://www.elastic.co/guide/cn/elasticsearch/guide/current/prefix-query.html\n\n```\n\n比如\n\n```\nGET /my_index/address/_search\n{\n    \"query\": {\n        \"prefix\": {\n            \"postcode\": \"W1\"\n        }\n    }\n}\n\n```\n\n　　&nbsp;\n\n一些比较复杂的DSL\n\n```\nGET index_*/_search\n{\n  \"query\": {\n        \"bool\": {\n            \"must\": [{\n                \"range\" : {\n                  \"publish_date\" : {\n                      \"gt\" : \"2014-01-01\",\n                      \"lt\" : \"2019-01-07\"\n                  }\n                }\n            },\n            { \"multi_match\": {\n              \"query\": \"免费\",\n              \"fields\":[\"name1\",\"name2\",\"name3\",\"name4\",\"name5\",\"name6\"]\n              }\n            },\n            { \"multi_match\": {\n              \"query\": \"英语\",\n              \"fields\":[\"name1\",\"name2\",\"name3\",\"name4\",\"name5\",\"name6\"]\n              }\n            }\n            \n            ],\n            \"must_not\": { \"match\": {\"tags\": \"\" }},\n            \"filter\": {\n                \"range\": { \"count\": { \"gte\": \"30\" ,\"lte\": \"1000\"}} \n            }\n        }\n    },\n  \"aggs\": {\n    \"by_tags\": {\n      \"terms\": { \"field\": \"field1\"\n      },\n      \"aggs\": {\n      \"sales\": {\n         \"date_histogram\": {\n            \"field\": \"date\",\n            \"interval\": \"day\", \n            \"format\": \"yyyy-MM-dd\" \n         }\n      }\n   }\n    }\n  },\n  \"_source\": [],\n  \"size\": 1\n}\n\n```\n\n带有去重的\n\n```\nGET xxxx_2019-09-10/_search\n{\n  \"query\": {\n        \"bool\": {\n            \"must\": [\n              {\n                \"range\" : {\n                  \"xxxx\" : {\n                      \"gt\" : \"2014-01-01\",\n                      \"lt\" : \"2019-01-07\"\n                  }\n                }\n              },\n              { \n                \"terms\": {\n                  \"xxxx\": [\"xxx\",\"xxx\"]\n                }\n              },\n              { \n                \"terms\": {\n                  \"xxx\": [\"xxx\",\"xxx\"]\n                }\n              },\n              { \n                \"terms\": {\n                  \"xxx\": [\"xxx\"]\n                }\n              },\n              {\n                \"bool\": {\n                  \"should\": [\n                      {\n                          \"range\": { \n                            \"xxx\": { \"gte\": 1 ,\"lte\": 2.99 }}\n                      },\n                      {\"range\": { \n                        \"xxx\": { \"gte\": 3.99 ,\"lte\": 7.99 }}\n                      }\n              ]}},{\n                \"bool\": {\n                  \"should\": [\n                      {\n                          \"range\": { \n                            \"xxx\": { \"gte\": 0 ,\"lte\": 100 }}\n                      },\n                      {\"range\": { \n                        \"xxx\": { \"gte\": 1000 ,\"lte\": 10000 }}\n                      }\n              ]}}\n          ],\n          \"must_not\": { \"match\": {\"xx\": \"\" }}\n              \n          \n          \n        }\n    },\n    \"collapse\":{\n        \"field\":\"xxx\"\n    },\n  \"aggs\": {\n    \"by_tags\": {\n      \"terms\": { \"field\": \"xxx\"\n      },\n      \"aggs\": {\n      \"sales\": {\n         \"date_histogram\": {\n            \"field\": \"xxx\",\n            \"interval\": \"month\", \n            \"format\": \"yyyy-MM-dd\" \n         }\n      }\n   }\n    }\n  },\n  \"_source\":[\"xxx\"],\n  \"size\": 10\n}\n\n```\n\n&nbsp;\n\n<11>带嵌套对象查询\n\n参考：[https://www.elastic.co/guide/cn/elasticsearch/guide/current/nested-query.html](https://www.elastic.co/guide/cn/elasticsearch/guide/current/nested-query.html)\n\n由于嵌套对象 被索引在独立隐藏的文档中，我们无法直接查询它们。 相应地，我们必须使用 [ `nested` 查询](https://www.elastic.co/guide/en/elasticsearch/reference/5.6/query-dsl-nested-query.html) 去获取它们：\n\n对于nested对象的查询，需要套上一层nested\n\n```\nGET /xxxxx/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"nested\": {\n            \"path\": \"t4\", \n            \"query\": {\n              \"bool\": {\n                \"must\": [ \n                  {\n                    \"match\": {\n                      \"t4.t1\": \"HelloWorld\"\n                    }\n                  }\n                ]\n              }\n            }\n          }\n        }\n      ]\n}}\n}\n\n```\n\n或者\n\n```\nGET /xxxxx/_search\n{\n    \"query\": {\n    \"nested\": {\n      \"path\": \"t4\",\n      \"query\": {\n        \"multi_match\" : {\n            \"query\" : \"HelloWorld\",\n            \"fields\": [\"t4.t1\", \"sex\"]\n        }\n    }\n    }}\n}\n\n```\n\n&nbsp;\n\nEs优化：\n\n[Elasticsearch 技术分析（七）： Elasticsearch 的性能优化](https://www.cnblogs.com/jajian/p/10465519.html)\n\n### 查看索引是否关闭\n\n```\nhttp://localhost:9200/_cat/indices/index_name?h=status\n\n```\n\n&nbsp;\n\n**重建索引**\n\n因为数值类型的es字段，在query的字符串不能转换成数值的时候，需要把字段的类型从long改成keyword，先修改索引模板的字段的类型，然后执行reindex命令\n\n```\nPOST _reindex\n{\n  \"source\": {\n    \"index\": \"twitter1\"\n  },\n  \"dest\": {\n    \"index\": \"twitter1_new\"\n  }\n}\n\n```\n\n　　\n\n&nbsp;\n","tags":["ELK"]},{"title":"GraphX学习笔记——Programming Guide","url":"/GraphX学习笔记——Programming Guide.html","content":"学习的资料是官网的Programming Guide\n\n```\nhttps://spark.apache.org/docs/latest/graphx-programming-guide.html\n\n```\n\n<!--more-->\n&nbsp;首先是GraphX的**简介**\n\nGraphX是Spark中专门负责图和图并行计算的组件。\n\nGraphX通过引入了图形概念来继承了Spark RDD：一个连接节点和边的有向图\n\n为了支持图计算，GraphX引入了一些算子：&nbsp;[subgraph](https://spark.apache.org/docs/latest/graphx-programming-guide.html#structural_operators),&nbsp;[joinVertices](https://spark.apache.org/docs/latest/graphx-programming-guide.html#join_operators), and&nbsp;[aggregateMessages](https://spark.apache.org/docs/latest/graphx-programming-guide.html#aggregateMessages)等\n\n和&nbsp;[Pregel](https://spark.apache.org/docs/latest/graphx-programming-guide.html#pregel)&nbsp;API，此外还有一些[algorithms](https://spark.apache.org/docs/latest/graphx-programming-guide.html#graph_algorithms) 和 [builders](https://spark.apache.org/docs/latest/graphx-programming-guide.html#graph_builders) 来简化图分析任务。\n\n&nbsp;\n\n关于构建 **节点Vertex** 和 **边Edge**\n\n1.如果需要将节点定义成一个类\n\n```\npackage graphx\n\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\nimport org.graphstream.graph.implementations.{AbstractEdge, SingleGraph, SingleNode}\n\n/**\n  * Created by common on 18-1-22.\n  */\n\n// 抽象节点\nclass VertexProperty()\n// User节点\ncase class UserProperty(val name: String) extends VertexProperty\n// Product节点\ncase class ProductProperty(val name: String, val price: Double) extends VertexProperty\n\nobject GraphxLearning {\n\n  def main(args: Array[String]): Unit = {\n\n    val conf = new SparkConf().setAppName(\"GraphX\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n\n    // The graph might then have the type:\n    var graph: Graph[VertexProperty, String] = null\n\n  }\n}\n\n```\n\n和节点一样，边也可以定义成一个class，同时Graph类需要和定义的节点和边的类型相对应\n\n```\nclass Graph[VD, ED] {    // VD表示节点类型，ED表示边类型\n  val vertices: VertexRDD[VD]\n  val edges: EdgeRDD[ED]\n}\n\n```\n\n&nbsp;\n\n2.如果节点的类型比较简单，例如只是一个String或者(String,String)，就不需要定义成一个类\n\n```\npackage graphx\n\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\nimport org.graphstream.graph.implementations.{AbstractEdge, SingleGraph, SingleNode}\n\n/**\n  * Created by common on 18-1-22.\n  */\nobject GraphxLearning {\n\n  def main(args: Array[String]): Unit = {\n\n    val conf = new SparkConf().setAppName(\"GraphX\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n\n    // Create an RDD for the vertices\n    val users: RDD[(VertexId, (String, String))] =\n      sc.parallelize(Array((3L, (\"rxin\", \"student\")), (7L, (\"jgonzal\", \"postdoc\")),\n        (5L, (\"franklin\", \"prof\")), (2L, (\"istoica\", \"prof\"))))\n    // Create an RDD for edges\n    val relationships: RDD[Edge[String]] =\n      sc.parallelize(Array(Edge(3L, 7L, \"collab\"), Edge(5L, 3L, \"advisor\"),\n        Edge(2L, 5L, \"colleague\"), Edge(5L, 7L, \"pi\")))\n    //Define a default user in case there are relationship with missing user\n    val defaultUser = (\"John Doe\", \"Missing\")\n\n    // 使用多个RDDs建立一个Graph，Graph的类型分别是节点加上边的类型，有两种节点，一种有ID，一种没有\n    val srcGraph: Graph[(String, String), String] = Graph(users, relationships, defaultUser)\n\n  }\n}\n\n```\n\n&nbsp;**图**的一些算子\n|<table style=\"width: 962px;\" border=\"1\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td valign=\"top\" width=\"215\">图信息<td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">numEdges: Long</td><td valign=\"top\" width=\"294\">计算整个图中边的数目</td>\n\n计算整个图中边的数目\n<td valign=\"top\" width=\"215\">numVertices: Long</td><td valign=\"top\" width=\"294\">计算整个图中顶点的数目</td>\n\n计算整个图中顶点的数目\n<td valign=\"top\" width=\"215\">inDegrees: VertexRDD[Int]</td><td valign=\"top\" width=\"294\">计算所有点的入度，若顶点无入度，则不会出现在结果中</td>\n\n计算所有点的入度，若顶点无入度，则不会出现在结果中\n<td valign=\"top\" width=\"215\">outDegrees: VertexRDD[Int]</td><td valign=\"top\" width=\"294\">计算所有点的出度，和inDegrees相似，若顶点无出度则不会出现在结果中</td>\n\n计算所有点的出度，和inDegrees相似，若顶点无出度则不会出现在结果中\n<td valign=\"top\" width=\"215\">degrees: VertexRDD[Int]</td><td valign=\"top\" width=\"294\">计算所有顶点的出入度之和，孤立的顶点（无边与之相连）不会出现在结果中</td>\n\n计算所有顶点的出入度之和，孤立的顶点（无边与之相连）不会出现在结果中\n<td valign=\"top\" width=\"215\">查看图中的集合</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">` vertices: VertexRDD[VD]`</td><td valign=\"top\" width=\"294\">&nbsp;节点`VertexRDD`</td>\n<td valign=\"top\" width=\"215\">` edges: EdgeRDD[ED] `</td><td valign=\"top\" width=\"294\">&nbsp;边EdgeRDD</td>\n<td valign=\"top\" width=\"215\">`<code class=\"language-scala\" data-lang=\"scala\">triplets: RDD[EdgeTriplet[VD, ED]]`</code></td><td valign=\"top\" width=\"294\">三元组RDD</td>\n<td valign=\"top\" width=\"215\">图存储</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">`persist<code class=\"language-scala\" data-lang=\"scala\">(newLevel: StorageLevel = StorageLevel.MEMORY_ONLY): Graph[VD, ED]`</code></td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">`cache(): Graph[VD, ED]`</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">`unpersistVertices(blocking: Boolean = true): Graph[VD, ED]`</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">`操作partition的算子`</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">`partitionBy(partitionStrategy: PartitionStrategy): Graph[VD, ED]`</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">操作Vertex和Edge的算子，以生成新的Graph</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">`mapVertices[VD2](map: (VertexId, VD) => VD2): Graph[VD2, ED]`</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">` mapEdges[ED2](map: Edge[ED] => ED2): Graph[VD, ED2]`</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">`mapEdges[ED2](map: (PartitionID, Iterator[Edge[ED]]) => Iterator[ED2]): Graph[VD, ED2]`</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">`mapTriplets[ED2](map: EdgeTriplet[VD, ED] => ED2): Graph[VD, ED2]`</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">`mapTriplets[ED2](map: (PartitionID, Iterator[EdgeTriplet[VD, ED]]) => Iterator[ED2]) : Graph[VD, ED2]`</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">修改图结构的算子</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">&nbsp;`reverse: Graph[VD, ED]`&nbsp;</td><td valign=\"top\" width=\"294\">&nbsp;改变有向边的方向</td>\n\n&nbsp;\n<td valign=\"top\" width=\"215\">`subgraph( epred: EdgeTriplet[VD,ED] => Boolean = (x => true), vpred: (VertexId, VD) => Boolean = ((v, d) => true)) : Graph[VD, ED]`</td><td valign=\"top\" width=\"294\">子图</td>\n<td valign=\"top\" width=\"215\">`mask[VD2, ED2](other: Graph[VD2, ED2]): Graph[VD, ED]`</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">`groupEdges(merge: (ED, ED) => ED): Graph[VD, ED]`</td><td valign=\"top\" width=\"294\">&nbsp;graphx中两个节点之间可以存在多条边，可以用于将这多条边合并</td>\n<td valign=\"top\" width=\"215\">Join算子</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">`joinVertices[U](table: RDD[(VertexId, U)])(mapFunc: (VertexId, VD, U) => VD): Graph[VD, ED]`</td><td valign=\"top\" width=\"294\">使用顶点的更新数据生成新的顶点数据。将图数据与输入数据做内连接操作，过滤输入数据中不存在的顶点，并对连接结果使用指定的UDF进行计算，若输入数据中未包含图中某些顶点的更新数据，则在新图中使用顶点的旧数据</td>\n<td valign=\"top\" width=\"215\">`outerJoinVertices[U, VD2](other: RDD[(VertexId, U)]) (mapFunc: (VertexId, VD, Option[U]) => VD2) : Graph[VD2, ED]`</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">聚合算子</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">collectNeighborIds(edgeDirection: EdgeDirection): VertexRDD[Array[VertexId]]</td><td valign=\"top\" width=\"294\">收集每个顶点的相邻顶点的ID数据，edgeDirection用来控制收集的方向</td>\n\n收集每个顶点的相邻顶点的ID数据，edgeDirection用来控制收集的方向\n<td valign=\"top\" width=\"215\">collectNeighbors(edgeDirection: EdgeDirection): VertexRDD[Array[(VertexId, VD)]]</td><td valign=\"top\" width=\"294\">收集每个顶点的相邻顶点的数据，当图中顶点的出入度较大时，可能会占用很大的存储空间，参数edgeDirection用于控制收集方向</td>\n\n收集每个顶点的相邻顶点的数据，当图中顶点的出入度较大时，可能会占用很大的存储空间，参数edgeDirection用于控制收集方向\n<td valign=\"top\" width=\"215\">`aggregateMessages[Msg: ClassTag]( sendMsg: EdgeContext[VD, ED, Msg] => Unit, mergeMsg: (Msg, Msg) => Msg, tripletFields: TripletFields = TripletFields.All) : VertexRDD[A]`</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">`迭代图并行计算的算子`</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">`pregel[A](initialMsg: A, maxIterations: Int, activeDirection: EdgeDirection)( vprog: (VertexId, VD, A) => VD, sendMsg: EdgeTriplet[VD, ED] => Iterator[(VertexId,A)], mergeMsg: (A, A) => A) : Graph[VD, ED]`</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">`基础图算法`</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">`pageRank(tol: Double, resetProb: Double = 0.15): Graph[Double, Double]`</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n\n&nbsp;\n<td valign=\"top\" width=\"215\">`connectedComponents(): Graph[VertexId, ED]`</td><td valign=\"top\" width=\"294\">&nbsp;联通，无向联通的节点将会有一个相同的VertexId</td>\n<td valign=\"top\" width=\"215\">`triangleCount(): Graph[Int, ED]`</td><td valign=\"top\" width=\"294\">&nbsp;</td>\n<td valign=\"top\" width=\"215\">`stronglyConnectedComponents(numIter: Int): Graph[VertexId, ED] }`</td><td valign=\"top\" width=\"294\">&nbsp;强联通，有向联通的节点将会有一个相同的VertexId</td>\n<td valign=\"top\" width=\"215\">LabelPropagation</td><td valign=\"top\" width=\"294\">&nbsp;标签传播算法算法终止条件：它要求所有的node都满足，node的label一定是它的邻居label中出现次数最多的(或最多的之一)，这意味着，每个node的邻居中，和它处于同一个community的数量一定大于等于处于其它community的数量</td>\n\n&nbsp;标签传播算法\n<td valign=\"top\" width=\"215\">ShortestPaths</td><td valign=\"top\" width=\"294\">&nbsp;最短路径算法</td>\n<td valign=\"top\" width=\"215\">SVDPlusPlus</td><td valign=\"top\" width=\"294\">&nbsp;SVD算法</td>\n|&nbsp;|&nbsp;\n|&nbsp;|&nbsp;\n","tags":["Spark","图存储及计算"]},{"title":"GraphX学习笔记——可视化","url":"/GraphX学习笔记——可视化.html","content":"首先自己造了一份简单的社交关系的图\n\n第一份是人物数据，id和姓名，person.txt\n\n```\n1 孙俪\n2 邓超\n3 佟大为\n4 冯绍峰\n5 黄晓明\n6 angelababy\n7 李冰冰\n8 范冰冰\n\n```\n\n<!--more-->\n&nbsp;第二份是社交关系数据，两个人的id和社交关系，social.txt\n\n```\n1 丈夫 2\n2 妻子 1\n1 搭档 3\n3 同学 4\n3 好友 5\n5 好友 3\n5 妻子 6\n5 好友 7\n7 好友 8\n\n```\n\n&nbsp;使用SparkX和GraphStream来处理数据\n\n```\npackage graphx\n\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\nimport org.graphstream.graph.implementations.{AbstractEdge, SingleGraph, SingleNode}\n\n/**\n  * Created by common on 18-1-22.\n  */\nobject GraphxLearning {\n\n  def main(args: Array[String]): Unit = {\n\n    val conf = new SparkConf().setAppName(\"GraphX\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n\n    val path1 = \"input/graphx/person.txt\"\n    val path2 = \"input/graphx/social.txt\"\n\n\n    // 顶点RDD[顶点的id,顶点的属性值]\n    val users: RDD[(VertexId, (String, String))] = sc.textFile(path1).map { line =>\n      val vertexId = line.split(\" \")(0).toLong\n      val vertexName = line.split(\" \")(1)\n      (vertexId, (vertexName, vertexName))\n    }\n\n    // 边RDD[起始点id,终点id，边的属性（边的标注,边的权重等）]\n    val relationships: RDD[Edge[String]] = sc.textFile(path2).map { line =>\n      val arr = line.split(\" \")\n      val edge = Edge(arr(0).toLong, arr(2).toLong, arr(1))\n      edge\n    }\n\n    // 默认（缺失）用户\n    //Define a default user in case there are relationship with missing user\n    val defaultUser = (\"John Doe\", \"Missing\")\n\n    //使用RDDs建立一个Graph（有许多建立Graph的数据来源和方法，后面会详细介绍）\n    val srcGraph = Graph(users, relationships, defaultUser)\n\n    val graph: SingleGraph = new SingleGraph(\"graphDemo\")\n\n    //    load the graphx vertices into GraphStream\n    for ((id, name) <- srcGraph.vertices.collect()) {\n      val node = graph.addNode(id.toString).asInstanceOf[SingleNode]\n      node.addAttribute(\"ui.label\", name._1)\n    }\n\n    //    load the graphx edges into GraphStream edges\n    for (Edge(x, y, relation) <- srcGraph.edges.collect()) {\n      val edge = graph.addEdge(x.toString ++ y.toString, x.toString, y.toString, true).asInstanceOf[AbstractEdge]\n      edge.addAttribute(\"ui.label\", relation)\n    }\n\n    graph.setAttribute(\"ui.quality\")\n    graph.setAttribute(\"ui.antialias\")\n\n    graph.display()\n\n\n  }\n\n}\n\n```\n\n可视化的结果，该图数据节点数很少，本来想尝试一份百万节点的数据，结果遇到了爆内存的问题\n\n后来发现爆内存是肯定的，而且显示的点太多也不太利于debug，解决方法是使用subgraph()方法来对图进行裁剪以减小节点和边的数量\n\n<img src=\"/images/517519-20180123230936569-1566720681.png\" alt=\"\" width=\"717\" height=\"563\" />\n","tags":["Spark","图存储及计算"]},{"title":"Gephi学习笔记","url":"/Gephi学习笔记.html","content":"使用gephi对图数据进行可视化操作，下面网址是gephi的说明文档\n\n```\nhttps://seinecle.github.io/gephi-tutorials/generated-pdf/semantic-web-importer-en.pdf\n\n```\n\n使用的gephi版本号：0.9.1\n\n系统：Ubuntu 16.04\n\n内存：8G\n\n<!--more-->\n&nbsp;\n\n1.启动Gephi，在semantic web import中输入，该web接口是dbpedia的RDF格式数据，然后点击run\n\n```\nhttp://dbpedia.org/sparql\n\n```\n\n&nbsp;<img src=\"/images/517519-20180119205116084-325771715.png\" alt=\"\" width=\"593\" height=\"487\" />\n\n接下来在 **图** 界面中可以看到RDF数据的可视化界面，注意这里只有100条RDF数据，包含了101个节点和100条边，这是由于查询的sparql语句中的limit 100\n\n<img src=\"/images/517519-20180119211322881-841256618.png\" alt=\"\" width=\"841\" height=\"471\" />\n\n<img src=\"/images/517519-20180119211457303-45582917.png\" alt=\"\" width=\"257\" height=\"121\" />\n\n当然你可以选择limit 300条数据，来看一下可视化的结果，这时查出来的结果中包含314个节点和300条边\n\n<img src=\"/images/517519-20180119212424209-1487600013.png\" alt=\"\" width=\"774\" height=\"417\" />\n\n可以选择不同的布局\n\nForce Atlas 重力地图集\n\n<img src=\"/images/517519-20180119205518771-559373823.png\" alt=\"\" width=\"475\" height=\"345\" />\n\nForce Atlas2 重力地图集2\n\n<img src=\"/images/517519-20180119205626271-480498616.png\" alt=\"\" width=\"474\" height=\"381\" />\n\nFruchterman Reingold\n\n<img src=\"/images/517519-20180119205838849-1423192807.png\" alt=\"\" width=\"528\" height=\"415\" />\n\nOpenOrd\n\n<img src=\"/images/517519-20180119205950318-454125700.png\" alt=\"\" width=\"590\" height=\"386\" />\n\n随机布局\n\n<img src=\"/images/517519-20180119210116756-116510414.png\" alt=\"\" width=\"599\" height=\"389\" />\n\n&nbsp;\n","tags":["开发工具"]},{"title":"Centos7.0下MySQL的安装","url":"/Centos7.0下MySQL的安装.html","content":"**1.下载mysql的repo源**\n\n```\nwget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm\n\n```\n\n**2.安装mysql-community-release-el7-5.noarch.rpm包**\n\n```\nsudo rpm -ivh mysql-community-release-el7-5.noarch.rpm\n\n```\n\n安装这个包后，会获得两个mysql的yum repo源：/etc/yum.repos.d/mysql-community.repo，/etc/yum.repos.d/mysql-community-source.repo。\n\n**3.安装mysql**\n\n```\nsudo yum install mysql-server\n\n```\n\n根据提示安装就可以了,不过安装完成后没有密码,需要重置密码\n\n**4.重置mysql密码**\n\n```\nmysql -u root\n\n```\n\n登录时有可能报这样的错：ERROR 2002 (HY000): Can&lsquo;t connect to local MySQL server through socket &lsquo;/var/lib/mysql/mysql.sock&lsquo; (2)，原因是/var/lib/mysql的访问权限问题。下面的命令把/var/lib/mysql的拥有者改为当前用户：\n\n```\nsudo chown -R root:root /var/lib/mysql\n\n```\n\n重启mysql服务\n\n```\nservice mysqld restart\n\n```\n\n接下来登录重置密码：\n\n```\n$ mysql -u root  //直接回车进入mysql控制台\nmysql > use mysql;\nmysql > update user set password=password('123456') where user='root';\nmysql > FLUSH PRIVILEGES;\n\n```\n\n**如果mysql -u root遇到/var/lib/mysql/mysql.sock错误的时候**\n\n```\nln -s /tmp/mysql.sock /var/lib/mysql/mysql.sock \n\n```\n\n**远程使用workbench登录MySQL**\n\n```\nmysql -u root -p #登录MySQL\nuse mysql;        #库\nselect host,user,password from user; #表中存放的是有访问权限的用户和ip\ngrant all privileges on *.* to 'root'@'你的IP' identified by 'root' with grant option; # 添加用户，其中identified by 'root'中的root是'root'@'你的IP'这个用户登录时候使用的密码\nflush privileges;\nset password for 'root'@'你的IP'=password('你的密码'); # 更改密码\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["MySQL"]},{"title":"Ubuntu16.04安装apache-airflow","url":"/Ubuntu16.04安装apache-airflow.html","content":"## 1.安装apache-airflow 1.8.0\n\n服务器使用的是centos系统，需要安装好pip和setuptools，同时注意更新安装的版本\n\n接下来参考安装好Airflow\n\n```\nAirflow 1.8 工作流平台搭建 http://blog.csdn.net/kk185800961/article/details/78431484\nairflow最简安装方法 centos 6.5 http://blog.csdn.net/Excaliburace/article/details/53818530\n\n```\n\n以mysql作为数据库，airflow默认使用sqlite作为数据库\n\n1.建表\n\n```\n# 创建相关数据库及账号  \nmysql> create database airflow default charset utf8 collate utf8_general_ci;  \nmysql> create user airflow@'localhost' identified by 'airflow';  \nmysql> grant all on airflow.* to airflow@'localhost';  \nmysql> flush privileges;  \n\n```\n\n2.安装airflow，需要环境隔离的时候请使用virtualenv ./**en**v创建隔离环境\n\n```\nsudo pip install apache-airflow==1.8.0\n\n```\n\n3.使用pip来安装，安装的路径在python路径下site-packages文件夹，在使用上述命令一遍就能看到\n\n```\n~/anaconda2/lib/python2.7/site-packages/airflow\n\n```\n\n4.在/etc/proofile中添加，之后source一下\n\n```\n#Airflow\nexport AIRFLOW_HOME=/home/lintong/airflow\n\n```\n\n5.创建数据库，这一步会创建上面路径的airflow文件夹，以及文件夹中的一些文件\n\n```\nairflow initdb\n\n```\n\n<!--more-->\n&nbsp;查看是否安装成功\n\n```\nairflow version\n\n```\n\n5.配置元数据库地址\n\n```\n/home/lintong/airflow\nvim airflow.cfg\n\n```\n\n修改下图中的配置\n\n<img src=\"/images/517519-20180526162428809-2057770369.png\" alt=\"\" />\n\n```\nsql_alchemy_conn = mysql://airflow:airflow@localhost:3306/airflow\n\n```\n\n6.安装python的mysql驱动\n\n```\npip install mysql-python\n\n```\n\n再次初始化数据库\n\n```\nairflow initdb\n\n```\n\n7.启动web界面\n\n```\nairflow webserver -p 8080\nhttp://localhost:8080/admin/\n\n```\n\n8.在airflow路径下新建一个dags文件夹，并创建一个DAG python脚本，参考：[[AirFlow]AirFlow使用指南三 第一个DAG示例](https://blog.csdn.net/sunnyyoona/article/details/76615699)\n\n```\n# -*- coding: utf-8 -*-\n\nimport airflow\nimport os\nimport time\nfrom airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import timedelta\n\n# -------------------------------------------------------------------------------\n# these args will get passed on to each operator\n# you can override them on a per-task basis during operator initialization\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': airflow.utils.dates.days_ago(0),\n    'email': ['xxxxxxx'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n    # 'queue': 'bash_queue',\n    # 'pool': 'backfill',\n    # 'priority_weight': 10,\n    # 'end_date': datetime(2016, 1, 1),\n    # 'wait_for_downstream': False,\n    # 'dag': dag,\n    # 'adhoc':False,\n    # 'sla': timedelta(hours=2),\n    # 'execution_timeout': timedelta(seconds=300),\n    # 'on_failure_callback': some_function,\n    # 'on_success_callback': some_other_function,\n    # 'on_retry_callback': another_function,\n    # 'trigger_rule': u'all_success'\n}\n\n# -------------------------------------------------------------------------------\n# dag\n\ndag = DAG(\n    'example_print_dag',\n    default_args=default_args,\n    description='my first DAG',\n    schedule_interval=timedelta(days=1))\n\n\ndef each_content(content, path):\n    return (\"echo \\\"\" + content + \" \" + time.asctime(time.localtime(time.time())) + \"\\\" >> \" + path)\n\n# -------------------------------------------------------------------------------\n# first operator\n\n#print1_operator = PythonOperator(\n#    task_id='print1_task',\n#    python_callable=each_content(\"1\", \"/home/lintong/桌面/test.txt\"),\n#    dag=dag)\n\nprint1_operator = BashOperator(\n    task_id='print1_task',\n    bash_command='echo 1 >> /home/lintong/test.txt',\n    dag=dag)\n\n# -------------------------------------------------------------------------------\n# second operator\n\nprint2_operator = BashOperator(\n    task_id='print2_task',\n    bash_command='echo 2 >> /home/lintong/test.txt',\n    dag=dag)\n\n# -------------------------------------------------------------------------------\n# third operator\n\nprint3_operator = BashOperator(\n    task_id='print3_task',\n    bash_command='echo 3 >> /home/lintong/test.txt',\n    dag=dag)\n\n# -------------------------------------------------------------------------------\n# dependencies\n#each_content(\"1\", \"/home/lintong/桌面/test.txt\")\nprint2_operator.set_upstream(print1_operator)\nprint3_operator.set_upstream(print1_operator)\n\n#if __name__ == \"__main__\":\n#    dag.cli()\n\n```\n\n9.在web界面中查看是否出现这个DAG\n\n<img src=\"/images/517519-20180530003137872-208243656.png\" alt=\"\" />\n\n10.出现的时候，DAG的状态是off，需要将起状态设置为on，并点击后面的 绿色三角形 启动按钮\n\n11.启动调度器\n\n```\nairflow scheduler\n\n```\n\n12.查看文件test.txt，其中会顺序出现1 2 3或者1 3 2\n\n&nbsp;\n\n安装完成后使用下面shell脚本来启动Airflow，端口为8080\n\n```\n#!/bin/bash\n\n#set -x\n#set -e\nset -u\n\n# 使用./start_airflow.sh \"stop_all\"或者\"start_all\"\nif [ $1 == \"stop_all\" ]; then\n    # 获取所有进程，取出Airflow，去除grep，截取PID，干掉\n    ps -ef | grep -Ei 'airflow' | grep -v 'grep' | awk '{print $2}' | xargs kill\nfi\n\nif [ $1 == \"start_all\" ]; then\n    cd /home/lintong/airflow/logs\n    nohup airflow webserver >>webserver.log 2>&amp;1 &amp;\n    nohup airflow worker >>worker.log 2>&amp;1 &amp;\n    nohup airflow scheduler >>scheduler.log 2>&amp;1 &amp;\n    echo \"后台启动Airflow\"\nfi\n\n```\n\n&nbsp;添加用户的python脚本\n\n```\nfrom airflow import models,   settings\nfrom airflow.contrib.auth.backends.password_auth import PasswordUser\nuser = PasswordUser(models.User())\nuser.username = 'lintong'\nuser.email = 'xxxxx@gmail.com'\nuser.password = 'XXXXX'\nsession = settings.Session()\nsession.add(user)\nsession.commit()\nsession.close()\nexit()\n\n```\n\n&nbsp;\n\n## 2.安装apache-airflow 1.10.0\n\n如果要安装1.10版本的，请使用python3.6\n\n如果pip3找不到了\n\n```\nsudo python3 -m pip install --upgrade --force-reinstall pip\n\n```\n\n如果虚拟环境中不存在pip3\n\n```\nsudo apt-get install python3-venv\n\n```\n\n安装Python3.6\n\n```\nsudo add-apt-repository ppa:deadsnakes/ppa\nsudo apt-get update\nsudo apt-get install python3.6\n\n```\n\n安装pip3.6\n\n```\nwget https://bootstrap.pypa.io/get-pip.py\nsudo python3.6 get-pip.py\n\n```\n\n安装python-dev，不然会报error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n\n```\nsudo apt-get install python3.6-dev\n\n```\n\n添加AIRFLOW_HOME\n\n```\n#Airflow\nexport AIRFLOW_HOME=/home/lintong/airflow\n\n```\n\n安装airflow 1.10.10\n\n```\nsudo pip3.6 install --force-reinstall apache-airflow==1.10.0 -i https://pypi.tuna.tsinghua.edu.cn/simple\n\n```\n\nairflow version，然后airflow_home目录下会自动出现airflow.cfg文件，修改airflow.cfg\n\n```\nsql_alchemy_conn = mysql://airflow:airflow@localhost:3306/airflow\n\n```\n\n安装mysqlclient，因为python3没有mysql-python\n\n```\npip3.6 install mysqlclient\n\n```\n\n初始化db\n\n```\nairflow initdb\n\n```\n\n设置supervisor启动\n\nairflow_scheduler\n\n```\nlintong@master:/etc/supervisor/conf.d$ cat airflow_scheduler.conf\n[program:airflow_scheduler]\ndirectory=/home/lintong/airflow\ncommand = airflow scheduler\nuser = lintong\nautostart = false\nautorestart = true\nstderr_logfile = /var/log/supervisor/airflow_scheduler.stderr.log\nstdout_logfile = /var/log/supervisor/airflow_scheduler.stdout.log\n\n```\n\nairflow_worker\n\n当设置 airflow 的 executors 设置为 CeleryExecutor 时才需要开启 worker 守护进程\n\n```\nlintong@master:/etc/supervisor/conf.d$ cat airflow_worker.conf\n[program:airflow_worker]\ndirectory=/home/lintong/airflow\ncommand = airflow worker\nuser = lintong\nautostart = false\nautorestart = true\nstderr_logfile = /var/log/supervisor/airflow_worker.stderr.log\nstdout_logfile = /var/log/supervisor/airflow_worker.stdout.log\n\n```\n\nairflow_webserver\n\n```\nlintong@master:/etc/supervisor/conf.d$ cat airflow_webserver.conf\n[program:airflow_webserver]\ndirectory=/home/lintong/airflow\ncommand = airflow webserver -p 10017\nuser = lintong\nautostart = false\nautorestart = true\nstderr_logfile = /var/log/supervisor/airflow_webserver.stderr.log\nstdout_logfile = /var/log/supervisor/airflow_webserver.stdout.log\n\n```\n\n　　\n\n一些需要修改的配置\n\n1.可以在页面看到configurationview\n\n```\nexpose_config = True\n\n```\n\n2.不显示airflow exmaple的dag，需要airflow resetdb，然后airflow initdb\n\n```\nload_examples = False\n\n```\n\n3.修改LocalExecutor成CeleryExecutor\n\n```\npip3 install celery\npip3 install redis\n\n```\n\n修改配置\n\n```\nexecutor = CeleryExecutor\nbroker_url = redis://127.0.0.1:6379/0\ncelery_result_backend = redis://127.0.0.1:6379/0\n\n```\n\n使用LocalExecutor的时候不需要额外启动airflow worker\n\n参考\n\n```\nhttps://blog.csdn.net/zzq900503/article/details/104537121\n\n```\n\n4.并发相关的配置\n\n```\n[celery]\nworker_concurrency = 32 # 每台worker的能同时运行的最大实例数量，如果work单独部署的话，可以和所在机器的核数保持一致\n\n[core]\nparallelism = 128 # 同时运行的最大实例数量，如果你有4台worker，每台worker的最高并发是32的话，就是128\n\n[scheduler] # 调度器解析dag的线程数量\nparsing_processes = 20\n\n```\n\n　　\n","tags":["Airbnb"]},{"title":"在Ubuntu上使用shadow$ocks","url":"/在Ubuntu上使用shadow$ocks.html","content":"安装\n\n```\npip install shadow$ocks\n\n```\n\n创建文件\n\n```\ntouch /etc/shadow$ocks.json\n\n```\n\n<!--more-->\n&nbsp;\n\n```\n{\n\"server\":\"服務器IP或域名\",\n\"server_port\":端口號,\n\"local_address\": \"127.0.0.1\",\n\"local_port\":1080,\n\"password\":\"密碼\",\n\"timeout\":300,\n\"method\":\"aes-256-cfb\",\n\"fast_open\": false\n}\n\n\n```\n\n&nbsp;安装proxychains\n\n```\nsudo apt-get install proxychains\n\n```\n\n&nbsp;编辑/etc/proxychains.conf，最后一行改为`socks5 127.0.0.1 1080`\n\n&nbsp;然后在root下运行\n\n```\nsslocal -c /etc/shadow$ocks.json -d start\n\n```\n\n&nbsp;如果遇到chrome或者微信上不了网\n\n```\ngoogle-chrome --proxy-server=\"socks5://127.0.0.1:1080\"\nelectronic-wechat --proxy-server=\"socks5://127.0.0.1:1080\"\n\n```\n\n以上使用--proxy-server的方法不是很方便\n\n在Firefox和chrome中可以使用**SwitchOmega插件**中的proxy参数\n\n&nbsp;\n\n将socks代理转换成http代理\n\n参考：[ubuntu网络代理](http://zteman.com/2016/11/ubuntu-%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86/)\n\n安装privoxy\n\n```\nsudo apt-get install python-m2crypto privoxy\n\n```\n\n配置privoxy<br />\nprivoxy的配置文件位于/etc/privoxy/config，用vi打开配置文件\n\n```\nsudo vim /etc/privoxy/config\n\n```\n\n监听端口\n\n```\n在配置文件中有这样一行listen-address localhost:8118 , 这一行代表的意思是，privoxy会监听本地的8118端口，接受请求。\n\n```\n\n转发位置\n\n```\n在配置文件中添加信息 forward-socks5 / 127.0.0.1:1080 . 这一行的意思是将上一步8118端口监听到的请求转发到1080端口，然后由shadow$ocks通过socks方式进行处理 * 注意：末尾的&rdquo;.&rdquo;符号不能丢掉，否则会报参数缺失错误*\n\n```\n\n重启privoxy\n\n```\nsudo systemctl restart privoxy.service\n\n```\n\n在终端使用proxy\n\n```\nexport https_proxy=http://127.0.0.1:2340;export http_proxy=http://127.0.0.1:2340;export all_proxy=socks5://127.0.0.1:2341\n\n```\n\n&nbsp;\n","tags":["Linux"]},{"title":"同时安装anaconda2和anaconda3","url":"/同时安装anaconda2和anaconda3.html","content":"安装的过程请参考 [Ubuntu14.04下同时安装Anaconda2与Anaconda3](http://www.linuxdiyf.com/linux/29891.html)\n\n启动的时候cd到$HOME/anaconda2/envs/py3k/bin下\n\n```\nsource activate py3k #启动\ndeactivate py3k    #退出\n\n```\n\n然后记得在/etc/profile中加上\n\n```\n# added by Anaconda2 4.3.1 installer\nexport PATH=\"/home/common/anaconda2/bin:$PATH\"\n\n```\n\n<!--more-->\n&nbsp;如果想安装包,直接pip install\n","tags":["Python"]},{"title":"Hive学习笔记——安装和内部表CRUD","url":"/Hive学习笔记——安装和内部表CRUD.html","content":"1.首先需要安装Hadoop和Hive\n\n安装的时候参考 [http://blog.csdn.net/jdplus/article/details/46493553](http://blog.csdn.net/jdplus/article/details/46493553)\n\n安装的版本是apache-hive-2.1.1-bin.tar.gz,解压到/usr/local目录下\n\n然后在/etc/profile文件中添加\n\n```\nexport HIVE_HOME=/usr/local/hive\nexport PATH=$PATH:$HIVE_HOME/bin\n\n```\n\n2.修改配置文件\n\n在bin/hive-config.sh文件中添加\n\n```\nexport JAVA_HOME=/usr/lib/jvm/jdk1.8.0_121\nexport HIVE_HOME=/usr/local/hive\nexport HADOOP_HOME=/usr/local/hadoop\n\n```\n\n添加`hive-env.sh文件`\n\n```\ncp hive-env.sh.template hive-env.sh\n\n```\n\n修改conf目录下的hive-site.xml的内容,该模式是本地模式,且使用JDBC连接元数据,本地模式可以查看Hive编程指南P24-27\n\n实际数据还是存放在HDFS中,MySQL中存放的是元数据表,即schema信息\n\n```\n<configuration>\n<property>\n    <name>javax.jdo.option.ConnectionURL</name>\n    <value>jdbc:mysql://localhost:3306/hive</value>\n    <description>JDBC connect string for a JDBC metastore</description>\n</property>\n<property>\n    <name>javax.jdo.option.ConnectionDriverName</name>\n    <value>com.mysql.jdbc.Driver</value>\n    <description>Driver class name for a JDBC metastore</description>\n</property>\n<property>\n    <name>javax.jdo.option.ConnectionUserName</name>\n    <value>root</value>\n    <description>Username to use against metastore database</description>\n</property>\n<property>\n    <name>javax.jdo.option.ConnectionPassword</name>\n    <value>XXXX</value>\n    <description>password to use against metastore database</description>\n</property>\n#如果不配置下面的部分会产生错误1.\n<property>\n    <name>hive.exec.local.scratchdir</name>\n    <value>/usr/local/hive/tmp/local</value>\n    <description>Local scratch space for Hive jobs</description>\n  </property>\n<property>\n    <name>hive.downloaded.resources.dir</name>\n    <value>/usr/local/hive/tmp/downloaded</value>\n    <description>Temporary local directory for added resources in the remote file system.</description>\n</property>\n<property>\n    <name>hive.querylog.location</name>\n    <value>/usr/local/hive/tmp/location</value>\n    <description>Location of Hive run time structured log file</description>\n</property>\n<property>\n   <name>hive.server2.logging.operation.log.location</name>\n    <value>/usr/local/hive/tmp/operation_logs</value>\n    <description>Top level directory where operation logs are stored if logging functionality is enabled</description>\n</property>\n</configuration>\n\n```\n\n注意/usr/local/hive/tmp/local , /usr/local/hive/tmp/downloaded , /usr/local/hive/tmp/location , /usr/local/hive/tmp/operation_logs 这四个文件夹需要自己创建\n\n修改hive-log4j.properties\n\n```\n#cp hive-log4j.properties.template hive-log4j.properties\n#vim hive-log4j.properties\nhive.log.dir=自定义目录/log/\n\n```\n\n在HDFS上建立/tmp和/user/hive/warehouse目录，并赋予组用户写权限\n\n注意这里面的/user/hive/warehouse是由hive-site.xml中的${hive.metastore.warehouse.dir}指定的数据仓库的目录\n\n```\nhadoop fs -mkdir       /tmp\nhadoop fs -mkdir       /user/hive/warehouse\nhadoop fs -chmod g+w   /tmp\nhadoop fs -chmod g+w   /user/hive/warehouse\n\n```\n\n<!--more-->\n&nbsp;Mysql配置\n\n```\n#创建数据库\nmysql> create database hive;\n#赋予访问权限\nmysql> grant all privileges on hive.* to root@localhost identified by '密码' with grant option;\nmysql> flush privileges;\n#将JDBC复制到Hive库目录用于java程序与mysql的连接\ncp mysql-connector-java-5.1.35/mysql-connector-java-5.1.35-bin.jar /usr/local/apache-hive-1.1.0-bin/lib/\n\n```\n\n在hive的bin目录下,初始化元数据\n\n```\n./schematool -initSchema -dbType mysql\n\n```\n\n&nbsp;\n\n如果想使用hive的web界面的话,参考 [http://blog.csdn.net/yyywyr/article/details/51416721](http://blog.csdn.net/yyywyr/article/details/51416721)\n\n&nbsp;\n\nHadoop和Hive的own和grp都是Hadoop\n\nHive的安装很简单,在清华镜像站下载Hive的二进制文件,然后解压到/usr/local目录下\n\n修改own和grp就行\n\n启动hive之前需要先启动Hadoop,启动后显示\n\n```\nhive> \n\n```\n\n启动hive metastore\n\n```\nbin/hive --service metastore\n```\n","tags":["Hive"]},{"title":"Python爬虫——布隆过滤器","url":"/Python爬虫——布隆过滤器.html","content":"布隆过滤器的实现方法1:自己实现\n\n参考 [http://www.cnblogs.com/naive/p/5815433.html](http://www.cnblogs.com/naive/p/5815433.html)\n\nbllomFilter两个参数分别代表,**布隆过滤器的大小和hash函数的个数**\n\n```\n#coding:utf-8\n#!/usr/bin/env python\n\nfrom bitarray import bitarray\n# 3rd party\nimport mmh3\nimport scrapy\nfrom BeautifulSoup import BeautifulSoup as BS\nimport os\nls = os.linesep\n\nclass BloomFilter(set):\n\n    def __init__(self, size, hash_count):\n        super(BloomFilter, self).__init__()\n        self.bit_array = bitarray(size)\n        self.bit_array.setall(0)\n        self.size = size\n        self.hash_count = hash_count\n\n    def __len__(self):\n        return self.size\n\n    def __iter__(self):\n        return iter(self.bit_array)\n\n    def add(self, item):\n        for ii in range(self.hash_count):\n            index = mmh3.hash(item, ii) % self.size\n            self.bit_array[index] = 1\n\n        return self\n\n    def __contains__(self, item):\n        out = True\n        for ii in range(self.hash_count):\n            index = mmh3.hash(item, ii) % self.size\n            if self.bit_array[index] == 0:\n                out = False\n\n        return out\n\nclass DmozSpider(scrapy.Spider):\n    name = \"baidu\"\n    allowed_domains = [\"baidu.com\"]\n    start_urls = [\n        \"http://baike.baidu.com/item/%E7%BA%B3%E5%85%B0%E6%98%8E%E7%8F%A0\"\n    ]\n\n    def parse(self, response):\n\n        # fname = \"/media/common/娱乐/Electronic_Design/Coding/Python/Scrapy/tutorial/tutorial/spiders/temp\"\n        #\n        # html = response.xpath('//html').extract()[0]\n        # fobj = open(fname, 'w')\n        # fobj.writelines(html.encode('utf-8'))\n        # fobj.close()\n\n        bloom = BloomFilter(1000, 10)\n        animals = ['dog', 'cat', 'giraffe', 'fly', 'mosquito', 'horse', 'eagle',\n                   'bird', 'bison', 'boar', 'butterfly', 'ant', 'anaconda', 'bear',\n                   'chicken', 'dolphin', 'donkey', 'crow', 'crocodile']\n        # First insertion of animals into the bloom filter\n        for animal in animals:\n            bloom.add(animal)\n\n        # Membership existence for already inserted animals\n        # There should not be any false negatives\n        for animal in animals:\n            if animal in bloom:\n                print('{} is in bloom filter as expected'.format(animal))\n            else:\n                print('Something is terribly went wrong for {}'.format(animal))\n                print('FALSE NEGATIVE!')\n\n        # Membership existence for not inserted animals\n        # There could be false positives\n        other_animals = ['badger', 'cow', 'pig', 'sheep', 'bee', 'wolf', 'fox',\n                         'whale', 'shark', 'fish', 'turkey', 'duck', 'dove',\n                         'deer', 'elephant', 'frog', 'falcon', 'goat', 'gorilla',\n                         'hawk']\n        for other_animal in other_animals:\n            if other_animal in bloom:\n                print('{} is not in the bloom, but a false positive'.format(other_animal))\n            else:\n                print('{} is not in the bloom filter as expected'.format(other_animal))\n\n```\n\n<!--more-->\n&nbsp;\n\n布隆过滤器的实现方法2:**使用pybloom**\n\n参考 [http://www.jianshu.com/p/f57187e2b5b9](http://www.jianshu.com/p/f57187e2b5b9)\n\n```\n#coding:utf-8\n#!/usr/bin/env python\n\nfrom pybloom import BloomFilter\n\nimport scrapy\nfrom BeautifulSoup import BeautifulSoup as BS\nimport os\nls = os.linesep\n\nclass DmozSpider(scrapy.Spider):\n    name = \"baidu\"\n    allowed_domains = [\"baidu.com\"]\n    start_urls = [\n        \"http://baike.baidu.com/item/%E7%BA%B3%E5%85%B0%E6%98%8E%E7%8F%A0\"\n    ]\n\n    def parse(self, response):\n\n        # fname = \"/media/common/娱乐/Electronic_Design/Coding/Python/Scrapy/tutorial/tutorial/spiders/temp\"\n        #\n        # html = response.xpath('//html').extract()[0]\n        # fobj = open(fname, 'w')\n        # fobj.writelines(html.encode('utf-8'))\n        # fobj.close()\n\n        # bloom = BloomFilter(100, 10)\n        bloom = BloomFilter(1000, 0.001)\n        animals = ['dog', 'cat', 'giraffe', 'fly', 'mosquito', 'horse', 'eagle',\n                   'bird', 'bison', 'boar', 'butterfly', 'ant', 'anaconda', 'bear',\n                   'chicken', 'dolphin', 'donkey', 'crow', 'crocodile']\n        # First insertion of animals into the bloom filter\n        for animal in animals:\n            bloom.add(animal)\n\n        # Membership existence for already inserted animals\n        # There should not be any false negatives\n        for animal in animals:\n            if animal in bloom:\n                print('{} is in bloom filter as expected'.format(animal))\n            else:\n                print('Something is terribly went wrong for {}'.format(animal))\n                print('FALSE NEGATIVE!')\n\n        # Membership existence for not inserted animals\n        # There could be false positives\n        other_animals = ['badger', 'cow', 'pig', 'sheep', 'bee', 'wolf', 'fox',\n                         'whale', 'shark', 'fish', 'turkey', 'duck', 'dove',\n                         'deer', 'elephant', 'frog', 'falcon', 'goat', 'gorilla',\n                         'hawk']\n        for other_animal in other_animals:\n            if other_animal in bloom:\n                print('{} is not in the bloom, but a false positive'.format(other_animal))\n            else:\n                print('{} is not in the bloom filter as expected'.format(other_animal))\n\n```\n\n&nbsp;\n\n输出\n\n```\ndog is in bloom filter as expected\ncat is in bloom filter as expected\ngiraffe is in bloom filter as expected\nfly is in bloom filter as expected\nmosquito is in bloom filter as expected\nhorse is in bloom filter as expected\neagle is in bloom filter as expected\nbird is in bloom filter as expected\nbison is in bloom filter as expected\nboar is in bloom filter as expected\nbutterfly is in bloom filter as expected\nant is in bloom filter as expected\nanaconda is in bloom filter as expected\nbear is in bloom filter as expected\nchicken is in bloom filter as expected\ndolphin is in bloom filter as expected\ndonkey is in bloom filter as expected\ncrow is in bloom filter as expected\ncrocodile is in bloom filter as expected\nbadger is not in the bloom filter as expected\ncow is not in the bloom filter as expected\npig is not in the bloom filter as expected\nsheep is not in the bloom filter as expected\nbee is not in the bloom filter as expected\nwolf is not in the bloom filter as expected\nfox is not in the bloom filter as expected\nwhale is not in the bloom filter as expected\nshark is not in the bloom filter as expected\nfish is not in the bloom filter as expected\nturkey is not in the bloom filter as expected\nduck is not in the bloom filter as expected\ndove is not in the bloom filter as expected\ndeer is not in the bloom filter as expected\nelephant is not in the bloom filter as expected\nfrog is not in the bloom filter as expected\nfalcon is not in the bloom filter as expected\ngoat is not in the bloom filter as expected\ngorilla is not in the bloom filter as expected\nhawk is not in the bloom filter as expected\n\n```\n\n&nbsp;\n","tags":["Python"]},{"title":"Ubuntu下安装和使用zookeeper和kafka","url":"/Ubuntu下安装和使用zookeeper和kafka.html","content":"## **1.下载 kafka和zookeeper**\n\n这里下载的是 kafka_2.10-0.10.0.0.tgz 和 zookeeper-3.4.10.tar.gz\n\n可以在清华镜像站下载\n\n```\nhttps://mirrors.tuna.tsinghua.edu.cn/apache/\n\n```\n\n或者apache官网\n\n```\nhttps://kafka.apache.org/downloads\nhttps://zookeeper.apache.org/releases.html\n\n```\n\n然后分别解压到/usr/local目录下\n\n<!--more-->\n&nbsp;\n\n## **2.安装zookeeper**\n\n进入zookeeper目录,在conf目录下将zoo_sample.cfg文件拷贝,并更名为zoo.cfg\n\n参考[ https://my.oschina.net/phoebus789/blog/730787](https://my.oschina.net/phoebus789/blog/730787)\n\nzoo.cfg文件的内容\n\n```\n# The number of ticks that the initial \n# synchronization phase can take\ninitLimit=10\n# The number of ticks that can pass between \n# sending a request and getting an acknowledgement\nsyncLimit=5\n# the directory where the snapshot is stored.\n# do not use /tmp for storage, /tmp here is just \n# example sakes.\ndataDir=/home/common/zookeeper/zookeeperdir/zookeeper-data\ndataLogDir=/home/common/zookeeper/zookeeperdir/logs\n# the port at which the clients will connect\nclientPort=2181\nserver.1=10.10.100.10:2888:3888\n# the maximum number of client connections.\n# increase this if you need to handle more clients\n#maxClientCnxns=60\n#\n# Be sure to read the maintenance section of the \n# administrator guide before turning on autopurge.\n#\n# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance\n#\n# The number of snapshots to retain in dataDir\n#autopurge.snapRetainCount=3\n# Purge task interval in hours\n# Set to \"0\" to disable auto purge feature\n#autopurge.purgeInterval=1\n\n```\n\n新建下面这两个目录\n\n```\n/home/common/zookeeper/zookeeperdir/zookeeper-data\n/home/common/zookeeper/zookeeperdir/logs\n\n```\n\n在zookeeper-data目录下新建一个myid文件,内容为1,代表这个服务器的编号是1,具体参考上面网址中的内容\n\n最后在/etc/profile中添加环境变量,并source\n\n```\nexport ZOOKEEPER_HOME=/usr/local/zookeeper\nexport PATH=${ZOOKEEPER_HOME}/bin:$PATH\n\n```\n\n现在zookeeper就安装好了,现在**启动zookeeper**\n\n```\nbin/zkServer.sh start\n\n```\n\n查看状态\n\n```\nbin/zkServer.sh status\n\n```\n\n**启动客户端脚本**\n\n```\nbin/zkCli.sh -server localhost:2181\n\n```\n\n**停止zookeeper**\n\n```\nbin/zkServer.sh stop\n\n```\n\n&nbsp;\n\n## **3.启动kafka**\n\n1.现在安装kafka,同样是解压之后就安装好了\n\n参考 [http://www.jianshu.com/p/efc8b9dbd3bd](http://www.jianshu.com/p/efc8b9dbd3bd)\n\n2.进入kafka目录下\n\nkafka需要使用Zookeeper,首先需要启动Zookeeper服务,上面的操作就已经启动了Zookeeper服务\n\n如果没有的话,可以使用kafka自带的脚本启动一个简单的单一节点Zookeeper实例\n\n```\nbin/zookeeper-server-start.sh config/zookeeper.properties\n\n```\n\n**启动 Kafka服务**\n\n```\nbin/kafka-server-start.sh config/server.properties\n\n```\n\n**停止 Kafka服务**\n\n```\nbin/kafka-server-stop.sh config/server.properties\n\n```\n\n&nbsp;\n\n## 4.kafka常用命令\n\n### **1.创建一个主题**\n\n首先创建一个名为`test`的topic，只使用单个分区和一个复本\n\n```\nbin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test\n\n```\n\n&nbsp;现在可以运行list topic命令看到我们的主题\n\n```\nbin/kafka-topics.sh --list --zookeeper localhost:2181\n\n```\n\n### **2.使用kafka-console-producer发送数据**\n\n```\nbin/kafka-console-producer.sh --broker-list localhost:9092 --topic test\nThis is a message\nThis is another message\n\n```\n\n如果要批量导入文件数据到kafka，参考：[2.1 本地环境下kafka批量导入数据](https://www.jianshu.com/p/ee8cc5272df0)\n\n```\nbin/kafka-console-producer.sh --broker-list localhost:9092 --topic test_topic < file_pat\n\n```\n\n如果要模拟实时数据到打入kafka的情况，可以写一个shell脚本\n\n```\n#!/usr/bin/env bash\n\ncat XXXX.log | while read line\ndo\n    sleep 0.1\n    echo \"${line}\"\n    echo \"${line}\" | /home/lintong/software/apache/kafka_2.11-0.10.0.0/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic topicA\ndone\n\n```\n\n&nbsp;\n\n### **3.使用kafka-console-consumer消费数据**\n\n旧版消费者\n\n```\nbin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning 2>/dev/null\n\n```\n\n新版消费者\n\n```\nbin/kafka-console-consumer.sh --new-consumer --bootstrap-server localhost:9092 --topic input --from-beginning 2>/dev/null\n\n```\n\n消费带权限kafka topic\n\n```\n/opt/cloudera/parcels/KAFKA/bin/kafka-console-consumer --new-consumer --bootstrap-server xxxx:9092 --topic my_topic --consumer.config ./client.jaas > ./test.log\n\n```\n\n其他参数\n\n```\n--max-messages XXX   指定在退出前最多读取的消息数\n--partition XXX    指定消费的分区\n--property print.timestamp=true 打印时间戳，如CreateTime:1612167905050\n\n```\n\n同时打印kafka message中的key和value\n\n```\nkafka-console-consumer.sh --bootstrap-server xxx:9092 --topic my_topic_name --partition 1 --offset 1589306 --max-messages 100000 --property print.key=true --property key.separator=\"-\" > /home/lintong/tmp.log\n\n```\n\n　　\n\n### **4.查看指定的topic的offset信息**\n\n**<img src=\"/images/517519-20180903202825403-5962815.png\" alt=\"\" width=\"820\" height=\"124\" />**\n\n对于结尾是ZK的消费者,其消费者的信息是存储在Zookeeper中的\n\n<img src=\"/images/517519-20180903203108136-2001642599.png\" alt=\"\" />\n\n对于结尾是KF的消费者,其消费者的信息是存在在Kafka的broker中的\n\n都可以使用下面的命令进行查看\n\n```\nbin/kafka-consumer-offset-checker.sh --zookeeper localhost:2181 --group xxx --topic xxx\n\n```\n\n结果\n\n```\nbin/kafka-consumer-offset-checker.sh --zookeeper localhost:2181 --group test-consumer-group --topic xxx\n[2018-09-03 20:34:57,595] WARN WARNING: ConsumerOffsetChecker is deprecated and will be dropped in releases following 0.9.0. Use ConsumerGroupCommand instead. (kafka.tools.ConsumerOffsetChecker$)\nGroup           Topic                          Pid Offset          logSize         Lag             Owner\ntest-consumer-group xxx              0   509             0               -509            none\n\n```\n\n或者\n\n```\n./bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper localhost:2181 --group xxxx --topic xxxx\n\n```\n\n结果\n\n```\nbin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper localhost:2181 --group test-consumer-group\n[2018-09-03 20:45:02,967] WARN WARNING: ConsumerOffsetChecker is deprecated and will be dropped in releases following 0.9.0. Use ConsumerGroupCommand instead. (kafka.tools.ConsumerOffsetChecker$)\nGroup           Topic                          Pid Offset          logSize         Lag             Owner\ntest-consumer-group xxx              0   509             509             0               none\n\n```\n\nlag是负数的原因是 topic中的消息数量过期(超过kafka默认的7天后被删除了),变成了0,所以Lag=logSize减去Offset,所以就变成了负数\n\n### **5.删除一个topic**\n\n需要在 conf/server.properties 文件中设置\n\n```\n# For delete topic\ndelete.topic.enable=true\n\n```\n\n否则在执行了以下删除命令后，再 list 查看所有的topic，还是会看到该topic\n\n```\nbin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic topicB\n\n```\n\n再到 配置文件 中的kafka数据存储地址去删除物理数据了，我的地址为\n\n```\n/tmp/kafka-logs\n\n```\n\n最后需要到zk里删除kafka的元数据\n\n```\n./bin/zkCli.sh #进入zk shell\nls /brokers/topics\nrmr /brokers/topics/topicA\n\n```\n\n&nbsp;参考：[kafka 手动删除topic的数据](https://my.oschina.net/remainsu/blog/1571228)\n\n&nbsp;\n\n### **6.查看某个group的信息**\n\n**新版**\n\n```\nbin/kafka-consumer-groups.sh --new-consumer --bootstrap-server localhost:9092 --describe --group xxx\n\n```\n\n**结果**\n\n```\nbin/kafka-consumer-groups.sh --new-consumer --bootstrap-server localhost:9092 --describe --group group_id\nGROUP          TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET   LAG             　　　　OWNER\ngroup_id 　　　  xxx              0          509             509             0               consumer-1_/127.0.0.1\n\n```\n\n如果这时候消费者进程关闭了之后,使用上面的命令和下面的-list命令将不会查出这个group_id,但是当消费者进程重新开启后,这个group_id又能重新查到,且消费的offset不会丢失\n\n**旧版**\n\n```\nbin/kafka-consumer-groups.sh --zookeeper 127.0.0.1:2181 --group xxx --describe\n\n```\n\n### **7.查看consumer group的列表**\n\nZK的消费者可以使用下面命令查看,比如上面的例子中的 test-consumer-group\n\n```\nbin/kafka-consumer-groups.sh --zookeeper 127.0.0.1:2181 --list\n\n```\n\nKF的消费者可以使用下面命令查看,比如上面的例子中的 console-consumer-xxx ,但是只会查看到类似于 KMOffsetCache-master 的结果,这是由于这种消费者的信息是存放在 __consumer_offsets 中\n\n对于如何查看存储于 __consumer_offsets 中的新版消费者的信息,可以参考huxihx的博文: [Kafka 如何读取offset topic内容 (__consumer_offsets)](http://www.cnblogs.com/huxi2b/p/6061110.html)\n\n```\nbin/kafka-consumer-groups.sh --new-consumer --bootstrap-server localhost:9092 --list\n\n```\n\n### **8.在zk中删除一个consumer group**\n\n```\nrmr /consumers/test-consumer-group\n\n```\n\n### **9.查看topic的offset的最小值**\n\n参考：[重置kafka的offset](https://www.cnblogs.com/keitsi/p/5685576.html)\n\n```\nbin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9092 -topic xxxx --time -2\nxxxx:0:0\n\n```\n\n### **10.查看topic的offset的最大值**\n\n```\nbin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9092 -topic xxxx --time -1\n\n```\n\n### **11.<strong>重置topic的某个消费者的offset为0**</strong>\n\n需要高版本的kafka才有该命令，在高版本的kafka client对低版本的kafka集群执行该命令是会生效的\n\n而且需要该group是inactive的，即该消费组没有消费者，不然会报 Error: Assignments can only be reset if the group 'xxxxxx' is inactive, but the current state is Stable.\n\n```\nkafka-consumer-groups --bootstrap-server localhost:9092 --group xxx --topic xxx --reset-offsets --to-earliest --execute\n\n```\n\n如果是要调整某个topic的某个partition，只需要在topic名字后面加上（:partition_id），如\n\n```\nkafka-consumer-groups --bootstrap-server localhost:9092 --group xxx --topic xxx:0 --reset-offsets --to-earliest --execute\n\n```\n\n### **<strong>12.重置topic的某个消费者的offset为指定值**</strong>\n\n```\nkafka-consumer-groups.sh --bootstrap-server localhost:9092 --group groupName --reset-offsets --to-offset 1000 --topic topicName --execute\n\n```\n\n### **13.指定offset和partition进行消费**\n\n指定offset的时候必须指定partition\n\n```\n/opt/cloudera/parcels/KAFKA/bin/kafka-console-consumer --topic xxxx --partition 2 --offset 820000  --bootstrap-server xxx:9092 > ./test2.log\n\n```\n\n### **14.查看kafka topic的consumer的某个时间的offset**\n\n注意这个--to-datetime是utc时间，需要减去8个小时\n\n```\n/opt/cloudera/parcels/KAFKA/bin/kafka-consumer-groups --bootstrap-server xxxx:9092 --group xxxx --topic xxxx --command-config ./client.jaas  --reset-offsets --to-datetime 2020-01-01T00:00:00.000\n\n```\n\nclient.jaas\n\n```\nproperties {\n\tsecurity.protocol=SASL_PLAINTEXT\n\tsasl.mechanism=PLAIN\n\tsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"xxxx\" serviceName=\"kafka\" password=\"xxxx\";\n}\n\n```\n\n### **15.将某个topic的某个消费者的offset置为lastest**\n\n```\n/opt/cloudera/parcels/KAFKA/bin/kafka-consumer-groups --bootstrap-server xxxx:9092 --group xxxx --topic xxxx --reset-offsets --to-latest --execute\n\n```\n\n### **16.迁移topic副本**\n\n比如topic1有3个partition，这个3个partition分布在broker150、broker151、broker152上，此时想要将broker152换成broker164，比如说在kafka集群**添加了新节点**或者遇到**磁盘分布不均匀**的情况的时候\n\n在kafka manager上面点击 <img src=\"/images/517519-20200720152053429-1448890669.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n<img src=\"/images/517519-20200720152408719-856712972.png\" alt=\"\" loading=\"lazy\" />\n\n修改\n\n<img src=\"/images/517519-20200720152442586-1774205835.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp; 再点击 <img src=\"/images/517519-20200720152119470-51895690.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;如果数据量大的话会比较耗时\n\n<img src=\"/images/517519-20200720152210175-741203373.png\" width=\"400\" height=\"158\" loading=\"lazy\" />\n\n### **17.生成迁移方案**\n\n参考：[kafka partiton迁移方法与原理](https://www.cnblogs.com/set-cookie/p/9614241.html)\n\n&nbsp;\n\n### **18.查看同步延迟（under replicated）的partition**\n\n```\n/opt/cloudera/parcels/KAFKA/lib/kafka/bin/kafka-topics.sh --zookeeper xxxx:2181 --describe --under-replicated-partitions\n\n```\n\n&nbsp;\n\n### 19.增加kafka topic的副本数量\n\n参考：[通过CMAK修改Kafka的Topic的副本数](https://blog.csdn.net/player2135/article/details/123147844)\n\n&nbsp;\n","tags":["kafka","zookeeper"]},{"title":"Ubuntu16.04安装xgboost","url":"/Ubuntu16.04安装xgboost.html","content":"**1.Python下安装方法**\n\n```\ngit clone --recursive https://github.com/dmlc/xgboost\ncd xgboost\nmake -j4\ncd python-package/\nsudo python setup.py install\n\n```\n\n如果在import xgboost后，遇到问题\n\n```\nOSError: /home/common/anaconda2/lib/python2.7/site-packages/scipy/sparse/../../../../libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by /home/common/coding/coding/Scala/xgboost/python-package/xgboost/../../lib/libxgboost.so)\n\n```\n\n解决方法\n\n```\nconda install libgcc\n\n```\n\n**2.Java下安装方法**\n\n**请先在Python下安装好，因为上面的gcc版本问题会影响到java下xgboost的编译和安装**\n\n**先更新**\n\n```\ngit pull &amp;&amp; git submodule init &amp;&amp; git submodule update &amp;&amp; git submodule status\n\n```\n\n**然后参考**\n\n```\nhttp://xgboost.readthedocs.io/en/latest/jvm/\n\n```\n\n<!--more-->\n&nbsp;\n\n几个xgboost的Scala实现方法\n\n```\nhttps://www.elenacuoco.com/2016/10/10/scala-spark-xgboost-classification/\nhttp://blog.csdn.net/luoyexuge/article/details/71422270&nbsp;\n\n```\n","tags":["ML Infra"]},{"title":"查看pip安装的Python库","url":"/查看pip安装的Python库.html","content":"查看安装的库\n\n```\npip list或者pip freeze\n\n```\n\n查看过时的库\n\n```\npip list --outdated\n\n```\n\n批量更新的Python脚本\n\n```\nimport pip\nfrom subprocess import call\n \nfor dist in pip.get_installed_distributions():\n    call(\"sudo pip install --upgrade \" + dist.project_name, shell=True)\n\n```\n\n<!--more-->\n&nbsp;更新pip\n\n```\npip install --upgrade pip\n\n```\n\n&nbsp;\n","tags":["Python"]},{"title":"Kaggle学习笔记——房屋价格预测","url":"/Kaggle学习笔记——房屋价格预测.html","content":"Kaggle的房价数据集使用的是[Ames Housing<!--more-->\n&nbsp;dataset](http://www.amstat.org/publications/jse/v19n3/decock.pdf)，是美国爱荷华州的艾姆斯镇2006-2010年的房价\n\n## 1.特征探索和分析\n\n### 1.了解特征的含义\n\n首先使用Python的pandas加载一下训练样本和测试样本，数据的格式是csv格式的，且第一列是特征的名称\n\n<img src=\"/images/517519-20230323215533168-1043888981.png\" width=\"500\" height=\"50\" />\n\n查看一下特征的维度\n\n```\nimport pandas as pd\n\n# 加载数据\ntrain_data = pd.read_csv(\"./raw_data/train.csv\")\n\n# 去掉id和售价，只看特征\ndata: DataFrame = train_data.drop(columns=[\"Id\", \"SalePrice\"])\n\nprint(data.shape)\n\n```\n\n输出如下，除去Id和SalePrice，总共有79维的特征\n\n```\n(1460, 79)\n\n```\n\n翻译一下给的房屋数据的特征，这里定义了一个dict，方便理解每个特征的含义\n\n```\ndict = {\n    \"MSSubClass\": \"参与销售住宅的类型:有年代新旧等信息\",\n    \"MSZoning\": \"房屋类型:农用,商用等\",\n    \"LotFrontage\": \"距离街道的距离\",\n    \"LotArea\": \"房屋的面积\",\n    \"Street\": \"通向房屋的Street是用什么铺的\",\n    \"Alley\": \"通向房屋的Alley是用什么铺的\",\n    \"LotShape\": \"房屋的户型,规整程度\",\n    \"LandContour\": \"房屋的平坦程度\",\n    \"Utilities\": \"设施,通不通水电气\",\n    \"LotConfig\": \"死路,处于三岔口等\",\n    \"LandSlope\": \"坡度\",\n    \"Neighborhood\": \"邻居\",\n    \"Condition1\": \"\",\n    \"Condition2\": \"\",\n    \"BldgType\": \"住宅类型,住的家庭数,是否别墅等\",\n    \"HouseStyle\": \"住宅类型,隔断等\",\n    \"OverallQual\": \"房屋的质量\",\n    \"OverallCond\": \"房屋位置的质量\",\n    \"YearBuilt\": \"建造的时间\",\n    \"YearRemodAdd\": \"改造的时间\",\n    \"RoofStyle\": \"屋顶的类型\",\n    \"RoofMatl\": \"屋顶的材料\",\n    \"Exterior1st\": \"外观覆盖的材质\",\n    \"Exterior2nd\": \"如果超过一种,则有第二种材质\",\n    \"MasVnrType\": \"表层砌体类型\",\n    \"MasVnrArea\": \"表层砌体面积\",\n    \"ExterQual\": \"外观材料质量\",\n    \"ExterCond\": \"外观材料情况\",\n    \"Foundation\": \"地基类型\",\n    \"BsmtQual\": \"地下室质量\",\n    \"BsmtCond\": \"地下室的基本情况\",\n    \"BsmtExposure\": \"地下室采光\",\n    \"BsmtFinType1\": \"地下室的完成情况比例\",\n    \"BsmtFinSF1\": \"地下室的完成面积\",\n    \"BsmtFinType2\": \"如果有多个地下室的话\",\n    \"BsmtFinSF2\": \"如果有多个地下室的话\",\n    \"BsmtUnfSF\": \"未完成的地下室面积\",\n    \"TotalBsmtSF\": \"地下室面积\",\n    \"Heating\": \"供暖类型\",\n    \"HeatingQC\": \"供暖质量\",\n    \"CentralAir\": \"是否有中央空调\",\n    \"Electrical\": \"电气系统\",\n    \"_1stFlrSF\": \"1楼面积\",\n    \"_2ndFlrSF\": \"2楼面积\",\n    \"LowQualFinSF\": \"低质量完成的面积(楼梯占用的面积)\",\n    \"GrLivArea\": \"地面以上居住面积\",\n    \"BsmtFullBath\": \"地下室都是洗手间\",\n    \"BsmtHalfBath\": \"地下室一半是洗手间\",\n    \"FullBath\": \"洗手间都在一层以上\",\n    \"HalfBath\": \"一半洗手间在一层以上\",\n    \"BedroomAbvGr\": \"卧室都在一层以上\",\n    \"KitchenAbvGr\": \"厨房在一层以上\",\n    \"KitchenQual\": \"厨房质量\",\n    \"TotRmsAbvGrd\": \"所有房间都在一层以上\",\n    \"Functional\": \"房屋的功能性等级\",\n    \"Fireplaces\": \"壁炉位置\",\n    \"FireplaceQu\": \"壁炉质量\",\n    \"GarageType\": \"车库类型\",\n    \"GarageYrBlt\": \"车库建造时间\",\n    \"GarageFinish\": \"车库的室内装修\",\n    \"GarageCars\": \"车库的汽车容量\",\n    \"GarageArea\": \"车库面积\",\n    \"GarageQual\": \"车库质量\",\n    \"GarageCond\": \"车库情况\",\n    \"PavedDrive\": \"铺路的材料\",\n    \"WoodDeckSF\": \"木地板面积\",\n    \"OpenPorchSF\": \"露天门廊面积\",\n    \"EnclosedPorch\": \"独立门廊面积\",\n    \"_3SsnPorch\": \"three season门廊面积\",\n    \"ScreenPorch\": \"纱门门廊面积\",\n    \"PoolArea\": \"游泳池面积\",\n    \"PoolQC\": \"游泳池质量\",\n    \"Fence\": \"栅栏质量\",\n    \"MiscFeature\": \"上面不包含其他功能\",\n    \"MiscVal\": \"上面不包含其他功能的价格\",\n    \"MoSold\": \"月销量\",\n    \"YrSold\": \"年销量\",\n    \"SaleType\": \"销售方式\",\n    \"SaleCondition\": \"销售情况\"\n}\n\n```\n\n### 2.查看特征是离散特征还是连续特征\n\n```\n# 查看特征是离散特征还是连续特征\ntrain_data.info()\n\n```\n\n&nbsp;<img src=\"/images/517519-20230402224842886-452119346.png\" width=\"300\" height=\"331\" loading=\"lazy\" />\n\n### 3.查看特征的统计指标\n\n```\n# 查看特征的统计指标\npd.set_option('expand_frame_repr', False)  # 不换行\npd.set_option('display.max_columns', None)  # 显示所有列\ntrain_data_statistic = train_data.describe()\nprint(train_data_statistic)\n\n```\n\n读取了数据之后先describe一下，查看均值，样本标准偏差，最小值和最大值等，参考：[Python Pandas 常用统计数据方法汇总（求和，计数，均值，中位数，分位数，最大/最小，方差，标准差等）](https://blog.csdn.net/qq_42067550/article/details/106260512)\n\n其中count（非空值数）、unique（去重后的个数）、top（频数最高者）、freq（最高频数）\n\n可以发现MasVnrType这个特征出现最多的是None，有864/1460都是None字符串，这个特征在后面需要进行删除处理\n\n```\n                 Id   MSSubClass MSZoning  LotFrontage        LotArea Street Alley LotShape LandContour Utilities LotConfig LandSlope Neighborhood Condition1 Condition2 BldgType HouseStyle  OverallQual  OverallCond    YearBuilt  YearRemodAdd RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType   MasVnrArea ExterQual ExterCond Foundation BsmtQual BsmtCond BsmtExposure BsmtFinType1   BsmtFinSF1 BsmtFinType2   BsmtFinSF2    BsmtUnfSF  TotalBsmtSF Heating HeatingQC CentralAir Electrical     1stFlrSF     2ndFlrSF  LowQualFinSF    GrLivArea  BsmtFullBath  BsmtHalfBath     FullBath     HalfBath  BedroomAbvGr  KitchenAbvGr KitchenQual  TotRmsAbvGrd Functional   Fireplaces FireplaceQu GarageType  GarageYrBlt GarageFinish   GarageCars   GarageArea GarageQual GarageCond PavedDrive   WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  ScreenPorch     PoolArea PoolQC  Fence MiscFeature       MiscVal       MoSold       YrSold SaleType SaleCondition      SalePrice\ncount   1460.000000  1460.000000     1460  1201.000000    1460.000000   1460    91     1460        1460      1460      1460      1460         1460       1460       1460     1460       1460  1460.000000  1460.000000  1460.000000   1460.000000      1460     1460        1460        1460       1452  1452.000000      1460      1460       1460     1423     1423         1422         1423  1460.000000         1422  1460.000000  1460.000000  1460.000000    1460      1460       1460       1459  1460.000000  1460.000000   1460.000000  1460.000000   1460.000000   1460.000000  1460.000000  1460.000000   1460.000000   1460.000000        1460   1460.000000       1460  1460.000000         770       1379  1379.000000         1379  1460.000000  1460.000000       1379       1379       1460  1460.000000  1460.000000    1460.000000  1460.000000  1460.000000  1460.000000      7    281          54   1460.000000  1460.000000  1460.000000     1460          1460    1460.000000\nunique          NaN          NaN        5          NaN            NaN      2     2        4           4         2         5         3           25          9          8        5          8          NaN          NaN          NaN           NaN         6        8          15          16          4          NaN         4         5          6        4        4            4            6          NaN            6          NaN          NaN          NaN       6         5          2          5          NaN          NaN           NaN          NaN           NaN           NaN          NaN          NaN           NaN           NaN           4           NaN          7          NaN           5          6          NaN            3          NaN          NaN          5          5          3          NaN          NaN            NaN          NaN          NaN          NaN      3      4           4           NaN          NaN          NaN        9             6            NaN\ntop             NaN          NaN       RL          NaN            NaN   Pave  Grvl      Reg         Lvl    AllPub    Inside       Gtl        NAmes       Norm       Norm     1Fam     1Story          NaN          NaN          NaN           NaN     Gable  CompShg     VinylSd     VinylSd       None          NaN        TA        TA      PConc       TA       TA           No          Unf          NaN          Unf          NaN          NaN          NaN    GasA        Ex          Y      SBrkr          NaN          NaN           NaN          NaN           NaN           NaN          NaN          NaN           NaN           NaN          TA           NaN        Typ          NaN          Gd     Attchd          NaN          Unf          NaN          NaN         TA         TA          Y          NaN          NaN            NaN          NaN          NaN          NaN     Gd  MnPrv        Shed           NaN          NaN          NaN       WD        Normal            NaN\nfreq            NaN          NaN     1151          NaN            NaN   1454    50      925        1311      1459      1052      1382          225       1260       1445     1220        726          NaN          NaN          NaN           NaN      1141     1434         515         504        864          NaN       906      1282        647      649     1311          953          430          NaN         1256          NaN          NaN          NaN    1428       741       1365       1334          NaN          NaN           NaN          NaN           NaN           NaN          NaN          NaN           NaN           NaN         735           NaN       1360          NaN         380        870          NaN          605          NaN          NaN       1311       1326       1340          NaN          NaN            NaN          NaN          NaN          NaN      3    157          49           NaN          NaN          NaN     1267          1198            NaN\nmean     730.500000    56.897260      NaN    70.049958   10516.828082    NaN   NaN      NaN         NaN       NaN       NaN       NaN          NaN        NaN        NaN      NaN        NaN     6.099315     5.575342  1971.267808   1984.865753       NaN      NaN         NaN         NaN        NaN   103.685262       NaN       NaN        NaN      NaN      NaN          NaN          NaN   443.639726          NaN    46.549315   567.240411  1057.429452     NaN       NaN        NaN        NaN  1162.626712   346.992466      5.844521  1515.463699      0.425342      0.057534     1.565068     0.382877      2.866438      1.046575         NaN      6.517808        NaN     0.613014         NaN        NaN  1978.506164          NaN     1.767123   472.980137        NaN        NaN        NaN    94.244521    46.660274      21.954110     3.409589    15.060959     2.758904    NaN    NaN         NaN     43.489041     6.321918  2007.815753      NaN           NaN  180921.195890\nstd      421.610009    42.300571      NaN    24.284752    9981.264932    NaN   NaN      NaN         NaN       NaN       NaN       NaN          NaN        NaN        NaN      NaN        NaN     1.382997     1.112799    30.202904     20.645407       NaN      NaN         NaN         NaN        NaN   181.066207       NaN       NaN        NaN      NaN      NaN          NaN          NaN   456.098091          NaN   161.319273   441.866955   438.705324     NaN       NaN        NaN        NaN   386.587738   436.528436     48.623081   525.480383      0.518911      0.238753     0.550916     0.502885      0.815778      0.220338         NaN      1.625393        NaN     0.644666         NaN        NaN    24.689725          NaN     0.747315   213.804841        NaN        NaN        NaN   125.338794    66.256028      61.119149    29.317331    55.757415    40.177307    NaN    NaN         NaN    496.123024     2.703626     1.328095      NaN           NaN   79442.502883\nmin        1.000000    20.000000      NaN    21.000000    1300.000000    NaN   NaN      NaN         NaN       NaN       NaN       NaN          NaN        NaN        NaN      NaN        NaN     1.000000     1.000000  1872.000000   1950.000000       NaN      NaN         NaN         NaN        NaN     0.000000       NaN       NaN        NaN      NaN      NaN          NaN          NaN     0.000000          NaN     0.000000     0.000000     0.000000     NaN       NaN        NaN        NaN   334.000000     0.000000      0.000000   334.000000      0.000000      0.000000     0.000000     0.000000      0.000000      0.000000         NaN      2.000000        NaN     0.000000         NaN        NaN  1900.000000          NaN     0.000000     0.000000        NaN        NaN        NaN     0.000000     0.000000       0.000000     0.000000     0.000000     0.000000    NaN    NaN         NaN      0.000000     1.000000  2006.000000      NaN           NaN   34900.000000\n25%      365.750000    20.000000      NaN    59.000000    7553.500000    NaN   NaN      NaN         NaN       NaN       NaN       NaN          NaN        NaN        NaN      NaN        NaN     5.000000     5.000000  1954.000000   1967.000000       NaN      NaN         NaN         NaN        NaN     0.000000       NaN       NaN        NaN      NaN      NaN          NaN          NaN     0.000000          NaN     0.000000   223.000000   795.750000     NaN       NaN        NaN        NaN   882.000000     0.000000      0.000000  1129.500000      0.000000      0.000000     1.000000     0.000000      2.000000      1.000000         NaN      5.000000        NaN     0.000000         NaN        NaN  1961.000000          NaN     1.000000   334.500000        NaN        NaN        NaN     0.000000     0.000000       0.000000     0.000000     0.000000     0.000000    NaN    NaN         NaN      0.000000     5.000000  2007.000000      NaN           NaN  129975.000000\n50%      730.500000    50.000000      NaN    69.000000    9478.500000    NaN   NaN      NaN         NaN       NaN       NaN       NaN          NaN        NaN        NaN      NaN        NaN     6.000000     5.000000  1973.000000   1994.000000       NaN      NaN         NaN         NaN        NaN     0.000000       NaN       NaN        NaN      NaN      NaN          NaN          NaN   383.500000          NaN     0.000000   477.500000   991.500000     NaN       NaN        NaN        NaN  1087.000000     0.000000      0.000000  1464.000000      0.000000      0.000000     2.000000     0.000000      3.000000      1.000000         NaN      6.000000        NaN     1.000000         NaN        NaN  1980.000000          NaN     2.000000   480.000000        NaN        NaN        NaN     0.000000    25.000000       0.000000     0.000000     0.000000     0.000000    NaN    NaN         NaN      0.000000     6.000000  2008.000000      NaN           NaN  163000.000000\n75%     1095.250000    70.000000      NaN    80.000000   11601.500000    NaN   NaN      NaN         NaN       NaN       NaN       NaN          NaN        NaN        NaN      NaN        NaN     7.000000     6.000000  2000.000000   2004.000000       NaN      NaN         NaN         NaN        NaN   166.000000       NaN       NaN        NaN      NaN      NaN          NaN          NaN   712.250000          NaN     0.000000   808.000000  1298.250000     NaN       NaN        NaN        NaN  1391.250000   728.000000      0.000000  1776.750000      1.000000      0.000000     2.000000     1.000000      3.000000      1.000000         NaN      7.000000        NaN     1.000000         NaN        NaN  2002.000000          NaN     2.000000   576.000000        NaN        NaN        NaN   168.000000    68.000000       0.000000     0.000000     0.000000     0.000000    NaN    NaN         NaN      0.000000     8.000000  2009.000000      NaN           NaN  214000.000000\nmax     1460.000000   190.000000      NaN   313.000000  215245.000000    NaN   NaN      NaN         NaN       NaN       NaN       NaN          NaN        NaN        NaN      NaN        NaN    10.000000     9.000000  2010.000000   2010.000000       NaN      NaN         NaN         NaN        NaN  1600.000000       NaN       NaN        NaN      NaN      NaN          NaN          NaN  5644.000000          NaN  1474.000000  2336.000000  6110.000000     NaN       NaN        NaN        NaN  4692.000000  2065.000000    572.000000  5642.000000      3.000000      2.000000     3.000000     2.000000      8.000000      3.000000         NaN     14.000000        NaN     3.000000         NaN        NaN  2010.000000          NaN     4.000000  1418.000000        NaN        NaN        NaN   857.000000   547.000000     552.000000   508.000000   480.000000   738.000000    NaN    NaN         NaN  15500.000000    12.000000  2010.000000      NaN           NaN  755000.000000\n\n```\n\n## 2.**数据清洗和缺失值填充**\n\n### 1.处理缺失值\n\n查看有无空值，pandas的空值为NaN（字符串\"NA\"），所以我们先统计一下NaN值，参考：[特征预处理&mdash;&mdash;异常值处理](https://www.cnblogs.com/tonglin0325/p/6298290.html)\n\n```\n# 对有Nan值的列的空值比例进行排序，并添加上特征的名字和类型\nnan_total = data.isna().sum().sort_values(ascending=False)\nnan_percent = (data.isna().sum() / data.isnull().count()).sort_values(ascending=False)\nfeature_name = pd.Series(dict).T.sort_values(ascending=False)\nfeature_type = data.dtypes.sort_values(ascending=False)\nmissing_data = pd.concat(\n    [nan_total, nan_percent, feature_name, feature_type],\n    axis=1,\n    keys=['NanTotal', 'NanPercent', 'FeatureName', 'FeatureType']\n)\nprint(missing_data.head(20))\n\n```\n\n<img src=\"/images/517519-20230405220227917-454716152.png\" width=\"500\" height=\"227\" loading=\"lazy\" />\n\n可以看出，PoolQC这一列有很多值是NaN，当空值比例远大于15%的时候，就可以考虑将这个特征删掉\n\n```\n# 删除NaN比例远超过15%的特征\ntrain_data: DataFrame = train_data.drop(\n    columns=['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'MasVnrType', 'FireplaceQu']\n)\nprint(train_data.shape)\n\n```\n\n还剩下73维特征\n\n```\n# 对LotFrontage进行双变量分析\nsns.scatterplot(y=train_data['SalePrice'], x=train_data['LotFrontage'])\nplt.show()\n\n```\n\n<img src=\"/images/517519-20230405233555962-1328060755.png\" width=\"300\" height=\"223\" loading=\"lazy\" />\n\n可以使用删除红圈的离群点的LotFrontage的平均值来对缺失值进行填充\n\n```\n# 处理缺失值\n# LotFrontage\n# sns.scatterplot(y=train_data['SalePrice'], x=data['LotFrontage'])\n# plt.show()\ntrain = data.drop(data[data['LotFrontage'] > 300].index)\ndata['LotFrontage'] = data['LotFrontage'].fillna(train['LotFrontage'].mean())\n\n```\n\n处理其他缺失值\n\n```\n# GarageType\n# sns.scatterplot(y=train_data['SalePrice'], x=data['GarageType'])\n# plt.show()\ndata['GarageType'].fillna(value='None', inplace=True)\n\n# GarageYrBlt\n# sns.scatterplot(y=train_data['SalePrice'], x=data['GarageYrBlt'])\n# plt.show()\ndata['GarageYrBlt'] = data['GarageYrBlt'].fillna(data['GarageYrBlt'].mean())\n\n# GarageFinish\n# sns.scatterplot(y=train_data['SalePrice'], x=data['GarageFinish'])\n# plt.show()\ndata['GarageFinish'].fillna(value='None', inplace=True)\n\n# GarageQual\n# sns.scatterplot(y=train_data['SalePrice'], x=data['GarageQual'])\n# plt.show()\ndata['GarageQual'].fillna(value='None', inplace=True)\n\n# GarageCond\n# sns.scatterplot(y=train_data['SalePrice'], x=data['GarageCond'])\n# plt.show()\ndata['GarageCond'].fillna(value='None', inplace=True)\n\n# BsmtFinType2\n# sns.scatterplot(y=train_data['SalePrice'], x=data['BsmtFinType2'])\n# plt.show()\ndata['BsmtFinType2'].fillna(value='None', inplace=True)\n\n# BsmtExposure\n# sns.scatterplot(y=train_data['SalePrice'], x=data['BsmtExposure'])\n# plt.show()\ndata['BsmtExposure'].fillna(value='None', inplace=True)\n\n# BsmtQual\n# sns.scatterplot(y=train_data['SalePrice'], x=data['BsmtQual'])\n# plt.show()\ndata['BsmtQual'].fillna(value='None', inplace=True)\n\n# BsmtFinType1\n# sns.scatterplot(y=train_data['SalePrice'], x=data['BsmtFinType1'])\n# plt.show()\ndata['BsmtFinType1'].fillna(value='None', inplace=True)\n\n# BsmtCond\n# sns.scatterplot(y=train_data['SalePrice'], x=data['BsmtCond'])\n# plt.show()\ndata['BsmtCond'].fillna(value='None', inplace=True)\n\n# MasVnrArea\n# sns.scatterplot(y=train_data['SalePrice'], x=data['MasVnrArea'])\n# plt.show()\ndata['MasVnrArea'].fillna(value=0, inplace=True)\n\n# Electrical\n# sns.scatterplot(y=train_data['SalePrice'], x=data['Electrical'])\n# plt.show()\ndata['Electrical'].fillna(value='SBrkr', inplace=True)\n\n```\n\n现在就将所有的缺失值填充好了\n\n### 4.处理离群值\n\n我们首先对需要预测的值进行离群值分析，首先绘制预测值SalePrice的箱型图\n\n## **3.数据转换**\n\n5.特征分布处理\n\n正态分布\n\n## **4.特征选择**\n\n### 1.去掉变化小的特征\n\n[](https://www.cnblogs.com/tonglin0325/p/6214978.html)假设某特征的特征值只有0和1，并且在所有输入样本中，95%的实例的该特征取值都是1，那就可以认为这个特征作用不大。如果100%都是1，那这个特征就没意义了。参考：[特征预处理&mdash;&mdash;特征选择和特征理解 ](https://www.cnblogs.com/tonglin0325/p/6214978.html)\n\n比如 Street 和 Utilities，unique是2且freq很高，应该去掉这类特征\n\n&nbsp;<img src=\"/images/517519-20230325001557162-951922778.png\" width=\"600\" height=\"95\" />\n\n```\n# 删除变化小的特征\ntrain_data: DataFrame = train_data.drop(\n    columns=['Street', 'Utilities']\n)\n\n```\n\n### 2.相关性分析\n\n&nbsp;\n\n先看一下**房价的分布情况**，15W刀左右的居多，可以看出这个数据集是很久以前的了\n\n<img src=\"/images/517519-20170529170228414-1916146287.png\" width=\"400\" height=\"175\" />\n\n1.先分析**数值型**的特征,比如LotFrontage(通往房屋的street的距离)和LotArea(房子的建筑面积，包括车库院子啥的)\n\n分析:因为距离有的值为NA的缘故，对其值为NA的处理成了距离为0。\n\n按我们的思路，应该是距离越近价格越高(出行比较方便)，可是事实上并不是这样子，可能是私家车在外国家庭的普及率问题，和street的距离同房价之间的关系并不是很明显\n\n<img src=\"/images/517519-20170529162211336-398652915.png\" width=\"300\" height=\"216\" />\n\n接下来分析**房屋的建筑面积和房价**之间的关系，为了直观些，我把单位从平方英尺转换成了平方米。\n\n可以看到数据集中房屋的面积的峰值在900平方米左右，且房屋的建筑面积和房价是正相关的（废话）\n\n<img src=\"/images/517519-20170529164539477-1994704691.png\" width=\"600\" height=\"400\" />\n\n接下来再看看**1层和2层面积和房价**之间的关系\n\n可以看到，1楼面积和2层面积（如果有的话），同房价之间的关系也很线性，除少数离群的点之外\n\n&nbsp;<img src=\"/images/517519-20170529171410805-1848160120.png\" alt=\"\" width=\"418\" height=\"494\" />&nbsp;&nbsp;&nbsp; <img src=\"/images/517519-20170529171700774-1032569400.png\" alt=\"\" width=\"467\" height=\"492\" />\n","tags":["ML"]},{"title":"Spark学习笔记——泰坦尼克生还预测","url":"/Spark学习笔记——泰坦尼克生还预测.html","content":"```\npackage kaggle\n\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.{SQLContext, SparkSession}\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.classification.{LogisticRegressionWithLBFGS, LogisticRegressionWithSGD, NaiveBayes, SVMWithSGD}\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.stat.Statistics\n\n\n/**\n  * Created by mi on 17-5-23.\n  */\n\n\nobject Titanic {\n\n\n  def main(args: Array[String]) {\n\n    //    val sparkSession = SparkSession.builder.\n    //      master(\"local\")\n    //      .appName(\"spark session example\")\n    //      .getOrCreate()\n    //    val rawData = sparkSession.read.csv(\"/home/mi/下载/kaggle/Titanic/nohead-train.csv\")\n    //    val d = rawData.map{p => p.asInstanceOf[person]}\n    //    d.show()\n\n    val conf = new SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n    val sqlContext = new SQLContext(sc)\n\n    //屏蔽日志\n    Logger.getLogger(\"org.apache.spark\").setLevel(Level.WARN)\n    Logger.getLogger(\"org.eclipse.jetty.server\").setLevel(Level.OFF)\n\n    // 读取数据\n    val df = sqlContext.load(\"com.databricks.spark.csv\", Map(\"path\" -> \"/home/mi/下载/kaggle/Titanic/train.csv\", \"header\" -> \"true\"))\n\n    // 分析年龄数据\n    val ageAnalysis = df.rdd.filter(d => d(5) != null).map { d =>\n      val age = d(5).toString.toDouble\n      Vectors.dense(age)\n    }\n    val ageMean = Statistics.colStats(ageAnalysis).mean(0)\n    val ageMax = Statistics.colStats(ageAnalysis).max(0)\n    val ageMin = Statistics.colStats(ageAnalysis).min(0)\n    val ageDiff = ageMax - ageMin\n\n    // 分析船票价格数据\n    val fareAnalysis = df.rdd.filter(d => d(9) != null).map { d =>\n      val fare = d(9).toString.toDouble\n      Vectors.dense(fare)\n    }\n    val fareMean = Statistics.colStats(fareAnalysis).mean(0)\n    val fareMax = Statistics.colStats(fareAnalysis).max(0)\n    val fareMin = Statistics.colStats(fareAnalysis).min(0)\n    val fareDiff = fareMax - fareMin\n\n\n    // 数据预处理\n    val trainData = df.rdd.map { d =>\n      val label = d(1).toString.toInt\n      val sex = d(4) match {\n        case \"male\" => 0.0\n        case \"female\" => 1.0\n      }\n      val age = d(5) match {\n        case null => (ageMean - ageMin) / ageDiff\n        case _ => (d(5).toString().toDouble - ageMin) / ageDiff\n      }\n      val fare = d(9) match {\n        case null => (fareMean - fareMin) / fareDiff\n        case _ => (d(9).toString().toDouble - fareMin) / fareDiff\n      }\n\n      LabeledPoint(label, Vectors.dense(sex, age, fare))\n    }\n\n    // 切分数据集和测试集\n    val Array(trainingData, testData) = trainData.randomSplit(Array(0.8, 0.2))\n\n    // 训练数据\n    val numIterations = 8\n    val lrModel = new LogisticRegressionWithLBFGS().setNumClasses(2).run(trainingData)\n    //    val svmModel = SVMWithSGD.train(trainingData, numIterations)\n\n    val nbTotalCorrect = testData.map { point =>\n      if (lrModel.predict(point.features) == point.label) 1 else 0\n    }.sum\n    val nbAccuracy = nbTotalCorrect / testData.count\n\n    println(\"SVM模型正确率：\" + nbAccuracy)\n\n    // 预测\n    // 读取数据\n    val testdf = sqlContext.load(\"com.databricks.spark.csv\", Map(\"path\" -> \"/home/mi/下载/kaggle/Titanic/test.csv\", \"header\" -> \"true\"))\n\n    // 分析测试集年龄数据\n    val ageTestAnalysis = testdf.rdd.filter(d => d(4) != null).map { d =>\n      val age = d(4).toString.toDouble\n      Vectors.dense(age)\n    }\n    val ageTestMean = Statistics.colStats(ageTestAnalysis).mean(0)\n    val ageTestMax = Statistics.colStats(ageTestAnalysis).max(0)\n    val ageTestMin = Statistics.colStats(ageTestAnalysis).min(0)\n    val ageTestDiff = ageTestMax - ageTestMin\n\n    // 分析船票价格数据\n    val fareTestAnalysis = testdf.rdd.filter(d => d(8) != null).map { d =>\n      val fare = d(8).toString.toDouble\n      Vectors.dense(fare)\n    }\n    val fareTestMean = Statistics.colStats(fareTestAnalysis).mean(0)\n    val fareTestMax = Statistics.colStats(fareTestAnalysis).max(0)\n    val fareTestMin = Statistics.colStats(fareTestAnalysis).min(0)\n    val fareTestDiff = fareTestMax - fareTestMin\n\n    // 数据预处理\n    val data = testdf.rdd.map { d =>\n      val sex = d(3) match {\n        case \"male\" => 0.0\n        case \"female\" => 1.0\n      }\n      val age = d(4) match {\n        case null => (ageTestMean - ageTestMin) / ageTestDiff\n        case _ => (d(4).toString().toDouble - ageTestMin) / ageTestDiff\n      }\n      val fare = d(8) match {\n        case null => (fareTestMean - fareTestMin) / fareTestDiff\n        case _ => (d(8).toString().toDouble - fareTestMin) / fareTestDiff\n      }\n\n      Vectors.dense(sex, age, fare)\n    }\n\n    val predictions = lrModel.predict(data).map(p => p.toInt)\n    // 保存预测结果\n    predictions.coalesce(1).saveAsTextFile(\"file:///home/mi/下载/kaggle/Titanic/test_predict\")\n  }\n}\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["Spark"]},{"title":"Spark学习笔记——手写数字识别","url":"/Spark学习笔记——手写数字识别.html","content":"```\nimport org.apache.spark.ml.classification.RandomForestClassifier\nimport org.apache.spark.ml.regression.RandomForestRegressor\nimport org.apache.spark.mllib.classification.{LogisticRegressionWithLBFGS, NaiveBayes, SVMWithSGD}\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.optimization.L1Updater\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.tree.{DecisionTree, RandomForest}\nimport org.apache.spark.mllib.tree.configuration.Algo\nimport org.apache.spark.mllib.tree.impurity.Entropy\n\n/**\n  * Created by common on 17-5-17.\n  */\n\ncase class LabeledPic(\n                       label: Int,\n                       pic: List[Double] = List()\n                     )\n\nobject DigitRecognizer {\n\n  def main(args: Array[String]): Unit = {\n\n    val conf = new SparkConf().setAppName(\"DigitRecgonizer\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n    // 去掉第一行，sed 1d train.csv > train_noheader.csv\n    val trainFile = \"file:///media/common/工作/kaggle/DigitRecognizer/train_noheader.csv\"\n    val trainRawData = sc.textFile(trainFile)\n    // 通过逗号对数据进行分割，生成数组的rdd\n    val trainRecords = trainRawData.map(line => line.split(\",\"))\n\n    val trainData = trainRecords.map { r =>\n      val label = r(0).toInt\n      val features = r.slice(1, r.size).map(d => d.toDouble)\n      LabeledPoint(label, Vectors.dense(features))\n    }\n\n\n    //    // 使用贝叶斯模型\n    //    val nbModel = NaiveBayes.train(trainData)\n    //\n    //    val nbTotalCorrect = trainData.map { point =>\n    //      if (nbModel.predict(point.features) == point.label) 1 else 0\n    //    }.sum\n    //    val nbAccuracy = nbTotalCorrect / trainData.count\n    //\n    //    println(\"贝叶斯模型正确率：\" + nbAccuracy)\n    //\n    //    // 对测试数据进行预测\n    //    val testRawData = sc.textFile(\"file:///media/common/工作/kaggle/DigitRecognizer/test_noheader.csv\")\n    //    // 通过逗号对数据进行分割，生成数组的rdd\n    //    val testRecords = testRawData.map(line => line.split(\",\"))\n    //\n    //    val testData = testRecords.map { r =>\n    //      val features = r.map(d => d.toDouble)\n    //      Vectors.dense(features)\n    //    }\n    //    val predictions = nbModel.predict(testData).map(p => p.toInt)\n    //    // 保存预测结果\n    //    predictions.coalesce(1).saveAsTextFile(\"file:///media/common/工作/kaggle/DigitRecognizer/test_predict\")\n\n\n    //    // 使用线性回归模型\n    //    val lrModel = new LogisticRegressionWithLBFGS()\n    //      .setNumClasses(10)\n    //      .run(trainData)\n    //\n    //    val lrTotalCorrect = trainData.map { point =>\n    //      if (lrModel.predict(point.features) == point.label) 1 else 0\n    //    }.sum\n    //    val lrAccuracy = lrTotalCorrect / trainData.count\n    //\n    //    println(\"线性回归模型正确率：\" + lrAccuracy)\n    //\n    //    // 对测试数据进行预测\n    //    val testRawData = sc.textFile(\"file:///media/common/工作/kaggle/DigitRecognizer/test_noheader.csv\")\n    //    // 通过逗号对数据进行分割，生成数组的rdd\n    //    val testRecords = testRawData.map(line => line.split(\",\"))\n    //\n    //    val testData = testRecords.map { r =>\n    //      val features = r.map(d => d.toDouble)\n    //      Vectors.dense(features)\n    //    }\n    //    val predictions = lrModel.predict(testData).map(p => p.toInt)\n    //    // 保存预测结果\n    //    predictions.coalesce(1).saveAsTextFile(\"file:///media/common/工作/kaggle/DigitRecognizer/test_predict1\")\n\n\n    //    // 使用决策树模型\n    //    val maxTreeDepth = 10\n    //    val numClass = 10\n    //    val dtModel = DecisionTree.train(trainData, Algo.Classification, Entropy, maxTreeDepth, numClass)\n    //\n    //    val dtTotalCorrect = trainData.map { point =>\n    //      if (dtModel.predict(point.features) == point.label) 1 else 0\n    //    }.sum\n    //    val dtAccuracy = dtTotalCorrect / trainData.count\n    //\n    //    println(\"决策树模型正确率：\" + dtAccuracy)\n    //\n    //    // 对测试数据进行预测\n    //    val testRawData = sc.textFile(\"file:///media/common/工作/kaggle/DigitRecognizer/test_noheader.csv\")\n    //    // 通过逗号对数据进行分割，生成数组的rdd\n    //    val testRecords = testRawData.map(line => line.split(\",\"))\n    //\n    //    val testData = testRecords.map { r =>\n    //      val features = r.map(d => d.toDouble)\n    //      Vectors.dense(features)\n    //    }\n    //    val predictions = dtModel.predict(testData).map(p => p.toInt)\n    //    // 保存预测结果\n    //    predictions.coalesce(1).saveAsTextFile(\"file:///media/common/工作/kaggle/DigitRecognizer/test_predict2\")\n\n\n//    // 使用随机森林模型\n//    val numClasses = 30\n//    val categoricalFeaturesInfo = Map[Int, Int]()\n//    val numTrees = 50\n//    val featureSubsetStrategy = \"auto\"\n//    val impurity = \"gini\"\n//    val maxDepth = 10\n//    val maxBins = 32\n//    val rtModel = RandomForest.trainClassifier(trainData, numClasses, categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)\n//\n//    val rtTotalCorrect = trainData.map { point =>\n//      if (rtModel.predict(point.features) == point.label) 1 else 0\n//    }.sum\n//    val rtAccuracy = rtTotalCorrect / trainData.count\n//\n//    println(\"随机森林模型正确率：\" + rtAccuracy)\n//\n//    // 对测试数据进行预测\n//    val testRawData = sc.textFile(\"file:///media/common/工作/kaggle/DigitRecognizer/test_noheader.csv\")\n//    // 通过逗号对数据进行分割，生成数组的rdd\n//    val testRecords = testRawData.map(line => line.split(\",\"))\n//\n//    val testData = testRecords.map { r =>\n//      val features = r.map(d => d.toDouble)\n//      Vectors.dense(features)\n//    }\n//    val predictions = rtModel.predict(testData).map(p => p.toInt)\n//    // 保存预测结果\n//    predictions.coalesce(1).saveAsTextFile(\"file:///media/common/工作/kaggle/DigitRecognizer/test_predict\")\n\n\n  }\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["Spark"]},{"title":"Stanford Corenlp学习笔记——词性标注","url":"/Stanford Corenlp学习笔记——词性标注.html","content":"使用**Stanford Corenlp**对中文进行词性标注\n\n语言为Scala，使用的jar的版本是3.6.0，而且是手动添加jar包，使用sbt添加其他版本的时候出现了各种各样的问题\n\n添加的jar包有5个\n\n<img src=\"/images/517519-20170513230440769-1952922214.png\" alt=\"\" />\n\n代码\n\n```\nimport edu.stanford.nlp.pipeline.{Annotation, StanfordCoreNLP}\n\n/**\n  * Created by common on 17-5-13.\n  */\nobject NLPLearning {\n\n  def main(args: Array[String]): Unit = {\n    val props=\"StanfordCoreNLP-chinese.properties\"\n    val pipeline = new StanfordCoreNLP(props)\n\n    val annotation = new Annotation(\"这家酒店很好，我很喜欢。\")\n\n    pipeline.annotate(annotation)\n    pipeline.prettyPrint(annotation, System.out)\n\n  }\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n\n**关于词性标记**\n\n**动词，形容词（4种）：VA，VC，VE，VV**\n\n**1、谓词性形容词：VA**\n\n谓词性形容词大致上相当于英语中的形容词和中文语法中、文学作品里的静态动词。我们的谓词性形容词包括两类：\n\n第一类：没有宾语且能被&ldquo;很&rdquo;修饰的谓语。\n\n第二类：源自第一类的、通过重叠（如红彤彤）或者通过名词加形容词模式意味着&ldquo;像N一样A&rdquo;（如雪白）的谓语。这个类型的谓词性形容词没有宾语，但是有一些不能被&ldquo;很&rdquo;修饰，因为这些词的强调意思已经内嵌在词内了。\n\n注意：当集合（VA）中的一个词修饰名词但没有用&ldquo;的&rdquo;，那么它被标注为JJ（名作定）或是一个名词，而不是VA。当集合（VA）中的一个词有一个宾语，那么它被标注为VV，而不是VA。譬如，这&nbsp;项/M&nbsp;活动&nbsp;丰富/VV&nbsp;了/AS&nbsp;他&nbsp;的/DEG&nbsp;生活。\n\n**2、系动词：VC**\n\n&ldquo;是&rdquo;和&ldquo;为&rdquo;被标记为VC。如果&ldquo;非&rdquo;的意思是&ldquo;不是&rdquo;并且句子里没有其他动词时，&ldquo;非&rdquo;也被标注为VC。\n\n&ldquo;是&rdquo;有几种用法：\n\n&middot;连接两个名词短语或者主语：他&nbsp;是/VC&nbsp;学生。\n\n&middot;在分裂句中：他&nbsp;是/VC&nbsp;昨天&nbsp;来&nbsp;的/SP。\n\n&middot;为了强调：他&nbsp;是/VC&nbsp;喜欢&nbsp;看&nbsp;书。\n\n现在，在所有这些情况中，&ldquo;是&rdquo;被标注为VC。\n\n**3 、&ldquo;有&rdquo;作为主要动词：VE**\n\n只有当&ldquo;有，没{有}&rdquo;和&ldquo;无&rdquo;作为主要动词时（包括占有的&ldquo;有&rdquo;和表存在的&ldquo;有&rdquo;等等），被标注为VE。\n\n**4、其他动词：VV**\n\nVV包括其他动词，诸如情态动词，提升谓词（如&ldquo;可能&rdquo;），控制动词（如&ldquo;要&rdquo;、&ldquo;想&rdquo;），行为动词（如&ldquo;走&rdquo;），心理动词（如&ldquo;喜欢&rdquo;、&ldquo;了解&rdquo;、&ldquo;怨恨&rdquo;），等等。\n\n&nbsp;\n\n**名词（3种）：NR，NT，NN**\n\n**1、专有名词：NR**\n\n专有名词是名词的子集。一个专有名词可以是一个特定的人名，政治或地理上定义的地方（城市、国家、河流、山脉等），或者是一种组织（企业、政府或其他组织实体）。一个专有名词通常是独一无二，并且不能被Det+M所修饰的。\n\n&middot;以下名字是专有名词：\n\n地区/国家/村庄/城市，山脉/河流，报纸/杂志，&nbsp;组织/公司，学校/联盟/基金会，个人/家庭。\n\n&middot;以下名字不是专有名词：\n\n国籍（如中国人），种族（如白人），职称（如教授），疾病，职业，器官（如肺），乐器（如钢琴），游戏（如足球），花（如玫瑰），等等。\n\n**2、时间名词：NT**\n\n时间名词可以是介词的宾语，譬如在、从、到、等到。它们可以被问及，如&ldquo;这个时候&rdquo;，也可以被用以提问&ldquo;什么时候&rdquo;。它们也可以直接修饰VP（动词短语）或者S（主语）。像其他名词一样，时间名词可以是某些动词的论元。\n\n时间名词可以是时间的名称（如1990年、一月、汉朝）或是由&ldquo;PN+LC，N+LC，DT+N&rdquo;等结构组成。\n\n例子：一月、汉朝、当今、何时、今后\n\n**3、其他名词：NN**\n\n其他名词包括所有其他名词。其他名词NN，除了地方名词，一般不能修饰动词短语（有&ldquo;地/DEV&rdquo;或者没&ldquo;地/DEV&rdquo;）。\n\n&nbsp;\n\n**定位（1）：LC**\n\n**方位词：LC**\n\n很多名词单独使用时不能作为介词如&ldquo;在&rdquo;、&ldquo;到&rdquo;的论元，也不能直接修饰VP（动词短语）或者S（主语）。方位词的一个功能是连接前述的名词短语或者主语，从而使整个短语可以作为这些介词的论元或者来修饰动词短语或主语。\n\n一些方位词可以独立使用作为介词或动词的论元。一些方位词可以被&ldquo;最&rdquo;修饰。方位词不能被Det+M所修饰。\n\n方位词分为两类：\n\n&middot;方位词：这类方位词表示方向、位置等。它们来自名词。一些可以单独使用作为介词或动词的论元。一些可以被&ldquo;最&rdquo;修饰。它们不能被Det+M所修饰。\n\n&nbsp;&mdash;单音节方位词：如：前，后，里，外，内，北，东，边，侧，底，间，末，旁。\n\n&nbsp;&mdash;双音节方位词：它们由以下部分组成：\n\n&nbsp;&nbsp;&nbsp;*单音节方位词加上诸如&ldquo;以、之&rdquo;等的语素。\n\n&nbsp;&nbsp;&nbsp;&nbsp;例子：之间，以北。\n\n&nbsp;&nbsp;&nbsp;*两个单音节方位词。\n\n&nbsp;&nbsp;&nbsp;&nbsp;例子：前后，左右，上下，东北。\n\n&middot;其他：我们把以下情况标注为LC。\n\n&nbsp;&nbsp;.&nbsp;为止：到&nbsp;目前&nbsp;为止。\n\n&nbsp;&nbsp;.&nbsp;开始：从&nbsp;四月&nbsp;开始。\n\n&nbsp;&nbsp;.&nbsp;来：5年&nbsp;来。\n\n&nbsp;&nbsp;.&nbsp;以来：&nbsp;1998年&nbsp;以来。\n\n&nbsp;&nbsp;.&nbsp;起：&nbsp;一九九三年&nbsp;起。\n\n&nbsp;&nbsp;.&nbsp;在内：包括&nbsp;他&nbsp;在内。\n\n&nbsp;\n\n**代词（1种）：PN**\n\n代词的功能是作为名词短语的替代物或者表示事先详细说明的或者从上下文可知晓的被叫的人或事。它们一般不受Det+M或者形容词性短语修饰。\n\n代词包括人称代词（如我、你），当作为名词短语单独使用时为指示代词（如这、那），所有格代名词（如其）以及反身代词（如我自己、自己）。\n\n&nbsp;\n\n**限定词和数词（3种）：DT，CD，OD**\n\n**1、限定词：DT**\n\n限定词包括指示词（如这、那、该）和诸如&ldquo;每、各、前、后&rdquo;等词。限定词不包括基数词和序列词。\n\n参见限定词部分。\n\n**2、基数词：CD**\n\nCD包括基数词并随意与一些概数词连用，如&ldquo;来、多、好几&rdquo;和诸如&ldquo;好些、若干、半、许多、很多（如很多&nbsp;学生）&rdquo;等词。\n\n例子：1245，一百。 \n\n**3、序列词：OD**\n\n序列词被标注为OD。我们把第+CD看做一个词，并标注它为OD。\n\n例子：第一百。\n\n&nbsp;\n\n**度量词（1）：M**\n\n度量词跟在数字后形成Det+M结构修饰名词或动词，包括类词（如&ldquo;个&rdquo;），表示一群的度量词，如&ldquo;群&rdquo;，以及公里、升等度量词。\n\n一些度量词可以被有限的形容词（如一/CD**小/JJ**瓶/M水/NN），临时量词可以被名词和形容词修饰（如：一/CD**铁/NN**箱子/M书/NN）。\n\n&nbsp;\n\n**副词（1）：AD <br />**\n\n副词包括情态副词、频率副词、程度副词、连接副词等，大部分副词的功能是修饰动词短语或主语。\n\n如：仍然、很、最、大大、又、约\n\n&nbsp;\n\n**介词（1）：P**\n\n介词可以把名词短语或从句作为论元。\n\n注释：把和被不标注为P，详见2.11部分。\n\n如：从、对\n\n&nbsp;\n\n**连词（2）：CC，CS**\n\n**1、并列连接词：CC**\n\nCC的主要模式是：XP{，}，CC&nbsp;XP。\n\n如：与、和、或、或者、还是（or）\n\n**2、从属连词：CS**\n\n从属连词连接两个句子，一个句子从属于另一个，这样的连词标记为CS。CS模式是：CS&nbsp;S1，S2和S2&nbsp;CS，S1。\n\n如：如果/CS，&hellip;&hellip;就/AD&hellip;&hellip;\n\n&nbsp;\n\n**助词（8）：DEC，DEG，DER，DEV，SP，AS，ETC，SP，MSP**\n\n**1、&ldquo;****的****&rdquo;****作为补语标记/名词化标记：DEC（的，之）**\n\n如：吃的DEC\n\n模式是：S/VP&nbsp;DEC{NP}\n\n注：的还有其他标记\n\n&middot;DEC&nbsp;他的/DEG车\n\n&middot;SP&nbsp;&nbsp;&nbsp;他是/VC一定要来的/SP。\n\n&middot;AS&nbsp;&nbsp;&nbsp;他是/VC在这里下的/AS车。\n\n**2、&ldquo;的&rdquo;作为关联标记或所有格标记：DEG**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;模式：NP/PP/JJ/DT&nbsp;DEG{NP}。\n\n**3、 补语短语&nbsp;得：DER**\n\n&nbsp;&nbsp;&nbsp;在V-得-R和V-得结构中，&ldquo;得&rdquo;标记为DER。\n\n&nbsp;&nbsp;&nbsp;注：有些以&ldquo;得&rdquo;结尾的搭配不是V-得结构，如记得，获得是动词。 \n\n**4、方式&ldquo;地&rdquo;：DEV**\n\n当&ldquo;地&rdquo;出现在&ldquo;XP地VP&rdquo;，XP修饰VP。在一些古典文学中，&ldquo;的&rdquo;也用于这种情景，此时&ldquo;的&rdquo;也标注为DEV。\n\n**5、动态助词：AS**\n\n动态助词仅包括&ldquo;着，了，过，的&rdquo;。\n\n**6、句末助词：SP**\n\nSP经常出现在句末，如：他好吧[SP]？\n\n有时，句末助词用于表停顿，如：他吧[SP]，人很好。\n\n如：了，呢，吧，啊，呀，吗 \n\n**7、 ETC**\n\nETC用于标注等，等等。\n\n**8、其他助词：MSP**\n\n&ldquo;所，以，来，而&rdquo;，当它们出现在VP前时，标注为MSP。\n\n所：他所[MSP]需要的/DEC\n\n以或来：用&hellip;&hellip;以/MSP（或来）维持\n\n而：为&hellip;&hellip;而[MSP]奋斗\n\n&nbsp;\n\n**其他（8）：IJ，ON，PU，JJ，FW，LB，SB，BA**\n\n**1、感叹词：IJ**\n\n出现在句首位置的感叹词，如：啊。\n\n**2、拟声词：ON**\n\n①&nbsp;修饰&ldquo;ON地V&rdquo;中的VP：雨哗哗[ON]地[DEV]下了[AS]一夜\n\n②&nbsp;修饰&ldquo;ON中的N&rdquo;中的NP：砰[ON]的/DEG一声！\n\n③&nbsp;自行成句：砰砰[ON]！\n\n④&nbsp;一般不能被副词修饰，如：哗啦啦，咯吱。\n\n**&nbsp;3、****长&ldquo;被&rdquo;结构：LB**\n\n仅包括&ldquo;被，叫，给，为（口语中）&rdquo;，当它们出现在被字结构NP0+LB+NP1+VP中\n\n如：他被/LB&nbsp;我训了/AS&nbsp;一顿/M&nbsp;.\n\n注：当叫作为兼语动词时，&ldquo;叫&rdquo;标注为VV。\n\n如：他叫/VV你去。\n\n**4、短&ldquo;被&rdquo;结构：SB（仅包括口语中的&ldquo;被，给&rdquo;）**\n\n&nbsp;&nbsp;&nbsp;&nbsp;NP0+SB+VP，他被/SB&nbsp;训了/AS一顿/M。\n\n注：&ldquo;给&rdquo;有其他标记：LB，VV和P。\n\n如：你给/P他写封/M信。\n\n**5、把字结构：BA**\n\n仅包括&ldquo;把，将&rdquo;，当它们出现在把字结构中（NP0+BA+NP1+VP）。\n\n如：他把/BA你骗了/AS。\n\n注：&ldquo;将&rdquo;有其他标记：AD和VV，如：他将/VV了[AS]我的[DEG]军。\n\n**6、其他名词修饰语：JJ**\n\n包括三种类型：\n\n①区别词&nbsp;只修饰模式JJ+的+{N}或JJ+N中的名词，且一定要有&ldquo;的&rdquo;，它们不能被程度副词修饰。\n\n如：共同/JJ的/DEG目标/NN，她是[VC]女/JJ的/DEG。\n\n②带有连字符的复合词\n\n通常为双音节词&nbsp;JJ+N&nbsp;如留美/JJ学者/NN\n\n③形容词：新/JJ消息/NN\n\n模式：JJ+N\n\n注：当&ldquo;的/DEC&rdquo;在形容词和名词中间时，形容词标记为VA。\n\n**7、外来词：FW**\n\nFW仅被用于：当词性标注标记在上下文中不是很清楚时。外来词不包括外来词的翻译，不包括混合中文的词（如卡拉OK/NN，A型/NN），不包括词义和词性在文中都是清楚的词。\n\n**8、标点：PU**\n\n当标点是词的一部分时，不用标注为PU，如123,456/CD。\n\n&nbsp;\n\n<img src=\"/images/517519-20170513232228316-58060057.png\" alt=\"\" width=\"522\" height=\"218\" />\n\n<img src=\"/images/517519-20170513232238347-1362818555.png\" alt=\"\" width=\"501\" height=\"312\" />\n\n&nbsp;\n","tags":["nlp"]},{"title":"Solr学习笔记——导入JSON数据","url":"/Solr学习笔记——导入JSON数据.html","content":"1.导入JSON数据的方式有两种，一种是在web管理界面中导入，另一种是使用curl命令来导入\n\n```\ncurl http://localhost:8983/solr/baikeperson/update/json?commit=true --data-binary @/home/XXX/下载/person/test1.json -H 'Content-type:text/json; charset=utf-8'\n\n```\n\n2.导入的时候注意格式\n\n使用curl可以导入的格式\n\n```\n{\n　　\"add\": {\n　　　　\"overwrite\": true,\n　　　　\"doc\": {\n　　　　　　\"id\": 1,\n　　　　　　\"name\": \"Some book\",\n　　　　　　\"author\": [\"John\", \"Marry\"]\n　　　　}\n　　},\n　　\"add\": {\n　　　　\"overwrite\": true,\n　　　　\"boost\": 2.5,\n　　　　\"doc\": {\n　　　　　　\"id\": 2,\n　　　　　　\"name\": \"Important Book\",\n　　　　　　\"author\": [\"Harry\", \"Jane\"]\n　　　　}\n　　},\n　　\"add\": {\n　　　　\"overwrite\": true,\n　　　　\"doc\": {\n　　　　　　\"id\": 3,\n　　　　　　\"name\": \"Some other book\",\n　　　　　　\"author\": \"Marry\"\n　　　　}\n　　}\n}\n\n```\n\n<!--more-->\n&nbsp;\n\n在web界面中可以导入的格式\n\n```\n{\"title\":\"许宝江\",\"url\":\"7254863\",\"chineseName\":\"许宝江\",\"sex\":\"男\",\"occupation\":\" 滦县农业局局长\",\"nationality\":\"中国\"}\n\n```\n\n不可以导入的格式\n\n```\n{\"title\":\"鲍志成\",\"url\":\"2074015\",\"chineseName\":\"鲍志成\",\"occupation\":\"医师\",\"nationality\":\"中国\",\"birthDate\":\"1901年\",\"deathDate\":\"1973年\",\"graduatedFrom\":\"香港大学\"}\n{\"title\":\"许宝江\",\"url\":\"7254863\",\"chineseName\":\"许宝江\",\"sex\":\"男\",\"occupation\":\"&nbsp;滦县农业局局长\",\"nationality\":\"中国\"}\n\n```\n\n**格式转换的Scala代码**\n\n```\nimport java.io.{File, PrintWriter}\nimport scala.io.Source\n\n/**\n  * Created by common on 17-5-10.\n  */\nobject SplitJson {\n\n  def main(args: Array[String]): Unit = {\n\n    val inputPath = \"/home/common/下载/person/part-r-00000-47c2fce6-87cb-4a33-af2c-309a621b070f.json\"\n\n    val outputPath = \"/home/common/下载/person/split.json\"\n    val pw = new PrintWriter(new File(outputPath))\n\n    val s = Source.fromFile(new File(inputPath)).getLines()\n    pw.append(\"{\\\"add\\\": {\\\"overwrite\\\": true,\\\"doc\\\":\")\n    s.foreach { x =>\n      if (s.hasNext) pw.append(s\"$x\").write(\"},\\\"add\\\": {\\\"overwrite\\\": true,\\\"doc\\\": \\n\")\n      else pw.append(s\"$x\").write(\"}}\\n\")\n    }\n    pw.flush\n    pw.close\n  }\n\n\n}\n\n```\n\n&nbsp;**导入成功**将会返回，导入之后需要等上一段时间才会生成索引\n\n```\n{\"responseHeader\":{\"status\":0,\"QTime\":86}}\n\n```\n\n注意有可能还需要在下面的地址中加上\n\n```\n/var/solr/data/baikeperson/conf\n\n```\n\n```\n<requestHandler name=\"/update/json\" class=\"solr.JsonUpdateRequestHandler\" />\n\n```\n\n导入了28W条人物百科数据\n\n<img src=\"/images/517519-20170510221056191-1305284810.png\" alt=\"\" width=\"886\" height=\"302\" />\n\n&nbsp;查询一下岳云鹏\n\n<img src=\"/images/517519-20170510221212097-734059287.png\" alt=\"\" width=\"969\" height=\"527\" />\n","tags":["Solr"]},{"title":"Solr学习笔记——查询","url":"/Solr学习笔记——查询.html","content":"1.进入Solr管理界面[http://localhost:8983/solr/](http://localhost:8983/solr/)\n\n<img src=\"/images/517519-20170510115102269-1984174909.png\" alt=\"\" width=\"675\" height=\"598\" />\n\n可以看到Query中有若干的参数，其意义如下（参考：[http://www.jianshu.com/p/3c4cae5dee8d](http://www.jianshu.com/p/3c4cae5dee8d)）\n\n**Solr的查询语法：**\n\nSolr默认有三种查询解析器（Query Parser）：\n\n- Standard Query Parser\n- DisMax Query Parser\n- Extended DisMax Query Parser (eDisMax)\n\n第一种是标准的Parser，最后一种是最强大的，也是Sunspot默认使用的Parser。\n\n#### 支持的参数：\n\n- defType: 选择查询解析器类型，例如dismax, edismax\n- q：主查询参数（field_name:value）\n- sort：排序，例如score desc，price asc\n- start：起始的数据偏移offset，用于分页\n- raws：一次返回的数量，用于分页\n- fq：filter query 返回结果的过滤查询\n- fl：fields to list 返回的字段（*, score）\n- debug：返回调试信息，debug=timing，debug=results\n- timeAllowed：超时时间\n- wt：response writer返回的响应格式\n\n下面是DisMax Parser可以使用的：\n\n- qf：query fields，指定查询的字段，指定solr从哪些field中搜索，没有值的时候使用df\n- mm：最小匹配比例\n- pf：phrase fields\n- ps：phrase slop\n- qs：query phrase slop\n\n#### 特殊符号意义：\n\n- ?：te?t 单个字符匹配\n- *：tes* 多个字符匹配\n- ~：fuzzy searches（模糊匹配），roam~，roams/foam/foams\n- count:{1 TO 10}：range search 范围检索\n- ^：Boosting a Term（升级权重），jakarta^4 apache, \"酒店\"^4 \"宾馆\"\n- ^=：Constant Score with（指定分数），(description:blue OR color:blue)^=1.0 text:shoes\n\n#### 逻辑操作\n\n- AND 或者 &amp;&amp;\n- NOT 或者 !\n- OR 或者 !!\n- + 必须满足\n- - 剔除，比如 title: -安徽，返回的是title中不含有\"安徽\"的所有结果\n\n<!--more-->\n&nbsp;\n","tags":["Solr"]},{"title":"Ubuntu下安装Solr","url":"/Ubuntu下安装Solr.html","content":"1.在**清华开源软件镜像站**或者[http://www.us.apache.org/dist/](http://www.us.apache.org/dist/)\n\n下载Solr的安装包，我下载的是solr-6.5.1.tgz\n\n2.解压并移动到/usr/local目录下\n\n3.安装Solr需要安装Java环境，假设Java环境是安装好的\n\n4.解压solr-6.5.1.tgz目录中的install_solr_service.sh文件\n\n```\ntar zxvf solr-6.5.1.tgz solr-6.5.1/bin/install_solr_service.sh --strip-components=2\n\n```\n\n5.运行这个脚本，进行安装\n\n```\nsudo bash ./install_solr_service.sh solr-6.5.1.tgz\n\n```\n\n6.如果安装失败的话，使用下面的命令删除Solr，然后重新安装\n\n```\nsudo service solr stop\nsudo rm -r /var/solr\nsudo rm -r /opt/solr-6.5.1\nsudo rm -r /opt/solr\nsudo rm /etc/init.d/solr\nsudo deluser --remove-home solr\nsudo deluser --group solr\n\n```\n\n<!--more-->\n&nbsp;7.安装成功了的话，使用下面的命令来检查 Solr 服务的状态\n\n```\nservice solr status\n\n```\n\n<img src=\"/images/517519-20170510100636363-1364510586.png\" alt=\"\" />&nbsp;\n\n8.solr的管理界面\n\n```\nhttp://localhost:8983/solr/\n\n```\n\n9.使用 Solr 用户添加多个集合\n\n```\nsudo su - solr -c \"/opt/solr/bin/solr create -c myfirstcollection -n data_driven_schema_configs\"\n\nCopying configuration to new core instance directory:\n/var/solr/data/myfirstcollection\n\nCreating new core 'myfirstcollection' using command:\nhttp://localhost:8983/solr/admin/cores?action=CREATE&amp;name=myfirstcollection&amp;instanceDir=myfirstcollection\n\n{\n  \"responseHeader\":{\n    \"status\":0,\n    \"QTime\":2127},\n  \"core\":\"myfirstcollection\"}\n\n```\n\n10.对应的删除命令\n\n```\n/opt/solr-6.5.1/bin$ solr delete -c myfirstcollection\n\n```\n\n&nbsp;\n\n我们已经成功的为我们的第一个集合创建了新核心实例目录，并可以将数据添加到里面。要查看库中的**默认模式文件**，可以在这里找到：&nbsp;\n\n```\ncd /opt/solr/server/solr/configsets/data_driven_schema_configs/conf/\n\n```\n\n10.在web管理界面中点击Core Admin，可以看到我们刚刚创建的集合\n\n<img src=\"/images/517519-20170510103202847-1131365606.png\" alt=\"\" width=\"744\" height=\"296\" />\n\n11.除了使用命令行来添加集合的方式之外，还有可**以在web管理界面中添加集合**\n\n方法：Core Admin&mdash;&mdash;Core Selector&mdash;&mdash;Submit Document\n\n```\n    {\n    \"number\": 1,\n    \"Name\": \"George Washington\",\n    \"birth_year\": 1989,\n    \"Starting_Job\": 2002,\n    \"End_Job\": \"2009-04-30\",\n    \"Qualification\": \"Graduation\",\n    \"skills\": \"Linux and Virtualization\"\n    }\n\n```\n\n<img src=\"/images/517519-20170510103535551-1790657667.png\" alt=\"\" width=\"455\" height=\"544\" />\n","tags":["Linux","Solr"]},{"title":"Spark学习笔记——文本处理技术","url":"/Spark学习笔记——文本处理技术.html","content":"1.建立TF-IDF模型\n\n```\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.mllib.linalg.{SparseVector => SV}\nimport org.apache.spark.mllib.feature.HashingTF\nimport org.apache.spark.mllib.feature.IDF\n\n/**\n  * Created by common on 17-5-6.\n  */\nobject TFIDF {\n\n  def main(args: Array[String]): Unit = {\n    val conf = new SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n\n//    val path = \"hdfs://master:9000/user/common/20Newsgroups/20news-bydate-train/*\"\n    val path = \"file:///media/common/工作/kaggle/test/*\"\n    val rdd = sc.wholeTextFiles(path)\n\n    // 提取文本信息\n    val text = rdd.map { case (file, text) => text }\n    //    print(text.count())\n\n    val regex = \"\"\"[^0-9]*\"\"\".r\n\n    // 排除停用词\n    val stopwords = Set(\n      \"the\", \"a\", \"an\", \"of\", \"or\", \"in\", \"for\", \"by\", \"on\", \"but\", \"is\", \"not\",\n      \"with\", \"as\", \"was\", \"if\",\n      \"they\", \"are\", \"this\", \"and\", \"it\", \"have\", \"from\", \"at\", \"my\",\n      \"be\", \"that\", \"to\"\n    )\n\n    // 以使用正则表达切分原始文档来移除这些非单词字符\n    val nonWordSplit = text.flatMap(t =>\n      t.split(\"\"\"\\W+\"\"\").map(_.toLowerCase))\n\n    // 过滤掉数字和包含数字的单词\n    val filterNumbers = nonWordSplit.filter(token =>\n      regex.pattern.matcher(token).matches)\n\n    // 基于出现的频率，排除很少出现的单词，需要先计算一遍整个测试集\n    val tokenCounts = filterNumbers.map(t => (t, 1)).reduceByKey(_ + _)\n    val rareTokens = tokenCounts.filter { case (k, v) => v < 2 }.map {\n      case (k, v) => k\n    }.collect.toSet\n\n    // 每一个文档的预处理函数\n    def tokenize(line: String): Seq[String] = {\n      line.split(\"\"\"\\W+\"\"\")\n        .map(_.toLowerCase)\n        .filter(token => regex.pattern.matcher(token).matches)\n        .filterNot(token => stopwords.contains(token))\n        .filterNot(token => rareTokens.contains(token))\n        .filter(token => token.size >= 2) //删除只有一个字母的单词\n        .toSeq\n    }\n\n    // 每一篇文档经过预处理之后，每一个文档成为一个Seq[String]\n    val tokens = text.map(doc => tokenize(doc)).cache()\n\n    println(tokens.distinct.count)\n    // 第一篇文档第一部分分词之后的结果\n    println(tokens.first())\n    println(tokens.first().length)\n\n    // 生成2^18维的特征\n    val dim = math.pow(2, 18).toInt\n    val hashingTF = new HashingTF(dim)\n\n    // HashingTF 的 transform 函数把每个输入文档(即词项的序列)映射到一个MLlib的Vector对象\n    val tf = hashingTF.transform(tokens)\n    // tf的长度是文档的个数，对应的是文档和维度的矩阵\n    tf.cache\n\n    // 取得第一个文档的向量\n    val v = tf.first.asInstanceOf[SV]\n    println(v.size)\n    // v.value和v.indices的长度相等，value是词频，indices是词频非零的下标\n    println(v.values.size)\n    println(v.indices.size)\n    println(v.values.toSeq)\n    println(v.indices.take(10).toSeq)\n\n    // 对每个单词计算逆向文本频率\n    val idf = new IDF().fit(tf)\n    // 转换词频向量为TF-IDF向量\n    val tfidf = idf.transform(tf)\n    val v2 = tfidf.first.asInstanceOf[SV]\n    println(v2.values.size)\n    println(v2.values.take(10).toSeq)\n    println(v2.indices.take(10).toSeq)\n\n    // 计算整个文档的TF-IDF最小和最大权值\n    val minMaxVals = tfidf.map { v =>\n      val sv = v.asInstanceOf[SV]\n      (sv.values.min, sv.values.max)\n    }\n    val globalMinMax = minMaxVals.reduce { case ((min1, max1),\n    (min2, max2)) =>\n      (math.min(min1, min2), math.max(max1, max2))\n    }\n    println(globalMinMax)\n\n    // 比较几个单词的TF-IDF权值\n    val common = sc.parallelize(Seq(Seq(\"you\", \"do\", \"we\")))\n    val tfCommon = hashingTF.transform(common)\n    val tfidfCommon = idf.transform(tfCommon)\n    val commonVector = tfidfCommon.first.asInstanceOf[SV]\n    println(commonVector.values.toSeq)\n\n    val uncommon = sc.parallelize(Seq(Seq(\"telescope\", \"legislation\",\"investment\")))\n    val tfUncommon = hashingTF.transform(uncommon)\n    val tfidfUncommon = idf.transform(tfUncommon)\n    val uncommonVector = tfidfUncommon.first.asInstanceOf[SV]\n    println(uncommonVector.values.toSeq)\n\n  }\n\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["Spark"]},{"title":"Spark学习笔记——spark listener","url":"/Spark学习笔记——spark listener.html","content":"spark可以使用SparkListener API在spark运行的过程中监控spark任务当前的运行状态，参考：[SparkListener监听使用方式及自定义的事件处理动作](https://www.cnblogs.com/yyy-blog/p/10253830.html)\n\n编写 MySparkAppListener\n\n```\npackage com.bigdata.spark\n\nimport org.apache.spark.internal.Logging\nimport org.apache.spark.scheduler.{SparkListener, SparkListenerApplicationEnd, SparkListenerApplicationStart}\n\nclass MySparkAppListener extends SparkListener with Logging {\n\n  // 启动事件\n  override def onApplicationStart(applicationStart: SparkListenerApplicationStart): Unit = {\n    val appId = applicationStart.appId\n    logInfo(\"spark job start => \" + appId.get)\n  }\n\n  // 结束事件\n  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n    logInfo(\"spark job end => \" + applicationEnd.time)\n  }\n}\n\n```\n\n添加 spark.extraListeners 参数\n\n```\n    val sparkSession = SparkSession.builder()\n      .master(\"local\")\n      .config(\"spark.extraListeners\", \"com.bigdata.spark.MySparkAppListener\")\n      .appName(\"spark session example\")\n      .getOrCreate()\n\n```\n\n运行任务后就可以在日志当中看到对应的日志\n\n```\n21/12/27 23:13:46 INFO MySparkAppListener: spark job start => local-1640618026361\n\n21/12/27 23:13:48 INFO MySparkAppListener: spark job end => 1640618028287\n\n```\n\n还有其他的事件\n\n```\nabstract class SparkListener extends SparkListenerInterface {\n  //阶段完成时触发的事件\n  override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = { }\n\n  //阶段提交时触发的事件\n  override def onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted): Unit = { }\n\n  //任务启动时触发的事件\n  override def onTaskStart(taskStart: SparkListenerTaskStart): Unit = { }\n\n  //下载任务结果的事件\n  override def onTaskGettingResult(taskGettingResult: SparkListenerTaskGettingResult): Unit = { }\n\n  //任务结束的事件\n  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = { }\n\n  //job启动的事件\n  override def onJobStart(jobStart: SparkListenerJobStart): Unit = { }\n\n  //job结束的事件\n  override def onJobEnd(jobEnd: SparkListenerJobEnd): Unit = { }\n\n  //环境变量被更新的事件\n  override def onEnvironmentUpdate(environmentUpdate: SparkListenerEnvironmentUpdate): Unit = { }\n\n  //块管理被添加的事件\n  override def onBlockManagerAdded(blockManagerAdded: SparkListenerBlockManagerAdded): Unit = { }\n\n  override def onBlockManagerRemoved(\n      blockManagerRemoved: SparkListenerBlockManagerRemoved): Unit = { }\n\n  //取消rdd缓存的事件\n  override def onUnpersistRDD(unpersistRDD: SparkListenerUnpersistRDD): Unit = { }\n\n  //app启动的事件\n  override def onApplicationStart(applicationStart: SparkListenerApplicationStart): Unit = { }\n\n  //app结束的事件 [以下各事件也如同函数名所表达各个阶段被触发的事件不在一一标注]\n  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = { } \n\n  override def onExecutorMetricsUpdate(\n      executorMetricsUpdate: SparkListenerExecutorMetricsUpdate): Unit = { }\n\n  override def onExecutorAdded(executorAdded: SparkListenerExecutorAdded): Unit = { }\n\n  override def onExecutorRemoved(executorRemoved: SparkListenerExecutorRemoved): Unit = { }\n\n  override def onExecutorBlacklisted(\n      executorBlacklisted: SparkListenerExecutorBlacklisted): Unit = { }\n\n  override def onExecutorUnblacklisted(\n      executorUnblacklisted: SparkListenerExecutorUnblacklisted): Unit = { }\n\n  override def onNodeBlacklisted(\n      nodeBlacklisted: SparkListenerNodeBlacklisted): Unit = { }\n\n  override def onNodeUnblacklisted(\n      nodeUnblacklisted: SparkListenerNodeUnblacklisted): Unit = { }\n\n  override def onBlockUpdated(blockUpdated: SparkListenerBlockUpdated): Unit = { }\n\n  override def onOtherEvent(event: SparkListenerEvent): Unit = { }\n}\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["Spark"]},{"title":"Ubuntu安装redis缓存数据库","url":"/Ubuntu安装redis缓存数据库.html","content":"参考：[http://blog.csdn.net/xiangwanpeng/article/details/54586087](http://blog.csdn.net/xiangwanpeng/article/details/54586087)\n\n1.在下载目录下\n\n```\nsudo wget http://download.redis.io/releases/redis-3.2.6.tar.gz\n\n```\n\n2.解压，并复制到/usr/local目录下\n\n```\ntar -zxvf redis-3.2.6.tar.gz\nmv redis-3.2.6 /usr/local/\n\n```\n\n3.编译和安装\n\n```\ncd /redis\nsudo make\nsudo make install\n\n```\n\n4.在redis安装文件夹中修改文件redis.conf，使得redis在后台运行\n\n```\nvim redis.conf\n#修改daemonize yes\n\n```\n\n5.启动redis\n\n```\nredis-server redis.conf\nredis-cli -p 6379\n\n```\n\n<!--more-->\n&nbsp;\n\n6.安装redis desktop manager\n\n按照 http://blog.csdn.net/u013410747/article/details/51706964 的步骤\n\n安装deb安装包,需要依赖libicu52\n\n在`/etc/apt/目录`下,先拷贝一份`cp sources.list sources.list1`\n\n然后使用网易的源\n\n```\ndeb http://mirrors.163.com/ubuntu/ trusty main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ trusty main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiverse\n\n```\n\n&nbsp;更新源\n\n```\nsudo apt-get update\n\n```\n\n```\nsudo apt-get -f install libicu52\n\n```\n\n&nbsp;最后安装 redis-desktop-manager_0.8.3-120_amd64.deb\n\n&nbsp;\n\n给redis设置密码，需要在redis.conf的配置文件中添加\n\n```\nrequirepass xxxx\n\n```\n\n否则会报没有权限\n\n```\n(error) NOAUTH Authentication required\n\n```\n\n需要添加密码参数\n\n```\nredis-cli -p 6379 -a xxxx\n\n```\n\n&nbsp;\n\nredis默认的databases有16个，可以在配置文件中配置\n\n```\ndatabases 16\n\n```\n\n使用select来选择当前的库\n\n```\n127.0.0.1:6379> select 1\nOK\n127.0.0.1:6379[1]> select 0\nOK\n\n```\n\n可以使用 keys *来查看redis中的数据，不过由于redis是单线程的，所以禁止在生产环境中使用keys命令\n\n```\nkeys *\n\n```\n\n可以使用scan命令来替代keys命令\n\n```\nscan 0 match user* count 10\n\n```\n\n查看key的数据类型，不同的key所对应的命令不一样，否则会报&nbsp;(error) WRONGTYPE Operation against a key holding the wrong kind of value\n\n```\ntype XXXXX\n\n```\n\n&nbsp;zset的zcan命令，ZSCAN key cursor [MATCH pattern] [COUNT count]\n\n```\nhttps://www.runoob.com/redis/sorted-sets-zscan.html\n\n```\n\n比如\n\n```\nZSCAN XXXX 0\n\n```\n\n　　\n\n　　\n","tags":["Redis"]},{"title":"广告系统架构——CTR预估","url":"/广告系统架构——CTR预估.html","content":"<img src=\"/images/517519-20230826113217322-1261025150.png\" width=\"600\" height=\"318\" loading=\"lazy\" />\n\n参考：[ 广告点击率预估是怎么回事？ ](https://www.linkedin.com/pulse/%E5%B9%BF%E5%91%8A%E7%82%B9%E5%87%BB%E7%8E%87%E9%A2%84%E4%BC%B0%E6%98%AF%E6%80%8E%E4%B9%88%E5%9B%9E%E4%BA%8B-chen-ouyang/?originalSubdomain=cn)\n\n<img src=\"/images/517519-20230828121109981-590212648.png\" width=\"600\" height=\"313\" loading=\"lazy\" />\n\n参考：[推荐系统(10):样本拼接工程实践](https://zhuanlan.zhihu.com/p/594275446)\n\n<!--more-->\n&nbsp;\n","tags":["广告系统"]},{"title":"Spark学习笔记——构建分类模型","url":"/Spark学习笔记——构建分类模型.html","content":"Spark中常见的三种**分类模型**:线性模型、决策树和朴素贝叶斯模型。\n\n**线性模型**，简单而且相对容易扩展到非常大的数据集；线性模型又可以分成：**1.逻辑回归；2.线性支持向量机**\n\n**决策树**是一个强大的非线性技术,训练过程计算量大并且较难扩展(幸运的是,MLlib会替我们考虑扩展性的问题)，但是在很多情况下性能很好；\n\n**朴素贝叶斯模型**简单、易训练，并且具有高效和并行的优点(实际中，模型训练只需要遍历所有数据集一次)。当采用合适的特征工程，这些模型在很多应用中都能达到不错的性能。而且，朴素贝叶斯模型可以作为一个很好的模型测试基准，用于比较其他模型的性能。\n\n<!--more-->\n&nbsp;\n\n现在我们采用的数据集是**stumbleupon**，这个数据集是主要是一些**网页的分类数据**。\n\n内容样例：String = \"http://www.bloomberg.com/news/2010-12-23/ibm-predicts-holographic-calls-air-breathing-batteries-by-2015.html\"&nbsp;&nbsp; &nbsp;\"4042\"&nbsp;&nbsp; &nbsp;\"{\"\"title\"\":\"\"IBM Sees Holographic Calls Air Breathing Batteries ibm sees holographic calls, air-breathing batteries\"\",\"\"body\"\":\"\"A sign stands outside the International Business Machines Corp IBM Almaden Research Center campus in San Jose California Photographer Tony Avelar Bloomberg Buildings stand at the International Business Machines Corp IBM Almaden Research Center campus in the Santa Teresa Hills of San Jose California Photographer Tony Avelar Bloomberg By 2015 your mobile phone will project a 3 D image of anyone who calls and your laptop will be powered by kinetic energy At least that s what International Business Machines Corp sees in its crystal ...\n\n开始四列分别包含 **URL 、页面的 ID 、原始的文本内容**和分配给页面的**类别**。接下来 22 列包含**各种各样的数值或者类属特征**。最后一列为目标值， -1 为长久， 0 为短暂。\n\n&nbsp;\n\n```\nval rawData = sc.textFile(\"/user/common/stumbleupon/train_noheader.tsv\")\nval records = rawData.map(line => line.split(\"\\t\"))\nrecords.first()\n\n```\n\n由于数据格式的问题，我们做一些数据清理的工作，在处理过程中**把额外的( \" )去掉**。数据集中还有一些用 \"?\" 代替的缺失数据，本例中，我们直接**用 0 替换那些缺失数据** 。\n\n在清理和处理缺失数据后,我们**提取最后一列的标记变量**以及**第 5 列到第 25 列的特征矩阵**。将标签变量转换为 Int 值，特征向量转换为 Double 数组。\n\n最后，我们**将标签和和特征向量转换为 LabeledPoint 实例**，从而将特征向量存储到 MLlib 的 Vector 中。\n\n```\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nval data = records.map { r =>\n    val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n    val label = trimmed(r.size - 1).toInt\n    val features = trimmed.slice(4, r.size - 1).map(d => \n        if (d ==\"?\") 0.0 else d.toDouble)\n    LabeledPoint(label, Vectors.dense(features))\n}    \n\n```\n\n（**朴素贝叶斯**特殊的数据处理）在对数据集做进一步处理之前，我们发现数值数据中包含负的特征值。我们知道，朴素贝叶斯模型要求特征值非负，否则碰到负的特征值程序会抛出错误。因此，需要为朴素贝叶斯模型构建一份输入特征向量的数据，将负特征值设为 0\n\n```\nval nbData = records.map { r =>\n    val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n    val label = trimmed(r.size - 1).toInt\n    val features = trimmed.slice(4, r.size - 1).map(d => \n        if (d ==\"?\") 0.0 else d.toDouble).map(d => \n            if (d < 0) 0.0 else d)\n    LabeledPoint(label, Vectors.dense(features))\n}        \n\n```\n\n&nbsp;分别训练**逻辑回归、SVM、朴素贝叶斯模型和决策树**\n\n```\nimport org.apache.spark.mllib.classification.LogisticRegressionWithSGD\nimport org.apache.spark.mllib.classification.SVMWithSGD\nimport org.apache.spark.mllib.classification.NaiveBayes\nimport org.apache.spark.mllib.tree.DecisionTree\nimport org.apache.spark.mllib.tree.configuration.Algo\nimport org.apache.spark.mllib.tree.impurity.Entropy\nval numIterations = 10\nval maxTreeDepth = 5\n\n```\n\n训练**逻辑回归模型**\n\n```\nval lrModel = LogisticRegressionWithSGD.train(data, numIterations)\n\n```\n\n训练**SVM模型**\n\n```\nval svmModel = SVMWithSGD.train(data, numIterations)\n\n```\n\n训练**朴素贝叶斯模型 **\n\n```\nval nbModel = NaiveBayes.train(nbData)\n\n```\n\n&nbsp;训练**决策树模型**\n\n```\nval dtModel = DecisionTree.train(data, Algo.Classification, Entropy, maxTreeDepth)\n\n```\n\n&nbsp;\n\n**验证预测结果的正确性**，以逻辑回归为例子，说明预测的结果是错误的\n\n```\nval dataPoint = data.first\nval prediction = lrModel.predict(dataPoint.features)\n# 输出 prediction: Double = 1.0\nval trueLabel = dataPoint.label\n# 输出 trueLabel: Double = 0.0\n\n```\n\n&nbsp;\n\n**评估分类模型的性能**\n\n**1.逻辑回归模型**\n\n```\nval lrTotalCorrect = data.map { point =>\n    if (lrModel.predict(point.features) == point.label) 1 else 0\n}.sum\nval lrAccuracy = lrTotalCorrect / data.count\n\n```\n\n&nbsp;\n\n```\nlrAccuracy: Double = 0.5146720757268425\n\n```\n\n**2.SVM模型**\n\n```\nval svmTotalCorrect = data.map { point =>\n    if (svmModel.predict(point.features) == point.label) 1 else 0\n}.sum\nval svmAccuracy = svmTotalCorrect / data.count\n\n```\n\n&nbsp;\n\n```\nsvmAccuracy: Double = 0.5146720757268425\n\n```\n\n**3.贝叶斯模型**\n\n```\nval nbTotalCorrect = nbData.map { point =>\n    if (nbModel.predict(point.features) == point.label) 1 else 0\n}.sum\nval nbAccuracy = nbTotalCorrect / data.count\n\n```\n\n&nbsp;\n\n```\nnbAccuracy: Double = 0.5803921568627451\n\n```\n\n**4.决策树模型**\n\n```\nval dtTotalCorrect = data.map { point =>\n    val score = dtModel.predict(point.features)\n    val predicted = if (score > 0.5) 1 else 0\n    if (predicted == point.label) 1 else 0\n}.sum\nval dtAccuracy = dtTotalCorrect / data.count\n\n```\n\n&nbsp;\n\n```\ndtAccuracy: Double = 0.6482758620689655\n\n```\n\n&nbsp;\n\n**准确率和召回率**&nbsp;\n\n&nbsp;\n\n**改进模型性能以及参数调优**\n\n1.特征标准化\n\n研究特征是如何分布的，先将特征向量用 RowMatrix 类表示成 MLlib 中的分布矩阵。 RowMatrix 是一个由向量组成的 RDD ，其中每个向量是分布矩阵的一行。\n\nRowMatrix 类中有一些方便操作矩阵的方法，其中一个方法可以计算矩阵每列的**统计特性**：\n\n```\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\nval vectors = data.map(lp => lp.features)\nval matrix = new RowMatrix(vectors)\nval matrixSummary = matrix.computeColumnSummaryStatistics()　　\n#computeColumnSummaryStatistics 方法计算特征矩阵每列的不同统计数据,包括均值和方差,所有统计值按每列一项的方式存储在一个 Vector 中\n\n```\n\n&nbsp;\n\n```\nprintln(matrixSummary.mean)　　#输出矩阵每列的均值\nprintln(matrixSummary.min)　　#输出矩阵每列的最小值\nprintln(matrixSummary.max)    #输出矩阵每列的最大值\nprintln(matrixSummary.variance)    #输出矩阵每列的方差\nprintln(matrixSummary.numNonzeros)    #输出矩阵每列中非 0 项的数目\n\n```\n\n&nbsp;\n\n对特征矩阵进行**归一化**\n\n```\nimport org.apache.spark.mllib.feature.StandardScaler\nval scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)\n# 传入两个参数,一个表示是否从数据中减去均值,另一个表示是否应用标准差缩放\nval scaledData = data.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))\n\n```\n\n&nbsp;\n\n```\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n\nval lrModelScaled = LogisticRegressionWithSGD.train(scaledData, numIterations)\nval lrTotalCorrectScaled = scaledData.map { point =>\n    if (lrModelScaled.predict(point.features) == point.label) 1 else 0\n}.sum\nval lrAccuracyScaled = lrTotalCorrectScaled / numData\nval lrPredictionsVsTrue = scaledData.map { point =>\n    (lrModelScaled.predict(point.features), point.label)\n}\nval lrMetricsScaled = new BinaryClassificationMetrics(lrPredictionsVsTrue)\nval lrPr = lrMetricsScaled.areaUnderPR\nval lrRoc = lrMetricsScaled.areaUnderROC\n\nprintln(f\"${lrModelScaled.getClass.getSimpleName}\\nAccuracy:${lrAccuracyScaled * 100}%2.4f%%\\nArea under PR: ${lrPr *100.0}%2.4f%%\\nArea under ROC: ${lrRoc * 100.0}%2.4f%%\")\n\n```\n\n可以看出，**特征标准化提升了逻辑回归模型的准确率和AUC**\n\n```\nLogisticRegressionModel\nAccuracy:62.0419%\nArea under PR: 72.7254%\nArea under ROC: 61.9663%\n\n```\n\n&nbsp;\n","tags":["Spark"]},{"title":"Python爬虫——使用selenium和phantomjs爬取js动态加载的网页","url":"/Python爬虫——使用selenium和phantomjs爬取js动态加载的网页.html","content":"**1.安装selenium**\n\n```\npip install selenium\nCollecting selenium\n  Downloading selenium-3.4.1-py2.py3-none-any.whl (931kB)\n    100% |████████████████████████████████| 942kB 573kB/s \nInstalling collected packages: selenium\nSuccessfully installed selenium-3.4.1\n\n```\n\n**2.安装phantomjs**\n\n下载地址：[http://phantomjs.org/download.html](http://phantomjs.org/download.html)\n\n下载的版本是：phantomjs-2.1.1-linux-x86_64.tar.bz2\n\n解压下载好的文件，并把文件夹移动到/usr/local目录下\n\n```\nsudo mv phantomjs-2.1.1-linux-x86_64 /usr/local/phantomjs\n\n```\n\n<!--more-->\n&nbsp;在/etc/profile下添加，之后source /etc/profile\n\n```\nexport PHANTOMJS_HOME=/usr/local/phantomjs\nexport PATH=$PATH:$PHANTOMJS_HOME/bin\n\n```\n\n测试是否安装成功\n\n```\nphantomjs -v\n2.1.1\n\n```\n\n&nbsp;\n","tags":["Python"]},{"title":"Spark学习笔记——Spark上数据的获取、处理和准备","url":"/Spark学习笔记——Spark上数据的获取、处理和准备.html","content":"数据获得的方式多种多样，常用的**公开数据集**包括：\n\n**1.UCL机器学习知识库**:包括近300个不同大小和类型的数据集,可用于分类、回归、聚类和推荐系统任务。数据集列表位于:http://archive.ics.uci.edu/ml/\n\n**2.Amazon AWS公开数据集**:包含的通常是大型数据集,可通过Amazon S3访问。这些数据集包括人类基因组项目、Common Crawl网页语料库、维基百科数据和Google Books Ngrams。相关信息可参见:http://aws.amazon.com/publicdatasets/\n\n**3.Kaggle**:这里集合了Kaggle举行的各种机器学习竞赛所用的数据集。它们覆盖分类、回归、排名、推荐系统以及图像分析领域,可从Competitions区域下载: http://www.kaggle.com/competitions\n\n**4.KDnuggets**:这里包含一个详细的公开数据集列表,其中一些上面提到过的。该列表位于:http://www.kdnuggets.com/datasets/index.html\n\n<!--more-->\n&nbsp;\n\n下面采用的数据集是**MovieLens 100k数据集**，MovieLens 100k数据集包含表示多个用户对多部电影的10万次评级数据,也包含电影元数据和用户属性信息。\n\n<img src=\"/images/517519-20170429112121412-1968267194.png\" alt=\"\" width=\"565\" height=\"383\" />\n\n在目录下，可以**查看文件中的前5行的数据**\n\n```\nhead -5 u.user\n1|24|M|technician|85711\n2|53|F|other|94043\n3|23|M|writer|32067\n4|24|M|technician|43537\n5|33|F|other|15213\n\n```\n\n现在使用**Spark交互式终端**来对数据进行**可视化**的操作，以直观的了解数据的情况\n\n**1.安装ipython**\n\nIPython是针对Python的一个高级交互式壳程序,包含内置一系列实用功能的pylab,其中有NumPy和SciPy用于数值计算,以及matplotlib用于交互式绘图和可视化\n\n```\nsudo apt-get install ipython\n```\n\n**2.安装anaconda**，安装的文件是Anaconda2-4.3.1-Linux-x86_64.sh，可以在清华的开源软件镜像站下载\n\n一个预编译的科学Python套件\n\n```\nbash Anaconda2-4.3.1-Linux-x86_64.sh\n#一路回车\n#文件讲会安装在～目录下\n#在询问是否把anaconda的bin添加到用户的环境变量中，选择yes\nsource ~/.bashrc\n\n```\n\n在/etc/profile中添加\n\n```\nexport PATH=/home/lintong/anaconda2/bin:$PATH\n\n```\n\n3.启动Hadoop，在Hadoop的安装目录的sbin目录下启动start-all.sh\n\n4.启动pyspark，注意使用的spark的版本是2.1.0，所以参数和低版本的会有不同，下图是启动后的界面\n\n```\nPYSPARK_DRIVER_PYTHON=/usr/bin/ipython PYSPARK_DRIVER_PYTHON_OPTS=\"--pylab\" pyspark\n\n```\n\n&nbsp;<img src=\"/images/517519-20170429113500240-1013324541.png\" alt=\"\" />\n\n5.把训练数据集文件放在Hadoop文件系统中\n\n```\nhadoop fs -put /XXXtinput/ml-100k /user/XXX\n\n```\n\n6.代码\n\n```\nuser_data = sc.textFile(\"/user/common/ml-100k/u.user\")\nuser_data.first()\n\n```\n\n&nbsp;<img src=\"/images/517519-20170429113608615-1045705745.png\" alt=\"\" />\n\n```\nuser_fields = user_data.map(lambda line: line.split(\"|\"));\\\nages = user_fields.map(lambda x: int(x[1])).collect();\\\nhist(ages, bins=20, color='lightblue', normed=True);\\\nfig = matplotlib.pyplot.gcf();\\\nfig.set_size_inches(16, 10)\n\n```\n\n&nbsp;<img src=\"/images/517519-20170429113749897-53328367.png\" alt=\"\" width=\"540\" height=\"457\" />\n\n```\ncount_by_occupation = user_fields.map(lambda fields: (fields[3], 1)).reduceByKey(lambda x, y: x + y).collect()\n#或者 count_by_occupation = user_fields.map(lambda fields: fields[3]).countByValue()\nx_axis1 = np.array([c[0] for c in count_by_occupation])\ny_axis1 = np.array([c[1] for c in count_by_occupation])\n#升序排序\nx_axis = x_axis1[np.argsort(y_axis1)]\ny_axis = y_axis1[np.argsort(y_axis1)]\n\npos = np.arange(len(x_axis))\nwidth = 1.0\nax = plt.axes()\nax.set_xticks(pos + (width / 2))\nax.set_xticklabels(x_axis)\nplt.bar(pos, y_axis, width, color='lightblue')\nplt.xticks(rotation=30)\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(16, 10)\n\n```\n\n&nbsp;<img src=\"/images/517519-20170429164735397-190037609.png\" alt=\"\" width=\"720\" height=\"446\" />\n","tags":["Spark"]},{"title":"Spark学习笔记——构建基于Spark的推荐引擎","url":"/Spark学习笔记——构建基于Spark的推荐引擎.html","content":"**推荐模型**\n\n推荐模型的种类分为：\n\n**1.基于内容的过滤**：基于内容的过滤利用物品的内容或是属性信息以及某些相似度定义,来求出与该物品类似的物品。\n\n**2.协同过滤**：协同过滤是一种借助众包智慧的途径。它利用大量已有的用户偏好来估计用户对其未接触过的物品的喜好程度。其内在思想是相似度的定义。\n\n在基于用户的方法的中,如果两个用户表现出相似的偏好(即对相同物品的偏好大体相同),那就认为他们的兴趣类似。\n\n同样也可以借助基于物品的方法来做推荐。这种方法通常根据现有用户对物品的偏好或是评级情况,来计算物品之间的某种相似度。\n\n**3.矩阵分解：**\n\n3.1. 显式矩阵分解\n\n例如我们可以得到多个用户对多部电影的评级的数据，这样我们就可以得到一个**用户&mdash;电影评级的矩阵**。\n\n<img src=\"/images/517519-20170430154036428-1109250879.png\" alt=\"\" />\n\n我们所得到的这个矩阵是稀疏的，假设得到的**&ldquo;用户&mdash;物品&rdquo;矩阵的维度为U&times;I**，我们需要对其进行降维，然后得到一个**表示用户的U&times;k维矩阵**和一个**表示物品的k&times;I维矩阵**。\n\n**要计算给定用户对某个物品的预计评级 **：\n\n只需要从用户因子矩阵和物品因子矩阵分别选取相应的行(用户因子向量)与列(物品因子向量),然后计算两者的点积即可\n\n<img src=\"/images/517519-20170430155942584-698896923.png\" alt=\"\" width=\"400\" height=\"301\" />\n\n而**对于物品之间相似度的计算**,可以用最近邻模型中用到的相似度衡量方法。不同的是,这里可以直接利用物品因子向量,将相似度计算转换为对两物品因子向量之间相似度的计算\n\n<img src=\"/images/517519-20170430160031975-1941482174.png\" alt=\"\" width=\"391\" height=\"299\" />\n\n<!--more-->\n&nbsp;\n\n1.使用的是Spark-shell和Scala语言，同样需要把文件放在Hadoop文件系统中\n\n**启动Spark-shell**\n\n```\nval rawData = sc.textFile(\"/user/common/ml-100k/u.data\")\nrawData.first()\n\n```\n\n```\n#输出 res1: String = 196\t242\t3\t881250949\n#该数据由用户ID、影片ID、星级和时间戳等字段依次组成\n\n```\n\n&nbsp;提取出前三个字段\n\n```\nval rawRatings = rawData.map(_.split(\"\\t\").take(3))\n\n```\n\n&nbsp;使用ALS模型进行训练\n\n```\nimport org.apache.spark.mllib.recommendation.ALS\nval ratings = rawRatings.map { case Array(user, movie, rating) => Rating(user.toInt, movie.toInt, rating.toDouble) }\n```\n\n```\nscala> ratings.first()\n#输出 res24: org.apache.spark.mllib.recommendation.Rating = Rating(196,242,3.0)\n\n```\n\n&nbsp;<img src=\"/images/517519-20170430160705319-2123034484.png\" alt=\"\" width=\"626\" height=\"301\" />\n\n```\nval model = ALS.train(ratings, 50, 10, 0.01)\n\n```\n\n&nbsp;结果，每个用户和每部电影都会有对应的因子数组(分别含943个和1682个因子)\n\n```\nscala> model.userFeatures\nres26: org.apache.spark.rdd.RDD[(Int, Array[Double])] = users MapPartitionsRDD[454] at mapValues at ALS.scala:269\n\nscala> model.userFeatures.count\nres27: Long = 943\n\nscala> model.productFeatures.count\nres28: Long = 1682\n\n```\n\n**1. 从MovieLens 100k数据集生成电影推荐**\n\n&nbsp;MLlib的推荐模型基于矩阵分解,因此可用模型所求得的因子矩阵来**计算用户对物品的预计评级**。\n\n```\nscala> val predictedRating = model.predict(789, 123)\npredictedRating: Double = 2.4585387904925593\n\n```\n\n**predict 函数**同样可以以 (user, item) ID对类型的RDD对象为输入,这时它将为每一对都生成相应的预测得分。我们可以借助这个函数来同时为多个用户和物品进行预测。&nbsp;\n\n要**为某个用户生成前K个推荐物品** , 可借助 MatrixFactorizationModel 所提供的**recommendProducts 函数**来实现。该函数需两个输入参数: user 和 num 。其中 user 是用户ID,而 num 是要推荐的物品个数。\n\n```\nval userId = 789\nval K = 10\nval topKRecs = model.recommendProducts(userId, K)\nprintln(topKRecs.mkString(\"\\n\"))\n\nscala> println(topKRecs.mkString(\"\\n\"))\nRating(789,179,5.5976762921826575)\nRating(789,1022,5.431324881530808)\nRating(789,182,5.4204134645044615)\nRating(789,942,5.340233945688523)\nRating(789,188,5.254370757962667)\nRating(789,183,5.208694711418427)\nRating(789,428,5.1758907376213825)\nRating(789,198,5.150276931639322)\nRating(789,59,5.123932678029936)\nRating(789,715,5.103514906503706)\n\n```\n\n**&nbsp;2. 检验推荐内容**\n\n读入电影数据，导入为Map[Int, String] 类型,即从电影ID到标题的映射\n\n```\nval movies = sc.textFile(\"/user/common/ml-100k/u.item\")\nval titles = movies.map(line => line.split(\"\\\\|\").take(2)).map(array=> (array(0).toInt,array(1))).collectAsMap()\n\n```\n\n&nbsp;查看123对应的电影的名称\n\n```\nscala> titles(123)\nres30: String = Frighteners, The (1996)\n\n```\n\n&nbsp;\n\n```\nval moviesForUser = ratings.keyBy(_.user).lookup(789)\nprintln(moviesForUser.size)    #这个用户对33部电影做过评级\n#获取评级最高的前10部电影\nscala> moviesForUser.sortBy(-_.rating).take(10).map(rating => (titles(rating.product),rating.rating)).foreach(println)\n(Godfather, The (1972),5.0)\n(Trainspotting (1996),5.0)\n(Dead Man Walking (1995),5.0)\n(Star Wars (1977),5.0)\n(Swingers (1996),5.0)\n(Leaving Las Vegas (1995),5.0)\n(Bound (1996),5.0)\n(Fargo (1996),5.0)\n(Last Supper, The (1995),5.0)\n(Private Parts (1997),4.0)\n\nscala> topKRecs.map(rating => (titles(rating.product), rating.rating)).foreach(println)\n(Clockwork Orange, A (1971),5.5976762921826575)\n(Fast, Cheap &amp; Out of Control (1997),5.431324881530808)\n(GoodFellas (1990),5.4204134645044615)\n(What's Love Got to Do with It (1993),5.340233945688523)\n(Full Metal Jacket (1987),5.254370757962667)\n(Alien (1979),5.208694711418427)\n(Harold and Maude (1971),5.1758907376213825)\n(Nikita (La Femme Nikita) (1990),5.150276931639322)\n(Three Colors: Red (1994),5.123932678029936)\n(To Die For (1995),5.103514906503706)\n\n```\n\n&nbsp;\n","tags":["Spark"]},{"title":"Spark学习笔记——基于MLlib的机器学习","url":"/Spark学习笔记——基于MLlib的机器学习.html","content":"使用MLlib库中的机器学习算法对垃圾邮件进行分类\n\n分类的垃圾邮件的如图中分成4个文件夹，两个文件夹是训练集合，两个文件夹是测试集合\n\n<img src=\"/images/517519-20170428224913912-1476201945.png\" alt=\"\" />\n\nbuild.sbt文件\n\n```\nname := \"spark-first\"\n\nversion := \"1.0\"\n\nscalaVersion := \"2.11.8\"\n\nlibraryDependencies ++= Seq(\n  \"org.apache.spark\" % \"spark-core_2.11\" % \"2.1.0\",\n  \"org.apache.hadoop\" % \"hadoop-common\" % \"2.7.2\",\n  \"mysql\" % \"mysql-connector-java\" % \"5.1.31\",\n  \"org.apache.spark\" %% \"spark-sql\" % \"2.1.0\",\n  \"org.apache.spark\" %% \"spark-streaming\" % \"2.1.0\",\n  \"org.apache.spark\" % \"spark-mllib_2.11\" % \"2.1.0\"\n)\n\n```\n\n代码\n\n```\nimport org.apache.hadoop.io.{IntWritable, LongWritable, MapWritable, Text}\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkConf\nimport org.apache.spark._\nimport org.apache.hadoop.mapreduce.Job\nimport org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat\nimport org.apache.spark.sql.SQLContext\nimport java.util.Properties\n\nimport org.apache.spark.streaming.StreamingContext\nimport org.apache.spark.streaming.StreamingContext._\nimport org.apache.spark.streaming.dstream.DStream\nimport org.apache.spark.streaming.Duration\nimport org.apache.spark.streaming.Seconds\n\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.feature.HashingTF\nimport org.apache.spark.mllib.classification.LogisticRegressionWithSGD\n\n/**\n  * Created by common on 17-4-6.\n  */\nobject SparkRDD {\n\n  def main(args: Array[String]) {\n    val conf = new SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n\n    val spam = sc.textFile(\"input/email/spam\")\n    val normal = sc.textFile(\"input/email/ham\")\n\n    // 创建一个HashingTF实例来把邮件文本映射为包含10000个特征的向量\n    val tf = new HashingTF(numFeatures = 10000)\n    // 各邮件都被切分为单词，每个单词被映射为一个特征\n    val spamFeatures = spam.map(email => tf.transform(email.split(\" \")))\n    val normalFeatures = normal.map(email => tf.transform(email.split(\" \")))\n    // 创建LabeledPoint数据集分别存放阳性（垃圾邮件）和阴性（正常邮件）的例子\n    val positiveExamples = spamFeatures.map(features => LabeledPoint(1, features))\n    val negativeExamples = normalFeatures.map(features => LabeledPoint(0, features))\n    val trainingData = positiveExamples.union(negativeExamples)\n    trainingData.cache() // 因为逻辑回归是迭代算法，所以缓存训练数据RDD\n    // 使用SGD算法运行逻辑回归\n    val model = new LogisticRegressionWithSGD().run(trainingData)\n    // 以阳性（垃圾邮件）和阴性（正常邮件）的例子分别进行测试\n    val posTest = tf.transform(\n      \"Experience with BiggerPenis Today! Grow 3-inches more ...\".split(\" \"))\n    val negTest = tf.transform(\n      \"That is cold.  Is there going to be a retirement party? ...\".split(\" \"))\n    println(\"Prediction for positive test example: \" + model.predict(posTest))\n    println(\"Prediction for negative test example: \" + model.predict(negTest))\n\n  }\n}\n\n```\n\n<!--more-->\n&nbsp;\n\n结果\n\n<img src=\"/images/517519-20170428225310319-1623202871.png\" alt=\"\" />\n\n&nbsp;\n","tags":["Spark"]},{"title":"Spark学习笔记——Spark Streaming","url":"/Spark学习笔记——Spark Streaming.html","content":"许多应用需要**即时处理**收到的数据，例如用来实时追踪页面访问统计的应用、训练机器学习模型的应用， 还有自动检测异常的应用。**Spark Streaming** 是 Spark 为这些应用而设计的模型。它允许用户使用一套和批处理非常接近的 API 来编写流式计算应用，这样就可以大量重用批处理应用的技术甚至代码。 <br />**Spark Streaming** 使用离散化流（ discretized stream）作为抽象表示， 叫作 **DStream**。 DStream 是随时间推移而收到的数据的序列。在内部，每个时间区间收到的数据都作为 RDD 存在，而 **DStream 是由这些 RDD 所组成的序列**（因此得名&ldquo;离散化&rdquo;）。DStream 可以从各种输入源创建，比如 Flume、 Kafka 或者 HDFS。创建出来的 DStream 支持两种操作，一种是**转化操作（ transformation）** ，**会生成一个新的DStream**，另一种是**输出操作（ output operation）**，**可以把数据写入外部系统中**。DStream提供了许多与 RDD 所支持的操作相类似的操作支持，还增加了与时间相关的新操作，比如滑动窗口。 <br /><br />\n\n**build.sbt**\n\n```\nname := \"spark-first\"\n\nversion := \"1.0\"\n\nscalaVersion := \"2.11.8\"\n\nlibraryDependencies ++= Seq(\n  \"org.apache.spark\" % \"spark-core_2.11\" % \"2.1.0\",\n  \"org.apache.hadoop\" % \"hadoop-common\" % \"2.7.2\",\n  \"mysql\" % \"mysql-connector-java\" % \"5.1.31\",\n  \"org.apache.spark\" %% \"spark-sql\" % \"2.1.0\",\n  \"org.apache.spark\" %% \"spark-streaming\" % \"2.1.0\"\n\n)\n\n```\n\n<!--more-->\n&nbsp;\n\n**代码，使用Spark Streaming对端口发过来的数据进行词频统计**\n\n```\nimport org.apache.hadoop.io.{IntWritable, LongWritable, MapWritable, Text}\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkConf\nimport org.apache.spark._\n\nimport org.apache.spark.streaming.StreamingContext\nimport org.apache.spark.streaming.StreamingContext._\nimport org.apache.spark.streaming.dstream.DStream\nimport org.apache.spark.streaming.Duration\nimport org.apache.spark.streaming.Seconds\n\n/**\n  * Created by common on 17-4-6.\n  */\nobject SparkRDD {\n\n  def main(args: Array[String]) {\n    val conf = new SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n\n    // Spark streaming\n    // 从SparkConf创建StreamingContext并指定1秒钟的批处理大小\n    val ssc = new StreamingContext(conf, Seconds(1))\n    // 连接到本地机器7777端口上后，使用收到的数据创建DStream\n    val lines = ssc.socketTextStream(\"localhost\", 7777)\n    // 对每一行数据执行Split操作\n    val words = lines.flatMap(_.split(\" \"))\n    // 统计word的数量\n    val pairs = words.map(word => (word, 1))\n    val wordCounts = pairs.reduceByKey(_ + _)\n    // 输出结果\n    wordCounts.print()\n    ssc.start()        // 开始\n    ssc.awaitTermination() // 计算完毕退出\n  }\n}\n\n```\n\n&nbsp;\n\n**首先在终端中运行命令，向7777端口发送数据**\n\n```\nnc -l 7777\n\n```\n\n**nc命令参数**\n\n```\n-g<网关>：设置路由器跃程通信网关，最多设置8个； \n-G<指向器数目>：设置来源路由指向器，其数值为4的倍数； \n-h：在线帮助； \n-i<延迟秒数>：设置时间间隔，以便传送信息及扫描通信端口； \n-l：使用监听模式，监控传入的资料； \n-n：直接使用ip地址，而不通过域名服务器； \n-o<输出文件>：指定文件名称，把往来传输的数据以16进制字码倾倒成该文件保存； \n-p<通信端口>：设置本地主机使用的通信端口； \n-r：指定源端口和目的端口都进行随机的选择； \n-s<来源位址>：设置本地主机送出数据包的IP地址； \n-u：使用UDP传输协议； \n-v：显示指令执行过程； \n-w<超时秒数>：设置等待连线的时间； \n-z：使用0输入/输出模式，只在扫描通信端口时使用。\n\n```\n\n**然后运行Spark Streaming程序**\n\n**<img src=\"/images/517519-20170426110323584-1929070568.png\" alt=\"\" />**\n\n接着在终端中输入\n\n```\nHello World 1 #回车\nHello World 2\n\n```\n\n中断程序，在Spark Streaming输出看见\n\n<img src=\"/images/517519-20170426110936850-1686124331.png\" alt=\"\" />\n\n<img src=\"/images/517519-20170426110942194-973753305.png\" alt=\"\" />\n\n&nbsp;\n\n也可以自己**创建一个网络连接**，并随机生成一些数据病通过这个连接发送出去。\n\n注意下面的测试文件应该放在class目录下\n\n```\nimport java.io.PrintWriter\nimport java.net.ServerSocket\n\nimport scala.util.Random\n\n/**\n  * Created by common on 17-4-30.\n  */\nobject StreamingProducer {\n  def main(args: Array[String]) {\n    val random = new Random()\n    // 每秒最大活动数\n    val MaxEvents = 6\n    // 读取可能的名称\n    val namesResource =\n      this.getClass.getResourceAsStream(\"/name.csv\")\n    val names = scala.io.Source.fromInputStream(namesResource)\n      .getLines()\n      .toList\n      .head\n      .split(\",\")\n      .toSeq\n    // 生成一系列可能的产品\n    val products = Seq(\n      \"iPhone Cover\" -> 9.99,\n      \"Headphones\" -> 5.49,\n      \"Samsung Galaxy Cover\" -> 8.95,\n      \"iPad Cover\" -> 7.49\n    )\n\n    /** 生成随机产品活动 */\n    def generateProductEvents(n: Int) = {\n      (1 to n).map { i =>\n        val (product, price) = products(random.nextInt(products.size))\n        val user = random.shuffle(names).head\n        (user, product, price)\n      }\n    }\n\n    // 创建网络生成器\n    val listener = new ServerSocket(9999)\n    println(\"Listening on port: 9999\")\n    while (true) {\n      val socket = listener.accept()\n      new Thread() {\n        override def run = {\n          println(\"Got client connected from: \" +\n            socket.getInetAddress)\n          val out = new PrintWriter(socket.getOutputStream(), true)\n          while (true) {\n            Thread.sleep(1000)\n            val num = random.nextInt(MaxEvents)\n            // 用户和产品活动的随机组合\n            val productEvents = generateProductEvents(num)\n            // 向端口发送数据\n            productEvents.foreach { event =>\n              out.write(event.productIterator.mkString(\",\"))\n              out.write(\"\\n\")\n            }\n            // 清空缓存\n            out.flush()\n            println(s\"Created $num events...\")\n          }\n          socket.close()\n        }\n      }.start()\n    }\n  }\n}\n\n```\n\n**&nbsp;流处理程序**代码\n\n```\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\n\n/**\n  * Created by common on 17-4-30.\n  */\nobject SimpleStreamingApp {\n  def main(args: Array[String]) {\n    // 每隔10秒触发一次计算，使用了print算子\n    val ssc = new StreamingContext(\"local[2]\",\n      \"First Streaming App\", Seconds(10))\n    val stream = ssc.socketTextStream(\"localhost\", 9999)\n    // 简单地打印每一批的前几个元素\n    // 批量运行\n    stream.print()\n    ssc.start()\n    ssc.awaitTermination()\n  }\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20170430213655725-684092232.png\" alt=\"\" />\n\n&nbsp;\n","tags":["Spark"]},{"title":"Scala学习笔记——断言和单元测试","url":"/Scala学习笔记——断言和单元测试.html","content":"**1.断言**\n\n**assert(conditon)**将在条件不成立的时候，抛出assertionError\n\n**assert(conditon,explanation)**讲在条件不成立的时候，抛出explanation作为说明\n\n```\npackage com.scala.first\n\n/**\n  * Created by common on 17-4-19.\n  */\nobject Assert {\n\n  def main(args: Array[String]): Unit = {\n    val a = new Assert()\n    a.above1(0)\n\n  }\n\n}\n\nclass Assert {\n  val value = 1\n\n  def above(that: Int): Unit = {\n    val thatVal = that\n    val thisVal = this.value\n    //如果条件不满足，Exception in thread \"main\" java.lang.AssertionError: assertion failed\n    assert(thatVal == thisVal)\n  }\n\n  //另一种断言\n  //如果条件不满足，Exception in thread \"main\" java.lang.AssertionError: assertion failed\n  def above1(that: Int): Unit = {\n    {\n      val thatVal = that\n      val thisVal = this.value\n    } ensuring(that == this.value)\n\n  }\n}\n\n```\n\n<!--more-->\n&nbsp;\n\n**2.单元测试**\n\n**Scala中提供了多种单元测试的方法，比如ScalaTest**\n\n**ScalaTest提供了多种单元测试的方法，最简单的就是创建org.scalatest.suite类，并在这些类中定义测试方法**\n\n**如果cmd+shift+T的快捷键无效的话，在需要测试的类上右键，Go to Test，创建一个测试类**\n","tags":["Scala"]},{"title":"Scala学习笔记——样本类和模式匹配","url":"/Scala学习笔记——样本类和模式匹配.html","content":"**1.样本类**\n\n在申明的类前面加上一个**case修饰符**，带有这种修饰符的类被称为**样本类（case class）。**\n\n被申明为样本类的类的**特点**：1.会添加和类名一致的工厂方法；2.样本类参数列表中的所有参数隐式获得了val前缀，因此它被当做字段维护；3.编译器被这个样本类添加了toString、hashcode、equals方法的实现；4.支持了模式匹配\n\n<!--more-->\n&nbsp;\n\n**2.模式匹配**\n\n一个**模式匹配**包含了一系列**备选项**，每个都开始于**关键字case**。每个备选项都**包含了一个模式及一到多个表达**式，它们将在模式匹配过程中被计算。\n\n其中**箭头符号=>**隔开了模式和表达式。\n\n```\npackage com.scala.first\n\n/**\n  * Created by common on 17-4-19.\n  */\nobject CaseClass {\n\n  def main(args: Array[String]): Unit = {\n    println(cal(\"+\"))\n\n    prtList(List(0, 1))\n\n    println(prtType(\"abc\"))\n    println(prtType(Map(1 -> 1)))\n  }\n\n  def cal(exp: String): Int = {\n    val add = \"+\"\n    val result = 1\n    exp match {\n      case \"+\" => result + 2 //常量模式仅仅匹配自身\n      case \"-\" => result - 2\n      case \"*\" => result * 2\n      case \"/\" => result / 2\n      case add => result + 2 //变量模式可以匹配任意对象\n      case _ => result //通配模式匹配任意对象\n    }\n  }\n\n  //序列模式\n  def prtList(list: List[Int]) = list match {\n    case List(0, _, _) => println(\"this is a list\") //序列模式，可以匹配List或者Array\n    case List(1, _*) => println(\"this is a list, too\") //匹配一个不指定长度的序列\n    case _ => println(\"other\")\n  }\n\n  //元组模式\n  def prtTuple(tup: Any) = tup match {\n    case (0, _, _) => println(\"this is a tuple\") //元组模式\n    case _ => println(\"other\")\n  }\n\n  //类型模式，可以用在类型测试和类型转换\n  def prtType(x: Any) = x match {\n    case s: String => s.length\n    case m: Map[_, _] => m.size\n    case _ => 1\n  }\n\n}\n\n```\n\n&nbsp;\n","tags":["Scala"]},{"title":"Scala学习笔记——类型","url":"/Scala学习笔记——类型.html","content":"**1.Option类型**\n\nOption类型可以有**两种类型**，一种是**Some(x)**，一种是**None对象**\n\n比如Scala的Map的get方法发现了指定键，返回Some(x)，没有发现，返回None对象\n\n<!--more-->\n&nbsp;\n\n**2.列表**\n\nList类型中的所有元素都具有**相同的类型**。\n\n**空列表**的类型为**List[Nothing]**。对于任意类型T的List[T],List[Nothing]都是其子类。\n\n```\nval list = List[String](\"1\",\"2\",\"3\")\nval list1 = \"1\"::\"2\"::\"3\"::Nil        //所有的列表都是由两个基础构造块Nil和::构造出来的，Nil表示空列表\n\n```\n\n&nbsp;\n\n**列表的基本操作**\n\nhead　　返回列表的第一个元素，仅能作用在非空列表上\n\ntail　　返回除第一个之外所有元素组成的列表，仅能作用在非空列表上\n\nisEmpty　　判断是否为空\n\n&nbsp;\n\n**列表模式**\n\n列表可以使用模式匹配做拆分\n\n```\nval List(a,b,c) = list1   //两个List的长度相等，里面的元素一一对应\nval a1 :: b1 :: rest = list1  //如果不知道长度，还是用::来做匹配，rest会是一个List\n\n```\n\n&nbsp;\n\n**List类的一阶方法**\n\n1.连接列表使用 **:::**\n\n2.列表长度，**length**\n\n3.**head和tail**，获得第一个和除了第一个之外的列表\n\n**last和init**，获得最后一个和除了最后一个之外的列表\n\n4.翻转列表，**reverse**\n\n5.**drop和take**可以返回任意长度的前缀或后缀\n\n6.**splitAt**可以在指定位置拆分列表\n\n```\nval list = List[String](\"1\",\"2\",\"3\")\nprintln(list.splitAt(1))\n\n```\n\n7.**apply**实现了随机元素的选择，按下标选择元素\n\n**indices**方法可以返回指定列表的所有有效索引值组成的列表\n\n8.啮合列表：zip，zip操作可以把两个列表组成一个**对偶列表**\n\n如果想把列表元素和索引值啮合在一起，可以使用**zipWithIndex**\n\n9.显示列表：**toString**和**mkString**\n\n```\nval list = List[String](\"1\",\"2\",\"3\")\nprintln(list.toString())\n//输出 List(1, 2, 3)\n\n```\n\n```\nval list = List[String](\"1\",\"2\",\"3\")\nprintln(list.mkString(\"0\",\",\",\"4\"))   //前缀字符串，分隔符，后缀字符串\n//输出 01,2,34\n\n```\n\n&nbsp;\n\n```\nval buf = new StringBuilder\nprintln(\"abcde\".addString(buf,\"{\",\";\",\"}\"))\n#输出 {a;b;c;d;e}\n\n```\n\n10.转换列表：**toArray、copyToArray**\n\n```\nval arr = \"abcde\".toArray   //转换成Array\narr.foreach(print)\n\nval l = \"abcde\".toList   //转换成List\nl.foreach(print)\n\n```\n\n&nbsp;\n\n```\n    val arr1 = Array[String](\"1\")\n    val arr2 = Array[String](\"0\",\"0\",\"2\")\n    arr1.copyToArray(arr2,1)　　　　　//把第一个arr的元素复制到第二个arr的相应位置\n    arr2.foreach(print)　　　　　　　　//输出012\n\n```\n\n&nbsp;\n\n**List类的高阶方法**\n\n**1.列表间映射：map、flatMap和foreach**\n\n**map**把**函数f**应用在列表的**每个元素**之后生成的结果，组成一个**新的列表**，然后返回\n\n```\n    val words = List(\"Hello\",\"World\")\n    println(words.map(_.toList))\n    println(words.flatMap(_.toList))\n\n//List(List(H, e, l, l, o), List(W, o, r, l, d))\n//List(H, e, l, l, o, W, o, r, l, d)\n\n```\n\n**2.列表过滤：filter、partition、find、takeWhile、dropWhile和span**\n\n**filter过滤出符合条件的元素组成的列表**\n\n```\n    val words = List(\"Hello\",\"World\")\n    val filt = words.filter(_ == \"Hello\")\n    println(filt)　　　　　　//输出 List(Hello)\n\n```\n\n**partition返回列表对**\n\n```\n    val words = List(\"Hello\",\"World\")\n    val filt = words.partition(_ == \"Hello\")\n    println(filt)        //输出列表对 (List(Hello),List(World))\n\n```\n\n**find返回第一个满足条件的元素**\n\n```\n    val words = List(\"Hello\",\"World\")\n    val filt = words.find(_ == \"Hello\")\n    println(filt)            //输出 Some(Hello)\n\n```\n\n**takeWhile和dropWhile返回满足条件的最长前缀和最长后缀**\n\n**span返回takeWhile和dropWhile的组合**\n\n&nbsp;\n\n**列表的论断：**\n\n**forall，如果列表中的所有元素都满足条件，返回true**\n\n**exists，如果列表中有一个元素满足条件，返回true**\n\n```\n    val filt = words.forall(_ == \"Hello\")\n    println(filt)        //返回false\n\n```\n\n&nbsp;\n\n**折叠列表　/:和:\\**\n\n```\n    val result = (\"\" /: list) (_ + \" \" + _)   //初始值，加上每个值和间隔的结果\n    println(result)        //返回  [ 1 2 3]\n\n```\n\n&nbsp;\n\n```\n    val result = (list :\\ \"\") (_ + \" \" + _)   //初始值，加上每个值和间隔的结果\n    println(result)        //返回 [1 2 3 ]\n\n```\n\n&nbsp;\n\n**列表排序**\n\n```\n    val result = list.sortWith(_ > _)\n    println(result)\n\n```\n\n&nbsp;\n\n**List对象的方法**\n\n**List.apply**，通过元素创建列表\n\n```\nval list = List.apply(\"4\",\"5\",\"6\")\n\n```\n\n**List.range**，创建数值范围，生成 List[1,2,3,4,5]\n\n```\nval list = List.range(1,6)\n\n```\n\n**解除啮合列表，List.unzip**\n\n**连接列表 List.concat**\n\n```\n    val list = List[Char]('a','b','c')\n    val list1 = List[Char]('z','x','c')\n    val list2 = List.concat(list,list1)\n\n    println(list2)    //输出    List(a, b, c, z, x, c)\n\n```\n\n&nbsp;\n\n**3.集合类型**\n\n**集合类型包括：数组（Array）、列表（List）、集（Set）、映射（Map）**\n\n<img src=\"/images/517519-20170422115654259-1006225115.png\" alt=\"\" width=\"520\" height=\"190\" />\n\n```\n    val list = List(1,2)\n\n    val ite = list.iterator\n    while (ite.hasNext) {\n      println(ite.next())\n    }\n\n```\n\n**数组和列表**是不可变对象，操作的时候使用buf来构造\n\n**队列（Queue）**\n\n添加元素&mdash;&mdash;enqueue()\n\n从头部移除元素&mdash;&mdash;dequeue()\n\n返回添加了元素之后的队列&mdash;&mdash;append()\n\n```\nimport scala.collection.mutable.Queue<br />\n    val empty = new Queue[Int]\n    empty.enqueue(0)\n    empty.enqueue(1)\n    empty.enqueue(2)\n    empty.dequeue()\n    println(empty)\n\n```\n\n**&nbsp;栈**和队列差不多\n\n**集（Set）**\n\n**<img src=\"/images/517519-20170422163357977-1218192081.png\" alt=\"\" width=\"737\" height=\"498\" />**\n\n**映射（Map）**\n\n<img src=\"/images/517519-20170422163538431-1523270328.png\" alt=\"\" width=\"693\" height=\"161\" />\n\n<img src=\"/images/517519-20170422163551056-897267725.png\" alt=\"\" width=\"650\" height=\"556\" />\n\n**元组**，可以保存不同类型的对象\n\n```\n    val tuple = (1,\"2\")\n    println(tuple._1)\n    println(tuple._2)\n\n```\n\n&nbsp;\n","tags":["Scala"]},{"title":"Spark学习笔记——读写HBase","url":"/Spark学习笔记——读写HBase.html","content":"**1.首先在HBase中建立一张表，名字为student**\n\n**参考 [Hbase学习笔记&mdash;&mdash;基本CRUD操作](http://www.cnblogs.com/tonglin0325/p/6701666.html)**\n\n一个**cell的值,**取决于**Row,Column family,Column Qualifier和Timestamp**\n\n**HBase表结构**\n\n**<img src=\"/images/517519-20170418200643915-626033901.png\" alt=\"\" />**\n\n**2.往HBase中写入数据，写入的时候，需要写family和column**\n\n**build.sbt**\n\n```\nlibraryDependencies ++= Seq(\n  \"org.apache.spark\" %% \"spark-core\" % \"2.1.0\",\n  \"mysql\" % \"mysql-connector-java\" % \"5.1.31\",\n  \"org.apache.spark\" %% \"spark-sql\" % \"2.1.0\",\n  \"org.apache.hbase\" % \"hbase-common\" % \"1.3.0\",\n  \"org.apache.hbase\" % \"hbase-client\" % \"1.3.0\",\n  \"org.apache.hbase\" % \"hbase-server\" % \"1.3.0\",\n  \"org.apache.hbase\" % \"hbase\" % \"1.2.1\"\n)\n\n```\n\n**在hbaseshell中写数据的时候，写的是String，但是在idea中写代码的话，如果写的是int类型的，就会出现\\x00...的情况**\n\n```\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.sql._\nimport java.util.Properties\n\nimport com.google.common.collect.Lists\nimport org.apache.spark.sql.types.{ArrayType, StringType, StructField, StructType}\nimport org.apache.hadoop.hbase.HBaseConfiguration\nimport org.apache.hadoop.hbase.client.{Get, Put, Result, Scan}\nimport org.apache.hadoop.hbase.io.ImmutableBytesWritable\nimport org.apache.hadoop.hbase.mapred.TableOutputFormat\nimport org.apache.hadoop.hbase.mapreduce.TableInputFormat\nimport org.apache.hadoop.hbase.util.Bytes\nimport org.apache.hadoop.mapred.JobConf\n\n/**\n  * Created by mi on 17-4-11.\n  */\n\ncase class resultset(name: String,\n                     info: String,\n                     summary: String)\n\ncase class IntroItem(name: String, value: String)\n\n\ncase class BaikeLocation(name: String,\n                         url: String = \"\",\n                         info: Seq[IntroItem] = Seq(),\n                         summary: Option[String] = None)\n\ncase class MewBaikeLocation(name: String,\n                            url: String = \"\",\n                            info: Option[String] = None,\n                            summary: Option[String] = None)\n\n\nobject MysqlOpt {\n\n  def main(args: Array[String]): Unit = {\n\n    // 本地模式运行,便于测试\n    val conf = new SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n    // 创建 spark context\n    val sc = new SparkContext(conf)\n    val sqlContext = new SQLContext(sc)\n    import sqlContext.implicits._\n\n    //定义数据库和表信息\n    val url = \"jdbc:mysql://localhost:3306/baidubaike?useUnicode=true&amp;characterEncoding=UTF-8\"\n    val table = \"baike_pages\"\n\n    // 读取Hbase文件，在hbase的/usr/local/hbase/conf/hbase-site.xml中写的地址\n    val hbasePath = \"file:///usr/local/hbase/hbase-tmp\"\n\n    // 创建hbase configuration\n    val hBaseConf = HBaseConfiguration.create()\n    hBaseConf.set(TableInputFormat.INPUT_TABLE, \"student\")\n\n    // 初始化jobconf，TableOutputFormat必须是org.apache.hadoop.hbase.mapred包下的！\n    val jobConf = new JobConf(hBaseConf)\n    jobConf.setOutputFormat(classOf[TableOutputFormat])\n    jobConf.set(TableOutputFormat.OUTPUT_TABLE, \"student\")\n\n    val indataRDD = sc.makeRDD(Array(\"1,99,98\",\"2,97,96\",\"3,95,94\"))\n\n    val rdd = indataRDD.map(_.split(',')).map{arr=>{\n      /*一个Put对象就是一行记录，在构造方法中指定主键\n       * 所有插入的数据必须用org.apache.hadoop.hbase.util.Bytes.toBytes方法转换\n       * Put.add方法接收三个参数：列族，列名，数据\n       */\n      val put = new Put(Bytes.toBytes(arr(0)))\n      put.add(Bytes.toBytes(\"course\"),Bytes.toBytes(\"math\"),Bytes.toBytes(arr(1)))\n      put.add(Bytes.toBytes(\"course\"),Bytes.toBytes(\"english\"),Bytes.toBytes(arr(2)))\n      //转化成RDD[(ImmutableBytesWritable,Put)]类型才能调用saveAsHadoopDataset\n      (new ImmutableBytesWritable, put)\n    }}\n\n    rdd.saveAsHadoopDataset(jobConf)\n\n    sc.stop()\n\n  }\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n\n**<img src=\"/images/517519-20170418200812134-1999506675.png\" alt=\"\" />**\n\n&nbsp;\n\n&nbsp;\n\n**3.从Hbase中读取数据**\n\n```\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.sql._\nimport java.util.Properties\n\nimport com.google.common.collect.Lists\nimport org.apache.spark.sql.types.{ArrayType, StringType, StructField, StructType}\nimport org.apache.hadoop.hbase.HBaseConfiguration\nimport org.apache.hadoop.hbase.client.{Get, Put, Result, Scan}\nimport org.apache.hadoop.hbase.io.ImmutableBytesWritable\nimport org.apache.hadoop.hbase.mapred.TableOutputFormat\nimport org.apache.hadoop.hbase.mapreduce.TableInputFormat\nimport org.apache.hadoop.hbase.util.Bytes\nimport org.apache.hadoop.mapred.JobConf\n\n/**\n  * Created by mi on 17-4-11.\n  */\n\ncase class resultset(name: String,\n                     info: String,\n                     summary: String)\n\ncase class IntroItem(name: String, value: String)\n\n\ncase class BaikeLocation(name: String,\n                         url: String = \"\",\n                         info: Seq[IntroItem] = Seq(),\n                         summary: Option[String] = None)\n\ncase class MewBaikeLocation(name: String,\n                            url: String = \"\",\n                            info: Option[String] = None,\n                            summary: Option[String] = None)\n\n\nobject MysqlOpt {\n\n  def main(args: Array[String]): Unit = {\n\n    // 本地模式运行,便于测试\n    val conf = new SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n    // 创建 spark context\n    val sc = new SparkContext(conf)\n    val sqlContext = new SQLContext(sc)\n    import sqlContext.implicits._\n\n    //定义数据库和表信息\n    val url = \"jdbc:mysql://localhost:3306/baidubaike?useUnicode=true&amp;characterEncoding=UTF-8\"\n    val table = \"baike_pages\"\n\n    // 读取Hbase文件，在hbase的/usr/local/hbase/conf/hbase-site.xml中写的地址\n    val hbasePath = \"file:///usr/local/hbase/hbase-tmp\"\n\n    // 创建hbase configuration\n    val hBaseConf = HBaseConfiguration.create()\n    hBaseConf.set(TableInputFormat.INPUT_TABLE, \"student\")\n\n    // 从数据源获取数据并转化成rdd\n    val hBaseRDD = sc.newAPIHadoopRDD(hBaseConf, classOf[TableInputFormat], classOf[ImmutableBytesWritable], classOf[Result])\n\n    println(hBaseRDD.count())\n\n    // 将数据映射为表  也就是将 RDD转化为 dataframe schema\n    hBaseRDD.foreach{case (_,result) =>{\n      //获取行键\n      val key = Bytes.toString(result.getRow)\n      //通过列族和列名获取列\n      val math = Bytes.toString(result.getValue(\"course\".getBytes,\"math\".getBytes))\n      println(\"Row key:\"+key+\" Math:\"+math)\n    }}\n\n    sc.stop()\n\n  }\n\n\n}\n\n```\n\n&nbsp;输出\n\n```\nRow key:   \u0001 Math:99\nRow key:   \u0002 Math:97\nRow key:   \u0003 Math:95\nRow key:1 Math:99\nRow key:1000 Math:99\nRow key:2 Math:97\nRow key:3 Math:95\n\n```\n\n&nbsp;\n","tags":["HBase","Spark"]},{"title":"Spark学习笔记——读写HDFS","url":"/Spark学习笔记——读写HDFS.html","content":"**使用Spark读写HDFS中的parquet文件**\n\n**文件夹中的parquet文件**\n\n<img src=\"/images/517519-20170418143215462-2130167231.png\" alt=\"\" />\n\n**build.sbt文件**\n\n```\nname := \"spark-hbase\"\n\nversion := \"1.0\"\n\nscalaVersion := \"2.11.8\"\n\nlibraryDependencies ++= Seq(\n  \"org.apache.spark\" %% \"spark-core\" % \"2.1.0\",\n  \"mysql\" % \"mysql-connector-java\" % \"5.1.31\",\n  \"org.apache.spark\" %% \"spark-sql\" % \"2.1.0\",\n  \"org.apache.hbase\" % \"hbase-common\" % \"1.3.0\",\n  \"org.apache.hbase\" % \"hbase-client\" % \"1.3.0\",\n  \"org.apache.hbase\" % \"hbase-server\" % \"1.3.0\",\n  \"org.apache.hbase\" % \"hbase\" % \"1.2.1\"\n)\n\n```\n\n<!--more-->\n&nbsp;\n\n**Scala实现方法**\n\n```\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.sql._\nimport java.util.Properties\n\nimport com.google.common.collect.Lists\nimport org.apache.spark.sql.types.{ArrayType, StringType, StructField, StructType}\nimport org.apache.hadoop.hbase.HBaseConfiguration\nimport org.apache.hadoop.hbase.client.{Result, Scan}\nimport org.apache.hadoop.hbase.io.ImmutableBytesWritable\nimport org.apache.hadoop.hbase.mapreduce.TableInputFormat\n\n\n/**\n  * Created by mi on 17-4-11.\n  */\n\ncase class resultset(name: String,\n                     info: String,\n                     summary: String)\n\ncase class IntroItem(name: String, value: String)\n\n\ncase class BaikeLocation(name: String,\n                         url: String = \"\",\n                         info: Seq[IntroItem] = Seq(),\n                         summary: Option[String] = None)\n\ncase class MewBaikeLocation(name: String,\n                         url: String = \"\",\n                         info: Option[String] = None,\n                         summary: Option[String] = None)\n\n\nobject MysqlOpt {\n\n  def main(args: Array[String]): Unit = {\n    val conf = new SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n    val sqlContext = new SQLContext(sc)\n    import sqlContext.implicits._\n\n    //定义数据库和表信息\n    val url = \"jdbc:mysql://localhost:3306/baidubaike?useUnicode=true&amp;characterEncoding=UTF-8\"\n    val table = \"baike_pages\"\n\n    //读取parquetFile，并写入Mysql\n    val sparkSession = SparkSession.builder()\n      .master(\"local\")\n      .appName(\"spark session example\")\n      .getOrCreate()\n    val parquetDF = sparkSession.read.parquet(\"/home/mi/coding/coding/baikeshow_data/baikeshow\")\n//    parquetDF.collect().take(20).foreach(println)\n    //parquetDF.show()\n\n    //BaikeLocation是读取的parquet文件中的case class\n    val ds = parquetDF.as[BaikeLocation].map { line =>\n      //把info转换为新的case class中的类型String\n      val info = line.info.map(item => item.name + \":\" + item.value).mkString(\",\")\n      //注意需要把字段放在一个case class中，不然会丢失列信息\n      MewBaikeLocation(name = line.name, url = line.url, info = Some(info), summary = line.summary)\n    }.cache()\n\n    ds.show()\n//    ds.take(2).foreach(println)\n\n    //写入Mysql\n    //    val prop = new Properties()\n    //    prop.setProperty(\"user\", \"root\")\n    //    prop.setProperty(\"password\", \"123456\")\n    //    ds.write.mode(SaveMode.Append).jdbc(url, \"baike_location\", prop)\n\n    //写入parquetFile\n    ds.repartition(10).write.parquet(\"/home/mi/coding/coding/baikeshow_data/baikeshow1\")\n\n  }\n\n}\n\n```\n\n&nbsp;\n\n**df.show打印出来的信息，如果没放在一个case class中的话，name,url,info,summary这列信息会变成1,2,3,4<br />**\n\n**<img src=\"/images/517519-20170418145512024-1514596659.png\" alt=\"\" />**\n\n**使用spark-shell查看写回去的parquet文件的信息**\n\n```\n#进入spark-shell\nimport org.apache.spark.sql.SQLContext\nval sqlContext = new SQLContext(sc)\nval path = \"file:///home/mi/coding/coding/baikeshow_data/baikeshow1\"\nval df = sqlContext.parquetFile(path)\ndf.show\ndf.count\n\n```\n\n&nbsp;<img src=\"/images/517519-20170418152010977-971443432.png\" alt=\"\" />\n\n**<img src=\"/images/517519-20170418152040399-1400639693.png\" alt=\"\" />**\n\n如果**只想显示某一列**的话，可以这么做\n\n```\ndf.select(\"title\").take(100).foreach(println)　　//只显示title这一列的信息\n\n```\n\n&nbsp;\n","tags":["Spark"]},{"title":"Scala学习笔记——简化代码、柯里化、继承、特质","url":"/Scala学习笔记——简化代码、柯里化、继承、特质.html","content":"**1.简化代码**\n\n```\npackage com.scala.first\n\nimport java.io.File\nimport javax.management.Query\n\n/**\n  * Created by common on 17-4-5.\n  */\nobject FileMatcher {\n\n  def main(args: Array[String]) {\n\n    for (file <- filesHere)\n      println(file)\n\n    println()\n\n\n    for (file <- filesMatching(\"src\", _.endsWith(_)))\n      println(file)\n\n    for (file <- filesEnding(\"src\"))\n      println(file)\n\n  }\n\n  private def filesHere = (new File(\".\")).listFiles\n\n  //matcher是传入一个函数，返回boolean值，比如_.endsWith(_)\n  private def filesMatching(query: String, matcher: (String, String) => Boolean) = {\n    for (file <- filesHere; if matcher(file.getName, query)) yield file\n  }\n\n  //上面的函数不够简洁，下面是更加简洁的定义\n  private def filesMatch(matcher: String => Boolean) = {\n    for (file <- filesHere; if matcher(file.getName)) yield file\n  }\n\n  //然后可以定义使用不同matcher()的方法\n  def filesEnding(query: String) = {\n    filesMatch(_.endsWith(query))\n  }\n\n  //使用exists来简化代码\n  def containsOdd(nums: List[Int]): Boolean = {\n    nums.exists(_ % 2 == 1)\n  }\n\n  def containsNeg(nums: List[Int]): Boolean = {\n    nums.exists(_ < 0)\n  }\n\n}\n\n```\n\n<!--more-->\n&nbsp;输出是\n\n```\n./.idea\n./build.sbt\n./target\n./input\n./project\n./src\n\n./src\n./src\n\n```\n\n&nbsp;\n\n**2.柯里化**\n\n```\n//柯里化，可以看成是两个函数,y是第一个函数sum(x: Int)的参数\n  def sum(x: Int)(y: Int) = {\n    x + y\n  }\n\n  //println(sum2(2))的输出是4\n  def sum2 = sum(2)_\n\n```\n\n&nbsp;\n\n**3.继承**\n\n```\npackage com.scala.first\n\n/**\n  * Created by common on 17-4-16.\n  */\nobject AbstractClass {\n\n  def main(args: Array[String]): Unit = {\n    val ae = new ArrayElement(Array(\"hello\", \"world\"))\n    println(ae.width)\n\n\n    val list1 = List(1,2,3,4)\n    val list2 = List(1,2,3,4,5)\n    //Scala中同时遍历两个list，如果长度不一样会截去\n    for((line1,line2) <- list1 zip list2){\n      println(line1+line2)\n    }\n  }\n\n}\n\n//定义一个抽象类\nabstract class Element {\n\n  //定义无参方法,抽象成员\n  def contents: Array[String]\n\n  def height: Int = contents.length\n\n  def width: Int = if (height == 0) 0 else contents(0).length\n\n}\n\n//扩展类，继承了上面的抽象类，需要实现抽象类中的方法\nclass ArrayElement(content: Array[String]) extends Element {\n  def contents: Array[String] = content\n}\n\n//更加简洁的写法，contents和Element中的contents保持一致\nclass ArrayElement2(val contents: Array[String]) extends Element\n\n//另一个例子\nclass cat {\n  //确保一个成员不被子类重写，需要把其定义成final\n//  final val dangerous = false\n  val dangerous = false\n}\n\nclass tiger(override val dangerous: Boolean, private var age: Int) extends cat\n\n```\n\n&nbsp;\n\n**4.特质**\n\n```\npackage com.scala.first\n\n/**\n  * Created by common on 17-4-17.\n  */\nobject Trait {\n\n  def main(args: Array[String]): Unit = {\n    val person = new Person\n\n    //变量an可以初始化为任何混入了特质的类的对象，person对象包含了Animal特质\n    val an: Animal = person\n    println(an.toString)\n\n    val p = new PP()\n    //输出People特质中的内容\n    p.say()\n\n  }\n\n}\n\ntrait Animal {\n\n  def run(): Unit = {\n    println(\"Animal can run\")\n  }\n}\n\n//具有特质Animal\nclass Person extends Animal {\n  override def toString = \"can say\"\n}\n\n//具有特质Animal\nclass Tiger extends Animal {\n  override def toString = \"can run fast\"\n}\n\n//一个类只能继承一个父类。但是能混入多个特质\n//特征的作用：\n//1.把胖接口转换成瘦接口\n//2.为类提供可堆叠的改变\nclass Live\n\ntrait HasLeg\n\n//注意不能给特质中传递任何的参数\nclass PersonTiger extends Live with Animal with HasLeg {\n  println(\"混入了多个特质\")\n}\n\nclass P {\n  def say(): Unit = {\n    println(\"I am a people\")\n  }\n}\n\n//特质在抽象方法中的动态绑定\ntrait People extends P {\n  abstract override def say(): Unit = {\n    println(\"I am a trait people\")\n  }\n}\n\nclass PP extends People{\n    //输出People特质中的内容\n}\n\n```\n\n&nbsp;\n","tags":["Scala"]},{"title":"Spark学习笔记——读写MySQL","url":"/Spark学习笔记——读写MySQL.html","content":"**1.使用Spark读取MySQL中某个表中的信息**\n\nbuild.sbt文件\n\n```\nname := \"spark-hbase\"\n\nversion := \"1.0\"\n\nscalaVersion := \"2.11.8\"\n\nlibraryDependencies ++= Seq(\n  \"org.apache.spark\" %% \"spark-core\" % \"2.1.0\",\n  \"mysql\" % \"mysql-connector-java\" % \"5.1.31\",\n  \"org.apache.spark\" %% \"spark-sql\" % \"2.1.0\"\n)\n\n```\n\n<!--more-->\n&nbsp;Mysql.scala文件\n\n```\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.sql.{SQLContext, SaveMode}\nimport java.util.Properties\n\n\n/**\n  * Created by mi on 17-4-11.\n  */\n\ncase class resultset(name: String,\n                     info: String,\n                     summary: String)\n\nobject MysqlOpt {\n\n  def main(args: Array[String]): Unit = {\n    val conf = new SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n    val sqlContext = new SQLContext(sc)\n    import sqlContext.implicits._\n\n    //定义数据库和表信息\n    val url = \"jdbc:mysql://localhost:3306/baidubaike?useUnicode=true&amp;characterEncoding=UTF-8\"\n    val table = \"baike_pages\"\n\n    //读MySQL的方法1\n    val reader = sqlContext.read.format(\"jdbc\")\n    reader.option(\"url\", url)\n    reader.option(\"dbtable\", table)\n    reader.option(\"driver\", \"com.mysql.jdbc.Driver\")\n    reader.option(\"user\", \"root\")\n    reader.option(\"password\", \"XXX\")\n    val df = reader.load()\n    df.show()\n\n    //读MySQL的方法2\n    //    val jdbcDF = sqlContext.read.format(\"jdbc\").options(\n    //      Map(\"url\"->\"jdbc:mysql://localhost:3306/baidubaike?useUnicode=true&amp;characterEncoding=UTF-8\",\n    //        \"dbtable\"->\"(select name,info,summary from baike_pages) as some_alias\",\n    //        \"driver\"->\"com.mysql.jdbc.Driver\",\n    //        \"user\"-> \"root\",\n    //        //\"partitionColumn\"->\"day_id\",\n    //        \"lowerBound\"->\"0\",\n    //        \"upperBound\"-> \"1000\",\n    //        //\"numPartitions\"->\"2\",\n    //        \"fetchSize\"->\"100\",\n    //        \"password\"->\"XXX\")).load()\n    //    jdbcDF.show()\n\n  }\n}\n\n```\n\n&nbsp;输出\n\n<img src=\"/images/517519-20170413095220158-1948678960.png\" alt=\"\" />\n\n&nbsp;\n\n**2.使用Spark写MySQL中某个表中的信息**\n\n```\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.sql.{SQLContext, SaveMode}\nimport java.util.Properties\n\n\n/**\n  * Created by mi on 17-4-11.\n  */\n\ncase class resultset(name: String,\n                     info: String,\n                     summary: String)\n\nobject MysqlOpt {\n\n  def main(args: Array[String]): Unit = {\n    val conf = new SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n    val sqlContext = new SQLContext(sc)\n    import sqlContext.implicits._\n\n    //定义数据库和表信息\n    val url = \"jdbc:mysql://localhost:3306/baidubaike?useUnicode=true&amp;characterEncoding=UTF-8\"\n    val table = \"baike_pages\"\n\n    //写MySQL的方法1\n    val list = List(\n      resultset(\"名字1\", \"标题1\", \"简介1\"),\n      resultset(\"名字2\", \"标题2\", \"简介2\"),\n      resultset(\"名字3\", \"标题3\", \"简介3\"),\n      resultset(\"名字4\", \"标题4\", \"简介4\")\n    )\n    val jdbcDF = sqlContext.createDataFrame(list)\n    jdbcDF.collect().take(20).foreach(println)\n    //    jdbcDF.rdd.saveAsTextFile(\"/home/mi/coding/coding/Scala/spark-hbase/output\")\n    val prop = new Properties()\n    prop.setProperty(\"user\", \"root\")\n    prop.setProperty(\"password\", \"123456\")\n    //jdbcDF.write.mode(SaveMode.Overwrite).jdbc(url,\"baike_pages\",prop)\n    jdbcDF.write.mode(SaveMode.Append).jdbc(url, \"baike_pages\", prop)\n\n\n  }\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20170413100945189-253766268.png\" alt=\"\" />\n","tags":["Spark"]},{"title":"HBase学习笔记——基本CRUD操作","url":"/HBase学习笔记——基本CRUD操作.html","content":"**进入HBase的安装目录，****启动HBase**\n\n```\nbin/start-hbase.sh\n\n```\n\n**打开shell命令行模式**\n\n```\nbin/hbase shell\n\n```\n\n**关闭HBase**\n\n```\nbin/stop-hbase.sh\n\n```\n\n<!--more-->\n&nbsp;\n\n一个**cell的值,**取决于**Row,Column family,Column Qualifier和Timestamp**\n\n**HBase表结构**\n\n**<img src=\"/images/517519-20170418200643915-626033901.png\" alt=\"\" />**\n\n&nbsp;\n\n**1.查看当前用户**\n\n```\nhbase(main):001:0> whoami\nhbase/master@HADOOP.COM (auth:KERBEROS)\n\n```\n\n**2. HBase中创建表，这里面的name，sex，age，dept，course都是column-family**\n\n```\ncreate 'student','name','sex','age','dept','course'\n\n```\n\n**<img src=\"/images/517519-20170412215706580-118635761.png\" alt=\"\" />**\n\n**3.列出表**\n\n```\nhbase(main):005:0> list\nTABLE\n0 row(s) in 0.0170 seconds\n\n=> []\n\n```\n\n**4.HBase中添加数据，当添加了数据之后，就有了column，&lsquo;1000&rsquo;是ROW<br />**\n\n```\nput 'student','1000','name','XiaoMing'　　#这么写的话，family为name,column为空\nput 'student','1000','course:math','99'　　#这么写的话，family为course,column为math\n\n```\n\n**<img src=\"/images/517519-20170418200013759-1075653049.png\" alt=\"\" />**\n\n**5.HBase中查看表**\n\n```\ndesc 'student'\n#或者\nscan 'student'\n\n```\n\n**<img src=\"/images/517519-20170412215956533-1049837360.png\" alt=\"\" />**\n\n&nbsp;\n\n**6.HBase中查看某一列**\n\n```\nscan 'student',{COLUMN=>'name'}\n\n```\n\n&nbsp;<img src=\"/images/517519-20170412223758548-2130330016.png\" alt=\"\" />\n\n**7.HBase中查看一行**\n\n```\nget 'student',1000\n\n```\n\n&nbsp;<img src=\"/images/517519-20170412220433283-2083549801.png\" alt=\"\" />\n\n**8.查看特定行的某几列**\n\n```\nget 'student',1000,'name','sex'\n\n```\n\n**9.HBase中删除某一行的某一列数据**\n\n```\ndelete 'student','1000','name'\n\n```\n\n**10.HBase中删除某一行的数据**\n\n```\ndeleteall 'student','1000'\n\n```\n\n**11.删除某个表**\n\n```\ndisable &lsquo;student&rsquo;\ndrop 'student'    #删除之前必须先disable\n\n```\n\n**12.统计记录数**\n\n```\ncount 'student'\n\n```\n\n**13.清空表**\n\n```\ntruncate 'student'\n\n```\n\n**14.limit查看1条**\n\n```\nscan 'student',{LIMIT=>1}\n\n```\n\n**15.给用户赋权**\n\n给hive用户赋予all权限\n\n```\nhbase(main):001:0> grant 'hive','A'\n0 row(s) in 0.2550 seconds\n\n```\n\n参考：[HBase的ACL说明](https://cloud.tencent.com/developer/article/1390784)\n\n&nbsp;\n","tags":["HBase"]},{"title":"Spark学习笔记——Spark on YARN","url":"/Spark学习笔记——Spark on YARN.html","content":"Spark运行的时候，采用的是主从结构，有一个节点负责中央协调， 调度各个分布式工作节点。这个中央协调节点被称为**驱动器（ Driver） 节点**。与之对应的工作节点被称为**执行器（ executor） 节点**。\n\n所有的 Spark 程序都遵循同样的结构：程序从输入数据创建一系列 RDD， 再使用转化操作派生出新的 RDD，最后使用行动操作收集或存储结果 RDD 中的数据。\n\n<img src=\"/images/517519-20170410135304876-756963143.png\" alt=\"\" />\n\n**1.驱动器节点：**\n\n**Spark 驱动器**是执行你的程序中的 main() 方法的进程。它执行用户编写的用来创建 SparkContext、创建 RDD，以及进行 RDD 的转化操作和行动操作的代码。其实，当你启动 Spark shell 时，你就启动了一个 Spark 驱动器程序\n\n**驱动器程序**在 Spark 应用中有下述两个职责：1.把用户程序转为任务 2.为执行器节点调度任务\n\n**2.执行器节点：**\n\n**Spark 执行器节点**是一种工作进程，负责在 Spark 作业中运行任务，任务间相互独立。 Spark 应用启动时， 执行器节点就被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。\n\n**执行器进程**有两大作用： 第一，它们负责运行组成 Spark 应用的任务，并将结果返回给驱动器进程； 第二，它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。\n\n**3.集群管理器：**\n\n驱动器节点和执行器节点是如何启动的呢？ Spark 依赖于集群管理器来启动执行器节点，而在某些特殊情况下，也依赖集群管理器来启动驱动器节点。\n\n<!--more-->\n&nbsp;\n\n**Spark架构**\n\n<img src=\"/images/517519-20201214145122261-52137553.png\" alt=\"\" loading=\"lazy\" />\n\n```\nhttp://spark.apache.org/docs/latest/cluster-overview.html\n\n```\n\n<img src=\"/images/517519-20201214145151702-1831309909.png\" alt=\"\" loading=\"lazy\" />\n\n转自&nbsp;&nbsp;\n\n```\nhttps://zhuanlan.zhihu.com/p/91143069\n\n```\n\n&nbsp;\n\nyarn-client&nbsp;适用于交互、调试，希望立即看到app的输出\n\nyarn-cluster&nbsp;适用于生产环境\n\n**<img src=\"/images/517519-20210120152731662-1030117903.png\" alt=\"\" loading=\"lazy\" />**\n\n&nbsp;\n\n参考：\n\n[yarn-cluster和yarn-client提交模式的区别](https://www.jianshu.com/p/6b796a5c3e80)\n\n　　\n\n&nbsp;\n","tags":["Spark"]},{"title":"Spark学习笔记——数据读取和保存","url":"/Spark学习笔记——数据读取和保存.html","content":"**spark所支持的文件格式**\n\n<img src=\"/images/517519-20170408171821253-1118136829.png\" alt=\"\" width=\"614\" height=\"293\" />\n\n<!--more-->\n&nbsp;\n\n## **1.文本文件**\n\n在 Spark 中读写文本文件很容易。\n\n当我们将一个**文本文件**读取为** RDD** 时，**输入的每一行** 都会成为 **RDD 的 一个元素**。\n\n也可以将**多个完整的文本文件**一次性读取为**一个 pair RDD**， 其中**键是文件名，值是文件内容**。\n\n&nbsp;在 Scala 中**读取一个文本文件**\n\n```\nval inputFile = \"file:///home/common/coding/coding/Scala/word-count/test.segmented\"\nval textFile = sc.textFile(inputFile)\n\n```\n\n&nbsp;在 Scala 中**读取给定目录中的所有文件**\n\n```\nval input = sc.wholeTextFiles(\"file:///home/common/coding/coding/Scala/word-count\")\n\n```\n\n**&nbsp;保存文本文件**，Spark 将传入的路径作为目录对待，会在那个目录下输出多个文件\n\n```\ntextFile.saveAsTextFile(\"file:///home/common/coding/coding/Scala/word-count/writeback\")\n//textFile.repartition(1).saveAsTextFile 就能保存成一个文件\n\n```\n\n对于**dataFrame文件**，先使用**.toJavaRDD** 转换成RDD，然后再使用 &nbsp;**coalesce(1).saveAsTextFile**\n\n&nbsp;\n\n## **2.JSON**\n\nJSON 是一种使用较广的半结构化数据格式。\n\n**读取JSON**，书中代码有问题所以找了另外的一段读取JSON的代码\n\n&nbsp;build.sbt\n\n```\n\"org.json4s\" %% \"json4s-jackson\" % \"3.2.11\"\n\n```\n\n&nbsp;代码\n\n```\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.json4s._\nimport org.json4s.jackson.JsonMethods._\nimport org.json4s.jackson.Serialization\nimport org.json4s.jackson.Serialization.{read, write}\n\n/**\n  * Created by common on 17-4-3.\n  */\n\ncase class Person(firstName: String, lastName: String, address: List[Address]) {\n  override def toString = s\"Person(firstName=$firstName, lastName=$lastName, address=$address)\"\n}\n\ncase class Address(line1: String, city: String, state: String, zip: String) {\n  override def toString = s\"Address(line1=$line1, city=$city, state=$state, zip=$zip)\"\n}\n\nobject WordCount {\n  def main(args: Array[String]) {\n    val inputJsonFile = \"file:///home/common/coding/coding/Scala/word-count/test.json\"\n    val conf = new SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n    val input5 = sc.textFile(inputJsonFile)\n    val dataObjsRDD = input5.map { myrecord =>\n      implicit val formats = DefaultFormats\n      // Workaround as      DefaultFormats is not serializable\n      val jsonObj = parse(myrecord)\n      //val addresses = jsonObj \\ \"address\"\n      //println((addresses(0) \\ \"city\").extract[String])\n      jsonObj.extract[Person]\n    }\n    dataObjsRDD.saveAsTextFile(\"file:///home/common/coding/coding/Scala/word-count/test1.json\")\n\n  }\n\n\n}\n\n```\n\n读取的JSON文件\n\n```\n{\"firstName\":\"John\",\"lastName\":\"Smith\",\"address\":[{\"line1\":\"1 main street\",\"city\":\"San Francisco\",\"state\":\"CA\",\"zip\":\"94101\"},{\"line1\":\"1 main street\",\"city\":\"sunnyvale\",\"state\":\"CA\",\"zip\":\"94000\"}]}\n{\"firstName\":\"Tim\",\"lastName\":\"Williams\",\"address\":[{\"line1\":\"1 main street\",\"city\":\"Mountain View\",\"state\":\"CA\",\"zip\":\"94300\"},{\"line1\":\"1 main street\",\"city\":\"San Jose\",\"state\":\"CA\",\"zip\":\"92000\"}]}\n\n```\n\n输出的文件\n\n```\nPerson(firstName=John, lastName=Smith, address=List(Address(line1=1 main street, city=San Francisco, state=CA, zip=94101), Address(line1=1 main street, city=sunnyvale, state=CA, zip=94000)))\nPerson(firstName=Tim, lastName=Williams, address=List(Address(line1=1 main street, city=Mountain View, state=CA, zip=94300), Address(line1=1 main street, city=San Jose, state=CA, zip=92000)))\n\n```\n\n&nbsp;\n\n## **3.逗号分割值与制表符分隔值**\n\n逗号分隔值（CSV）文件每行都有固定数目的字段，字段间用逗号隔开（在制表符分隔值文件，即 TSV 文 件中用制表符隔开）。\n\n如果恰好CSV 的所有数据字段均**没有包含换行符**，你也可以使用** textFile()** 读取并解析数据，\n\nbuild.sbt\n\n```\n\"au.com.bytecode\" % \"opencsv\" % \"2.4\"\n\n```\n\n**3.1 读取CSV文件**\n\n```\nimport java.io.StringReader\n\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.json4s._\nimport org.json4s.jackson.JsonMethods._\nimport org.json4s.jackson.Serialization\nimport org.json4s.jackson.Serialization.{read, write}\nimport au.com.bytecode.opencsv.CSVReader\n\n/**\n  * Created by common on 17-4-3.\n  */\n\nobject WordCount {\n  def main(args: Array[String]) {\n\n    val input = sc.textFile(\"/home/common/coding/coding/Scala/word-count/sample_map.csv\")\n    val result6 = input.map{ line =>\n      val reader = new CSVReader(new StringReader(line));\n      reader.readNext();\n    }\n    for(result <- result6){\n      for(re <- result){\n        println(re)\n      }\n    }\n\n  }\n\n}\n\n```\n\n**&nbsp;CSV文件内容**\n\n<img src=\"/images/517519-20170409150551050-1663583374.png\" alt=\"\" />\n\n**输出**\n\n```\n0\nFront Left\n/usr/share/alsa/samples/Front_Left.wav\n1\nFront Right\n/usr/share/alsa/samples/Front_Right.wav\n\n```\n\n&nbsp;\n\n**如果在字段中嵌有换行符**，就需要完整读入每个文件，然后解析各段。如果每个文件都很大，读取和解析的过程可能会很不幸地成为性能瓶颈。\n\n&nbsp;代码\n\n```\nimport java.io.StringReader\n\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.json4s._\nimport org.json4s.jackson.JsonMethods._\nimport org.json4s.jackson.Serialization\nimport org.json4s.jackson.Serialization.{read, write}\nimport scala.collection.JavaConversions._\nimport au.com.bytecode.opencsv.CSVReader\n\n/**\n  * Created by common on 17-4-3.\n  */\n\ncase class Data(index: String, title: String, content: String)\n\nobject WordCount {\n  def main(args: Array[String]) {\n\n    val conf = new SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n    val input = sc.wholeTextFiles(\"/home/common/coding/coding/Scala/word-count/sample_map.csv\")\n    val result = input.flatMap { case (_, txt) =>\n      val reader = new CSVReader(new StringReader(txt));\n      reader.readAll().map(x => Data(x(0), x(1), x(2)))\n    }\n    for(res <- result){\n      println(res)\n    }\n  }\n\n}\n\n```\n\n&nbsp;输出\n\n```\nData(0,Front Left,/usr/share/alsa/samples/Front_Left.wav)\nData(1,Front Right,/usr/share/alsa/samples/Front_Right.wav)\n\n```\n\n&nbsp;或者\n\n```\nimport java.io.StringReader\n\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.json4s._\nimport org.json4s.jackson.JsonMethods._\nimport org.json4s.jackson.Serialization\nimport org.json4s.jackson.Serialization.{read, write}\nimport scala.collection.JavaConversions._\nimport au.com.bytecode.opencsv.CSVReader\n\n/**\n  * Created by common on 17-4-3.\n  */\n\ncase class Data(index: String, title: String, content: String)\n\nobject WordCount {\n  def main(args: Array[String]) {\n\n    val conf = new SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n    val input = sc.wholeTextFiles(\"/home/common/coding/coding/Scala/word-count/sample_map.csv\")<br />　　//wholeTextFiles读出来是一个RDD(String,String)\n    val result = input.flatMap { case (_, txt) =>\n      val reader = new CSVReader(new StringReader(txt));\n      //reader.readAll().map(x => Data(x(0), x(1), x(2)))\n      reader.readAll()\n    }\n    result.collect().foreach(x => {\n      x.foreach(println); println(\"======\")\n    })\n\n  }\n}\n\n```\n\n&nbsp;输出\n\n```\n0\nFront Left\n/usr/share/alsa/samples/Front_Left.wav\n======\n1\nFront Right\n/usr/share/alsa/samples/Front_Right.wav\n======\n\n```\n\n&nbsp;\n\n**3.2 保存CSV**\n\n```\nimport java.io.{StringReader, StringWriter}\n\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.json4s._\nimport org.json4s.jackson.JsonMethods._\nimport org.json4s.jackson.Serialization\nimport org.json4s.jackson.Serialization.{read, write}\n\nimport scala.collection.JavaConversions._\nimport au.com.bytecode.opencsv.{CSVReader, CSVWriter}\n\n/**\n  * Created by common on 17-4-3.\n  */\n\ncase class Data(index: String, title: String, content: String)\n\nobject WordCount {\n  def main(args: Array[String]) {\n\n    val conf = new SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n    val inputRDD = sc.parallelize(List(Data(\"index\", \"title\", \"content\")))\n    inputRDD.map(data => List(data.index, data.title, data.content).toArray)\n      .mapPartitions { data =>\n        val stringWriter = new StringWriter();\n        val csvWriter = new CSVWriter(stringWriter);\n        csvWriter.writeAll(data.toList)\n        Iterator(stringWriter.toString)\n      }.saveAsTextFile(\"/home/common/coding/coding/Scala/word-count/sample_map_out\")\n  }\n}\n\n```\n\n&nbsp;输出\n\n```\n\"index\",\"title\",\"content\"\n\n```\n\n&nbsp;\n\n## **4.SequenceFile**\n\n## SequenceFile 是由没有相对关系结构的键值对文件组成的常用 Hadoop 格式。\n\nSequenceFile 文件有同步标记， Spark 可 以用它来定位到文件中的某个点，然后再与记录的边界对齐。这可以让 Spark 使 用多个节点高效地并行读取 SequenceFile 文件。SequenceFile 也是Hadoop MapReduce 作 业中常用的输入输出格式，所以如果你在使用一个已有的 Hadoop 系统，数据很有可能是以 S equenceFile 的格式供你使用的。\n\n<img src=\"/images/517519-20170422203805712-287788231.png\" alt=\"\" width=\"390\" height=\"270\" />\n\n```\nimport org.apache.hadoop.io.{IntWritable, Text}\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\n\n/**\n  * Created by common on 17-4-6.\n  */\nobject SparkRDD {\n\n  def main(args: Array[String]) {\n    val conf = new SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n\n    //写sequenceFile，\n    val rdd = sc.parallelize(List((\"Panda\", 3), (\"Kay\", 6), (\"Snail\", 2)))\n    rdd.saveAsSequenceFile(\"output\")\n\n    //读sequenceFile\n    val output = sc.sequenceFile(\"output\", classOf[Text], classOf[IntWritable]).\n      map{case (x, y) => (x.toString, y.get())}\n    output.foreach(println)\n\n  }\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20170422203334212-758938897.png\" alt=\"\" />\n\n&nbsp;\n\n## **5.对象文件**\n\n对象文件看起来就像是对 SequenceFile 的简单封装，它允许存储只包含值的 RDD。和 SequenceFile 不一样的是，对象文件是使用 Java 序列化写出的。\n\n如果你修改了你的类&mdash;&mdash;比如增减了几个字段&mdash;&mdash;已经生成的对象文件就不再可读了。\n\n读取文件&mdash;&mdash;用 SparkContext 中的 objectFile() 函数接收一个路径，返回对应的 RDD。\n\n写入文件&mdash;&mdash;要 保存对象文件， 只需在 RDD 上调用 saveAsObjectFile\n\n&nbsp;\n\n## **6.Hadoop输入输出格式**\n\n除了 Spark 封装的格式之外，也可以与任何 Hadoop 支持的格式交互。Spark 支持新旧两套Hadoop 文件 API，提供了很大的灵活性。\n\n**旧的API：hadoopFile**，使用旧的 API 实现的 Hadoop 输入格式\n\n**新的API：newAPIHadoopFile**，接收一个路径以及三个类。第一个类是&ldquo;格式&rdquo;类，代表输入格式。第二个类是键的类，最后一个类是值的类。如果需要设定额外的 Hadoop 配置属性，也可以传入一个 conf 对象。\n\nKeyValueTextInputFormat 是最简单的 Hadoop 输入格式之一，可以用于从文本文件中读取键值对数据。每一行都会被独立处理，键和值之间用制表符隔开。\n\n```\nimport org.apache.hadoop.io.{IntWritable, LongWritable, MapWritable, Text}\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkConf\nimport org.apache.spark._\nimport org.apache.hadoop.mapreduce.Job\nimport org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat\nimport org.apache.spark.rdd._\n\n/**\n  * Created by common on 17-4-6.\n  */\nobject SparkRDD {\n\n  def main(args: Array[String]) {\n    val conf = new SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n\n    //使用老式 API 读取 KeyValueTextInputFormat()，以JSON文件为例子\n    //注意引用的包是org.apache.hadoop.mapred.KeyValueTextInputFormat\n//    val input = sc.hadoopFile[Text, Text, KeyValueTextInputFormat](\"input/test.json\").map {\n//      case (x, y) => (x.toString, y.toString)\n//    }\n//    input.foreach(println)\n\n    // 读取文件，使用新的API，注意引用的包是org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat\n    val job = new Job()\n    val data = sc.newAPIHadoopFile(\"input/test.json\" ,\n      classOf[KeyValueTextInputFormat],\n      classOf[Text],\n      classOf[Text],\n      job.getConfiguration)\n    data.foreach(println)\n\n    //保存文件，注意引用的包是org.apache.hadoop.mapreduce.lib.output.TextOutputFormat\n    data.saveAsNewAPIHadoopFile(\n      \"input/test1.json\",\n      classOf[Text],\n      classOf[Text],\n      classOf[TextOutputFormat[Text,Text]],\n      job.getConfiguration)\n\n  }\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20170422230017556-2042919896.png\" alt=\"\" />\n\n&nbsp;\n\n**Hadoop 的非文件系统数据源**\n\n除 了** hadoopFile() 和 saveAsHadoopFile()** 这 一 大 类 函 数， 还 可 以 使 用 **hadoopDataset/saveAsHadoopDataSet 和 newAPIHadoopDataset/ saveAsNewAPIHadoopDataset** 来访问 Hadoop 所支持的**非文件系统的存储格式**。例如，许多像 **HBase 和 MongoDB** 这样的键值对存储都提供了用来直接读取 Hadoop 输入格式的接口。我们可以在 Spark 中很方便地使用这些格式。\n\n&nbsp;\n\n## **7.文件压缩**\n\nSpark 原生的输入方式（ textFile 和 sequenceFile）可以自动处理一些类型的压缩。在读取压缩后的数据时，一些压缩编解码器可以推测压缩类型。 <br />这些压缩选项只适用于支持压缩的 Hadoop 格式，也就是那些写出到文件系统的格式。写入数据库的 Hadoop 格式一般没有实现压缩支持。如果数据库中有压缩过的记录，那应该是数据库自己配置的。\n\n<img src=\"/images/517519-20170423103217788-1005961188.png\" alt=\"\" width=\"555\" height=\"304\" />\n\n## **8.读取har文件**\n\n```\nval df = spark.read.text(\"har:///har/xxx/test/2019-06-30.har\")\n\n```\n\n&nbsp;查看\n\n```\nscala> df.show()\n+--------------------+----------+----+\n|               value|        dt|hour|\n+--------------------+----------+----+\n|/home/logs/sc...|2019-06-30|  17|\n|/home/logs/sc...|2019-06-30|  17|\n|/home/logs/sc...|2019-06-30|  17|\n|/home/logs/sc...|2019-06-30|  17|\n|/home/logs/sc...|2019-06-30|  17|\n|/home/logs/sc...|2019-06-30|  17|\n|/home/logs/sc...|2019-06-30|  17|\n|/home/logs/sc...|2019-06-30|  17|\n|/home/logs/sc...|2019-06-30|  17|\n|/home/logs/sc...|2019-06-30|  17|\n|/home/logs/sc...|2019-06-30|  17|\n|/home/logs/sc...|2019-06-30|  17|\n|/home/logs/sc...|2019-06-30|  17|\n|/home/logs/sc...|2019-06-30|  17|\n|/home/logs/sc...|2019-06-30|  17|\n|/home/logs/sc...|2019-06-30|  17|\n|/home/logs/sc...|2019-06-30|  17|\n|/home/logs/sc...|2019-06-30|  17|\n|/home/logs/sc...|2019-06-30|  17|\n|/home/logs/sc...|2019-06-30|  17|\n+--------------------+----------+----+\nonly showing top 20 rows\n\n```\n\n&nbsp;\n\n## **10.dataframe操作**\n\ndf column类型转换\n\n```\n.withColumn(\"a\", $\"a\".cast(sql.types.DoubleType))\n.withColumn(\"b\", $\"b\".cast(sql.types.LongType))\n\n```\n\n　　\n\n## 11.spark读写avro&nbsp;parquet\n\nA Powerful Big Data Trio: Spark, Parquet and Avro\n\n&nbsp;\n","tags":["Spark"]},{"title":"Spark学习笔记——键值对操作","url":"/Spark学习笔记——键值对操作.html","content":"**键值对 RDD**是 Spark 中许多操作所需要的常见数据类型\n\n键值对 RDD 通常用来进行聚合计算。我们一般要先通过一些初始** ETL(抽取、转化、装载)**操作来将数据转化为键值对形式。\n\nSpark 为包含键值对类型的 RDD 提供了一些专有的操作。\n\n<!--more-->\n&nbsp;\n\n**1.创建Pair RDD**\n\n```\n    val input = sc.parallelize(List(1, 2, 3, 4))\n　　val pairs = input.map(x => (x+1, x))\n    for (pair <- pairs){\n      println(pair)\n    }\n　　//输出\n(2,1)\n(3,2)\n(4,3)\n(5,4)       \n\n```\n\n&nbsp;\n\n**2.Pair RDD的转化操作**\n\nPair RDD 可以使用所有标准 RDD 上的可用的转化操作。\n\n<img src=\"/images/517519-20170407183139019-524414097.png\" alt=\"\" width=\"647\" height=\"678\" />\n\n<img src=\"/images/517519-20170407183510457-112776039.png\" alt=\"\" width=\"657\" height=\"154\" />\n\n<img src=\"/images/517519-20170407183523332-199882762.png\" alt=\"\" width=\"657\" height=\"196\" />\n\n**Pair RDD也支持RDD所支持的函数**\n\n```\npairs.filter{case (key, value) => value.length < 20}\n\n```\n\n&nbsp;\n\n**3.聚合操作**\n\n类似fold() 、 combine() 、 reduce() 等行动操作，这些操作返回 RDD,因此它们是转化操作而不是行动操作。\n\n**reduceByKey()函数，接收一个函数，并使用这个函数对值进行合并**\n\n```\nval wordCount = textFile.flatMap(line => line.split(\" \")).map(word => (word, 1)).reduceByKey((a, b) => a + b)　　#切分成单词，转换成键值对并计数\n\n```\n\n**或者**\n\n```\ninput.flatMap(x => x.split(\" \")).countByValue()\n\n```\n\n**foldByKey()函数，也是接收一个函数，并使用这个函数对值进行合并，提供初始值**\n\n```\nrdd.mapValues(x => (x, 1)).reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2))\n\n```\n\n&nbsp;<img src=\"/images/517519-20170407193059863-1476536116.png\" alt=\"\" width=\"588\" height=\"342\" />\n\n**countByValue()函数**\n\n```\nval textFile = sc.textFile(inputFile)\nval result1 = textFile.flatMap(x => x.split(\" \")).countByValue()\nprintln(result1)<br />\n输出：Map(cc -> 3, aa -> 6, bb -> 3, ee -> 3, ff -> 2, hh -> 1, dd -> 1, gg -> 1)\n\n```\n\n&nbsp;**combineByKey()函数，使用 combineByKey() 求每个键对应的平均值**\n\n```\n    val data = Seq((\"a\", 3), (\"b\", 4), (\"a\", 1))\n    val input2 = sc.parallelize(data)\n    //使用 combineByKey() 求每个键对应的平均值\n    val result2 = input2.combineByKey(\n      (v) => (v, 1),\n      (acc: (Int, Int), v) => (acc._1 + v, acc._2 + 1),\n      (acc1: (Int, Int), acc2: (Int, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2)\n    ).map{ case (key, value) => (key, value._1 / value._2.toFloat) }\n    result2.collectAsMap().map(println(_))\n    \n    //输出\n    (b,4.0)\n    (a,2.0)\n\n```\n\n&nbsp;\n\n**4.数据分组**\n\n**groupByKey()函数，将数据根据键进行分组**\n\n```\nval data = Seq((\"a\", 3), (\"b\", 4), (\"a\", 1))\nval input2 = sc.parallelize(data)\nval result3 = input2.groupByKey()\nfor (result <- result3)\n   println(result)\n\n//输出：\n(a,CompactBuffer(3, 1))\n(b,CompactBuffer(4))\n\n```\n\n&nbsp;\n\n**5.连接**\n\n**join操作符**\n\n```\n    val input = sc.parallelize(List(1, 2, 3, 4))\n    val pairs1 = input.map(x => (x+1, x))\n    //输出(2,1),(3,2),(4,3),(5,4)\n    val pairs2 = input.map(x => (x+1, 1))\n    //输出(2,1),(3,1),(4,1),(5,1)\n    for(pair <- pairs1.join(pairs2)){\n      println(pair)\n    }\n    //输出(4,(3,1)),(3,(2,1)),(5,(4,1)),(2,(1,1))\n\n```\n\n&nbsp;有时，我们**不希望结果中的键必须在两个 RDD 中都存在，join的部分可以不存在**\n\n**leftOuterJoin(other) 和 rightOuterJoin(other)** 都会根据键连接两个 RDD，但是允许结果中存在其中的一个 pair RDD 所 缺失的键。\n\n**leftOuterJoin(other)结果**\n\n```\n(4,(3,Some(1)))\n(3,(2,Some(1)))\n(5,(4,Some(1)))\n(2,(1,Some(1)))\n\n```\n\n** rightOuterJoin(other)结果**\n\n```\n(4,(Some(3),1))\n(3,(Some(2),1))\n(5,(Some(4),1))\n(2,(Some(1),1))\n\n```\n\n&nbsp;\n\n**6.数据排序**\n\n**在 Scala 中以字符串顺序对整数进行自定义排序**\n\n```\n    val input = sc.parallelize(List(1, 2, 3, 4))\n    val pairs1 = input.map(x => (x + 1, x))\n    implicit val sortIntegersByString = new Ordering[Int] {\n      override def compare(a: Int, b: Int) = b.toString.compare(a.toString)\n    }\n    for(pair <- pairs1.sortByKey())\n      println(pair)\n<br />//输出<br />(5,4)<br />(4,3)<br />(3,2)<br />(2,1)\n```\n\n&nbsp;<img src=\"/images/517519-20170408165032441-580107157.png\" alt=\"\" width=\"627\" height=\"311\" />\n","tags":["Spark"]},{"title":"Spark学习笔记——RDD编程","url":"/Spark学习笔记——RDD编程.html","content":"**1.RDD&mdash;&mdash;弹性分布式数据集（Resilient Distributed Dataset）**\n\nRDD是一个分布式的元素集合，在Spark中，对数据的操作就是**创建RDD**、**转换已有的RDD**和**调用RDD操作进行求值**。\n\nSpark 中的 RDD 就是一个不可变的分布式对象集合。每个 RDD 都被分为多个分区,这些分区运行在集群中的不同节点上。\n\n```\nobject WordCount {\n  def main(args: Array[String]) {\n    val inputFile =  \"file:///home/common/coding/coding/Scala/word-count/test.segmented\"\n    val conf = new SparkConf().setAppName(\"WordCount\").setMaster(\"local\")　　　　#创建一个SparkConf对象来配置应用<br>　　　　#集群URL：告诉Spark连接到哪个集群，local是单机单线程，无需连接到集群，应用名：在集群管理器的用户界面方便找到应用\n    val sc = new SparkContext(conf)　　　　　　　　#然后基于这SparkConf创建一个SparkContext对象\n    val textFile = sc.textFile(inputFile)　　　　#读取输入的数据\n    val wordCount = textFile.flatMap(line => line.split(\" \")).map(word => (word, 1)).reduceByKey((a, b) => a + b)　　#切分成单词，转换成键值对并计数\n    wordCount.foreach(println)\n  }\n}\n\n```\n\n**创建一个RDD**\n\n```\nval textFile = sc.textFile(inputFile)\n\n```\n\n或者\n\n```\nval lines = sc.parallelize(List(\"pandas\", \"i like pandas\"))\n\n```\n\n<!--more-->\n&nbsp;\n\nRDD支持两种类型的操作: **转化操作(transformation)**和**行动操作(action)**。\n\n**转化操作，是返回一个新的RDD的操作：**\n\nfilter()函数\n\n```\nval RDD = textFile.filter(line => line.contains(\"Hadoop\"))\n\n```\n\n&nbsp;map()函数\n\n```\nval input = sc.parallelize(List(1, 2, 3, 4))\nval result = input.map(x => x * x)\nprintln(result.collect().mkString(\",\"))\n\n```\n\n&nbsp;输出\n\n```\n1,4,9,16\n\n```\n\n**map()和flatMap()的区别**\n\n```\n    val input1 = sc.parallelize(List(\"hello world\",\"hi\"))\n    val lines = input1.map(line => line.split(\" \"))\n    for(line <- lines)\n      println(line)　　//输出是两个List的地址\n    val lines_ = input1.flatMap(line => line.split(\" \"))\n    for(line_ <- lines_)\n      println(line_)　　//输出是[hello world hi]\n\n```\n\n**<img src=\"/images/517519-20170406233514972-813112650.png\" alt=\"\" width=\"760\" height=\"633\" />**\n\n&nbsp;\n\n**行动操作，是向驱动器程序返回结果或把结果写入外部系统的操作,会触发实际的计算：**first()、count()、take()、collect()[获取整个RDD中的数据，只有想在本地处理这些数据的时候，才可以使用，因为一般情况下RDD很大]\n\ntake()函数\n\n```\ntextFile.take(5).foreach(println)\n\n```\n\nreduce函数，接收一个函数作为参数\n\n```\nval input = sc.parallelize(List(1, 2, 3, 4))\nval sum = input.reduce((x, y) => x + y)\nprintln(sum)　　//输出1-4的累加和，10\n\n```\n\naggregate()函数，计算List的和以及List的元素个数，然后计算平均值\n\n```\n    val input = sc.parallelize(List(1, 2, 3, 4))\n    val result = input.aggregate((0, 0))(\n      (acc, value) => (acc._1 + value, acc._2 + 1),\n      (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2))\n    val avg = result._1 / result._2.toDouble\n    println(result)\n    println(avg)\n\n```\n\n输出\n\n```\n(10,4)\n2.5\n\n```\n\n对于\n\n```\nval sum1 = input.aggregate((0, 0))((x, y) => (x._1 + y, x._2 + 1), (x, y) => (x._1 + y._1, x._2 + y._2))\n\n```\n\n&nbsp;输出（10,4）\n\n理解\n\n```\n过程大概这样：\n\n首先，初始值是(0,0)，这个值在后面2步会用到。\n\n然后，(acc,number) => (acc._1 + number, acc._2 + 1)，number即是函数定义中的T，这里即是List中的元素。所以acc._1 + number, acc._2 + 1的过程如下。\n\n    1.   0+1,  0+1\n\n    2.  1+2,  1+1\n\n    3.  3+3,  2+1\n\n    4.  6+4,  3+1\n\n    5.  10+5,  4+1\n\n    6.  15+6,  5+1\n\n    7.  21+7,  6+1\n\n    8.  28+8,  7+1\n\n    9.  36+9,  8+1\n\n结果即是(45,9)。这里演示的是单线程计算过程，实际Spark执行中是分布式计算，可能会把List分成多个分区，假如3个，p1(1,2,3,4)，p2(5,6,7,8)，p3(9)，经过计算各分区的的结果（10,4），（26,4），（9,1），这样，执行(par1,par2) => (par1._1 + par2._1, par1._2 + par2._2)就是（10+26+9,4+4+1）即（45,9）.再求平均值就简单了。\n\n```\n\n&nbsp;\n\ntop()函数，可以返回RDD的前几个元素\n\nfold()函数，和reduce()函数的功能差不多，但是需要提供初始值\n\n```\nval numbers = List(1, 2, 3, 4)\n    println(\n      numbers.fold(1) {\n        (a, b) => a + b\n      }\n    )\n\n```\n\n&nbsp;输出11\n\n<img src=\"/images/517519-20170407174056519-1855793921.png\" alt=\"\" width=\"925\" height=\"784\" />\n\n**转化操作和行动操作的区别：**\n\n**1.转换操作**只会惰性计算这些 RDD\n\n**2.行动操作**会对 RDD 计算出一个结果,并把结果返回到驱动器程序中,或把结果存储到外部存储系统(如 HDFS)中\n\n默认情况下,Spark 的 RDD 会在你每次对它们进行行动操作时重新计算。如果想在多个行动操作中重用同一个 RDD,可以使用 **RDD.persist()** 让 Spark 把这个 RDD 缓存下来\n\n&nbsp;\n\n**2.向Spark传递函数**\n\n在 Scala 中,我们可以把定义的内联函数、方法的引用或静态方法传递给 Spark。\n\n**我们可以把需要的字段放到一个局部变量中,来避免传递包含该字段的整个对象**\n\n```\nclass SearchFunctions(val query: String) {\n\n    def isMatch(s: String): Boolean = {\n        s.contains(query)\n    }\n\n    def getMatchesFunctionReference(rdd: RDD[String]): RDD[String] = {\n        // 问题: \"isMatch\"表示\"this.isMatch\",因此我们要传递整个\"this\"\n        rdd.map(isMatch)\n    }\n\n    def getMatchesFieldReference(rdd: RDD[String]): RDD[String] = {\n        // 问题: \"query\"表示\"this.query\",因此我们要传递整个\"this\"\n        rdd.map(x => x.split(query))\n    }\n\n    def getMatchesNoReference(rdd: RDD[String]): RDD[String] = {\n        // 安全:只把我们需要的字段拿出来放入局部变量中\n        val query_ = this.query\n        rdd.map(x => x.split(query_))\n    }\n}            \n\n```\n\n&nbsp;\n\n**3.持久化（缓存）**\n\nSpark RDD 是**惰性求值**的,而有时我们希望能**多次使用同一个 RDD**的时候需要对RDD进行持久化\n\n**两次调用行动操作**，每次Spark都会重新计算RDD和它的所有依赖\n\n```\nval result = input.map(x => x*x)\nprintln(result.count())\nprintln(result.collect().mkString(\",\"))\n\n```\n\n使用**persist()**来进行持久化\n\n```\nval result = input.map(x => x * x)\nresult.persist(StorageLevel.DISK_ONLY)\nprintln(result.count())\nprintln(result.collect().mkString(\",\"))\n\n```\n\n&nbsp;如果要缓存的数据太多,内存中放不下,Spark 会自动利用最近最少使用(LRU)的缓存策略把最老的分区从内存中移除。\n\n&nbsp;RDD 还有一个方法叫作 unpersist() ,调用该方法可以手动把持久化的 RDD 从缓存中移除。\n\n&nbsp;\n","tags":["Spark"]},{"title":"Scala学习笔记——函数和闭包","url":"/Scala学习笔记——函数和闭包.html","content":"**1.本地函数**\n\n可以在一个方法内再次定义一个方法，这个方法就是外部方法的内部私有方法，省略了private关键字\n\n**2.头等函数**\n\n```\nvar increase = (x: Int) => x + 1\nSystem.out.println(increase(10))\n\n```\n\n集合类的**foreach方法**\n\n```\nvar list1 = List(1, 2)\nlist1.foreach((x: Int) => println(x))\n\n```\n\n<!--more-->\n&nbsp;集合类的**filter方法**\n\n```\nlist1.filter((x: Int) => x > 1)\n\n```\n\n&nbsp;\n\n**3.函数字面量的短格式，使得函数写的更加简洁**\n\n```\n//函数字面量的短格式\n    list1.filter(x => x > 1)\n\n```\n\n&nbsp;\n\n**4.占位符语法，如果想让函数字面量更加简洁，可以把下划线当做一个或更多参数的占位符**\n\n```\n//使用占位符语法\n    System.out.println(list1.filter(_ > 1))\n\n```\n\n&nbsp;\n\n**5.部分应用函数**\n\nScala中_不但可以代替单个参数，还可以**替换整个参数列表**\n\n```\n//部分应用函数\n    list1.foreach(println _)\n\n```\n\n&nbsp;\n\n**6.闭包**\n\n<img src=\"/images/517519-20170405104015582-791613437.png\" alt=\"\" width=\"645\" height=\"349\" />\n\n<img src=\"/images/517519-20170405104440300-118148211.png\" alt=\"\" width=\"651\" height=\"300\" />\n\n&nbsp;\n\n**7.重复参数，允许向函数传入可变长参数列表，实际上是向参数列表中传入了一个array**\n\n```\n  //重复参数\n  def echo(args:String*): Unit ={\n    for(arg <- args) println(arg)\n  }\n\necho(\"a\",\"b\")\necho(\"a\",\"b\",\"c\")\n\n```\n\n&nbsp;但是如果要向这个函数中传入array的话，需要\n\n```\nvar list2 = List(\"a\", \"b\")\necho(list2: _*)\n\n```\n\n&nbsp;\n","tags":["Scala"]},{"title":"Spark学习笔记——在远程机器中运行WordCount","url":"/Spark学习笔记——在远程机器中运行WordCount.html","content":"1.通过**realy机器**登录**relay-shell**\n\n```\nssh XXX@XXX\n\n```\n\n2.登录了跳板机之后，**连接可以用的机器**\n\n```\nXXXX.bj\n\n```\n\n3.在本地的idea生成好程序的jar包（word-count_2.11-1.0.jar）之后，把**jar包**和**需要put到远程机器的hdfs文件系统中的文件**通过scp命令从开发机传到远程的机器中\n\n```\nscp 开发机用户名@开发机ip地址:/home/XXXXX/文件 .    #最后一个.表示cd的根目录下\n\n```\n\n<!--more-->\n&nbsp;\n\n```\nobject WordCount {\n  def main(args: Array[String]) {\n//    val inputFile =  \"file:///home/mi/coding/coding/Scala/word-count/input/README.txt\"\n//    val inputFile =  \"file://README.txt\"\n    val inputFile =  \"/user/XXXX/lintong/README.txt\"\n    val conf = new SparkConf().setAppName(\"WordCount\").setMaster(\"yarn-client\")\n    val sc = new SparkContext(conf)\n    val textFile = sc.textFile(inputFile)\n    val wordCount = textFile.flatMap(line => line.split(\" \")).map(word => (word, 1)).reduceByKey((a, b) => a + b)\n//    wordCount.foreach(println)\n//    wordCount.saveAsTextFile(\"file:///home/mi/coding/coding/Scala/word-count/output/READMEOUT.txt\")\n    wordCount.saveAsTextFile(\"/user/XXXX/lintong/READMEOUT.txt\")\n  }\n}\n\n```\n\n&nbsp;\n\n4.通过put命令将**远程机器中的txt文件**，传到远程机器的**hdfs文件系统**\n\n```\nhadoop fs -put /homeXXX/文件名 ./lintong    #.注意.的目录地址是用户的根目录\n\n```\n\n5.这时可以使用下面命令**查看文件**\n\n```\nhadoop fs -ls ./lintong\n\n```\n\n6.接下来写**shell脚本**，来运行**spark-submit命令**，**写完shell脚本只要运行shell脚本就行**，shell脚本的目录和jar包的目录保持一致\n\nyarn-client 调试模式\n\nyarn-cluster 生产模式\n\n```\nspark-submit --cluster XXXXX \\\n    --master yarn-client \\\n    --num-executors 3 \\\n    --class \"包名.类名\" \\\n    --queue XXXXX \\\n    word-count_2.11-1.0.jar\n\n```\n\n7.最后在hdfs文件系统中查看生成的文件，注意\n\n```\nwordCount.saveAsTextFile(\"/user/XXXX/lintong/READMEOUT.txt\")\n\n```\n\n&nbsp;会是一个READMEOUT.txt目录，这个目录下面有part文件\n\n```\nhadoop fs -ls ./lintong/READMEOUT.txt\n\n```\n\n&nbsp;输出\n\n```\nlintong/READMEOUT.txt/_SUCCESS\nlintong/READMEOUT.txt/part-00000\n\n```\n\n&nbsp;\n\n**使用spark-submit部署应用**\n\n一般是在shell脚本中写好，然后运行shell脚本就行了\n\n**spark-submit的详细参数**参考&nbsp;[spark-submit使用及说明](https://my.oschina.net/cjun/blog/509247)\n\n&nbsp;\n\n**在spark任务中认证**\n\n```\nimport org.apache.hadoop.security.UserGroupInformation\nimport org.apache.hadoop.conf.Configuration\nSystem.setProperty(\"java.security.krb5.conf\", \"/etc/krb5.conf\")\nval configuration = new Configuration()\nconfiguration.set(\"hadoop.security.authentication\", \"Kerberos\")\nUserGroupInformation.setConfiguration(configuration)\nUserGroupInformation.loginUserFromKeytab(\"xxx@XXXX\", \"xxx.keytab\")\n\n```\n\n&nbsp;\n","tags":["Spark"]},{"title":"Ubuntu下安装HBase","url":"/Ubuntu下安装hbase.html","content":"1.在清华镜像站点下载hbase的安装文件，选择的是stable的版本，版本号是hbase-1.2.5/\n\n2.解压放在/usr/local的目录下\n\n3.修改权限\n\n```\nsudo chown -R hduser hadoop hbase-1.2.5/\n\n```\n\n4.修改文件夹的名称为hbase\n\n5.在～/.bashrc下添加，之后source一下\n\n```\nexport PATH=$PATH:/usr/local/hbase/bin\n\n```\n\n或者在 /etc/profile中添加\n\n```\nexport HBASE_HOME=/usr/local/hbase\nexport PATH=${HBASE_HOME}/bin:$PATH\n\n```\n\n6.修改文件夹的权限\n\n```\ncd /usr/local\nsudo chown -R hadoop ./hbase\n\n```\n\n7.测试一下是否安装成功\n\n```\nhbase version\nHBase 1.2.5...\n\n```\n\n<!--more-->\n&nbsp;\n\n8.HBase有三种**运行模式**，**单机模式、伪分布式模式、分布式模式**。\n\n**8.1.单机模式**\n\n**1. 配置/usr/local/hbase/conf/hbase-env.sh 。**\n\n配置Java环境变量，并添加配置HBASE_MANAGES_ZK为true，用vi命令打开并编辑hbase-env.sh，命令如下\n\nHBASE_MANAGES_ZK为true的时候使用的Hbase自带的zk，使用jps来查看的时候进程的名字前面都是带h的，比如：HRegionServer、HQuorumPeer、HMaster\n\n```\nvi /usr/local/hbase/conf/hbase-env.sh\n\n```\n\n```\nexport JAVA_HOME=/usr/lib/java8/jdk1.8.0_65\nexport HBASE_MANAGES_ZK=true\n\n```\n\n**2. 配置/usr/local/hbase/conf/hbase-site.xml。**\n\n```\nvi /usr/local/hbase/conf/hbase-site.xml\n\n```\n\n&nbsp;在启动HBase前需要设置属性hbase.rootdir，用于指定HBase数据的存储位置，因为如果不设置的话，hbase.rootdir默认为/tmp/hbase-${user.name},这意味着每次重启系统都会丢失数据。此处设置为HBase安装目录下的hbase-tmp文件夹即（/usr/local/hbase/hbase-tmp）,添加配置如下：\n\n下面还指定了Hbase自带zk的端口:2183，外部的zk的端口是2182\n\n```\n<configuration>\n<!--    <property>\n                <name>hbase.rootdir</name>\n                <value>hdfs://master:9000</value>\n        </property>-->\n        <property>\n                <name>hbase.rootdir</name>\n                <value>file:///home/lintong/software/apache/hbase-1.2.6/hbase-tmp</value>\n        </property>\n        <property>\n                <name>hbase.zookeeper.property.clientPort</name>\n                <value>2183</value>\n        </property>\n        <property>\n                <name>hbase.cluster.distributed</name>\n                <value>true</value>\n        </property>\n</configuration>\n\n```\n\n&nbsp;\n\n3. 接下来测试运行。首先切换目录至HBase安装目录/usr/local/hbase；再启动HBase。命令如下：\n\n```\ncd /usr/local/hbase\nbin/start-hbase.sh\nbin/hbase shell\n\n```\n\nsudo bin/start-hbase.sh用于**启动HBase**\n\nbin/hbase shell用于**打开shell命令行模式**，用户可以通过输入shell命令操作HBase数据库。\n\n**停止HBase运行**,命令如下：\n\n```\nsudo bin/stop-hbase.sh\n\n```\n\n&nbsp;\n","tags":["HBase"]},{"title":"Ubuntu下搜狗输入法只显示黑框，不显示输入的汉字选项","url":"/Ubuntu下搜狗输入法只显示黑框，不显示输入的汉字选项.html","content":"1. cd ~/.config\n\n2.删除三个文件夹: SogouPY, SogouPY.users, sogou-qimpanel\n\n然后重启输入法\n","tags":["Linux"]},{"title":"Scala学习笔记——内建控制结构","url":"/Scala学习笔记——内建控制结构.html","content":"Scala的**内建控制结构**包括：if、while、for、try、match和函数调用\n\n**1.if表达式**\n\n```\n    //常见的写法\n    var filename = \"name\"\n    if (!args.isEmpty)\n      filename = args(0)\n\n    //比较简洁的写法\n    var filename1 =\n      if (!args.isEmpty) args(0)\n      else \"name\"\n\n    //更简洁的写法，不要有中间变量\n    println(if(!args.isEmpty) args(0) else \"name\")\n\n```\n\n<!--more-->\n&nbsp;\n\n**2.while循环，while循环和其他语言的一样，有while和do while**\n\n**　　Scala中对再次赋值语句的返回值是Unit，比如下面这个例子**\n\n**<img src=\"/images/517519-20170404210836503-1884336325.png\" alt=\"\" width=\"783\" height=\"146\" />**\n\n&nbsp;\n\n**3.for表达式**\n\n```\n    //列出当前目录的文件和文件夹\n    val filesHere = (new java.io.File(\".\")).listFiles\n\n    for (file <- filesHere)\n      println(file)\n\n    //打印1到4\n    for (i <- 1 to 4)\n      println(i)\n\n    //打印1到3\n    for (i <- 1 until 4)\n      println(i)\n\n    //for循环中的过滤功能\n    for (file <- filesHere if file.getName.endsWith(\"project\"))\n      println(file)\n\n    //for循环中的过滤功能，多个条件用；号分隔\n    for (file <- filesHere\n         if file.isFile;\n         if file.getName.endsWith(\"sbt\")\n    ) println(file)\n\n    //嵌套枚举\n    for( a <- 1 to 3; b <- 1 to 3){\n      println( \"Value of a: \" + a );\n      println( \"Value of b: \" + b );\n    }\n\n    //for循环采用yield,可以从存储中返回for循环中的变量的值，输出List(1, 2, 4, 5, 6, 7)\n    val numList = List(1,2,3,4,5,6,7,8,9,10)\n    System.out.println(\n      for{\n        a <- numList if a != 3; if a < 8\n      }yield a\n    )\n\n```\n\n&nbsp;\n\n**4.使用try表达式处理异常**\n\n&nbsp;　　抛出异常\n\n```\n//抛出异常\n  def isEven(n : Int): Unit ={\n    val half =\n      if (n % 2 == 0) n / 2\n      else throw new RuntimeException(\"n必须是偶数\")\n  }\n\n```\n\n&nbsp;　　捕获异常，finally语句\n\n```\n　　val file = new FileReader(\"input.txt\")\n    try {\n      //使用文件\n    } catch {\n      //捕获异常\n      case ex: FileNotFoundException =>\n      case ex: IOException =>\n    } finally {\n      //确保文件关闭\n      file.close()\n    }\n\n```\n\n&nbsp;　　catch子语句的返回值\n\n```\n//try-catch-finally语句的返回值\n  def urlFor(path:String) =\n    try{\n      new URL(path)\n    }catch{\n      case e:MalformedURIException => new URL(\"www.scala-lang.org\")\n    }\n\n```\n\n　　避免使用finally子句返回值\n\n&nbsp;<img src=\"/images/517519-20170404223508503-1844360166.png\" alt=\"\" width=\"664\" height=\"226\" />\n\n&nbsp;\n\n**5.匹配（match）表达式**\n\n　　Scala的match表达式类似于**switch语句**，其中 _ 表示其他的情况\n\n　　match表达式中的每一个备选项中**break是隐含的**，也就是不允许从一个备选项中落到下一个备选项中\n\n```\n    //匹配表达式\n    val firstArg = if (args.length > 0) args(0) else \"\"\n    firstArg match {\n      case \"1\" => println(\"A\")\n      case \"2\" => println(\"B\")\n      case \"3\" => println(\"C\")\n      case _ => println(\"D\")\n    }\n\n```\n\n&nbsp;\n\n**6.Scala中不再使用break和continue**\n\n可以用if替换每个continue，用布尔变量来替换每个break\n","tags":["Scala"]},{"title":"Scala学习笔记——函数式对象","url":"/Scala学习笔记——函数式对象.html","content":"**用创建一个函数式对象（<strong>类Rational**）的过程来说明</strong>\n\n类Rational是一种表示有理数（Rational number）的类\n\n```\npackage com.scala.first\n\n/**\n  * Created by common on 17-4-3.\n  */\nobject Rational {\n  def main(args: Array[String]) {\n\n    var r1 = new Rational(1, 2)\n    var r2 = new Rational(1)\n    System.out.println(r1.toString)\n    System.out.println(r1.add(r2).toString)\n    var r3 = new Rational(2, 2)\n    System.out.println(r3)\n    System.out.println(r1 + r3)\n  }\n}\n\nclass Rational(n: Int, d: Int) {\n  //检查先决条件，不符合先决条件将抛出IllegalArgumentException\n  require(d != 0)\n  //最大公约数\n  private val g = gcd(n.abs, d.abs)\n\n  private def gcd(a: Int, b: Int): Int = {\n    if (b == 0) a else gcd(b, a % b)\n  }\n\n  //进行约分\n  val numer: Int = n / g\n  val denom: Int = d / g\n\n  //辅助构造器\n  def this(n: Int) = this(n, 1)\n\n  //定义操作符\n  def +(that: Rational): Rational = {\n    new Rational(\n      numer * that.denom + that.numer * denom,\n      denom * that.denom\n    )\n  }\n\n  //方法重载\n  def +(i: Int): Rational = {\n    new Rational(\n      numer + i * denom, denom\n    )\n  }\n\n  def *(that: Rational): Rational = {\n    new Rational(\n      numer * that.numer,\n      denom * that.denom\n    )\n  }\n\n  //方法重载\n  override def toString = numer + \"/\" + denom\n\n  //定义方法\n  def add(that: Rational): Rational = {\n    new Rational(\n      numer * that.denom + that.numer * denom,\n      denom * that.denom\n    )\n  }\n\n  //定义方法，自指向this可写可不写\n  def lessThan(that: Rational): Boolean = {\n    this.numer * that.denom < that.numer * this.denom\n  }\n\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["Scala"]},{"title":"Spark学习笔记——安装和WordCount","url":"/Spark学习笔记——安装和WordCount.html","content":"**1.**去清华的镜像站点下载文件spark-2.1.0-bin-without-hadoop.tgz，不要下spark-2.1.0-bin-hadoop2.7.tgz\n\n**2.**把文件解压到/usr/local目录下，解压之后的效果，Hadoop和Spark都在**Hadoop用户**下\n\n**下面的操作都在Hadoop用户下**\n\n```\ndrwxrwxrwx 13 hadoop hadoop 4096 4月<!--more-->\n&nbsp;&nbsp; 4 11:50 spark-2.1.0-bin-without-hadoop/\n\n```\n\n&nbsp;添加Hadoop用户和用户组\n\n```\n$ sudo addgroup hadoop\n$ sudo adduser --ingroup hadoop hadoop\n$ sudo adduser hadoop sudo\n\n```\n\n&nbsp;然后修改文件夹的用户，用户组以及权限\n\n```\nsudo chown -R hduser:hadoop spark-2.1.0-bin-without-hadoop\nsudo chmod 777 hadoop/\n\n```\n\n&nbsp;Hadoop文件夹如果权限不对的话，也需要修改\n\n**3.**在/etc/profile下添加路径\n\n```\nexport SPARK_HOME=/usr/local/spark-2.1.0-bin-without-hadoop\nexport PATH=${SPARK_HOME}/bin:$PATH\n\n```\n\n**4.**还需要修改Spark的配置文件spark-env.sh\n\n```\ncd /usr/local/spark-2.1.0-bin-without-hadoop\ncp ./conf/spark-env.sh.template ./conf/spark-env.sh\n\n```\n\n添加如下\n\n```\nexport SPARK_DIST_CLASSPATH=$(/home/lintong/software/apache/hadoop-2.9.1/bin/hadoop classpath)\n\n```\n\n&nbsp;（以上可以参考厦门大学林子雨老师的教程&mdash;&mdash;[Spark2.1.0入门：Spark的安装和使用](http://dblab.xmu.edu.cn/blog/1307-2/)），有些教程坑无数\n\n**5.**在~/coding/coding/Scala/word-count路径下准备一个文本文件，比如test.segmented文件\n\n**6.**在该目录下，在终端运行 **spark-shell**\n\n<img src=\"/images/517519-20170403153849753-1451710534.png\" alt=\"\" />\n\n创建一个RDD\n\n```\nscala> val textFile = sc.textFile(\"file:///home/common/coding/coding/Scala/word-count/test.segmented\")\n\n```\n\n保存RDD成文件\n\n```\ntextFile.saveAsTextFile(\"file:///home/common/coding/coding/Scala/word-count/writeback\")\n\n```\n\n&nbsp;这时候会发现在文件夹目录下多了writeback目录，目录下是这么几个文件\n\n<img src=\"/images/517519-20170403154242785-678053673.png\" alt=\"\" />\n\n&nbsp;现在，我们**建立hdfs文件夹**，来把 test.segmented 文件放进我们的hdfs文件夹中\n\n首先，**启动Hadoop的HDFS组件**，因为没有用到MapReduce组件，所以没有必要启动MapReducen或者YARN\n\n```\ncd /usr/local/hadoop\n./sbin/start-dfs.sh\n\n```\n\n&nbsp;在**HDFS文件系统**中，建立文件夹\n\n```\n./bin/hdfs dfs -mkdir -p /user/hadoop\n\n```\n\n使用命令**查看**一下HDFS文件系统中的目录和文件\n\n在Hadoop文件夹下运行命令\n\n```\n./bin/hdfs dfs -ls .       #或者\n./bin/hdfs dfs -ls /user/hadoop\n\n```\n\n&nbsp;或者直接\n\n```\nhadoop fs -ls /user/hadoop    #或者\nhadoop fs -ls .\n\n```\n\n&nbsp;把刚刚的 test.segmented** 文件上传到分布式文件系统HDFS**中（放到hadoop用户目录下）\n\n```\nhadoop fs -put /home/common/coding/coding/Java/WordCount/input/test.segmented .\n\n```\n\n&nbsp;再次**查看**一下\n\n```\nhadoop@master:~$ hadoop fs -ls /user/hadoop\nFound 2 items\ndrwxr-xr-x   - hadoop supergroup          0 2017-04-03 16:18 /user/hadoop/QuasiMonteCarlo_1491207499210_758373570\n-rw-r--r--   1 hadoop supergroup         59 2017-04-03 16:43 /user/hadoop/test.segmented\n\n```\n\n如果需要**删除**\n\n```\nhadoop fs -rm /user/hadoop/test.segmented\n\n```\n\n查看一个文件的大小\n\n```\nhadoop fs -du -h /logs/xxxx\n\n```\n\n现在**回到 ****spark-shell 窗口**，编写代码从HDFS文件系统加载&nbsp;test.segmented 文件\n\n并打印文件中的第一行内容\n\n```\nscala> val textFile = sc.textFile(\"hdfs://master:9000/user/hadoop/test.segmented\")\ntextFile: org.apache.spark.rdd.RDD[String] = hdfs://master:9000/user/hadoop/test.segmented MapPartitionsRDD[1] at textFile at <console>:24\n\nscala> textFile.first()\nres0: String = aa bb aa\n\n```\n\n&nbsp;如果是单机的话，其中下面两条语句和上面第一条语句是一样的，但是如果是Hadoop伪分布式或者分布式的话，就不行\n\n```\nval textFile = sc.textFile(\"/user/hadoop/test.segmented\")\n\n```\n\n&nbsp;再次把textFile写回到HDFS文件系统中\n\n```\ntextFile.saveAsTextFile(\"hdfs://master:9000/user/hadoop/writeback\")\n\n```\n\n再次查看\n\n```\nhadoop@master:~$ hadoop fs -ls /user/hadoop\nFound 3 items\ndrwxr-xr-x   - hadoop supergroup          0 2017-04-03 16:18 /user/hadoop/QuasiMonteCarlo_1491207499210_758373570\n-rw-r--r--   1 hadoop supergroup         59 2017-04-03 16:43 /user/hadoop/test.segmented\ndrwxr-xr-x   - hadoop supergroup          0 2017-04-03 17:10 /user/hadoop/writeback\n\n```\n\n如果进入writeback文件夹中查看的话，可以看到里面的文件的内容和test.segmented中的是一样的\n\n```\nhadoop@master:~$ hadoop fs -ls /user/hadoop/writeback\nFound 3 items\n-rw-r--r--   3 hadoop supergroup          0 2017-04-03 17:10 /user/hadoop/writeback/_SUCCESS\n-rw-r--r--   3 hadoop supergroup         36 2017-04-03 17:10 /user/hadoop/writeback/part-00000\n-rw-r--r--   3 hadoop supergroup         24 2017-04-03 17:10 /user/hadoop/writeback/part-00001\n\n```\n\n```\nhadoop@master:~$ hadoop fs -cat /user/hadoop/writeback/part-00000\naa bb aa\nbb aa aa\ncc bb ee\ndd ee cc\n\n```\n\n```\nhadoop@master:~$ hadoop fs -cat /user/hadoop/writeback/part-00001\naa\ncc\nee\nff\nff\ngg\nhh\naa\n\n```\n\n&nbsp;\n\n现在进入**WordCount阶段**，再次进入 **Spark-shell** 中\n\n```\nval textFile = sc.textFile(\"hdfs://master:9000/user/hadoop/test.segmented\")\nval wordCount = textFile.flatMap(line => line.split(\" \")).map(word => (word, 1)).reduceByKey((a, b) => a + b)\nwordCount.collect()\n\n```\n\n&nbsp;输出\n\n```\nres6: Array[(String, Int)] = Array((ee,3), (aa,6), (gg,1), (dd,1), (hh,1), (ff,2), (bb,3), (cc,3))\n\n```\n\n&nbsp;\n\n&nbsp;**在spark-shell下面运行成功之后，就需要试着在idea里面建立一个工程来运行这段代码**\n\n**在idea下面建立一个Scala的工程，构建的方式选择是sbt**\n\n**<img src=\"/images/517519-20170404140301285-1875803098.png\" alt=\"\" width=\"990\" height=\"320\" />**\n\n&nbsp;\n\n由于本机的Scala的版本是2.11.8\n\n<img src=\"/images/517519-20170404141009738-94649904.png\" alt=\"\" />\n\n所以在project structure里面设置成2.11.8\n\n<img src=\"/images/517519-20170404141118863-2091736797.png\" alt=\"\" width=\"697\" height=\"481\" />\n\n**接着在build.sbt里面写**\n\n```\nname := \"word-count\"\n\nversion := \"1.0\"\n\nscalaVersion := \"2.11.8\"\n\nlibraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"2.1.0\"\n\n```\n\n&nbsp;注意里面的scalaVersion如果是2.11.X的话，sbt就会去拉spark-core_2.11-2.1.0的包\n\n可以去公司的私服nexus里面去看看有没有这个包\n\n**然后在WordCount.scala文件中写入我们的代码**\n\n**注意如果是setMaster(\"local\")的话，需要在/etc/hosts中设置127.0.1.1，然后取消192.168.0.1**\n\n```\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\n/**\n  * Created by common on 17-4-3.\n  */\n\n\nobject WordCount {\n  def main(args: Array[String]) {\n    val inputFile =  \"file:///home/common/coding/coding/Scala/word-count/test.segmented\"\n    val conf = new SparkConf().setAppName(\"WordCount\").setMaster(\"local\")　　　　#创建一个SparkConf对象来配置应用<br />　　　　#集群URL：告诉Spark连接到哪个集群，local是单机单线程，无需连接到集群，应用名：在集群管理器的用户界面方便找到应用\n    val sc = new SparkContext(conf)　　　　　　　　#然后基于这SparkConf创建一个SparkContext对象\n    val textFile = sc.textFile(inputFile)　　　　#读取输入的数据\n    val wordCount = textFile.flatMap(line => line.split(\" \")).map(word => (word, 1)).reduceByKey((a, b) => a + b)　　#切分成单词，转换成键值对并计数\n    wordCount.foreach(println)\n  }\n}\n\n```\n\n然后在sbt中refresh，进行拉包，拉包的过程是无比缓慢的\n\n拉好了之后运行的结果\n\n<img src=\"/images/517519-20170404141529816-1881109987.png\" alt=\"\" />\n\n和在 spark-shell中运行的结果是一致的\n\n&nbsp;\n\n在林子雨老师的教程中，[Spark2.1.0入门：第一个Spark应用程序：WordCount](http://dblab.xmu.edu.cn/blog/1311-2/)\n\n最后是将整个应用程序打包成JAR，然后通过 spark-submit 提交到 Spark 中运行\n\n做法是在idea的终端中，对代码进行**打包**\n\n```\ncommon@master:~/coding/coding/Scala/word-count$ sbt package\n[info] Loading project definition from /home/common/coding/coding/Scala/word-count/project\n[info] Set current project to word-count (in build file:/home/common/coding/coding/Scala/word-count/)\n[info] Compiling 1 Scala source to /home/common/coding/coding/Scala/word-count/target/scala-2.11/classes...\n[info] Packaging /home/common/coding/coding/Scala/word-count/target/scala-2.11/word-count_2.11-1.0.jar ...\n[info] Done packaging.\n[success] Total time: 6 s, completed 2017-4-4 18:02:13\n\n```\n\n&nbsp;生成的jar包位置在\n\n```\n/home/common/coding/coding/Scala/word-count/target/scala-2.11\n\n```\n\n&nbsp;最后通过 spark-submit 运行程序，将jar包通过这个命令提交到 Spark 中运行\n\n```\ncommon@master:~/coding/coding/Scala/word-count$ spark-submit --class \"WordCount\"  /home/common/coding/coding/Scala/word-count/target/scala-2.11/word-count_2.11-1.0.jar\n\n```\n\n&nbsp;运行结果\n\n<img src=\"/images/517519-20170404180945082-9627841.png\" alt=\"\" />\n\n&nbsp;\n\n在执行spark任务的时候，如果遇到\n\n```\n报如下错误:\nException in thread \"main\" java.lang.Exception: When running with master 'yarn-client' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment\n\n```\n\n在spark的配置文件 conf/spark-env.sh 中添加\n\n```\nexport HADOOP_HOME=/home/lintong/software/apache/hadoop-2.9.1\nexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\nPATH=$PATH:$HIVE_HOME/bin:$HADOOP_HOME/bin\n\n```\n\n&nbsp;\n\nspark集群安装参考：[spark 集群搭建 详细步骤](https://blog.csdn.net/bitcarmanlee/article/details/51967323)\n\n主要是配置slave文件和spark-env文件\n\n集群内容spark-env文件，其中xxx是spark web ui的端口\n\n```\nexport SPARK_DIST_CLASSPATH=$(/usr/bin/hadoop classpath)\n\n#export JAVA_HOME=/usr/lib/jvm/java-8-oracle\nexport SCALA_HOME=/home/dl/packages/scala-2.11.8\n\nexport SPARK_MASTER_WEBUI_PORT=xxxx\n\nexport HADOOP_HOME=/opt/cloudera/parcels/CDH-5.12.0-1.cdh5.12.0.p0.29/lib/hadoop\nexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\nexport HIVE_HOME=/opt/cloudera/parcels/CDH-5.12.0-1.cdh5.12.0.p0.29/lib/hive\nPATH=$PATH:$HIVE_HOME/bin:$HADOOP_HOME/bin\n\n```\n\n&nbsp;\n\nspark-shell查看当前spark的配置\n\n```\nscala> spark.sql(\"set\").filter(\"key rlike 'metastore|jdo'\").rdd.take(100).foreach(println(_))\n[spark.sql.hive.metastore.sharedPrefixes,com.amazonaws.services.dynamodbv2]\n\n```\n\n　　\n","tags":["Spark"]},{"title":"Scala学习笔记——入门","url":"/Scala学习笔记——入门.html","content":"**0.在 scala> 下运行Scala程序**\n\n**首先cd到.scala文件所在的目录下**\n\n**scalac这个scala文件，然后import package的名字.object的名字**\n\n**然后就能使用 object的名字.def的名字来运行这个def**\n\n<!--more-->\n&nbsp;\n\n**1.表达式**\n\n```\nscala> 1 + 2\nres0: Int = 3\n\n```\n\n&nbsp;\n\n**2.变量定义,Scala中有两种变量,一种是var,一种val,val类似于Java里面的final变量,一旦初始化了,val就不能再次被赋值了**\n\n```\nscala> var str = \"Hello World\"\nstr: String = Hello World\n#lazy关键字修饰的变量，定义时不赋值，真正使用的时候才赋值\nvar str = \"Hello World\"\n\n```\n\n&nbsp;\n\n**3.函数定义**\n\n**<img src=\"/images/517519-20170401105850274-707947926.png\" alt=\"\" />**\n\n```\nobject HelloWord {\n\n  def main(args: Array[String]) {\n    println(\"Hello Word\")\n    println(max(1, 2))\n    println(args(0)+\" \"+args(1))\n  }\n\n  def max(x: Int, y: Int): Int = {\n    if (x > y)\n      x\n    else\n      y\n  }\n}\n\n```\n\n&nbsp;输出\n\n```\nHello Word\n2\n0 1\n\n```\n\n&nbsp;\n\n**4.while循环和if判断**\n\n```\n  def printArg(args: Array[String]): Unit = {\n    var i = 0\n    while (i < args.length) {\n      println(args(i))\n      i += 1\n    }\n  }\n\n```\n\n&nbsp;\n\n**5.foreach和for做枚举**\n\n```\n  def printArg1(args: Array[String]): Unit = {\n    //scala的foreach循环\n    args.foreach(arg => println(arg))\n    //foreach的简洁写法\n    args.foreach(println)\n    //args给arg传值\n    for(arg <- args)\n      println(arg)\n  }\n\n```\n\n&nbsp;\n\n**6.类型参数化数组**\n\n```\n  def main(args: Array[String]) {\n    var big = new BigInteger(\"123\")\n    val arr = new Array[String](3)\n    arr(0) = \"1\"\n    arr(1) = \"2\"\n    arr(2) = \"3\"\n    //或者\n    arr.update(0,\"1\")\n    arr.update(1,\"2\")\n    arr.update(2,\"3\")\n    for(i <- 0 to 2){\n      print (arr(i))\n    }\n    //最为简洁的创建和初始化数组的方法\n    var numArr = Array(\"1\",\"2\",\"3\")\n\n  }\n\n```\n\n&nbsp;\n\n**7.列表List**\n\n```\n    var list1 = List(1,2)\n    var list2 = List(3,4)\n    var list3 = list1:::list2\n    println (list3)\n    var list4 = 0::list3\n    println (list4)\n\n```\n\n输出\n\n```\nList(1, 2, 3, 4)\nList(0, 1, 2, 3, 4)\n\n```\n\n**&nbsp;一些方法**\n\n```\nprintln (list4(2))\nprintln (list4.count(s => s == 2))\n\n```\n\n&nbsp;输出\n\n```\nList(1, 2, 3, 4, 2)\n2\n2\n\n```\n\n&nbsp;<img src=\"/images/517519-20170401181106633-509462620.png\" alt=\"\" width=\"618\" height=\"476\" />\n\n<img src=\"/images/517519-20170401181114305-1772327897.png\" alt=\"\" width=\"623\" height=\"402\" />\n\n&nbsp;\n\n**8.元组Tuple**\n\n&nbsp;元组和列表都是不可变的，但是元组**可以包含不同类型的元素**\n\n```\nvar pair = (123,\"123\")\nprintln (pair._1)\nprintln (pair._2)\n\n```\n\n&nbsp;输出\n\n```\n123\n123\n\n```\n\n&nbsp;\n\n**9.集合（set）和映射（map）**\n\n**集合set**和映射具有可变性和不可变性\n\n```\nvar jetSet = Set(\"a\",\"b\")\njetSet += \"c\"　　　　　　//只有可变mutable集合有+=方法，如果是import scala.collection.immutable.Set，将会转换成mutable\nprintln(jetSet.contains(\"c\"))\n\n```\n\n输出\n\n```\ntrue\n\n```\n\n**&nbsp;映射map**\n\n```\nvar map1 = Map[Int,String]()\nmap1 += (1 -> \"1111\")　　　//映射和集合也是一样的，映射也有可变和不可变之分\nmap1 += (2 -> \"2222\")\nmap1 += (3 -> \"3333\")\nprintln (map1)\n\n```\n\n&nbsp;输出\n\n```\nMap(1 -> 1111, 2 -> 2222, 3 -> 3333)\n\n```\n\n**&nbsp;直接定义映射**\n\n```\nvar map2 = Map(1 -> 1111, 2 -> 2222, 3 -> 3333)\nprintln (map2)\n\n```\n\n**&nbsp;**\n\n**10函数式编程**\n\n从\n\n```\nwhile (i < args.length) {\n      println(args(i))\n      i += 1\n}\n\n```\n\n&nbsp;到\n\n```\nfor(arg <- args)\n      println(arg)\n\n```\n\n&nbsp;的过程就是**函数式编程**\n\n&nbsp;\n\n**11.从文件里读取文本行**\n\n```\nif(args.length > 0){\n      for(line <- Source.fromFile(args(0)).getLines){\n        println(line.length+\" \"+line)\n      }\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20170401190008695-1160044334.png\" alt=\"\" />\n\n```\n//行数数字的长度\n  def widthOfLength(s: String) = s.length.toString.length\n\n  def readFile(args: Array[String]): Unit = {\n    if (args.length > 0) {\n      var lines = Source.fromFile(args(0)).getLines.toList\n      //有空格\n      var maxWidth = 0\n      for (line <- lines) {\n        maxWidth = maxWidth.max(widthOfLength(line))\n      }\n      for (line <- lines) {\n        var numSpace = maxWidth - widthOfLength(line)\n        var padding = \" \" * numSpace\n        println(padding + line.length + \" | \" + line)\n      }\n      //无空格\n      //      for(line <- Source.fromFile(args(0)).getLines){\n      //        println(line.length+\" \"+line)\n      //      }\n    }\n  }\n\n```\n\n&nbsp;\n\n**12.类和对象**\n\n　　1.Scala中不用加上分号,但是如果一行中有多个语句的话,分号是必须的\n\n　　2.Scala比Java更为面向对象的特点之一是**Scala不能定义静态变量**,而是代之以**定义单例对象(singleton object)**,所要做的就是**用object关键字取代class关键字**.\n\n　　　　当**单例对象Object**和**某个类**共享同**一个名称**的时候,它就被称为是这个类的**伴生对象**.**类和它的伴生对象可以互相访问气私有对象**.\n\n```\nobject ChecksumAccumulator {\n  private var cache = Map[String, Int]()\n\n  def main(args: Array[String]) {\n    System.out.println(calculate(\"abc\"))\n    System.out.println(calculate(\"a\"))\n    System.out.println(cache)\n  }\n\n  def calculate(s: String): Int = {\n    if (cache.contains(s))\n      cache(s)\n    else {\n      val acc = new ChecksumAccumulator\n      for (c <- s)\n        acc.add(c.toByte)\n      val cs = acc.checkSum()\n      cache += (s -> cs)  //把map赋值给cache\n      cs\n    }\n  }\n\n}\n\nclass ChecksumAccumulator {\n  private var sum = 0\n\n  def add(b: Byte): Unit = sum += b //不超过-128-127\n\n  def checkSum(): Int = ~(sum &amp; 0xFF) + 1\n}\n\n```\n\n&nbsp;　　3.除了使用main方法来定义程序的入口之外，Scala还提供了**特质scala.Application**，可以减少一些输入工作。\n\n```\nimport ChecksumAccumulator.calculate\n\nobject FallWinterSpringSummer extends Application{\n  for (season <- List(\"fall\",\"winter\",\"spring\",\"winter\")){\n    println(season+\": \"+calculate(season))\n  }\n}\n\n```\n\n&nbsp;\n\n**13.基本类型**\n\n&nbsp;<img src=\"/images/517519-20170402161800258-858596862.png\" alt=\"\" />\n\n**在Scala中 == 是比较内容，而在Java中是比较引用，这是Scala和Java中不同的地方**\n\n**运算符优先级**\n\n<img src=\"/images/517519-20170415225611033-2093073672.png\" alt=\"\" width=\"433\" height=\"453\" />\n\n&nbsp;\n\n**14.字面量**\n\n**14.1&mdash;&mdash;<strong>整数字面量**</strong>\n\n类型Int，Long，Short和Byte的整数字面量有3种格式：10进制，16进制（以0x或者0X开头），8进制（以0开头）\n\n如果整数字面量是以L或者l结尾的话，就是**Long类型**\n\n如果Int类型的字面量被赋值给Short或者Byte类型的变量，字面量就会被当成是被赋值的类型，以便让字面量值处于有效范围内\n\n**14.2&mdash;&mdash;浮点<strong>字面量**</strong>\n\n浮点数字面量是由十进制数字、可选的小数点、可选的E或者e及指数部分组成的。\n\n**14.3&mdash;&mdash;<strong>字符字面量**</strong>\n\n字符字面量可以是在单引号之间的任何Unicode字符，如\n\n```\nval a = 'A'\nval a = '\\101'\nval a = '\\u0041'\nval B\\u0041\\u0044 = 1\n\n```\n\n&nbsp;<img src=\"/images/517519-20170402165542492-463605460.png\" alt=\"\" width=\"258\" height=\"254\" />\n\n**14.4&mdash;&mdash;<strong>字符串字面量**</strong>\n\n**<strong><img src=\"/images/517519-20170402170310195-1833888993.png\" alt=\"\" width=\"647\" height=\"565\" />**</strong>\n\n**14.5&mdash;&mdash;符号<strong>字面量**</strong>\n\n符号字面量被写成**&lsquo;<标识符>**，这里的<标识符>可以是**任何字母或者数字**的标识符。这种字面量被映射成预定义类scala.Symbol的实例，可以使用在更新数据库记录的方法中\n\n```\n  def updateRecordByName(r:Symbol,value:Any): Unit ={\n\n  }\n\n```\n\n&nbsp;\n\n```\nupdateRecordByName('a,\"a\")\n\n```\n\n**&nbsp;如果同一个符号字面量出现两次，那么两个字面量指向的是同一个Symbol对象**\n\n&nbsp;\n\n**15.操作符和方法，Scala的数学运算，关系和逻辑操作，位操作符，和Java的差不多<br />**\n\nScala的操作符+支持重载，比如1+1L的结果就是Long型的\n\n&nbsp;\n\n**16.对象相等性**\n\n如果想要比**较一下两个对象是否相等**，可以使用==，**==操作符**对所有的对象都起作用，而**不仅仅是基本类型**\n\n&nbsp;\n\n**17.富包装器**\n\n**<img src=\"/images/517519-20170403145419753-2093184497.png\" alt=\"\" />**\n\n&nbsp;\n\n&nbsp;\n\n**18. scala中的 <:**\n\n```\n边界是 \"<:\"，下边界是 \">:\"\nT <: Animal的意思是：T必须是Animal的子类\n　　\n1. class Home[P <: Person]  表示P是Person接口的一个具体实现\n2. type Currency <: AbstractCurrency 表示抽象类Currency是抽象类AbstractCurrency的子类\n\n```\n\n参考：\n\n```\nhttps://blog.csdn.net/i6448038/article/details/52061287\nhttps://stackoverflow.com/questions/6828875/what-does-mean-in-scala/6829035\n\n```\n\n&nbsp;\n\n**19. scala中的ClassTag**\n\n参考：\n\n[如何使用Scala的ClassTag](https://zhuanlan.zhihu.com/p/69792401)\n","tags":["Scala"]},{"title":"Scala学习笔记——安装","url":"/Scala学习笔记——安装.html","content":"安装scala，不要使用sudo apt-get install scala来安装\n\n1.从下面网址来下载Scala文件\n\n```\nhttp://www.scala-lang.org/download/2.11.8.html\n\n```\n\n2.下载下的 scala-2.11.8.tgz 文件解压，然后把文件mv到/usr/local目录下\n\n3.在/etc/profile目录下加上,不要忘记了source\n\n```\nexport SCALA_HOME=/usr/local/scala-2.11.8\nexport PATH=$SCALA_HOME/bin:$PATH\n\n```\n\n<!--more-->\n&nbsp;4.打软链接\n\n```\nsudo ln -s /usr/local/scala-2.11.8/bin/scala /usr/bin\n\n```\n\n&nbsp;5.在终端中输入scala\n\n```\nWelcome to Scala 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_65).\nType in expressions for evaluation. Or try :help.\n\nscala> \n\n```\n\n6.退出\n\n```\nscala> :q\n\n```\n\n　　\n","tags":["Scala"]},{"title":"Maven常用命令","url":"/Maven常用命令.html","content":"**Apache官方仓库**\n\n```\nhttps://repository.apache.org/\n\n```\n\n**Maven中央仓库**　\n\n```\nhttp://mvnrepository.com/\n\n```\n\n**Maven介绍，包括作用、核心概念、用法、常用命令、扩展及配置**\n\n```\nhttp://www.trinea.cn/android/maven/\n```\n\n**Maven常用命令**\n\n```\nhttp://www.cnblogs.com/kingfy/p/5665218.html\n\n```\n\n　　\n\n**1. 创建Maven的普通java项目：<!--more-->\n&nbsp;**\n\n有下面几种不同的DarchetypeArtifactId\n\n1.省略默认是maven-archetype-quickstart　　\n\n```\nmvn archetype:generate -DgroupId=com.xxxx.xxxx -DartifactId=test-project -DarchetypeCatalog=internal -DarchetypeArtifactId=maven-archetype-quickstart\n\n```\n\n工程目录结构\n\n<img src=\"/images/517519-20181027155856057-444193222.png\" alt=\"\" />\n\n2.maven-archetype-archetype&nbsp;&nbsp; &nbsp;包含一个archetype的例子，主要用于当我们要建立自己的archetype的时候\n\n工程目录结构\n\n<img src=\"/images/517519-20181027155522256-874996196.png\" alt=\"\" /><br />3.maven-archetype-j2ee-simple&nbsp;&nbsp; &nbsp;包含一个简单的j2ee应用的例子\n\n工程目录结构\n\n<img src=\"/images/517519-20181028155803987-814477204.png\" alt=\"\" />\n\n4.maven-archetype-plugin&nbsp;&nbsp; &nbsp;包含一个Maven plugin的例子\n\n工程目录结构\n\n<img src=\"/images/517519-20181028155217061-1466539984.png\" alt=\"\" />\n\n5.maven-archetype-plugin-site&nbsp;&nbsp; &nbsp;包含一个Maven plugin site的例子\n\n6.Maven-archetype-portlet&nbsp;&nbsp; &nbsp;包含一个portlet的例子\n\n打包失败<br />7.maven-archetype-simple&nbsp;&nbsp; &nbsp;包含一个简单maven项目\n\n创建失败<br />8.Maven-archetype-site&nbsp;&nbsp; &nbsp;包含一个maven site的例子，它能够展示一些支持的文档类型，包括APT、XDoc和FML\n\n工程结构目录\n\n<img src=\"/images/517519-20181028161947758-351389294.png\" alt=\"\" />\n\n9.Maven-archetype-site-simple&nbsp;&nbsp; &nbsp;包含一个maven site的例子\n\n&nbsp;工程目录结构\n\n<img src=\"/images/517519-20181028163250953-505624499.png\" alt=\"\" />\n\n&nbsp;<br />**2. 创建Maven的Web项目**\n\n```\n    mvn archetype:create \n    -DgroupId=packageName    \n    -DartifactId=webappName \n    -DarchetypeArtifactId=maven-archetype-webapp    \n\n```\n\n**3. 清理编译**\n\n```\nmvn clean compile\n\n```\n\n**4. 清理测试**\n\n```\nmvn clean test\n\n```\n\n**5. 清理打包**\n\n```\nmvn clean package\n\n```\n\n**6. 带有依赖打包**\n\n```\nmvn package assembly:single\n\n```\n\n**7.指定编译版本的打包**\n\n```\nmvn package -Dmaven.test.skip=true -Dmaven.compiler.source=1.8 -Dmaven.compiler.target=1.8\n\n```\n\n**8****. 清理安装，会安装到~/.m2/目录下面**\n\n```\nmvn clean install\n\n```\n\n**9. 生成eclipse项目**\n\n```\nmvn eclipse:eclipse  \n\n```\n\n**10. 生成idea项目**\n\n```\nmvn idea:idea  \n\n```\n\n**12. 编译测试的内容**\n\n```\nmvn test-compile  \n\n```\n\n**13. 只打jar包**\n\n&nbsp;\n\n```\nmvn jar:jar  \n\n```\n\n&nbsp;\n\n**14.&nbsp;跳过测试**\n\n```\nmvn install -Dmaven.test.skip=true\n\n```\n\n**15. 指定端口**\n\n```\nmvn -Dmaven.tomcat.port=9090\n\n```\n\n**16. 只测试而不编译，也不测试编译**\n\n```\nmvn test -skipping compile -skipping test-compile ( -skipping 的灵活运用，当然也可以用于其他组合命令) \n\n```\n\n**17. 清除eclipse的一些系统设置:**\n\n```\nmvn eclipse:clean\n\n```\n\n**18. 打包依赖，使用后执行成功后会在target文件夹下多出一个以-jar-with-dependencies结尾的JAR包. 这个JAR包就包含了项目所依赖的所有JAR的CLASS**\n\n```\nmvn assembly:assembly\n\n```\n\n**19.&nbsp;添加 jar 包到本地仓库**\n\n```\nmvn install:install-file \n　　-DgroupId=xxx\n　　-DartifactId=xxx 　　　　\n　　-Dversion=1.0 　　　  //版本号\n　　-Dpackaging=jar      //类型\n　　-Dfile=d:\\xxx-1.0.jar   //jar实际路径\n\nmvn install:install-file -Dfile=jar包的位置 -DgroupId=上面的groupId -DartifactId=上面的artifactId -Dversion=上面的version -Dpackaging=jar\n\n```\n\n20.如果遇到mvn package 内存爆掉的情况，增加内存，然后mvn -version查看内存是多少\n\nexport MAVEN_OPTS=\"-Xmx512m -XX:MaxPermSize=128m\"\n\n&nbsp;\n\n**添加打包插件**\n\n```\n<plugin>\n    <groupId>org.apache.maven.plugins</groupId>\n    <artifactId>maven-compiler-plugin</artifactId>\n    <configuration>\n        <source>1.8</source>\n        <target>1.8</target>\n    </configuration>\n</plugin>\n\n```\n\n　　\n\n**库版本选择**\n\n```\n<dependency>\n<groupId>org.codehaus.plexus</groupId>\n<artifactId>plexus-utils</artifactId>\n<version>[1.1,)</version>\n</dependency>\n表达式 含义\n(,1.0] version<=1.0\n[1.2,1.3] 1.2<=version<=1.3\n[1.0,2.0) 1.0<=version<2.0\n[1.5,) 1.5<=version\n(,1.1),(1.1,) version!=1.1\n\n```\n\n&nbsp;\n\n**pom.xml 基本节点**\n\n```\n<project> 根节点\n<modelversion> pom.xml 使用的对象模型版本\n<groupId> 创建项目的组织或团体的唯一 Id\n<artifactId> 项目唯一Id, 项目名\n<packaging> 打包扩展名(JAR、WAR、EAR)\n<version> 项目版本号\n<name> 显示名，用于生成文档\n<url> 组织站点，用于生成文档\n<description> 项目描述，用于生成文档\n<dependency>之<scope> 管理依赖部署\n\n```\n\n&nbsp;\n\n**依赖管理**\n\n```\n<scope> 可使用 5 个值:\ncompile 缺省值，用于所有阶段，随项目一起发布\nprovided 期望JDK、容器或使用者提供此依赖。如servlet.jar\nruntime 只在运行时使用\ntest 只在测试时使用，不随项目发布\nsystem 需显式提供本地jar，不在代码仓库中查找\n\n```\n\n可以参考《[依赖管理&mdash;&mdash;廖雪峰的官方网站](https://www.liaoxuefeng.com/wiki/1252599548343744/1309301178105890)》\n\n&nbsp;\n\n**自用maven的setting.xml配置文件**\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<!--\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n-->\n\n<!--\n | This is the configuration file for Maven. It can be specified at two levels:\n |\n |  1. User Level. This settings.xml file provides configuration for a single user,\n |                 and is normally provided in ${user.home}/.m2/settings.xml.\n |\n |                 NOTE: This location can be overridden with the CLI option:\n |\n |                 -s /path/to/user/settings.xml\n |\n |  2. Global Level. This settings.xml file provides configuration for all Maven\n |                 users on a machine (assuming they're all using the same Maven\n |                 installation). It's normally provided in\n |                 ${maven.home}/conf/settings.xml.\n |\n |                 NOTE: This location can be overridden with the CLI option:\n |\n |                 -gs /path/to/global/settings.xml\n |\n | The sections in this sample file are intended to give you a running start at\n | getting the most out of your Maven installation. Where appropriate, the default\n | values (values used when the setting is not specified) are provided.\n |\n |-->\n<settings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\"\n          xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n          xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\">\n  <!-- localRepository\n   | The path to the local repository maven will use to store artifacts.\n   |\n   | Default: ${user.home}/.m2/repository\n  <localRepository>/path/to/local/repo</localRepository>\n  -->\n\n  <!-- interactiveMode\n   | This will determine whether maven prompts you when it needs input. If set to false,\n   | maven will use a sensible default value, perhaps based on some other setting, for\n   | the parameter in question.\n   |\n   | Default: true\n  <interactiveMode>true</interactiveMode>\n  -->\n\n  <!-- offline\n   | Determines whether maven should attempt to connect to the network when executing a build.\n   | This will have an effect on artifact downloads, artifact deployment, and others.\n   |\n   | Default: false\n  <offline>false</offline>\n  -->\n\n  <!-- pluginGroups\n   | This is a list of additional group identifiers that will be searched when resolving plugins by their prefix, i.e.\n   | when invoking a command line like \"mvn prefix:goal\". Maven will automatically add the group identifiers\n   | \"org.apache.maven.plugins\" and \"org.codehaus.mojo\" if these are not already contained in the list.\n   |-->\n  <pluginGroups>\n    <!-- pluginGroup\n     | Specifies a further group identifier to use for plugin lookup.\n    <pluginGroup>com.your.plugins</pluginGroup>\n    -->\n  </pluginGroups>\n\n  <!-- proxies\n   | This is a list of proxies which can be used on this machine to connect to the network.\n   | Unless otherwise specified (by system property or command-line switch), the first proxy\n   | specification in this list marked as active will be used.\n   |-->\n  <proxies>\n    <!-- proxy\n     | Specification for one proxy, to be used in connecting to the network.\n     |\n    <proxy>\n      <id>optional</id>\n      <active>true</active>\n      <protocol>http</protocol>\n      <username>proxyuser</username>\n      <password>proxypass</password>\n      <host>proxy.host.net</host>\n      <port>80</port>\n      <nonProxyHosts>local.net|some.host.com</nonProxyHosts>\n    </proxy>\n    -->\n  </proxies>\n\n  <!-- servers\n   | This is a list of authentication profiles, keyed by the server-id used within the system.\n   | Authentication profiles can be used whenever maven must make a connection to a remote server.\n   |-->\n  <servers>\n    <!-- server\n     | Specifies the authentication information to use when connecting to a particular server, identified by\n     | a unique name within the system (referred to by the 'id' attribute below).\n     |\n     | NOTE: You should either specify username/password OR privateKey/passphrase, since these pairings are\n     |       used together.\n     |\n    <server>\n      <id>deploymentRepo</id>\n      <username>repouser</username>\n      <password>repopwd</password>\n    </server>\n    -->\n\n    <!-- Another sample, using keys to authenticate.\n    <server>\n      <id>siteServer</id>\n      <privateKey>/path/to/private/key</privateKey>\n      <passphrase>optional; leave empty if not used.</passphrase>\n    </server>\n    -->\n\t<server>    \n\t\t<id>nexus-releases</id>    \n\t\t<username>admin</username>    \n\t\t<password>xxx</password>    \n\t</server>    \n\t<server>    \n\t\t<id>nexus-snapshots</id>    \n\t\t<username>admin</username>    \n\t\t<password>xxx</password>    \n\t</server>\n\n  </servers>\n\n  <!-- mirrors\n   | This is a list of mirrors to be used in downloading artifacts from remote repositories.\n   |\n   | It works like this: a POM may declare a repository to use in resolving certain artifacts.\n   | However, this repository may have problems with heavy traffic at times, so people have mirrored\n   | it to several places.\n   |\n   | That repository definition will have a unique id, so we can create a mirror reference for that\n   | repository, to be used as an alternate download site. The mirror site will be the preferred\n   | server for that repository.\n   |-->\n  <mirrors>\n\t<!--tencent-->\n\t<mirror>\n\t\t<id>mynexus</id>\n\t\t<mirrorOf>mynexus</mirrorOf>\n\t\t<name>My Nexus</name>\n\t\t<url>http://tencent:40000/nexus/content/repositories/thirdparty/</url>\n\t</mirror>\n\t<!--默认的中央仓库-->  \t\n\t<mirror>      \n\t\t<id>repo2</id>      \n\t\t<mirrorOf>central</mirrorOf>      \n\t\t<name>Human Readable Name for this Mirror.</name>      \n\t\t<url>http://repo2.maven.org/maven2/</url>      \n\t</mirror>\n\t<!--aliyun-->\n\t<mirror>\n\t\t<id>nexus-aliyun</id>\n\t\t<mirrorOf>aliyun</mirrorOf>\n\t\t<name>Nexus aliyun</name>\n\t\t<url>http://maven.aliyun.com/nexus/content/groups/public</url>\n\t</mirror>\n\n \n\t<mirror>      \n\t\t<id>ui</id>      \n\t\t<mirrorOf>ui</mirrorOf>      \n\t\t<name>Human Readable Name for this Mirror.</name>      \n\t\t<url>http://uk.maven.org/maven2/</url>      \n\t</mirror>\n\n\t<mirror>      \n\t\t<id>ibiblio</id>      \n\t\t<mirrorOf>ibiblio</mirrorOf>      \n\t\t<name>Human Readable Name for this Mirror.</name>      \n\t\t<url>http://mirrors.ibiblio.org/pub/mirrors/maven2/</url>      \n\t</mirror>\n\n\n    <!-- mirror\n     | Specifies a repository mirror site to use instead of a given repository. The repository that\n     | this mirror serves has an ID that matches the mirrorOf element of this mirror. IDs are used\n     | for inheritance and direct lookup purposes, and must be unique across the set of mirrors.\n     |\n    <mirror>\n      <id>mirrorId</id>\n      <mirrorOf>repositoryId</mirrorOf>\n      <name>Human Readable Name for this Mirror.</name>\n      <url>http://my.repository.com/repo/path</url>\n    </mirror>\n     -->\n  </mirrors>\n\n  <!-- profiles\n   | This is a list of profiles which can be activated in a variety of ways, and which can modify\n   | the build process. Profiles provided in the settings.xml are intended to provide local machine-\n   | specific paths and repository locations which allow the build to work in the local environment.\n   |\n   | For example, if you have an integration testing plugin - like cactus - that needs to know where\n   | your Tomcat instance is installed, you can provide a variable here such that the variable is\n   | dereferenced during the build process to configure the cactus plugin.\n   |\n   | As noted above, profiles can be activated in a variety of ways. One way - the activeProfiles\n   | section of this document (settings.xml) - will be discussed later. Another way essentially\n   | relies on the detection of a system property, either matching a particular value for the property,\n   | or merely testing its existence. Profiles can also be activated by JDK version prefix, where a\n   | value of '1.4' might activate a profile when the build is executed on a JDK version of '1.4.2_07'.\n   | Finally, the list of active profiles can be specified directly from the command line.\n   |\n   | NOTE: For profiles defined in the settings.xml, you are restricted to specifying only artifact\n   |       repositories, plugin repositories, and free-form properties to be used as configuration\n   |       variables for plugins in the POM.\n   |\n   |-->\n  <profiles>\n\n    <!-- profile\n     | Specifies a set of introductions to the build process, to be activated using one or more of the\n     | mechanisms described above. For inheritance purposes, and to activate profiles via <activatedProfiles/>\n     | or the command line, profiles have to have an ID that is unique.\n     |\n     | An encouraged best practice for profile identification is to use a consistent naming convention\n     | for profiles, such as 'env-dev', 'env-test', 'env-production', 'user-jdcasey', 'user-brett', etc.\n     | This will make it more intuitive to understand what the set of introduced profiles is attempting\n     | to accomplish, particularly when you only have a list of profile id's for debug.\n     |\n     | This profile example uses the JDK version to trigger activation, and provides a JDK-specific repo.\n    <profile>\n      <id>jdk-1.4</id>\n\n      <activation>\n        <jdk>1.4</jdk>\n      </activation>\n\n      <repositories>\n        <repository>\n          <id>jdk14</id>\n          <name>Repository for JDK 1.4 builds</name>\n          <url>http://www.myhost.com/maven/jdk14</url>\n          <layout>default</layout>\n          <snapshotPolicy>always</snapshotPolicy>\n        </repository>\n      </repositories>\n    </profile>\n    -->\n\n    <!--\n     | Here is another profile, activated by the system property 'target-env' with a value of 'dev',\n     | which provides a specific path to the Tomcat instance. To use this, your plugin configuration\n     | might hypothetically look like:\n     |\n     | ...\n     | <plugin>\n     |   <groupId>org.myco.myplugins</groupId>\n     |   <artifactId>myplugin</artifactId>\n     |\n     |   <configuration>\n     |     <tomcatLocation>${tomcatPath}</tomcatLocation>\n     |   </configuration>\n     | </plugin>\n     | ...\n     |\n     | NOTE: If you just wanted to inject this configuration whenever someone set 'target-env' to\n     |       anything, you could just leave off the <value/> inside the activation-property.\n     |\n    <profile>\n      <id>env-dev</id>\n\n      <activation>\n        <property>\n          <name>target-env</name>\n          <value>dev</value>\n        </property>\n      </activation>\n\n      <properties>\n        <tomcatPath>/path/to/tomcat/instance</tomcatPath>\n      </properties>\n    </profile>\n    -->     \n\t\t<!--配置私服-->\n\n\t\t  <profile> \n\t\t\t<id>nexus</id>  \n\t\t\t<repositories> \n\t\t\t  <repository> \n\t\t\t\t<id>public</id>  \n\t\t\t\t<name>Public Repositories</name>  \n\t\t\t\t<url>http://tencent:40000/nexus/content/repositories/thirdparty</url> \n\t\t\t  </repository> \n\t\t\t</repositories>  \n\t\t\t<pluginRepositories> \n\t\t\t  <pluginRepository> \n\t\t\t\t<id>public</id>  \n\t\t\t\t<name>Public Repositories</name>  \n\t\t\t\t<url>http://tencent:40000/nexus/content/repositories/thirdparty</url> \n\t\t\t  </pluginRepository> \n\t\t\t</pluginRepositories> \n\t\t  </profile> \n\n\t</profiles>\n\n\t<activeProfiles>    \n\t\t<activeProfile>nexus</activeProfile>    \n\t</activeProfiles>\n\n  <!-- activeProfiles\n   | List of profiles that are active for all builds.\n   |\n  <activeProfiles>\n    <activeProfile>alwaysActiveProfile</activeProfile>\n    <activeProfile>anotherAlwaysActiveProfile</activeProfile>\n  </activeProfiles>\n  -->\n</settings>\n\n```\n\n&nbsp;\n\n**21. pom指定repository**\n\n```\n    <repositories>\n        <repository>\n            <!-- Maven 自带的中央仓库使用的Id为central 如果其他的仓库声明也是用该Id就会覆盖中央仓库的配置 -->\n            <id>confluent</id>\n            <name>confluent</name>\n            <url>https://packages.confluent.io/maven/</url>\n            <layout>default</layout>\n            <releases>\n                <enabled>true</enabled>\n            </releases>\n            <snapshots>\n                <enabled>false</enabled>\n            </snapshots>\n        </repository>\n    </repositories>\n\n```\n\n　　\n\n&nbsp;\n\n&nbsp;\n","tags":["Maven"]},{"title":"MySQL学习笔记——innoDB存储结构","url":"/MySQL学习笔记——innoDB存储结构.html","content":"## 1.MySQL的存储路径\n\n```\nmysql> SHOW VARIABLES LIKE 'datadir';\n+---------------+----------------------+\n| Variable_name | Value                |\n+---------------+----------------------+\n| datadir       | /var/lib/mysql/data/ |\n+---------------+----------------------+\n1 row in set (0.01 sec)\n\n```\n\n查看datadir目录下的所有文件夹\n\n```\nsh-4.2$ ls -l | grep '^d'\ndrwxr-x--- 2 mysql mysql     4096 Aug 24 12:36 default\ndrwxr-x--- 2 mysql mysql     4096 Jan 31  2024 mysql\ndrwxr-x--- 2 mysql mysql     4096 Jan 31  2024 performance_schema\ndrwxr-x--- 2 mysql mysql    12288 Jan 31  2024 sys\n\n```\n\n这和MySQL的database是对应的\n\n<img src=\"/images/517519-20240831133138950-993195137.png\" width=\"250\" height=\"123\" loading=\"lazy\" />\n\n其中**default**是创建的database，目录下会包含opt，frm和ibd文件\n\n**db.opt**，用来存储当前数据库的默认字符集和字符校验规则。\n\n**frm（Form）**文件存储表定义。\n\n**ibd（InnoDB Data）**存储数据和索引文件。\n\n```\nsh-4.2$ pwd\n/var/lib/mysql/data/default\nsh-4.2$ ls\ndb.opt  singer.frm  singer.ibd  song.frm  song.ibd  song_singer.frm  song_singer.ibd  t_user.frm  t_user.ibd  test.frm  test.ibd  user.frm  user.ibd\n\n```\n\n**information_schema** 是每个MySQL实例中的一个数据库，存储MySQL服务器维护的所有其他数据库的信息。INFORMATION_SCHEMA数据库包含几个只读的表。它们实际上是视图，而不是基表，因此没有与它们关联的文件，而且你不能在它们上设置触发器。此外，没有使用该名称的数据库目录。\n\n**mysql** 数据库为系统数据库。它包含存储MySQL服务器运行时所需信息的表。参考：[https://dev.mysql.com/doc/refman/5.7/en/system-schema.html](https://dev.mysql.com/doc/refman/5.7/en/system-schema.html)\n\n**performance_schema** 是一个用于在底层监控MySQL服务器执行的特性。参考：[https://dev.mysql.com/doc/refman/5.7/en/performance-schema-quick-start.html](https://dev.mysql.com/doc/refman/5.7/en/performance-schema-quick-start.html)\n\n**sys** schema，这是一组帮助dba和开发人员解释由Performance schema收集的数据的对象。Sys schema对象可用于典型的调优和诊断用例。参考：[https://dev.mysql.com/doc/refman/5.7/en/sys-schema.html](https://dev.mysql.com/doc/refman/5.7/en/sys-schema.html)\n\n参考：[MySQL系统库作用：performance_schema，sys，information_schema，mysql](https://www.cnblogs.com/zhuwenjoyce/p/14839670.html)\n\n其他文件\n\n```\nsh-4.2$ ls -l | grep -v '^d'\ntotal 41032\n-rw-r----- 1 mysql mysql       56 Jan 31  2024 auto.cnf\n-rw-r----- 1 mysql mysql        2 Aug 15 15:05 bc130f3f763a.pid\n-rw------- 1 mysql mysql     1680 Jan 31  2024 ca-key.pem\n-rw-r--r-- 1 mysql mysql     1112 Jan 31  2024 ca.pem\n-rw-r--r-- 1 mysql mysql     1112 Jan 31  2024 client-cert.pem\n-rw------- 1 mysql mysql     1680 Jan 31  2024 client-key.pem\n-rw-r----- 1 mysql mysql      477 Aug 15 15:05 ib_buffer_pool\n-rw-r----- 1 mysql mysql  8388608 Aug 24 15:25 ib_logfile0\n-rw-r----- 1 mysql mysql  8388608 Aug 24 15:05 ib_logfile1\n-rw-r----- 1 mysql mysql 12582912 Aug 24 15:25 ibdata1\n-rw-r----- 1 mysql mysql 12582912 Aug 24 15:06 ibtmp1\n-rw-r--r-- 1 mysql mysql        6 Jan 31  2024 mysql_upgrade_info\n-rw------- 1 mysql mysql     1676 Jan 31  2024 private_key.pem\n-rw-r--r-- 1 mysql mysql      452 Jan 31  2024 public_key.pem\n-rw-r--r-- 1 mysql mysql     1112 Jan 31  2024 server-cert.pem\n-rw------- 1 mysql mysql     1676 Jan 31  2024 server-key.pem\n\n```\n\n在 MySQL 的 InnoDB 存储引擎中，这些文件（`ib_buffer_pool`、`ib_logfile0`、`ib_logfile1`、`ibdata1`、`ibtmp1`）代表了数据库运行时的不同数据结构和存储机制。它们各自有不同的用途，用于管理和存储 InnoDB 的数据和日志。\n\n**`ib_buffer_pool`** 文件用于持久化存储 InnoDB 缓冲池（Buffer Pool）中的热数据页的状态。当 MySQL 服务器重启时，通过此文件恢复缓冲池的内容，减少重启后重新填充缓冲池所需的时间。\n\n**`ib_logfile0`** 和 `**ib_logfile1** 这两个文件是 InnoDB 的 **重做日志（Redo Logs）** 文件，记录了数据库事务的更改信息（插入、更新、删除等）。这些日志文件用于在崩溃恢复过程中重新应用未写入数据文件的更改。<br />`\n\n**`ibdata1`** 是 InnoDB **系统表空间**的默认文件。它用于存储多个 InnoDB 表的数据和索引，以及全局表元数据（如数据字典、回滚段等）。\n\n表数据即可以存储在系统表空间ibdata1中，也可以存储在独立表空间中ibd，这个由参数 innodb_file_per_table 来控制，MySQL5.7及以上这个默认值为1，所以MySQL每张表都是一个独立的ibd文件。\n\n```\nmysql> SHOW VARIABLES LIKE 'innodb_file_per_table';\n+-----------------------+-------+\n| Variable_name         | Value |\n+-----------------------+-------+\n| innodb_file_per_table | ON    |\n+-----------------------+-------+\n1 row in set (0.01 sec)\n\n```\n\n**`ibtmp1`** 是 InnoDB 的 **临时表空间**文件，专门用于存储临时表和临时数据。\n\n## 2.MySQL innoDB的存储结构\n\n### 1.innoDB架构\n\n<img src=\"/images/517519-20240831151630359-159144194.png\" alt=\"\" loading=\"lazy\" />\n\n参考：[https://dev.mysql.com/doc/refman/5.7/en/innodb-architecture.html](https://dev.mysql.com/doc/refman/5.7/en/innodb-architecture.html)\n\n### 2.表空间**（Tablespace）**\n\n表空间是 InnoDB 用于存储表数据和索引数据的物理存储区域。可以把表空间理解为一块大的存储区域，其中可以存放多个表和索引。\n\n表空间由多个文件组成，这些文件可能是**系统表空间**（如 `ibdata1`）、**独立表空间**（每个表对应一个 `.ibd` 文件）、**通用表空间**（用户定义的多个表空间文件）、**临时表空间**（专门用于存储临时表和临时数据等，如`ibtmp1`）、**Undo 表空间**（也叫做回滚表空间，是 InnoDB 存储引擎中用于存储事务的 **undo logs**（回滚日志） 的一种特殊类型的表空间）。\n\n### 3.段，区，页，行\n\n<img src=\"/images/517519-20240831182038126-1063719033.png\" alt=\"\" />\n\n#### 参考：[MySQL Storage Structure](https://medium.com/@r844312/mysql-storage-structure-abbd4846e47b)\n\n#### 1.页（page）\n\n每个**表空间（tablespace）**由**页（page）**组成。MySQL实例中的每个表空间都有相同的页面大小。默认情况下，所有表空间的页面大小都是16KB。在创建MySQL实例时，可以通过指定innodb_page_size选项将页面大小减小到8KB或4KB。您还可以将页面大小增加到32KB或64KB。磁盘和内存之间的数据传输是逐页进行的，**innodb_page_size**表示InnoDB在任何时候在磁盘(数据文件)和内存(Buffer Pool)之间传输数据的大小。\n\n每个表中的数据被划分为多个**页（page）**。组成每个表的**页（page）**被安排在一个称为**b树索引**的树状数据结构中。**表数据/聚簇索引（Table Data）**和**辅助索引（Secondary Index）**都使用这种类型的结构。**表示整个表的b树索引**称为**聚簇索引**，它是根据**主键列**组织的。聚簇索引数据结构的节点包含**行（row）**中所有列（字段）的值。**辅助索引**结构的节点包含**索引列和主键列**的值。\n\n参考：[https://dev.mysql.com/doc/refman/5.7/en/innodb-row-format.html](https://dev.mysql.com/doc/refman/5.7/en/innodb-row-format.html)\n\nB+ 树中包含**数据页（Data Page）**和**索引页（Index Page）**\n\n索引页位于 B+ 树的**非叶子节点**中，主要存储索引键和指向子节点的指针。\n\n数据页位于 B+ 树的**叶子节点**中，存储实际的数据记录。\n\n**Page的结构**如下图，其中User Records是一个单向链表，而\n\n<img src=\"/images/517519-20240907231022525-189840633.png\" alt=\"\" loading=\"lazy\" />\n\n参考：[MySQL Storage Structure](https://medium.com/@r844312/mysql-storage-structure-abbd4846e47b)\n\n#### 2.区（extent）\n\n这些页被分组为大小为1MB的**区段（extents）**，其中最大的页大小为16KB(64个连续的16KB页，或128个8KB页，或256个4KB页)。对于一个32KB的页面，区段大小是2MB。对于64KB的页面，区段大小为4MB。\n\nB+ 树中每一层都是通过双向链表连接起来的，如果是以页为单位来分配存储空间，那么链表中相邻的两个页之间的物理位置并不是连续的，可能离得非常远，那么磁盘查询时就会有大量的随机I/O，随机 I/O 是非常慢的。所以在表中数据量大的时候，为某个索引分配空间的时候就不再按照页为单位分配了，而是按照区（extent）为单位分配。每个区的大小为 1MB，对于 16KB 的页来说，连续的 64 个页会被划为一个区，这样就使得链表中相邻的页的物理位置也相邻，就能使用顺序 I/O 了。\n\n参考：[MySQL 一行记录是怎么存储的？](https://xiaolincoding.com/mysql/base/row_format.html#%E8%A1%A8%E7%A9%BA%E9%97%B4%E6%96%87%E4%BB%B6%E7%9A%84%E7%BB%93%E6%9E%84%E6%98%AF%E6%80%8E%E4%B9%88%E6%A0%B7%E7%9A%84)\n\n#### 3.段（segment）\n\n表空间是由各个**段（segment）**组成的，段是由多个区（extent）组成的。段一般分为数据段、索引段和回滚段等。\n\n- 索引段（Non-Leaf Node Segment）：存放 B+ 树的非叶子节点的区的集合；\n- 数据段（Leaf Node Segment）：存放 B+ 树的叶子节点的区的集合；\n- 回滚段（Rollback Segment）：存放的是回滚数据的区的集合，MVCC 利用了回滚段实现了多版本查询数据。回滚段作为undo log的一部分，undo log主要用于事务回滚和MVCC（多并发版本控制）。\n\n参考：[MySQL 一行记录是怎么存储的？](https://xiaolincoding.com/mysql/base/row_format.html#%E8%A1%A8%E7%A9%BA%E9%97%B4%E6%96%87%E4%BB%B6%E7%9A%84%E7%BB%93%E6%9E%84%E6%98%AF%E6%80%8E%E4%B9%88%E6%A0%B7%E7%9A%84)\n\n表空间中的&ldquo;文件&rdquo;在InnoDB中称为**段（segment）**。(这些段不同于回滚段，后者实际上包含许多表空间段。)\n\n参考：[https://dev.mysql.com/doc/refman/5.7/en/innodb-file-space.html](https://dev.mysql.com/doc/refman/5.7/en/innodb-file-space.html)\n\n#### 4.行（row）\n\nInnoDB存储引擎支持四种行格式：REDUNDANT, COMPACT, DYNAMIC 和 COMPRESSED。\n\nMySQL5.7默认的row format是dynamic\n\n```\nmysql> SHOW VARIABLES LIKE 'innodb_default_row_format';\n+---------------------------+---------+\n| Variable_name             | Value   |\n+---------------------------+---------+\n| innodb_default_row_format | dynamic |\n+---------------------------+---------+\n1 row in set (0.00 sec)\n\n```\n\n变长（Variable-length）列不符合列值存储在b树索引节点中的规则。变长列太长而不能放在b树页上，它们存储在单独分配的磁盘页上，称为溢出页(overflow page)。这样的列称为页外列。页外列的值存储在溢出页的单链表中，每个列都有自己的一个或多个溢出页的列表。根据列的长度，可变长列的所有值或其前缀都存储在B-tree中，以避免浪费存储空间和另行读取一页。\n\n参考：[https://dev.mysql.com/doc/refman/5.7/en/innodb-row-format.html](https://dev.mysql.com/doc/refman/5.7/en/innodb-row-format.html)\n\n### 4.Buffer Pool\n\nBuffer Pool（缓冲池）是主内存中的一个区域，InnoDB在访问表和索引数据时将在这里缓存数据。缓冲池允许频繁使用的数据直接从内存中访问，这加快了处理速度。在专用服务器上，高达80%的物理内存通常分配给缓冲池。\n\n为了提高大容量读操作的效率，缓冲池被划分为可以容纳多行数据的页。为提高缓存管理的效率，缓冲池实现为页（Page）的链表。很少使用的数据使用最近最少使用(least recently used, LRU)算法的变体从缓存中老化。在需要空间向缓冲池添加新页时，会将最近最少使用的页清除，并将新页添加到列表的中间。\n\n**Buffer Pool的作用：**\n\n- 当读取数据时，如果数据存在于 Buffer Pool 中，客户端就会直接读取 Buffer Pool 中的数据，否则再去磁盘中读取。\n- 当修改数据时，如果数据存在于 Buffer Pool 中，那直接修改 Buffer Pool 中数据所在的页，然后将其页设置为脏页（该页的内存数据和磁盘上的数据已经不一致），为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。\n\n参考：[MySQL 日志：undo log、redo log、binlog 有什么用？](https://xiaolincoding.com/mysql/log/how_update.html#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-buffer-pool)\n\n## 3.InnoDB和MyISAM存储引擎区别\n\n### 1.InnoDB和MyISAM的区别\n\n参考：[深入理解MySQL索引原理和实现&mdash;&mdash;为什么索引可以加速查询？](https://blog.csdn.net/tongdanping/article/details/79878302)\n\n\n\n|存储引擎|InnoDB|MyISAM\n| ---- | ---- | ---- \n|索引类型|InnoDB 主键是聚簇索引，二级索引是非聚簇索引|MyISAM 是非聚簇索引\n|支持事务|InnoDB 支持事务|MyISAM 不支持事务\n|锁|InnoDB 支持表锁，但默认使用行锁|MyISAM 支持表锁，不支持行锁\n|支持全文索引|InnoDB（5.6以后） 支持全文索引|MyISAM 支持全文索引\n|并发性能|InnoDB 适用于大量的插入、删除和更新操作，尤其是高并发的写操作场景，因为它支持行级锁和多版本并发控制（MVCC）|MyISAM 由于其表锁机制，非常适合只读操作的大量 select 查询。\n|外键约束|InnoDB 支持外键约束|MyISAM 不支持外键约束\n|<!--more-->\n&nbsp;|&nbsp;|&nbsp;\n\n### 2.存储区别\n\nMyISAM在磁盘上会存储3个文件：frm（Form）文件存储表定义，myd（MyISAM Data）存储数据文件，myi（MyISAM Index）存储索引文件\n\nInnodb在磁盘上会存储2个文件：frm（Form）文件存储表定义，ibd（InnoDB Data）存储数据和索引文件\n\n```\nroot@master:/var/lib/mysql/wherehows# ls\ncfg_application.frm                   flow_execution.frm                            FTS_00000000000001a5_CONFIG.ibd                 log_dataset_instance_load_status#P#P201609.ibd  stg_flow_job.frm\ncfg_application.ibd                   flow_execution_id_map.frm                     FTS_00000000000001a5_DELETED_CACHE.ibd          log_dataset_instance_load_status#P#P201610.ibd  stg_flow_job.par\ncfg_cluster.frm                       flow_execution_id_map.par                     FTS_00000000000001a5_DELETED.ibd                log_dataset_instance_load_status#P#P201611.ibd  stg_flow_job#P#p0.ibd\ncfg_cluster.ibd                       flow_execution_id_map#P#p0.MYD                FTS_00000000000001b1_BEING_DELETED_CACHE.ibd    log_dataset_instance_load_status#P#P201612.ibd  stg_flow_job#P#p1.ibd\ncfg_database.frm                      flow_execution_id_map#P#p0.MYI                FTS_00000000000001b1_BEING_DELETED.ibd          log_dataset_instance_load_status#P#P203507.ibd  stg_flow_job#P#p2.ibd\ncfg_database.ibd                      flow_execution_id_map#P#p1.MYD                FTS_00000000000001b1_CONFIG.ibd                 log_lineage_pattern.frm                         stg_flow_job#P#p3.ibd\ncfg_data_center.frm                   flow_execution_id_map#P#p1.MYI                FTS_00000000000001b1_DELETED_CACHE.ibd          log_lineage_pattern.ibd                         stg_flow_job#P#p4.ibd\ncfg_data_center.ibd                   flow_execution_id_map#P#p2.MYD                FTS_00000000000001b1_DELETED.ibd                log_reference_job_id_pattern.frm                stg_flow_job#P#p5.ibd\ncfg_deployment_tier.frm               flow_execution_id_map#P#p2.MYI                FTS_0000000000000272_BEING_DELETED_CACHE.ibd    log_reference_job_id_pattern.ibd                stg_flow_job#P#p6.ibd\ncfg_deployment_tier.ibd               flow_execution_id_map#P#p3.MYD                FTS_0000000000000272_BEING_DELETED.ibd          source_code_commit_info.frm                     stg_flow_job#P#p7.ibd\ncfg_job_type.frm                      flow_execution_id_map#P#p3.MYI                FTS_0000000000000272_CONFIG.ibd                 source_code_commit_info.ibd                     stg_flow_owner_permission.frm\ncfg_job_type.ibd                      flow_execution_id_map#P#p4.MYD                FTS_0000000000000272_DELETED_CACHE.ibd          #sql-4312_14688.frm                             stg_flow_owner_permission.par\ncfg_job_type_reverse_map.frm          flow_execution_id_map#P#p4.MYI                FTS_0000000000000272_DELETED.ibd                stg_cfg_object_name_map.frm                     stg_flow_owner_permission#P#p0.ibd\ncfg_job_type_reverse_map.ibd          flow_execution_id_map#P#p5.MYD                job_attempt_source_code.frm                     stg_cfg_object_name_map.ibd                     stg_flow_owner_permission#P#p1.ibd\ncfg_object_name_map.frm               flow_execution_id_map#P#p5.MYI                job_attempt_source_code.ibd                     stg_database_scm_map.frm                        stg_flow_owner_permission#P#p2.ibd\ncfg_object_name_map.ibd               flow_execution_id_map#P#p6.MYD                job_execution_data_lineage.frm                  stg_database_scm_map.ibd                        stg_flow_owner_permission#P#p3.ibd\ncfg_search_score_boost.frm            flow_execution_id_map#P#p6.MYI                job_execution_data_lineage.par                  stg_dataset_owner.frm                           stg_flow_owner_permission#P#p4.ibd\ncfg_search_score_boost.ibd            flow_execution_id_map#P#p7.MYD                job_execution_data_lineage#P#p0.ibd             stg_dataset_owner.ibd                           stg_flow_owner_permission#P#p5.ibd\ncomments.frm                          flow_execution_id_map#P#p7.MYI                job_execution_data_lineage#P#p1.ibd             stg_dataset_owner_unmatched.frm                 stg_flow_owner_permission#P#p6.ibd\ncomments.ibd                          flow_execution.par                            job_execution_data_lineage#P#p2.ibd             stg_dataset_owner_unmatched.ibd                 stg_flow_owner_permission#P#p7.ibd\ndataset_capacity.frm                  flow_execution#P#p0.ibd                       job_execution_data_lineage#P#p3.ibd             stg_dict_dataset_field_comment.frm              stg_flow.par\ndataset_capacity.ibd                  flow_execution#P#p1.ibd                       job_execution_data_lineage#P#p4.ibd             stg_dict_dataset_field_comment.par              stg_flow#P#p0.ibd\ndataset_case_sensitivity.frm          flow_execution#P#p2.ibd                       job_execution_data_lineage#P#p5.ibd             stg_dict_dataset_field_comment#P#p0.ibd         stg_flow#P#p1.ibd\ndataset_case_sensitivity.ibd          flow_execution#P#p3.ibd                       job_execution_data_lineage#P#p6.ibd             stg_dict_dataset_field_comment#P#p1.ibd         stg_flow#P#p2.ibd\ndataset_compliance.frm                flow_execution#P#p4.ibd                       job_execution_data_lineage#P#p7.ibd             stg_dict_dataset_field_comment#P#p2.ibd         stg_flow#P#p3.ibd\ndataset_compliance.ibd                flow_execution#P#p5.ibd                       job_execution_ext_reference.frm                 stg_dict_dataset_field_comment#P#p3.ibd         stg_flow#P#p4.ibd\ndataset_constraint.frm                flow_execution#P#p6.ibd                       job_execution_ext_reference.par                 stg_dict_dataset_field_comment#P#p4.ibd         stg_flow#P#p5.ibd\ndataset_constraint.ibd                flow_execution#P#p7.ibd                       job_execution_ext_reference#P#p0.ibd            stg_dict_dataset_field_comment#P#p5.ibd         stg_flow#P#p6.ibd\ndataset_deployment.frm                flow.frm                                      job_execution_ext_reference#P#p1.ibd            stg_dict_dataset_field_comment#P#p6.ibd         stg_flow#P#p7.ibd\ndataset_deployment.ibd                flow_job.frm                                  job_execution_ext_reference#P#p2.ibd            stg_dict_dataset_field_comment#P#p7.ibd         stg_flow_schedule.frm\ndataset_index.frm                     flow_job.par                                  job_execution_ext_reference#P#p3.ibd            stg_dict_dataset.frm                            stg_flow_schedule.par\ndataset_index.ibd                     flow_job#P#p0.ibd                             job_execution_ext_reference#P#p4.ibd            stg_dict_dataset_instance.frm                   stg_flow_schedule#P#p0.ibd\ndataset_inventory.frm                 flow_job#P#p1.ibd                             job_execution_ext_reference#P#p5.ibd            stg_dict_dataset_instance.par                   stg_flow_schedule#P#p1.ibd\ndataset_inventory.ibd                 flow_job#P#p2.ibd                             job_execution_ext_reference#P#p6.ibd            stg_dict_dataset_instance#P#p0.ibd              stg_flow_schedule#P#p2.ibd\ndataset_owner.frm                     flow_job#P#p3.ibd                             job_execution_ext_reference#P#p7.ibd            stg_dict_dataset_instance#P#p1.ibd              stg_flow_schedule#P#p3.ibd\ndataset_owner.ibd                     flow_job#P#p4.ibd                             job_execution.frm                               stg_dict_dataset_instance#P#p2.ibd              stg_flow_schedule#P#p4.ibd\ndataset_partition.frm                 flow_job#P#p5.ibd                             job_execution_id_map.frm                        stg_dict_dataset_instance#P#p3.ibd              stg_flow_schedule#P#p5.ibd\ndataset_partition.ibd                 flow_job#P#p6.ibd                             job_execution_id_map.par                        stg_dict_dataset_instance#P#p4.ibd              stg_flow_schedule#P#p6.ibd\ndataset_partition_layout_pattern.frm  flow_job#P#p7.ibd                             job_execution_id_map#P#p0.MYD                   stg_dict_dataset_instance#P#p5.ibd              stg_flow_schedule#P#p7.ibd\ndataset_partition_layout_pattern.ibd  flow_owner_permission.frm                     job_execution_id_map#P#p0.MYI                   stg_dict_dataset_instance#P#p6.ibd              stg_git_project.frm\ndataset_privacy_compliance.frm        flow_owner_permission.par                     job_execution_id_map#P#p1.MYD                   stg_dict_dataset_instance#P#p7.ibd              stg_git_project.ibd\ndataset_privacy_compliance.ibd        flow_owner_permission#P#p0.ibd                job_execution_id_map#P#p1.MYI                   stg_dict_dataset.par                            stg_job_execution_data_lineage.frm\ndataset_reference.frm                 flow_owner_permission#P#p1.ibd                job_execution_id_map#P#p2.MYD                   stg_dict_dataset#P#p0.ibd                       stg_job_execution_data_lineage.ibd\ndataset_reference.ibd                 flow_owner_permission#P#p2.ibd                job_execution_id_map#P#p2.MYI                   stg_dict_dataset#P#p1.ibd                       stg_job_execution_ext_reference.frm\ndataset_schema_info.frm               flow_owner_permission#P#p3.ibd                job_execution_id_map#P#p3.MYD                   stg_dict_dataset#P#p2.ibd                       stg_job_execution_ext_reference.par\ndataset_schema_info.ibd               flow_owner_permission#P#p4.ibd                job_execution_id_map#P#p3.MYI                   stg_dict_dataset#P#p3.ibd                       stg_job_execution_ext_reference#P#p0.ibd\ndataset_security.frm                  flow_owner_permission#P#p5.ibd                job_execution_id_map#P#p4.MYD                   stg_dict_dataset#P#p4.ibd                       stg_job_execution_ext_reference#P#p1.ibd\ndataset_security.ibd                  flow_owner_permission#P#p6.ibd                job_execution_id_map#P#p4.MYI                   stg_dict_dataset#P#p5.ibd                       stg_job_execution_ext_reference#P#p2.ibd\ndataset_tag.frm                       flow_owner_permission#P#p7.ibd                job_execution_id_map#P#p5.MYD                   stg_dict_dataset#P#p6.ibd                       stg_job_execution_ext_reference#P#p3.ibd\ndataset_tag.ibd                       flow.par                                      job_execution_id_map#P#p5.MYI                   stg_dict_dataset#P#p7.ibd                       stg_job_execution_ext_reference#P#p4.ibd\ndb.opt                                flow#P#p0.ibd                                 job_execution_id_map#P#p6.MYD                   stg_dict_dataset_sample.frm                     stg_job_execution_ext_reference#P#p5.ibd\ndict_business_metric.frm              flow#P#p1.ibd                                 job_execution_id_map#P#p6.MYI                   stg_dict_dataset_sample.ibd                     stg_job_execution_ext_reference#P#p6.ibd\ndict_business_metric.ibd              flow#P#p2.ibd                                 job_execution_id_map#P#p7.MYD                   stg_dict_field_detail.frm                       stg_job_execution_ext_reference#P#p7.ibd\ndict_dataset_field_comment.frm        flow#P#p3.ibd                                 job_execution_id_map#P#p7.MYI                   stg_dict_field_detail.par                       stg_job_execution.frm\ndict_dataset_field_comment.ibd        flow#P#p4.ibd                                 job_execution.par                               stg_dict_field_detail#P#p0.ibd                  stg_job_execution.par\ndict_dataset.frm                      flow#P#p5.ibd                                 job_execution#P#p0.ibd                          stg_dict_field_detail#P#p1.ibd                  stg_job_execution#P#p0.ibd\ndict_dataset.ibd                      flow#P#p6.ibd                                 job_execution#P#p1.ibd                          stg_dict_field_detail#P#p2.ibd                  stg_job_execution#P#p1.ibd\ndict_dataset_instance.frm             flow#P#p7.ibd                                 job_execution#P#p2.ibd                          stg_dict_field_detail#P#p3.ibd                  stg_job_execution#P#p2.ibd\ndict_dataset_instance.par             flow_schedule.frm                             job_execution#P#p3.ibd                          stg_dict_field_detail#P#p4.ibd                  stg_job_execution#P#p3.ibd\ndict_dataset_instance#P#p0.ibd        flow_schedule.par                             job_execution#P#p4.ibd                          stg_dict_field_detail#P#p5.ibd                  stg_job_execution#P#p4.ibd\ndict_dataset_instance#P#p1.ibd        flow_schedule#P#p0.ibd                        job_execution#P#p5.ibd                          stg_dict_field_detail#P#p6.ibd                  stg_job_execution#P#p5.ibd\ndict_dataset_instance#P#p2.ibd        flow_schedule#P#p1.ibd                        job_execution#P#p6.ibd                          stg_dict_field_detail#P#p7.ibd                  stg_job_execution#P#p6.ibd\ndict_dataset_instance#P#p3.ibd        flow_schedule#P#p2.ibd                        job_execution#P#p7.ibd                          stg_flow_dag_edge.frm                           stg_job_execution#P#p7.ibd\ndict_dataset_instance#P#p4.ibd        flow_schedule#P#p3.ibd                        job_execution_script.frm                        stg_flow_dag_edge.par                           stg_kafka_gobblin_compaction.frm\ndict_dataset_instance#P#p5.ibd        flow_schedule#P#p4.ibd                        job_execution_script.ibd                        stg_flow_dag_edge#P#p0.ibd                      stg_kafka_gobblin_compaction.ibd\ndict_dataset_instance#P#p6.ibd        flow_schedule#P#p5.ibd                        job_source_id_map.frm                           stg_flow_dag_edge#P#p1.ibd                      stg_kafka_gobblin_distcp.frm\ndict_dataset_instance#P#p7.ibd        flow_schedule#P#p6.ibd                        job_source_id_map.par                           stg_flow_dag_edge#P#p2.ibd                      stg_kafka_gobblin_distcp.ibd\ndict_dataset_sample.frm               flow_schedule#P#p7.ibd                        job_source_id_map#P#p0.MYD                      stg_flow_dag_edge#P#p3.ibd                      stg_kafka_gobblin_lumos.frm\ndict_dataset_sample.ibd               flow_source_id_map.frm                        job_source_id_map#P#p0.MYI                      stg_flow_dag_edge#P#p4.ibd                      stg_kafka_gobblin_lumos.ibd\ndict_dataset_schema_history.frm       flow_source_id_map.par                        job_source_id_map#P#p1.MYD                      stg_flow_dag_edge#P#p5.ibd                      stg_kafka_metastore_audit.frm\ndict_dataset_schema_history.ibd       flow_source_id_map#P#p0.MYD                   job_source_id_map#P#p1.MYI                      stg_flow_dag_edge#P#p6.ibd                      stg_kafka_metastore_audit.ibd\ndict_field_detail.frm                 flow_source_id_map#P#p0.MYI                   job_source_id_map#P#p2.MYD                      stg_flow_dag_edge#P#p7.ibd                      stg_product_repo.frm\ndict_field_detail.ibd                 flow_source_id_map#P#p1.MYD                   job_source_id_map#P#p2.MYI                      stg_flow_dag.frm                                stg_product_repo.ibd\ndir_external_group_user_map.frm       flow_source_id_map#P#p1.MYI                   job_source_id_map#P#p3.MYD                      stg_flow_dag.par                                stg_repo_owner.frm\ndir_external_group_user_map.ibd       flow_source_id_map#P#p2.MYD                   job_source_id_map#P#p3.MYI                      stg_flow_dag#P#p0.ibd                           stg_repo_owner.ibd\ndir_external_user_info.frm            flow_source_id_map#P#p2.MYI                   job_source_id_map#P#p4.MYD                      stg_flow_dag#P#p1.ibd                           stg_source_code_commit_info.frm\ndir_external_user_info.ibd            flow_source_id_map#P#p3.MYD                   job_source_id_map#P#p4.MYI                      stg_flow_dag#P#p2.ibd                           stg_source_code_commit_info.ibd\nfavorites.frm                         flow_source_id_map#P#p3.MYI                   job_source_id_map#P#p5.MYD                      stg_flow_dag#P#p3.ibd                           track_object_access_log.frm\nfavorites.ibd                         flow_source_id_map#P#p4.MYD                   job_source_id_map#P#p5.MYI                      stg_flow_dag#P#p4.ibd                           track_object_access_log.ibd\nfield_comments.frm                    flow_source_id_map#P#p4.MYI                   job_source_id_map#P#p6.MYD                      stg_flow_dag#P#p5.ibd                           user_login_history.frm\nfield_comments.ibd                    flow_source_id_map#P#p5.MYD                   job_source_id_map#P#p6.MYI                      stg_flow_dag#P#p6.ibd                           user_login_history.ibd\nfilename_pattern.frm                  flow_source_id_map#P#p5.MYI                   job_source_id_map#P#p7.MYD                      stg_flow_dag#P#p7.ibd                           user_settings.frm\nfilename_pattern.ibd                  flow_source_id_map#P#p6.MYD                   job_source_id_map#P#p7.MYI                      stg_flow_execution.frm                          user_settings.ibd\nflow_dag.frm                          flow_source_id_map#P#p6.MYI                   log_dataset_instance_load_status.frm            stg_flow_execution.par                          users.frm\nflow_dag.par                          flow_source_id_map#P#p7.MYD                   log_dataset_instance_load_status.par            stg_flow_execution#P#p0.ibd                     users.ibd\nflow_dag#P#p0.ibd                     flow_source_id_map#P#p7.MYI                   log_dataset_instance_load_status#P#P201601.ibd  stg_flow_execution#P#p1.ibd                     watch.frm\nflow_dag#P#p1.ibd                     FTS_0000000000000184_BEING_DELETED_CACHE.ibd  log_dataset_instance_load_status#P#P201602.ibd  stg_flow_execution#P#p2.ibd                     watch.ibd\nflow_dag#P#p2.ibd                     FTS_0000000000000184_BEING_DELETED.ibd        log_dataset_instance_load_status#P#P201603.ibd  stg_flow_execution#P#p3.ibd                     wh_etl_job_history.frm\nflow_dag#P#p3.ibd                     FTS_0000000000000184_CONFIG.ibd               log_dataset_instance_load_status#P#P201604.ibd  stg_flow_execution#P#p4.ibd                     wh_etl_job_history.ibd\nflow_dag#P#p4.ibd                     FTS_0000000000000184_DELETED_CACHE.ibd        log_dataset_instance_load_status#P#P201605.ibd  stg_flow_execution#P#p5.ibd                     wh_etl_job_schedule.frm\nflow_dag#P#p5.ibd                     FTS_0000000000000184_DELETED.ibd              log_dataset_instance_load_status#P#P201606.ibd  stg_flow_execution#P#p6.ibd                     wh_etl_job_schedule.ibd\nflow_dag#P#p6.ibd                     FTS_00000000000001a5_BEING_DELETED_CACHE.ibd  log_dataset_instance_load_status#P#P201607.ibd  stg_flow_execution#P#p7.ibd\nflow_dag#P#p7.ibd                     FTS_00000000000001a5_BEING_DELETED.ibd        log_dataset_instance_load_status#P#P201608.ibd  stg_flow.frm　　\n```\n\n### 3.锁区别\n\nMyISAM存储引擎使用的是表锁。\n\nInnodb存储引擎默认使用的是行锁，同时也支持表级锁。InnoDB使用表锁的场景，比如：\n\n1.显式表锁\n\n使用 `LOCK TABLES` 语句显式地请求 InnoDB 表锁。使用显式表锁通常是为了确保在一个会话内多个表的操作不被其他会话打扰。\n\n```\nLOCK TABLES table_name WRITE;\n-- 执行一些操作\nUNLOCK TABLES;\n\n```\n\n在这种情况下，InnoDB 会为指定的表加一个表锁，防止其他事务对该表进行操作，直到显式解锁。\n\n2.操作不使用索引的场景\n\n当执行的操作没有使用到索引时，InnoDB 可能会退化为使用表锁。这通常发生在以下几种情况下：\n\n**全表扫描**：在执行更新或删除操作时，如果没有使用索引（例如 `UPDATE table SET column = value` 或 `DELETE FROM table` 之类的语句没有 WHERE 条件，或者 WHERE 条件没有使用索引列），InnoDB 会使用表锁以确保数据一致性。\n\n```\nUPDATE my_table SET column_a = 'value';  -- 没有WHERE条件，可能触发表锁\n\n```\n\n**不使用索引的条件查询**：在 `UPDATE` 或 `DELETE` 语句中，使用了条件查询但条件不使用索引时，InnoDB 可能会锁住整张表。\n\n3.ALTER TABLE 操作\n\n`ALTER TABLE` 语句通常需要对表进行结构修改，这样的操作会自动导致表锁。这个锁是一个&ldquo;意向独占锁&rdquo;（意图锁），以便其他事务不能同时对表进行写操作。\n\n```\nALTER TABLE my_table ADD COLUMN new_column INT;\n\n```\n\n4.使用全表扫描的 `INSERT ... SELECT` 操作\n\n在执行 `INSERT ... SELECT` 时，如果 `SELECT` 语句对表进行了全表扫描，InnoDB 可能会使用表锁以保证插入的数据的一致性。\n\n```\nINSERT INTO my_table (column1, column2)\nSELECT column1, column2 FROM another_table;\n\n```\n\n如果 `another_table` 没有索引，并且 `SELECT` 语句涉及到全表扫描，那么 InnoDB 可能会锁住整张表。\n\n5.高并发下的死锁检测优化\n\n6.外键约束\n\n当执行涉及外键约束的操作（如 `INSERT`、`UPDATE` 或 `DELETE`）时，如果父表或子表需要修改且有外键引用的约束关系时，InnoDB 可能会锁住相关的表，特别是当操作没有命中索引的情况下。\n\n## 4.MySQL InnoDB的锁类型\n\n### 1.共享锁（Share Locks），又称为S锁\n\n共享锁允许持有锁的事务读取一行数据。\n\n如果事务T1持有行r上的一个共享锁，那么来自不同事务T2的对行r上的一个锁的请求将被处理如下：T2对S锁的请求可以立即被授予。因此，T1和T2都对r持有S锁。 T2对X锁的请求不能立即被授予。\n\n### 2.互斥锁（Exclusive Locks），又称为排他锁，独占锁，X锁\n\n排他(X)锁允许持有锁的事务更新或删除一行。如果事务T1持有行r上的排他(X)锁，那么某个不同事务T2对r上任何一种类型的锁的请求都不能立即被授予。相反，事务T2必须等待事务T1释放对行r的锁。\n\n### 3.意向锁（Intention Locks）\n\n有2种类型的意向锁：\n\n给表添加意图共享锁（IS锁）表示事务打算在表中的个别行上设置共享锁。 比如\n\n```\nSELECT ... LOCK IN SHARE MODE\n\n```\n\n给表添加意图排他锁（IX锁）表示事务打算对表中的个别行设置排他锁。比如\n\n```\nSELECT ... FOR UPDATE\n\n```\n\n**意图锁的协议**如下:\n\n1.在一个事务获得表中某一行的共享锁之前，它必须首先获得表上的IS锁或更强的锁。\n\n2.在事务获得表中某一行的排它锁之前，它必须首先获得表上的IX锁。\n\n**表级别的锁类型兼容性**总结在下面的矩阵中：\n\n<th scope=\"col\">&nbsp;</th><th scope=\"col\">`X`</th><th scope=\"col\">`IX`</th><th scope=\"col\">`S`</th><th scope=\"col\">`IS`</th>\n|------\n<th scope=\"row\">`X`</th>|Conflict|Conflict|Conflict|Conflict\n<th scope=\"row\">`IX`</th>|Conflict|Compatible|Conflict|Compatible\n<th scope=\"row\">`S`</th>|Conflict|Conflict|Compatible|Compatible\n<th scope=\"row\">`IS`</th>|Conflict|Compatible|Compatible|Compatible\n\n如果请求事务与现有锁**兼容**，则授予该锁，但如果与现有锁**冲突**，则不授予该锁。事务一直等待，直到有冲突的现有锁被释放。如果锁请求与已有的锁冲突，并且由于会导致死锁而无法授予，则会发生错误。\n\n意图锁不会阻塞除了全表请求(例如，LOCK TABLES ... WRITE)。**意图锁的主要目的**是表明有人正在锁定某一行，或者将要锁定表中的某一行。\n\n### 4.记录锁（Record Lock）\n\n记录锁是索引记录上的锁。例如，\n\n```\nSTART TRANSACTION; # 或者 begin;\nSELECT c1 FROM t WHERE c1 = 10 FOR UPDATE;\n\n```\n\n其防止任何其他事务插入、更新或删除t.c1值等于10的行\n\n记录锁总是锁定索引记录，即使定义的表没有索引。对于这种情况，InnoDB会创建一个隐藏的聚集索引，并使用这个索引来锁定记录。\n\n### 5.间隙锁（Gap Lock）\n\n间隙锁是对索引记录之间的间隙的锁，或者对第一个或最后一个索引记录之前或之后的间隙的锁。例如，\n\n```\nSTART TRANSACTION; # 或者 begin;\nSELECT c1 FROM t WHERE c1 BETWEEN 10 and 20 FOR UPDATE;\n\n```\n\n其防止其他事务将值15插入到列t.c1中，无论该列中是否已经有这样的值，因为范围中所有现有值之间的差距被锁定。\n\n间隙可能跨越单个索引值、多个索引值，甚至是空的。\n\n间隔锁是性能和并发性之间权衡的一部分，只用于如下2种事务隔离级别，而不用于其他级别。\n\n- 可重复读（REPEATABLE READ）：在这个级别中，间隙锁防止了幻读，确保同一查询在同一事务中多次执行时返回相同的结果。\n- 串行化（SERIALIZABLE）：这个级别进一步加强了对并发事务的控制，间隙锁会被用于确保事务之间完全隔离。\n\n这里还值得注意的是，不同的事务可以在gap上持有冲突的锁。例如，事务A可以在一个间隙上持有一个共享间隙锁(gap S-lock)，而事务B可以在同一个间隙上持有一个排他间隙锁(gap X-lock)。允许冲突间隔锁的原因是，如果从索引中清除记录，则不同事务在记录上持有的间隔锁必须合并。\n\n### 6.下一键锁（Next-Key Lock）\n\n下一键锁是索引记录上的记录锁（Record Locks）和索引记录前间隙上的间隙锁（Gap Locks）的组合。 例如，即id == 2的记录锁和id < 2的间隙锁\n\n```\nSTART TRANSACTION; # 或者 begin;\nSELECT * FROM employees WHERE id <= 2 FOR UPDATE;\n\n```\n\n默认情况下，InnoDB使用可重复读取事务隔离级别操作。在这种情况下，InnoDB使用下一键锁（next-key lock）来进行搜索和索引扫描，从而防止出现幻读。\n\n### 7.插入意向锁（&nbsp;Insert Intention Locks）\n\n插入意图锁是一种间隙锁，它由行插入之前的插入操作设置。该锁表示插入的意图，插入到相同索引间隙的多个事务如果不在间隙内的相同位置插入，则无需等待对方。\n\n假设有值为4和7的索引记录。分别尝试插入值5和6的独立事务，在获得插入行上的互斥锁之前，每个锁的插入意图锁间隔在4和7之间，但不会相互阻塞，因为行不冲突。\n\n### 8.自增锁（AUTO-INC锁）\n\nAUTO-INC锁是一种特殊的表级锁，由插入到具有AUTO_INCREMENT列的表中的事务获得。在最简单的情况下，如果一个事务正在向表中插入值，那么任何其他事务都必须等待在该表中进行自己的插入，以便第一个事务插入的行接收连续的主键值。\n\n### 9.空间索引的谓词锁（&nbsp;Predicate Locks for Spatial Indexes）\n\n为了支持具有空间索引的表的隔离级别，InnoDB使用谓词锁。空间索引包含最小边界矩形(minimum bounding rectangle, MBR)值，因此InnoDB通过对查询使用的MBR值设置一个谓词锁来强制索引的一致性读取。其他事务不能插入或修改符合查询条件的行。\n\n参考：[https://dev.mysql.com/doc/refman/5.7/en/innodb-locking.html](https://dev.mysql.com/doc/refman/5.7/en/innodb-locking.html)\n","tags":["MySQL"]},{"title":"Hadoop学习笔记——WordCount","url":"/Hadoop学习笔记——WordCount.html","content":"**1.在IDEA下新建工程,选择from Mevan**\n\n**GroupId:WordCount**\n\n**<strong>ArtifactId:com.hadoop.1st**</strong>\n\n**<strong>Project name:<strong>WordCount**</strong></strong>\n\n**<img src=\"/images/517519-20170326182151611-1075878261.png\" alt=\"\" width=\"523\" height=\"411\" />**\n\n**2.pom.xml文件**\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>WordCount</groupId>\n    <artifactId>com.hadoop.1st</artifactId>\n    <version>1.0-SNAPSHOT</version>\n\n    <repositories>\n        <repository>\n            <id>apache</id>\n            <url>http://maven.apache.org</url>\n        </repository>\n    </repositories>\n\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-core</artifactId>\n            <version>1.2.1</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-common</artifactId>\n            <version>2.7.1</version>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <artifactId>maven-dependency-plugin</artifactId>\n                <configuration>\n                    <excludeTransitive>false</excludeTransitive>\n                    <stripVersion>true</stripVersion>\n                    <outputDirectory>./lib</outputDirectory>\n                </configuration>\n\n            </plugin>\n        </plugins>\n    </build>\n</project>\n\n```\n\n**<!--more-->\n&nbsp;3.main/java目录下新建WordCount.java文件**\n\n```\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;\n\nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\n\n/**\n * Created by common on 17-3-26.\n */\npublic class WordCount {\n    public static class WordCountMap extends\n            Mapper<LongWritable, Text, Text, IntWritable> {\n\n        private final IntWritable one = new IntWritable(1);\n        private Text word = new Text();\n\n        public void map(LongWritable key, Text value, Context context)\n                throws IOException, InterruptedException {\n            String line = value.toString();\n            StringTokenizer token = new StringTokenizer(line);\n            while (token.hasMoreTokens()) {\n                word.set(token.nextToken());\n                context.write(word, one);\n            }\n        }\n    }\n\n    public static class WordCountReduce extends\n            Reducer<Text, IntWritable, Text, IntWritable> {\n\n        public void reduce(Text key, Iterable<IntWritable> values,\n                           Context context) throws IOException, InterruptedException {\n            int sum = 0;\n            for (IntWritable val : values) {\n                sum += val.get();\n            }\n            context.write(key, new IntWritable(sum));\n        }\n    }\n\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        Job job = new Job(conf);\n        job.setJarByClass(WordCount.class);\n        job.setJobName(\"wordcount\");\n\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        job.setMapperClass(WordCountMap.class);\n        job.setReducerClass(WordCountReduce.class);\n\n        job.setInputFormatClass(TextInputFormat.class);\n        job.setOutputFormatClass(TextOutputFormat.class);\n\n        FileInputFormat.addInputPath(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        job.waitForCompletion(true);\n    }\n}\n\n```\n\n**&nbsp;4.在src同级目录下新建input目录,以及下面的test.segmented文件**\n\n<img src=\"/images/517519-20170326182559002-33010031.png\" alt=\"\" />\n\n**test.segmented文件内容**\n\n```\naa\nbb\ncc\ndd\naa\ncc\nee\nff\nff\ngg\nhh\naa\n\n```\n\n**4.在run configuration下设置运行方式为Application <br />**\n\n**<img src=\"/images/517519-20170326182905955-532086657.png\" alt=\"\" width=\"860\" height=\"590\" />**\n\n**5.运行java文件,将会生成output目录,part-r-00000为运行的结果,下次运行必须删除output目录,否则会报错**\n\n**<img src=\"/images/517519-20170326183009221-1576860669.png\" alt=\"\" width=\"897\" height=\"488\" />**\n\n&nbsp;\n","tags":["Hadoop"]},{"title":"Hadoop学习笔记——安装Hadoop","url":"/Hadoop学习笔记——安装Hadoop.html","content":"```\nsudo mv /home/common/下载/hadoop-2.7.2.tar.gz /usr/local\nsudo tar -xzvf hadoop-2.7.2.tar.gz\nsudo mv hadoop-2.7.2 hadoop    #改个名\n\n```\n\n<!--more-->\n&nbsp;在etc/profile文件中添加\n\n```\nexport HADOOP_HOME=/usr/local/hadoop\nexport PATH=.:$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin\n\n```\n\n**&nbsp;1.修改/usr/local/hadoop/etc/hadoop/hadoop-env.sh文件**\n\n```\nexport JAVA_HOME=/usr/lib/jvm/jdk1.8.0_121\n\n```\n\n**2.修改/usr/local/hadoop/etc/hadoop/core-site.xml文件**\n\n```\n<configuration>\n\n        <property>\n                <name>fs.default.name</name>\n                <value>hdfs://master:9000</value>\n        </property>\n        <property>\n                <name>hadoop.tmp.dir</name>\n                <value>~/software/apache/hadoop-2.9.1/tmp</value>\n        </property>\n        <property>\n                <name>hadoop.native.lib</name>\n                <value>false</value>\n        </property>\n\n</configuration>\n\n```\n\n&nbsp;在/etc/hosts中添加自己的外网ip\n\n```\nXXXX    master\n\n```\n\n&nbsp;如果在工程中需要访问HDFS，需要在resources中添加 core-site.xml文件\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<!--\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n\n<!-- Put site-specific property overrides in this file. -->\n\n<configuration>\n\n  <property>\n    <name>fs.defaultFS</name>\n    <value>hdfs://master:9000</value>\n  </property>\n\n</configuration>\n\n```\n\n&nbsp;\n\n&nbsp;**3.修改/usr/local/hadoop/etc/hadoop/hdfs-site.xml文件**\n\n```\n<configuration>\n\n        <property>\n                <name>dfs.replication</name>\n                <value>1</value>\n        </property>\n        <property>\n                <name>dfs.name.dir</name>\n                <value>file:/home/lintong/software/apache/hadoop-2.9.1/tmp/dfs/name</value>\n        </property>\n        <property>\n                <name>dfs.data.dir</name>\n                <value>file:/home/lintong/software/apache/hadoop-2.9.1/tmp/dfs/data</value>\n        </property>\n        <property>\n                <name>dfs.namenode.checkpoint.dir</name>\n                <value>file:/home/lintong/software/apache/hadoop-2.9.1/tmp/dfs/namenode</value>\n        </property>\n        <property>\n                <name>dfs.permissions</name>\n                <value>false</value>\n        </property>\n\n</configuration>\n\n```\n\n&nbsp;\n\n&nbsp;**4./usr/local/hadoop/etc/hadoop/mapred-site.xml(修改mapred-site.xml.template的那个文件)**\n\n```\n<configuration>\n\n        <property>\n                <name>mapreduce.framework.name</name>\n                <value>yarn</value>\n        </property>\n\n</configuration>\n\n```\n\n&nbsp;\n\n**5. /usr/local/hadoop/etc/hadoop/yarn-site.xml**\n\n```\n<configuration>\n\n<!-- Site specific YARN configuration properties -->\n        <property>\n                <name>yarn.nodemanager.aux-services</name>\n                <value>mapreduce_shuffle</value>\n        </property>\n        <property>\n                <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n                <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n        </property>\n\n</configuration>\n\n```\n\n&nbsp;\n\n**6.使得/etc/profile生效**\n\n```\nsudo source /etc/profile\n\n```\n\n**&nbsp;/etc/profile文件内容**\n\n```\nexport JAVA_HOME=/usr/lib/jvm/jdk1.8.0_121\nexport JRE_HOME=${JAVA_HOME}/jre \nexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib \nexport PATH=${JAVA_HOME}/bin:$PATH\n\nexport PATH=/usr/local/texlive/2015/bin/x86_64-linux:$PATH \nexport MANPATH=/usr/local/texlive/2015/texmf-dist/doc/man:$MANPATH \nexport INFOPATH=/usr/local/texlive/2015/texmf-dist/doc/info:$INFOPATH\n\nexport HADOOP_HOME=/usr/local/hadoop\nexport PATH=.:$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin\n\nexport M2_HOME=/opt/apache-maven-3.3.9\nexport M2=$M2_HOME/bin\nexport PATH=$M2:$PATH\n\nexport GRADLE_HOME=/opt/gradle/gradle-3.4.1\nexport PATH=$GRADLE_HOME/bin:$PATH\n\n```\n\n**&nbsp;~/.bashrc文件内容**\n\n```\nexport HADOOP_INSTALL=/usr/local/hadoop\nexport PATH=$PATH:$HADOOP_INSTALL/bin\nexport PATH=$PATH:$HADOOP_INSTALL/sbin\nexport HADOOP_MAPRED_HOME=$HADOOP_INSTALL\nexport HADOOP_COMMON_HOME=$HADOOP_INSTALL\nexport HADOOP_HDFS_HOME=$HADOOP_INSTALL\nexport YARN_HOME=$HADOOP_INSTALL\n\n```\n\n**SSH和Hadoop用户设置可以参考 <br />**\n\n[**http://www.cnblogs.com/CheeseZH/p/5051135.html**](http://www.cnblogs.com/CheeseZH/p/5051135.html)\n\n[**http://www.powerxing.com/install-hadoop/**](http://www.powerxing.com/install-hadoop/)\n\n**免密登录** \n\n```\ncat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n$ ssh localhost\n\n```\n\n&nbsp;\n\n**<i>如果遇到dataNode不能启动的问题,参考**\n\n**[http://www.aboutyun.com/thread-12803-1-1.html](http://www.aboutyun.com/thread-12803-1-1.html)**\n\n去Hadoop/log目录下查看log日志文件,然后在/usr/local/hadoop/tmp/dfs/data/current目录下修改VERSION文件中的内容\n\n&nbsp;\n\n**<ii>**[ubuntu Hadoop启动报Error: JAVA_HOME is not set and could not be found解决办法](https://blog.csdn.net/shengmingqijiquan/article/details/52628377)\n\n修改/etc/hadoop/hadoop-env.sh中设JAVA_HOME为绝对路径\n\n&nbsp;\n\n**Hadoop目录下的权限**\n\n**<img src=\"/images/517519-20170326113506315-1986368815.png\" alt=\"\" width=\"540\" height=\"323\" />**\n\n&nbsp;\n\n**格式化一个新的分布式文件系统**\n\n```\nhdfs namenode -format\n\n```\n\n**运行Hadoop**\n\n**<img src=\"/images/517519-20170326114429486-685152261.png\" alt=\"\" width=\"542\" height=\"324\" />**\n\n**运行Hadoop示例**\n\n```\n./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar pi 2 5\n\n```\n\n**&nbsp;输出**\n\n```\nNumber of Maps  = 2\nSamples per Map = 5\nWrote input for Map #0\nWrote input for Map #1\nStarting Job\n17/03/26 11:49:47 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n17/03/26 11:49:47 INFO input.FileInputFormat: Total input paths to process : 2\n17/03/26 11:49:47 INFO mapreduce.JobSubmitter: number of splits:2\n17/03/26 11:49:48 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490497943530_0002\n17/03/26 11:49:48 INFO impl.YarnClientImpl: Submitted application application_1490497943530_0002\n17/03/26 11:49:48 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1490497943530_0002/\n17/03/26 11:49:48 INFO mapreduce.Job: Running job: job_1490497943530_0002\n17/03/26 11:49:55 INFO mapreduce.Job: Job job_1490497943530_0002 running in uber mode : false\n17/03/26 11:49:55 INFO mapreduce.Job:  map 0% reduce 0%\n17/03/26 11:50:02 INFO mapreduce.Job:  map 100% reduce 0%\n17/03/26 11:50:08 INFO mapreduce.Job:  map 100% reduce 100%\n17/03/26 11:50:08 INFO mapreduce.Job: Job job_1490497943530_0002 completed successfully\n17/03/26 11:50:08 INFO mapreduce.Job: Counters: 49\n\tFile System Counters\n\t\tFILE: Number of bytes read=50\n\t\tFILE: Number of bytes written=353898\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=524\n\t\tHDFS: Number of bytes written=215\n\t\tHDFS: Number of read operations=11\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=3\n\tJob Counters \n\t\tLaunched map tasks=2\n\t\tLaunched reduce tasks=1\n\t\tData-local map tasks=2\n\t\tTotal time spent by all maps in occupied slots (ms)=9536\n\t\tTotal time spent by all reduces in occupied slots (ms)=3259\n\t\tTotal time spent by all map tasks (ms)=9536\n\t\tTotal time spent by all reduce tasks (ms)=3259\n\t\tTotal vcore-milliseconds taken by all map tasks=9536\n\t\tTotal vcore-milliseconds taken by all reduce tasks=3259\n\t\tTotal megabyte-milliseconds taken by all map tasks=9764864\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=3337216\n\tMap-Reduce Framework\n\t\tMap input records=2\n\t\tMap output records=4\n\t\tMap output bytes=36\n\t\tMap output materialized bytes=56\n\t\tInput split bytes=288\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=2\n\t\tReduce shuffle bytes=56\n\t\tReduce input records=4\n\t\tReduce output records=0\n\t\tSpilled Records=8\n\t\tShuffled Maps =2\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=2\n\t\tGC time elapsed (ms)=319\n\t\tCPU time spent (ms)=2570\n\t\tPhysical memory (bytes) snapshot=719585280\n\t\tVirtual memory (bytes) snapshot=5746872320\n\t\tTotal committed heap usage (bytes)=513802240\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=236\n\tFile Output Format Counters \n\t\tBytes Written=97\nJob Finished in 21.472 seconds\nEstimated value of Pi is 3.60000000000000000000\n\n```\n\n&nbsp;\n\n可以访问 Web 界面 [http://localhost:50070](http://localhost:50070) 查看 NameNode 和 Datanode 信息，还可以在线查看 HDFS 中的文件\n\n<img src=\"/images/517519-20170326155254815-1543951658.png\" alt=\"\" width=\"716\" height=\"572\" />\n\n&nbsp;\n\n启动 **YARN** 之后，运行实例的方法还是一样的，仅仅是资源管理方式、任务调度不同。观察日志信息可以发现，不启用 YARN 时，是 &ldquo;mapred.LocalJobRunner&rdquo; 在跑任务，启用 YARN 之后，是 &ldquo;mapred.YARNRunner&rdquo; 在跑任务。启动 YARN 有个好处是可以通过 Web 界面查看任务的运行情况：[http://localhost:8088/cluster](http://localhost:8088/cluster)\n\n<img src=\"/images/517519-20170326155502830-213646622.png\" alt=\"\" width=\"822\" height=\"408\" />\n\n**点击history,查看每一个任务,如果遇到master:19888不能访问的情况,在目录下执行**\n\n```\nmr-jobhistory-daemon.sh start historyserver\n\n```\n\n**<img src=\"/images/517519-20170326155552033-1690347143.png\" alt=\"\" width=\"781\" height=\"437\" />**\n\n&nbsp;\n\nhdfs解除安全模式\n\n```\nbin/hadoop dfsadmin -safemode leave\n\n```\n\n&nbsp;&nbsp;\n\n关于**Hadoop的架构**请关注下面这篇博文的内容\n\n```\nHadoop HDFS概念学习系列之初步掌握HDFS的架构及原理1（一）\n\n```\n\n关于**Hadoop中HDFS的读取过程**请关注下面这篇博文的内容\n\n```\nHadoop HDFS概念学习系列之初步掌握HDFS的架构及原理2（二）\n\n```\n\n关于**Hadoop中HDFS的写入过程**请关注下面这篇博文的内容\n\n```\nHadoop HDFS概念学习系列之初步掌握HDFS的架构及原理3（三）\n\n```\n\n关于**Hadoop中SNN的作用**请关注下面这篇博文的内容\n\n```\nhttp://blog.csdn.net/xh16319/article/details/31375197\n\n```\n\n&nbsp;\n","tags":["Hadoop"]},{"title":"Ubuntu下从外网上北邮人BT","url":"/Ubuntu下从外网上北邮人BT.html","content":"## 1.使用VPN+ipv6（测试于2017-01，该方法已经不可用）\n\n首先你需要有北邮的VPN账号和密码，只要是北邮的学生都有\n\n账号和密码不懂的请查看 [VPN账号密码说明](http://nic.bupt.edu.cn/content/content.php?p=8_18_242)\n\n接下来登录[https://sslvpn.bupt.edu.cn](https://sslvpn.bupt.edu.cn)，输入账号和密码\n\n<img src=\"/images/517519-20170123212313159-1199788839.png\" alt=\"\" width=\"677\" height=\"380\" />\n\n已经登录好了\n\n<img src=\"/images/517519-20170123212452737-1944352699.png\" alt=\"\" width=\"692\" height=\"244\" />\n\n但是还是不能上BYR BT，是因为没有把ipv4转成ipv6，在Ubuntu下进行转换很简单，\n\n只需要安装miredo，\n\n输入命令\n\n```\nsudo apt-get install miredo\n\n```\n\n<!--more-->\n&nbsp;安装好了之后，就可以上BYR BT了，不过速度还是ipv4的速度\n\n<img src=\"/images/517519-20170123212738706-890767087.png\" alt=\"\" width=\"905\" height=\"426\" />\n\n## 2.新方法：使用支持ipv6的网络+byr.pt（测试于2024-04）\n\n方法1已经失效，可以使用方法2\n\n找一个支持ipv6的梯子，然后测试一下是否支持ipv6，可以使用test-ipv6进行测试\n\n[https://test-ipv6.com/index.html.zh_CN](https://test-ipv6.com/index.html.zh_CN)\n\n能正常显示ipv6地址就是支持ipv6\n\n<img src=\"/images/517519-20240427105936314-1536621275.png\" width=\"500\" height=\"301\" loading=\"lazy\" />\n\n访问下面这个BYR BT地址，注意是.pt，不是.bt\n\n[https://byr.pt/](https://byr.pt/)\n\n<img src=\"/images/517519-20240427110145876-2007425937.png\" width=\"1000\" height=\"506\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["Linux"]},{"title":"机器学习——利用SVD简化数据","url":"/机器学习——利用SVD简化数据.html","content":"**奇异值分解**（Singular Value Decompositon,**SVD**），可以实现用小得多的数据集来表示原始数据集。\n\n<!--more-->\n&nbsp;\n\n**优点**：简化数据，取出噪声，提高算法的结果\n\n**缺点**：数据的转换可能难以理解\n\n**适用数据类型**：数值型数据\n\n&nbsp;\n\n**SVD**最早的应用之一是**信息检索**，我们称利用SVD的方法为**隐形语义索引（LSI）**或者**隐形语义分析（LSA）**。\n\n在**LSI**中，一个矩阵是有**文档**和**词语**组成的。当我们在该矩阵上应用SVD的时候，就会构建出**多个奇异值**。这些**奇异值代表了文档中的概念或者主题**，这一特点可以用于更高效的文档检索。\n\n**SVD**的另一个应用就是**推荐系统**。简单版本的推荐系统能够计算项或者人之间的相似度。更先进的方法则先利用SVD从数据中构建一个主题空间，然后再在该空间下计算其相似度。\n\n&nbsp;\n\nSVD将原始的**数据集矩阵Data**分解成三个矩阵 <img src=\"/images/517519-20170119153025406-1628504586.gif\" alt=\"\" />、<img src=\"/images/517519-20170119153040796-1459722075.gif\" alt=\"\" /> 和 <img src=\"/images/517519-20170119153052437-1176182922.gif\" alt=\"\" />。\n\n如果**数据集矩阵Data**是M&times;N的，那么 <img src=\"/images/517519-20170119153025406-1628504586.gif\" alt=\"\" /> 是M&times;M的、<img src=\"/images/517519-20170119153040796-1459722075.gif\" alt=\"\" /> 是M&times;N的、<img src=\"/images/517519-20170119153052437-1176182922.gif\" alt=\"\" /> 是N&times;N的。\n\n　　<img src=\"/images/517519-20170119154105187-329396320.gif\" alt=\"\" />\n\n矩阵 <img src=\"/images/517519-20170119153040796-1459722075.gif\" alt=\"\" /> 中**只有从大到小排列的对角元素**。在某个奇异值的数目（r个）之后，其他的奇异值都置为0，这就意味这数据集中仅有r个重要特征，而其余特征则都是噪声或者冗余特征。\n\n&nbsp;\n\n**利用Python实现SVD**\n\n```\n>> X=[0.3619 0.2997 0.1331 0.3296;0.1695 0.3628 0.0817 0.2826;0.1159 0.5581 0.0828 0.3718;0.1508 0.1077 0.0539 0.1274]　　#Matlab\n\nX =\n\n    0.3619    0.2997    0.1331    0.3296\n    0.1695    0.3628    0.0817    0.2826\n    0.1159    0.5581    0.0828    0.3718\n    0.1508    0.1077    0.0539    0.1274\n\n```\n\n&nbsp;\n\n```\n>> [U,S,V] = svd (X)　　　　#Matlab\n\nU =\n\n   -0.5468    0.6999    0.1302   -0.4406\n   -0.4846   -0.0839    0.5883    0.6420\n   -0.6496   -0.6312   -0.3105   -0.2883\n   -0.2102    0.3234   -0.7352    0.5574\n\n\nS =\n\n    1.0245         0         0         0\n         0    0.2608         0         0\n         0         0    0.0001         0\n         0         0         0    0.0000\n\n\nV =\n\n   -0.3778    0.8233   -0.4206   -0.0508\n   -0.7076   -0.5297   -0.3661   -0.2911\n   -0.1733    0.1974    0.6302   -0.7307\n   -0.5715    0.0518    0.5403    0.615\n\n```\n\n**&nbsp;Python**\n\n```\n>>> from numpy import *\n>>> U,Sigma,VT = linalg.svd([[0.3619,0.2997,0.1331,0.3296],[0.1695,0.3628,0.0817,0.2826],[0.1159,0.5581,0.0828,0.3718],[0.1508,0.1077,0.0539,0.1274]])\n>>> U\narray([[-0.54683102,  0.69993064,  0.13018303, -0.44059655],\n       [-0.48455132, -0.08387773,  0.58827674,  0.64195407],\n       [-0.64962251, -0.63124863, -0.31049494, -0.28828573],\n       [-0.21018197,  0.32339881, -0.73523857,  0.55736971]])\n>>> Sigma\narray([  1.02445357e+00,   2.60778615e-01,   8.12946379e-05,\n         3.22769863e-05])\n>>> VT\narray([[-0.37777826, -0.70756881, -0.17325197, -0.57150129],\n       [ 0.82328242, -0.52968851,  0.19737725,  0.05175294],\n       [-0.42060604, -0.36612216,  0.63019332,  0.5402791 ],\n       [-0.05079576, -0.29108595, -0.73067254,  0.61547251]])\n\n```\n\n&nbsp;可以看到，在Sigma矩阵中8.12946379e-05 和 3.22769863e-05 值的量级太小了，所以可以忽略\n\n所以Data矩阵的值就成了&nbsp; <img src=\"/images/517519-20170119171054406-625569361.gif\" alt=\"\" />\n\n<img src=\"/images/517519-20170119171137562-1829581401.png\" alt=\"\" />\n\n```\n>>> U,Sigma,VT = linalg.svd([[0.3619,0.2997,0.1331,0.3296],[0.1695,0.3628,0.0817,0.2826],[0.1159,0.5581,0.0828,0.3718],[0.1508,0.1077,0.0539,0.1274]])\n>>> U\narray([[-0.54683102,  0.69993064,  0.13018303, -0.44059655],\n       [-0.48455132, -0.08387773,  0.58827674,  0.64195407],\n       [-0.64962251, -0.63124863, -0.31049494, -0.28828573],\n       [-0.21018197,  0.32339881, -0.73523857,  0.55736971]])\n>>> Sigma\narray([  1.02445357e+00,   2.60778615e-01,   8.12946379e-05,\n         3.22769863e-05])\n>>> VT\narray([[-0.37777826, -0.70756881, -0.17325197, -0.57150129],\n       [ 0.82328242, -0.52968851,  0.19737725,  0.05175294],\n       [-0.42060604, -0.36612216,  0.63019332,  0.5402791 ],\n       [-0.05079576, -0.29108595, -0.73067254,  0.61547251]])\n>>> Sig3 = mat([[Sigma[0],0,0],[0,Sigma[1],0],[0,0,Sigma[2]]])\n>>> U[:,:3]*Sig3*VT[:3,:]\nmatrix([[ 0.36189928,  0.29969586,  0.13308961,  0.32960875],\n        [ 0.16950105,  0.36280603,  0.08171514,  0.28258725],\n        [ 0.11589953,  0.55809729,  0.0827932 ,  0.37180573],\n        [ 0.15080091,  0.10770524,  0.05391314,  0.12738893]])\n\n```\n\n&nbsp;\n\n```\n>>> import numpy as np\n>>> U\narray([[-0.54683102,  0.69993064],\n       [-0.48455132, -0.08387773],\n       [-0.64962251, -0.63124863],\n       [-0.21018197,  0.32339881]])\n>>> Sigma\narray([[ 1.02445357,  0.        ],\n       [ 0.        ,  0.26077861]])\n>>> VT\narray([[-0.37777826, -0.70756881, -0.17325197, -0.57150129],\n       [ 0.82328242, -0.52968851,  0.19737725,  0.05175294]])\n\n>>> M = np.dot(U,Sigma)\n>>> np.dot(M,VT)　　#可以使用np.dot进行矩阵乘法\narray([[ 0.36190373,  0.29969974,  0.13308294,  0.32960304],\n       [ 0.16952117,  0.36282354,  0.081685  ,  0.28256141],\n       [ 0.11588891,  0.55808805,  0.08280911,  0.37181937],\n       [ 0.15077578,  0.10768336,  0.05395081,  0.12742122]])\n\n```\n\n**&nbsp;经过SVD之后生成的三个矩阵相乘，得到的结果和原来的矩阵差不多**\n\n&nbsp;\n\n**基于协同过滤（collaborative filtering）的推荐引擎**\n\n**协同过滤**是通过将用户和其他用户的数据进行对比来实现推荐的。这里的数据是从概念上组织成了类似矩阵的形式。当数据采用这种方式进行组织的时候，我们就可以**比较**用户或者物品之间的**相似度**。比如，如果**电影**和**用户看过的电影之间的相似度很高**，推荐算法就会认为用户喜欢这部电影。\n\n&nbsp;\n\n**相似度计算**\n\n**第一种**：使用**欧式距离**，**相似度**=1/（1+距离）\n\n　　当距离为0的时候，相似度为1；当距离很大的时候，相似度趋近于0\n\n**第二种**：**皮尔逊相关系数**\n\n　　**皮尔逊相关系数**度量的是两个向量之间的相似度，相对于欧式距离的一个优势是，它对用户评级的量级并不敏感。\n\n　　**皮尔逊相关系数**的取值范围在-1到+1之间，在NumPy中由**函数corrcoef()**计算\n\n**第三种**：**余弦相似度**\n\n　　**余弦相似度**计算的是两个向量夹角的余弦值，如果夹角为90度，则相似度为0；如果两个向量的方向相同，则相似度为1\n\n**　　余弦相似度**的取值范围在-1到+1之间，在NumPy中由**函数linalg.norm()**计算\n\n　　<img src=\"/images/517519-20170121211935562-1325014210.gif\" alt=\"\" />\n\n&nbsp;\n\n```\nfrom numpy import *\nfrom numpy import linalg as la\n\ndef ecludSim(inA,inB):　　　　#欧式距离\n    return 1.0/(1.0 + la.norm(inA - inB))\n\ndef pearsSim(inA,inB):　　　　#皮尔逊相关系数\n    if len(inA) < 3 : return 1.0\n    return 0.5+0.5*corrcoef(inA, inB, rowvar = 0)[0][1]\n\ndef cosSim(inA,inB):　　　　　　#余弦相似度\n    num = float(inA.T*inB)\n    denom = la.norm(inA)*la.norm(inB)\n    return 0.5+0.5*(num/denom)\n\n```\n\n&nbsp;\n\n```\n# coding:utf-8\n# !/usr/bin/env python\n\nimport svdRec\nfrom numpy import *\n\nif __name__ == '__main__':\n\tmyMat = mat(svdRec.loadExData())\n\tprint svdRec.ecludSim(myMat[:,0],myMat[:,4])\t#矩阵第一列和第五列的欧氏距离相似度\n\tprint svdRec.pearsSim(myMat[:,0],myMat[:,4])\t#矩阵第一列和第五列的皮尔逊相关系数相似度\n\tprint svdRec.cosSim(myMat[:,0],myMat[:,4])\t\t#矩阵第一列和第五列的余弦相似度\n\n```\n\n```\n0.129731907557\n0.205965381738\n0.5\n\n```\n\n&nbsp;\n","tags":["ML"]},{"title":"Python自然语言处理学习——jieba分词","url":"/Python自然语言处理学习——jieba分词.html","content":"**jieba**&mdash;&mdash;**&ldquo;结巴&rdquo;中文分词**是[sunjunyi](https://githuber.cn/people/167837)开发的一款**Python中文分词组件**，可以在Github上查看[jieba项目](https://github.com/fxsjy/jieba)。\n\n要使用jieba中文分词，首先需要安装jieba中文分词，作者给出了如下的**安装方法**：\n\n1.全自动安装：`easy_install jieba` 或者 `pip install jieba` / `pip3 install jieba`\n\n2.半自动安装：先下载 [http://pypi.python.org/pypi/jieba/](http://pypi.python.org/pypi/jieba/) ，解压后运行 `python setup.py install`\n\n3.手动安装：将 jieba 目录放置于当前目录或者 site-packages 目录\n\n<!--more-->\n&nbsp;\n\n**作者介绍其采用的算法**：\n\n1.基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)\n\n2.采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合\n\n3.对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法\n\n&nbsp;\n\n**主要功能**：\n\n**1.分词**\n\n　　**分词功能**主要有两个方法 **jieba.cut **和 **jieba.cut_for_search**，\n\n其中 **jieba.cut方法 **接受**三个输入参数**:\n\n　　1.需要分词的字符串；\n\n　　2.cut_all 参数用来控制是否采用全模式；\n\n　　3.HMM 参数用来控制是否使用 HMM 模型\n\n```\njieba.cut(\"我来到北京清华大学\", cut_all=True)\n\n```\n\n&nbsp;其中 **jieba.cut****<strong>_for_search**方法 </strong>接受**两个输入参数**:\n\n　　1.需要分词的字符串；\n\n　　2.是否使用 HMM 模型。\n\n　　该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细\n\n```\njieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\")\n\n```\n\n&nbsp;**jieba.cut方法 和** **jieba.cut****<strong>_for_search**方法 </strong>返回的结构都是一个可迭代的 **generator**，可以使用 for 循环来获得分词后得到的每一个词语(unicode)\n\n也可以使用 **jieba.lcut方法 和 ****jieba.lcut****<strong>_for_search**方法 </strong>直接返回 **list**\n\n&nbsp;\n\n**作者的说明：**待分词的字符串可以是 unicode 或 UTF-8 字符串、GBK 字符串。\n\n**注意**：不建议直接输入 GBK 字符串，可能无法预料地错误解码成 UTF-8\n\n&nbsp;\n\n**以下是作者给出的demo和运行结果**：\n\n```\n# coding:utf-8\n# !/usr/bin/env python\n\nimport jieba \n\nif __name__ == '__main__':\n\tseg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True) \n\tprint(\"Full Mode: \" + \"/ \".join(seg_list))\t\t#全模式 \n\tseg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False) \n\tprint(\"Default Mode: \" + \"/ \".join(seg_list))\t#精确模式 \n\tseg_list = jieba.cut(\"他来到了网易杭研大厦\")\t\t#默认是精确模式 \n\tprint(\", \".join(seg_list)) \n\tseg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\")\t#搜索引擎模式 \n\tprint(\", \".join(seg_list))\n\n```\n\n**输出的结果**\n\n<img src=\"/images/517519-20170118230415890-1812850785.png\" alt=\"\" width=\"947\" height=\"127\" />\n\n**可以看到：<br />**\n\n　　**全模式**：**试图将句子最精确地切开，适合文本分析**，输出的是所有可能的分词组合，比如清华大学，会被分成：清华，清华大学，华大，大学\n\n　　**默认模型（精确模型）**：**把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义**，比如清华大学，只会输出清华大学\n\n　　**搜索引擎模式**：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词\n\n还有另外一个方法 **`jieba.Tokenizer(dictionary=DEFAULT_DICT)，用于新建自定义分词器，可用于同时使用不同词典。`**\n\n**`<code>jieba.dt` 为默认分词器，所有全局分词相关函数都是该分词器的映射。</code>**\n\n&nbsp;\n","tags":["Python"]},{"title":"机器学习——利用PCA来简化数据","url":"/机器学习——利用PCA来简化数据.html","content":"**降维技术**的**好处**：\n\n　　1.使得数据集更易使用\n\n　　2.降低很多算法的计算开销\n\n　　3.取出噪声\n\n　　4.使得结果易懂\n\n<!--more-->\n&nbsp;\n\n在**已标注和未标注的数据**上都有**降维技术**，**降维的方法**：\n\n　　1.**主成分分析（Principal Component Analysis，PCA）**。在PCA中，数据从原来的坐标系**转换到新的坐标系**，新坐标系的选择是由数据本身决定的。第一个新坐标轴选择的是原始数据中**方差最大**的方向，第二个新坐标轴的选择和第一个坐标轴**正交且具有最大方差**的方向。该过程中一直重复，重复次数为原始数据中特征的数目。我们会发现，大部分方差都包含在最前面的几个新坐标轴中。因此，我们就可以忽略余下的坐标轴，即对数据进行了降维处理。\n\n　　2.**因子分析（Factor Analysis）**。在因子分析中，我们假设在观察数据的生成中有一些观察不到的**隐变量（latent variable）**。假设观察数据是这些隐变量和某些噪声的线性组合。那么**隐变量的数据可能比观察数据的数目少**，也就是说通过**找到隐变量就可以实现数据的降维**。\n\n　　3.**独立成分分析（Independent Component Analysis，ICA）**。ICA假设数据是从**N个数据源**生成的，这一点和因子分析有些类似。假设数据为多个数据源的混合观察结果，**这些数据源之间在统计上相互独立的**，而在PCA中只假设数据是不相关的。同因子分析一样，**如果数据源的数目少于观察数据的数目，则可以实现降维过程**。\n\n&nbsp;\n\n**主成分分析**\n\n**优点**：降低数据的复杂度，识别最重要的多个特征\n\n**缺点**：不一定需要，且可能损失有用信息\n\n**适用数据类型**：数值型数据\n\n&nbsp;\n\n对于下图中的**二维数据**，这个二维数据是**随机生成的**\n\n```\n# coding:utf-8\n# !/usr/bin/env python\n\n'''\nCreated on Jun 1, 2011\n\n@author: Peter\n'''\nfrom numpy import *\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nn = 1000 \t\t#number of points to create\nxcord0 = []\nycord0 = []\nxcord1 = []\nycord1 = []\nmarkers =[]\ncolors =[]\nfw = open('testSet.txt','w')\nfor i in range(n):\n    [r0,r1] = random.standard_normal(2)\t\t#随机生成一组二维数据(fFlyer,tats)\n    fFlyer = r0 + 9.0\n    tats = 1.0*r1 + fFlyer + 0\n    xcord0.append(fFlyer)\n    ycord0.append(tats)\n    fw.write(\"%f\\t%f\\n\" % (fFlyer, tats))\t#将二维数据(fFlyer,tats)写入文件中\n\nfw.close()\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.scatter(xcord0,ycord0, marker='^', s=90)\nplt.xlabel('hours of direct sunlight')\nplt.ylabel('liters of water')\nplt.show()\n\n```\n\n&nbsp;<img src=\"/images/517519-20170118163056781-1708175330.png\" alt=\"\" width=\"480\" height=\"407\" />\n\n&nbsp;\n\n&nbsp;使用下面的程序对**二维数据**进行**主成分分析，**输出**原始数据重构之后的矩阵<strong>lowDDataMat(蓝色)**</strong>，**第一主成分<strong>reconMat(红色)**</strong>\n\n```\ndef pca(dataMat, topNfeat=9999999):\t\t\t\t#数据矩阵, 输出前topNfeat个特征，如果topNfeat=1就是降维成1维\n    meanVals = mean(dataMat, axis=0)\t\t\t#求平均值\n    meanRemoved = dataMat - meanVals \t\t\t#去除平均值\n    covMat = cov(meanRemoved, rowvar=0)\t\t\t#计算协方差矩阵\n    eigVals,eigVects = linalg.eig(mat(covMat))\t#计算协方差矩阵的特征值和特征向量\n    eigValInd = argsort(eigVals)            \t#排序, 找出特征值大的. 其实就是与其他的变化最不相符\n    eigValInd = eigValInd[:-(topNfeat+1):-1]  \t#保留最上面的N个特征\n    redEigVects = eigVects[:,eigValInd]       \t#保留最上面的N个特征向量\n    lowDDataMat = meanRemoved * redEigVects\t\t#将数据转换到上述N个特征向量构建的新空间中\n    reconMat = (lowDDataMat * redEigVects.T) + meanVals\n    return lowDDataMat, reconMat\t\t\t\t#lowDDataMat是原始数据重构之后的矩阵(蓝色)，reconMat是第一主成分(红色)\n\n```\n\n&nbsp;\n\n```\n# coding:utf-8\n# !/usr/bin/env python\n\n'''\nCreated on Jun 1, 2011\n\n@author: Peter\n'''\nfrom numpy import *\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pca\n\ndataMat = pca.loadDataSet('testSet.txt')\nlowDMat, reconMat = pca.pca(dataMat, 1)\t\t#lowDDataMat是原始数据重构之后的矩阵(蓝色)，reconMat是第一主成分(红色)\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.scatter(dataMat[:,0], dataMat[:,1], marker='^', s=90)\nax.scatter(reconMat[:,0], reconMat[:,1], marker='o', s=50, c='red')\nplt.show()\n\n```\n\n<img src=\"/images/517519-20170118163714406-1403950244.png\" alt=\"\" width=\"477\" height=\"405\" />\n\n&nbsp;\n\n```\n# coding:utf-8\n# !/usr/bin/env python\n\n'''\nCreated on Jun 1, 2011\n\n@author: Peter\n'''\nfrom numpy import *\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pca\n\nn = 1000 #number of points to create\nxcord0 = []; ycord0 = []\nxcord1 = []; ycord1 = []\nxcord2 = []; ycord2 = []\nmarkers =[]\ncolors =[]\nfw = open('testSet3.txt','w')\nfor i in range(n):\t\t\t#随机生成1000个二维数据，这1000个二维数据会被标记上0/1/2三个标签\n    groupNum = int(3*random.uniform())\n    [r0,r1] = random.standard_normal(2)\n    if groupNum == 0:\n        x = r0 + 16.0\n        y = 1.0*r1 + x\n        xcord0.append(x)\n        ycord0.append(y)\n    elif groupNum == 1:\n        x = r0 + 8.0\n        y = 1.0*r1 + x\n        xcord1.append(x)\n        ycord1.append(y)\n    elif groupNum == 2:\n        x = r0 + 0.0\n        y = 1.0*r1 + x\n        xcord2.append(x)\n        ycord2.append(y)\n    fw.write(\"%f\\t%f\\t%d\\n\" % (x, y, groupNum))\n\nfw.close()\nfig = plt.figure()\n\nax = fig.add_subplot(211)\t\t#第一幅图\nax.scatter(xcord0,ycord0, marker='^', s=90)\nax.scatter(xcord1,ycord1, marker='o', s=50,  c='red')\nax.scatter(xcord2,ycord2, marker='v', s=50,  c='yellow')\n\nax = fig.add_subplot(212)\t\t#第二幅图\nmyDat = pca.loadDataSet('testSet3.txt')\n#myDat是(100,3)，降维之后lowDDat是(100,1)\nlowDDat,reconDat = pca.pca(myDat[:,0:2],1)\t#lowDDat是原始数据重构降维之后的矩阵，reconDat是第一主成分\nlabel0Mat = lowDDat[nonzero(myDat[:,2]==0)[0],:2][0] #get the items with label 0\nlabel1Mat = lowDDat[nonzero(myDat[:,2]==1)[0],:2][0] #get the items with label 1\nlabel2Mat = lowDDat[nonzero(myDat[:,2]==2)[0],:2][0] #get the items with label 2\n\n#ax.scatter(label0Mat[:,0],label0Mat[:,1], marker='^', s=90)\n#ax.scatter(label1Mat[:,0],label1Mat[:,1], marker='o', s=50,  c='red')\n#ax.scatter(label2Mat[:,0],label2Mat[:,1], marker='v', s=50,  c='yellow')\nax.scatter(label0Mat[:,0].flatten().A[0],zeros(shape(label0Mat)[0]), marker='^', s=90)\nax.scatter(label1Mat[:,0].flatten().A[0],zeros(shape(label1Mat)[0]), marker='o', s=50,  c='red')\nax.scatter(label2Mat[:,0].flatten().A[0],zeros(shape(label2Mat)[0]), marker='v', s=50,  c='yellow')\nplt.show()\n\n```\n\n&nbsp;<img src=\"/images/517519-20170118212507406-583095308.png\" alt=\"\" width=\"510\" height=\"432\" />\n\n&nbsp;\n\n**PCA可以从数据中识别其主要特征，它是通过沿着数据最大方差方向旋转坐标轴来实现的。选择方差最大的方向作为第一条坐标轴，后续坐标轴则与前面的坐标轴正交。协方差矩阵上的特征值分析可以用一系列的正交坐标轴来获取。**\n","tags":["ML"]},{"title":"特征预处理——异常值处理","url":"/特征预处理——异常值处理.html","content":"pandas是python特征预处理常用的框架\n\n## 1.查看数据\n\n加载数据\n\n```\n#-*- coding: utf-8 -*-\nimport pandas as pd\n\ntrain_data = pd.read_csv(\"./data/train.csv\")\nprint(train_data)\n\n```\n\n<img src=\"/images/517519-20221029164515738-1420097445.png\" width=\"400\" height=\"137\" loading=\"lazy\" />\n\npandas显示DataFrame数据的所有列，set_option其他参数参考：[Pandas函数set_option()学习笔记](https://blog.csdn.net/qq_42662568/article/details/104993201)\n\n```\npd.set_option('expand_frame_repr', False) # 不换行\n#pd.set_option('display.width', 200)    # 列最多显示多少字符后换行\n\npd.set_option('display.min_rows', 20) # 至少显示20行数据 \npd.set_option('display.max_columns', None) # 显示所有列 \n\nprint(train_data)\n\n```\n\n<img src=\"/images/517519-20221029192305392-188351075.png\" width=\"600\" height=\"296\" loading=\"lazy\" />\n\n## 2.查看空值\n\n在pandas中的空值为NaN，isna()等同于isnull()\n\n```\nprint(train_data.isna()) # 判断所有值是否是na，转换成boolean值\n\n```\n\n对于csv字符串NA，用pandas读取的时候，会转换成NaN，对于字符串None，则不是NaN\n\n<img src=\"/images/517519-20221029202924191-117381866.png\" width=\"300\" height=\"109\" loading=\"lazy\" />\n\n<img src=\"/images/517519-20221029202754664-1368512968.png\" width=\"500\" height=\"371\" loading=\"lazy\" />\n\n统计字段中Nan空值的总数\n\n```\nprint(train_data.isna().sum())\n\n```\n\n<img src=\"/images/517519-20221029204255818-422581419.png\" width=\"180\" height=\"257\" loading=\"lazy\" />\n\n查看某列是NaN的行\n\n```\nprint(train_data[train_data.MiscFeature.isnull()])\n\n```\n\n<img src=\"/images/517519-20221029210056011-545240882.png\" width=\"400\" height=\"218\" loading=\"lazy\" />\n\n查看有NaN值的列名\n\n```\nprint(train_data.columns[train_data.isnull().any()].tolist())\n\n['LotFrontage', 'Alley', 'MasVnrType', 'MasVnrArea', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Electrical', 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']\n\n```\n\n对有Nan值的列的空值比例进行排序\n\n```\ntotal = train_data.isna().sum().sort_values(ascending=False)\npercent = (train_data.isna().sum() / train_data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nprint(missing_data.head(20))\n\n              Total   Percent\nPoolQC         1453  0.995205\nMiscFeature    1406  0.963014\nAlley          1369  0.937671\nFence          1179  0.807534\nFireplaceQu     690  0.472603\nLotFrontage     259  0.177397\nGarageYrBlt      81  0.055479\nGarageCond       81  0.055479\nGarageType       81  0.055479\nGarageFinish     81  0.055479\nGarageQual       81  0.055479\nBsmtFinType2     38  0.026027\nBsmtExposure     38  0.026027\nBsmtQual         37  0.025342\nBsmtCond         37  0.025342\nBsmtFinType1     37  0.025342\nMasVnrArea        8  0.005479\nMasVnrType        8  0.005479\nElectrical        1  0.000685\nId                0  0.000000\n\n```\n\n## 3.删除和替换空值\n\n删除某一列为NaN值的行，只要有一列是NaN，该行数据就会被删除\n\n```\nprint(train_data.dropna(how='any'))\n\n```\n\n删除subset中某一列为NaN值的行，只要有一列是NaN，该行数据就会被删除\n\n```\nprint(train_data.dropna(subset=['Fence', 'MiscFeature'], how='any'))\n\n```\n\n删除所有列都是NaN值的行\n\n```\nprint(train_data.dropna(how='all'))\n\n```\n\n删除subset中所有列都是NaN值的行\n\n```\nprint(train_data.dropna(subset=['Fence', 'MiscFeature'], how='all'))\n\n```\n\n其他\n\n```\ndf.dropna(axis=1) #丢弃有缺失值的列\ndf.dropna(axis=1, how = 'all') #丢弃所有列中所有值均缺失的列\ndf.dropna(axis=0, subset=['name', 'age'])#丢弃name和age这两列中有缺失值的行\n\n```\n\n替换某列中的Nan值\n\n```\ntrain_data['Alley'].fillna(value='Not Found', inplace=True)\nprint(train_data)\n\n```\n\n<img src=\"/images/517519-20221029211751793-507471722.png\" width=\"500\" height=\"148\" loading=\"lazy\" />\n\n替换None字符串\n\n```\nprint(train_data.replace('None', np.nan))\n\n```\n\n<img src=\"/images/517519-20221029231036402-409933291.png\" width=\"400\" height=\"324\" loading=\"lazy\" />\n\n替换字符串的时候，如果替换改变原数据的值\n\n```\ntrain_data.replace('None', np.nan, inplace=True)\n\n```\n\n## 4.缺失值填充\n\n参考：[数据转化](https://wutaoblog.com.cn/2021/08/20/data_preprocess/)\n\n[6.4 缺失值插补](https://scikit-learn.org.cn/view/124.html)\n\n也可以使用sklearn对特征的缺失值进行填充，方法有2种：\n\n- 单变量：对第 i 个特征中的缺失值只使用该特征的某些信息来进行填充（impute.SimpleImputer）\n- 多变量：对第 i 个特征中的缺失值使用整个数据集的信息来填充（impute.IterativeImputer）\n\n### 数值型单变量缺失值填充\n\n```\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\n\n# 数值型单变量缺失值填充\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp.fit([[1, 2], [np.nan, 3], [7, 6]])\n>> SimpleImputer()\n\n# 缺失值填充 (1+7)/2=4和(2+3+6)/3=3.66666667\nX = [[np.nan, 2], [6, np.nan], [7, 6]]\nprint(imp.transform(X))\n[[4.         2.        ]\n [6.         3.66666667]\n [7.         6.        ]]\n\n```\n\n### 标称型单变量缺失值填充\n\n```\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nimport pandas as pd\n\n# 标称型变量缺失值填充\ndf = pd.DataFrame([[\"a\", \"x\"],\n                    [np.nan, \"y\"],\n                   [\"a\", np.nan],\n                    [\"b\", \"y\"]], dtype=\"category\")\n# 使用出现次数最多的值来填充缺失值\nimp = SimpleImputer(strategy=\"most_frequent\")\nprint(imp.fit_transform(df))\n\n[['a' 'x']\n ['a' 'y']\n ['a' 'y']\n ['b' 'y']]\n\n```\n\n### 多变量填充\n\n多变量填充是使用全部数据建模的方式进行填充缺失值：含有缺失值的特征被视为 y，而其他特征当作 x，对 （x, y）拟合回归模型，然后利用这个模型来预测 y 中的缺失值：\n\n```\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nimp = IterativeImputer(max_iter=10, random_state=0)\nimp.fit([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])\n>> IterativeImputer(random_state=0)\n\n# 拟合出第2个变量是第1个变量的2倍\nX_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]\nprint(np.round(imp.transform(X_test)))\n\n[[ 1.  2.]\n [ 6. 12.]\n [ 3.  6.]]\n\n```\n\n### 最近邻填充\n\n<!--more-->\n&nbsp;\n","tags":["Python"]},{"title":"机器学习——使用Apriori算法进行关联分析","url":"/机器学习——使用Apriori算法进行关联分析.html","content":"从大规模的数据集中**寻找隐含关系**被称作为**关联分析（association analysis）**或者**关联规则学习（association rule learning）**。\n\n<!--more-->\n&nbsp;\n\n**Apriori算法**\n\n**优点**：易编码实现\n\n**缺点**：在大数据集上可能较慢\n\n**使用数据类型**：数值型或者标称型数据\n\n&nbsp;\n\n**关联分析**寻找的是**隐含关系**，这些关系可以有两种形式：**频繁项集**或者**关联规则**。\n\n**频繁项集（frequent item sets）**是经常出现在一起的集合\n\n**关联规则（association rule）**暗示两种物品之间可能存在很强的关系\n\n&nbsp;\n\n**项集**的**支持度**和**可信度（置信度）**\n\n<img src=\"/images/517519-20170117205106739-1410094472.png\" alt=\"\" />\n\n&nbsp;\n\n**Apriori算法**的原理是**如果某个项集是频繁的**，那么它的**所有子集也是频繁的**；**如果一个项集是非频繁的**，那么它的**所有超集也是非频繁的**。\n\n&nbsp;\n\n<img src=\"/images/517519-20170117212439583-536376593.png\" alt=\"\" />\n","tags":["ML"]},{"title":"ubuntu安装mongo数据库","url":"/ubuntu安装mongo数据库.html","content":"安装**mongo数据库**，在shell下输入\n\n```\nsudo apt-get install mongodb\n\n```\n\n如果需要**在Python中使用mongo数据库**，还需要额外安装Python封装库\n\n```\npip install pymongo\n\n```\n\n<!--more-->\n&nbsp;检测安装是否成功，可以使用下面命令**在本地启动MongoDB**\n\n```\nmongod -dbpath .\n\n```\n\n&nbsp;\n\n在shell中输入**mongo**，就可以进入mongo数据库\n\n**查询数据库语句**\n\n```\n> show databases;\ncache\t0.0625GB\nlocal\t0.03125GB\n\n```\n\n**使用数据库语句**&nbsp;\n\n```\n> use cache;\nswitched to db cache\n\n```\n\n**建立一个叫做webpage的table**\n\n```\ndb.createCollection('webpage')\n\n```\n\n**查询全部table语句**\n\n```\n> show tables;\nsystem.indexes\nwebpage\n\n```\n\n**&nbsp;查询talbe中的内容**\n\n```\ndb.webpage.find();\n\n```\n\n**&nbsp;查询某些列的语句**\n\n```\ndb.webpage.find({},{_id:0})    #0代表不显示，1代表显示\n```\n\n**&nbsp;查询特定行的语句**\n\n```\ndb.webpage.find({ID:\"XXX\"})\n\n```\n\n**&nbsp;查询最后一条数据**\n\n```\ndb.baidutag.find({}).sort({\"_id\":-1}).limit(1)\n\n```\n\n**&nbsp;Mongo数据导出**\n\n```\nmongoexport -d local -c baidutagtemp -o /home/mi/baidutag.csv --csv -f \"title,author,ablum,tag,category,genre\"\n\n```\n\n&nbsp;添加过滤条件\n\n```\nmongoexport -d local -c wangyiartist -o /home/common/下载/huayuartist.csv --csv -f \"artist_name,artist_type,artist_id\" -q '{artist_type:\"欧美男歌手\"}'\n\n```\n\n&nbsp;多个条件\n\n```\nmongoexport -d local -c wangyiartist -o /home/common/下载/huayuartist.csv --csv -f \"artist_name,artist_type,artist_id\" -q '{\"$or\":[{artist_type:\"华语女歌手\"},{artist_type:\"华语男歌手\"}]}'\n\n```\n\n&nbsp;\n\n如果想在Amazon linux上安装mongo，参考文档：[https://mongodb.net.cn/manual/tutorial/install-mongodb-on-amazon/](https://mongodb.net.cn/manual/tutorial/install-mongodb-on-amazon/)\n","tags":["mongo"]},{"title":"Python爬虫——使用Cookie登录新浪微博","url":"/Python爬虫——使用Cookie登录新浪微博.html","content":"**1.首先在浏览器中进入WAP版微博的网址，因为手机版微博的内容较为简洁，方便后续使用正则表达式或者beautifulSoup等工具对所需要内容进行过滤**\n\n```\nhttps://login.weibo.cn/login/\n\n```\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20170113105113244-622915574.png\" alt=\"\" />\n\n**2.人工输入账号、密码、验证字符，最后最重要的是勾选（记住登录状态）**\n\n**<img src=\"/images/517519-20170113105555806-1723063114.png\" alt=\"\" />**\n\n**&nbsp;3.使用Wireshark工具或者火狐的HttpFox插件对GET请求进行分析，需要是取得GET请求中的Cookie信息**\n\n在**未登录新浪微博**的情况下，是可以通过网址查看一个用户的首页的，但是不能进一步查看该用户的关注和粉丝等信息，如果点击关注和粉丝，就会重定向回到登录页面\n\n比如使用下面函数对某个用户 http://weibo.cn/XXXXXX/fans 的**粉丝信息**进行访问，会**重定向回登录页面**\n\n```\n#获取网页函数\ndef getHtml(url,user_agent=\"wswp\",num_retries=2):       #下载网页，如果下载失败重新下载两次\n\tprint '开始下载网页：',url\n\t#\theaders = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; rv:24.0) Gecko/20100101 Firefox/24.0'}\n\theaders = {\"User-agent\":user_agent}\n\trequest = urllib2.Request(url,headers=headers)\t\t#request请求包\n\ttry:\n\t\thtml = urllib2.urlopen(request).read()          #GET请求\n\texcept urllib2.URLError as e:\n\t\tprint \"下载失败：\",e.reason\n\t\thtml = None\n\t\tif num_retries > 0:\n\t\t\tif hasattr(e,'code') and 500 <= e.code < 600:\n\t\t\t\treturn getHtml(url,num_retries-1)\n\treturn html\n\n```\n\n<img src=\"/images/517519-20170113112955010-1461329495.png\" alt=\"\" width=\"908\" height=\"168\" />\n\n所以需要在请求的包中的headers中加入Cookie信息，\n\n在勾选了**记住登录状态**之后，点击关注或者粉丝按钮，发出GET请求，并**使用wireshark对这个GET请求进行抓包**\n\n<img src=\"/images/517519-20170113113340916-1287108206.png\" alt=\"\" />\n\n可以抓到这个**GET请求**\n\n<img src=\"/images/517519-20170113113634775-1878534893.png\" alt=\"\" />\n\n右键**Follow TCP Stream**，图片中打码的部分就**Cookie信息**\n\n<img src=\"/images/517519-20170113114040713-328702850.png\" alt=\"\" width=\"486\" height=\"373\" />\n\n**4.加入<strong>Cookie信息**，重新获取网页</strong>\n\n有了**Cookie信息**，就可以对**Header信息**就行修改\n\n```\n#获取网页函数\ndef getHtml(url,user_agent=\"wswp\",num_retries=2):       #下载网页，如果下载失败重新下载两次\n\tprint '开始下载网页：',url\n\t#\theaders = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; rv:24.0) Gecko/20100101 Firefox/24.0'}\n\theaders = {\"User-agent\":user_agent,\"Cookie\":\"_T_WM=XXXXXXXX; SUB=XXXXXXXX; gsid_CTandWM=XXXXXXXXX\"}\n\trequest = urllib2.Request(url,headers=headers)\t\t#request请求包\n\ttry:\n\t\thtml = urllib2.urlopen(request).read()          #GET请求\n\texcept urllib2.URLError as e:\n\t\tprint \"下载失败：\",e.reason\n\t\thtml = None\n\t\tif num_retries > 0:\n\t\t\tif hasattr(e,'code') and 500 <= e.code < 600:\n\t\t\t\treturn getHtml(url,num_retries-1)\n\treturn html\n\n```\n\n&nbsp;\n\n```\nimport urllib2  \n\nif __name__ == '__main__':\n\tURL = 'http://weibo.cn/XXXXXX/fans'\t\t\t#URL替代\n\thtml = getHtml(URL)\n\tprint html\n\n```\n\n&nbsp;成功访问到**某个用户的粉丝信息**\n\n<img src=\"/images/517519-20170113114909697-994238539.png\" alt=\"\" width=\"962\" height=\"263\" />\n\n试一试访问一下最近一年很火的**papi酱的微博**，**她的个人信息页面**\n\n```\nimport urllib2 \n\nif __name__ == '__main__':\n\tURL = 'http://weibo.cn/2714280233/info'\t\t\t#URL替代\n\thtml = getHtml(URL)\n\tprint html\n\n```\n\n**<img src=\"/images/517519-20170113143230494-743999257.png\" alt=\"\" />**\n\n**5.添加<strong>user-agent信息**</strong>\n\n通过**GET请**求获取返回的网页，其中加入了**User-agent信息**，不然会抛出\"HTTP Error 403: Forbidden\"异常，\n\n因为有些网站为了防止这种没有User-agent信息的访问，会**验证请求信息中的UserAgent**(它的[信息包](http://baike.baidu.com/view/1654236.htm)括硬件平台、系统[软件](http://baike.baidu.com/view/37.htm)、[应用软件](http://baike.baidu.com/view/7886.htm)和用户个人偏好)，如果UserAgent存在异常或者是不存在,那么这次请求将会被拒绝。\n\n```\n#coding=utf-8\nimport urllib2\nimport re\n\n#使用Python2.7\ndef getHtml(url,user_agent=\"wswp\",num_retries=2):\t\t#下载网页，如果下载失败重新下载两次\n\tprint '开始下载网页：',url\n\theaders = {\"User-agent\":user_agent}\n#\theaders = {\n#   'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; rv:24.0) Gecko/20100101 Firefox/24.0',\n#   'cookie': cookie\n#\t}\n\trequest = urllib2.Request(url,headers=headers)\n\ttry:\n\t    html = urllib2.urlopen(request).read()\t\t\t#GET请求\n\texcept urllib2.URLError as e:\n\t\tprint \"下载失败：\",e.reason\n\t\thtml = None\n\t\tif num_retries > 0:\n\t\t\tif hasattr(e,'code') and 500 <= e.code < 600:\n\t\t\t\treturn getHtml(url,num_retries-1)\n\treturn html\n\nif __name__ == '__main__':\n\thtml = getHtml(\"http://www.baidu.com\")\n\tprint html\n\tprint \"结束\"\n\n```\n\n&nbsp;<img src=\"/images/517519-20170112170123822-369642613.png\" alt=\"\" width=\"290\" height=\"63\" />\n\n<img src=\"/images/517519-20170112170131541-2103744062.png\" alt=\"\" />\n\n...\n\n<img src=\"/images/517519-20170112170205369-232650603.png\" alt=\"\" width=\"595\" height=\"338\" />\n","tags":["Python"]},{"title":"Python爬虫——光学字符识别","url":"/Python爬虫——光学字符识别.html","content":"用homebrew 在电脑上安装tesseract库<!--more-->\n&nbsp;\n\n```\nbrew install tesseract\n```\n\n用pip安装支持python的tesseract 接口\n\n```\npip install pytesseract\n\n```\n\n　　\n\n使用**pytesseract库**对**图片文件**（jpg、png、bmp等）进行识别，**把图片转换成字符串输出。**\n\n```\nimport pytesseract\nfrom PIL import Image\n\nimg = Image.open('1.gif')\nprint(pytesseract.image_to_string(img))\n\n```\n\n**&nbsp;实际测试效果：**\n\n输入图片<img src=\"/images/517519-20170112233854885-417739597.png\" alt=\"\" width=\"118\" height=\"58\" />，输出<img src=\"/images/517519-20170112233932791-683604080.png\" alt=\"\" width=\"86\" height=\"31\" />，结果：成功\n\n输入图片<img src=\"/images/517519-20170112234003619-548205825.jpg\" alt=\"\" width=\"107\" height=\"59\" />，输出：为空，结果：失败\n\n输入图片<img src=\"/images/517519-20170112234042088-137114921.png\" alt=\"\" />，输出：为空，结果：失败\n\n输入图片<img src=\"/images/517519-20170113091822244-98784998.png\" alt=\"\" />，输出：<img src=\"/images/517519-20170113091843775-207757382.png\" alt=\"\" />，结果：错误\n\n&nbsp;\n\n如果需要支持中文，请下载下面的文件到目录&nbsp;/usr/local/Cellar/tesseract/5.1.0/share/tessdata\n\n```\nhttps://github.com/tesseract-ocr/tessdata/blob/main/chi_sim.traineddata\n\n```\n\n然后\n\n```\nprint(pytesseract.image_to_string(img, lang='chi_sim'))\n\n```\n\n　　\n","tags":["Python"]},{"title":"go学习笔记——pprof性能分析工具","url":"/go学习笔记——pprof性能分析工具.html","content":"可以使用pprof来分析golang程序的CPU性能，内存占用，block死锁，Goroutine性能等，pprof一般是在需要分析代码性能的时候才加入\n\n## 1.分析Gin web服务的性能\n\n可以使用 [gin-contrib/pprof](https://github.com/gin-contrib/pprof) 这个工具，参考：[Gin框架中使用pprof](https://liumurong.org/2019/12/gin_pprof/)\n\n添加依赖\n\n```\ngo get github.com/gin-contrib/pprof@v1.4.0\n\n```\n\n在gin的路由上添加 pprof.Register(app)\n\n```\npackage main\n\nimport (\n    \"net/http\"\n\n    \"github.com/gin-contrib/pprof\"\n    \"github.com/gin-gonic/gin\"\n)\n\nfunc main() {\n    app := gin.Default()\n\n    // 性能分析\n    pprof.Register(app)\n\n    app.GET(\"/test\", func(c *gin.Context) {\n        c.String(http.StatusOK, \"test\")\n    })\n    app.Run(\":3000\")\n}\n\n```\n\n启动web应用后访问如下path\n\n```\nhttp://localhost:3000/debug/pprof/\n\n```\n\n可以看到如下页面\n\n<img src=\"/images/517519-20240818145446806-955281051.png\" width=\"800\" height=\"321\" loading=\"lazy\" />\n\n使用go tool pprof命令采集20s的goroutine（Goroutine 是 Go 语言中的一种轻量级线程，提供了并发执行代码的能力）指标数据，如果不设置采集时间是默认采集30s，每10ms采集一次\n\n```\ngo tool pprof --seconds 20 http://localhost:18080/debug/pprof/goroutine\n\n```\n\n采集完毕后可以直接输入 web 来可视化查看数据，不过需要提前安装graphviz，否则会报<!--more-->\n&nbsp;failed to execute dot. Is Graphviz installed? Error: exec: \"dot\": executable file not found in $PATH\n\n```\nbrew install graphviz\n\n```\n\n如果使用brew安装graphviz失败的话，可以使用官方提供的编译安装的方式，我这边就是使用的这种方法\n\n```\nwget https://gitlab.com/api/v4/projects/4207231/packages/generic/graphviz-releases/12.0.0/graphviz-12.0.0.tar.gz\ntar -zxvf graphviz-12.0.0.tar.gz\n\n./configure\nmake\nmake install\n\n```\n\n参考：[https://graphviz.org/download/source/](https://graphviz.org/download/source/)\n\n<img src=\"/images/517519-20240818152152860-2025472556.png\" width=\"600\" height=\"179\" loading=\"lazy\" />\n\n输入web，会弹出一个生成的svg文件，将其保存到文件后用浏览器打开，如下\n\n<img src=\"/images/517519-20240818214553512-680202076.png\" width=\"350\" height=\"749\" loading=\"lazy\" />\n\n当然也可以采集heap的数据\n\n```\ngo tool pprof http://localhost:18080/debug/pprof/heap\n\n```\n\n如下\n\n<img src=\"/images/517519-20240818215059595-157737237.png\" width=\"900\" height=\"749\" loading=\"lazy\" />\n\n&nbsp;\n\n退出pprof可以输入\n\n```\nexit\n\n```\n\n&nbsp;\n\n## 2.分析http web服务的性能\n\n如果是使用go自带的http包实现的web服务，可以通过添加 net/http/pprof 的方式来开启pprof\n\n```\nimport _ \"net/http/pprof\"\n\n```\n\n如下\n\n```\npackage main\n\nimport (\n\t\"net/http\"\n\t_ \"net/http/pprof\"\n)\n\nfunc main() {\n\thttp.ListenAndServe(\"localhost:6060\", nil)\n}\n\n```\n\n之后访问\n\n```\nhttp://localhost:6060/debug/pprof/\n\n```\n\n## 3.分析CPU\n\n参考：[pprof 性能分析](https://geektutu.com/post/hpg-pprof.html) \n\n## 4.分析内存\n\n参考：[你不知道的 Go 之 pprof](https://darjun.github.io/2021/06/09/youdontknowgo/pprof/)\n","tags":["golang"]},{"title":"Java多线程——Semaphore信号灯","url":"/Java多线程——Semaphore信号灯.html","content":"**Semaphore [ˈseməfɔːr] **可以维护当前访问自身的线程个数，并提供了同步机制。**使用Semaphore可以控制同时访问资源的线程个数（即允许n个任务同时访问这个资源）**，例如，实现一个文件允许的并发访问数。\n\n<!--more-->\n&nbsp;\n\nSemaphore实现的功能就类似厕所有5个坑，假如有十个人要上厕所，那么同时能有多少个人去上厕所呢？同时只能有5个人能够占用，当5个人中的任何一个人让开后，其中在等待的另外5个人中又有一个可以占用了。\n\n另外等待的5个人中可以是随机获得优先机会，也可以是按照先来后到的顺序获得机会，这取决于构造Semaphore对象时传入的参数选项。\n\n&nbsp;\n\n单个信号量的Semaphore对象可以实现互斥锁的功能，并且可以是由一个线程获得了&ldquo;锁&rdquo;，再由另一个线程释放&ldquo;锁&rdquo;，这可应用于死锁恢复的一些场合。\n\n通过**acquire()获取访问许可**，使用**release()释放访问许可**，Semaphore可以初始化是0，然后通过release来变成1\n\nsp.availablePermits()可以获取当前可访问的许可的数量\n\nsp.drainPermits()可以把所有许可全部清零\n\n参考：[并发编程之Semaphore原理与应用](https://www.51cto.com/article/633255.html)\n\n```\npackage java_thread;\n\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.Semaphore;\n\npublic class SemaphoreTest {\n\tpublic static void main(String[] args) {\n\t\tExecutorService service = Executors.newCachedThreadPool();\n\t\tfinal  Semaphore sp = new Semaphore(3);\n\t\tfor(int i=0;i<10;i++){\n\t\t\tRunnable runnable = new Runnable(){\n\t\t\t\t\tpublic void run(){\n\t\t\t\t\ttry {\n\t\t\t\t\t\tsp.acquire();\n\t\t\t\t\t} catch (InterruptedException e1) {\n\t\t\t\t\t\te1.printStackTrace();\n\t\t\t\t\t}\n\t\t\t\t\tSystem.out.println(\"线程\" + Thread.currentThread().getName() + \n\t\t\t\t\t\t\t\"进入，当前已有\" + (3-sp.availablePermits()) + \"个并发\");\n\t\t\t\t\ttry {\n\t\t\t\t\t\tThread.sleep((long)(Math.random()*10000));\n\t\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t}\n\t\t\t\t\tSystem.out.println(\"线程\" + Thread.currentThread().getName() + \n\t\t\t\t\t\t\t\"即将离开\");\t\t\t\t\t\n\t\t\t\t\tsp.release();\n\t\t\t\t\t//下面代码有时候执行不准确，因为其没有和上面的代码合成原子单元\n\t\t\t\t\tSystem.out.println(\"线程\" + Thread.currentThread().getName() + \n\t\t\t\t\t\t\t\"已离开，当前已有\" + (3-sp.availablePermits()) + \"个并发\");\t\t\t\t\t\n\t\t\t\t}\n\t\t\t};\n\t\t\tservice.execute(runnable);\t\t\t\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["多线程"]},{"title":"Java多线程——其他工具类CyclicBarrier、CountDownLatch和Exchange","url":"/Java多线程——其他工具类CyclicBarrier、CountDownLatch和Exchange.html","content":"**CyclicBarrier**\n\n适用于：创建一组任务，它们并行地执行任务，然后**在进行下一个步骤之前等待**，直至所有任务完成。它使得所有的并行任务都将在栅栏处列队，因此可以一致地向前移动。\n\n表示大家彼此等待，大家集合好后才开始出发，分散活动后又在指定地点集合碰面，这就好比整个公司的人员利用周末时间集体郊游一样，先各自从家出发到公司集合后，再同时出发到公园游玩，在指定地点集合后再同时开始就餐&hellip;\n\n```\npackage java_thread;\nimport java.util.concurrent.CyclicBarrier;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\n\npublic class CyclicBarrierTest {\n\n\tpublic static void main(String[] args) {\n\t\tExecutorService service = Executors.newCachedThreadPool();\n\t\tfinal  CyclicBarrier cb = new CyclicBarrier(3);\n\t\tfor(int i=0;i<3;i++){\n\t\t\tRunnable runnable = new Runnable(){\n\t\t\t\t\tpublic void run(){\n\t\t\t\t\ttry {\n\t\t\t\t\t\tThread.sleep((long)(Math.random()*10000));\t\n\t\t\t\t\t\tSystem.out.println(\"线程\" + Thread.currentThread().getName() + \n\t\t\t\t\t\t\t\t\"即将到达集合地点1，当前已有\" + (cb.getNumberWaiting()+1) + \"个已经到达，\" + (cb.getNumberWaiting()==2?\"都到齐了，继续走啊\":\"正在等候\"));\t\t\t\t\t\t\n\t\t\t\t\t\tcb.await();\n\t\t\t\t\t\t\n\t\t\t\t\t\tThread.sleep((long)(Math.random()*10000));\t\n\t\t\t\t\t\tSystem.out.println(\"线程\" + Thread.currentThread().getName() + \n\t\t\t\t\t\t\t\t\"即将到达集合地点2，当前已有\" + (cb.getNumberWaiting()+1) + \"个已经到达，\" + (cb.getNumberWaiting()==2?\"都到齐了，继续走啊\":\"正在等候\"));\n\t\t\t\t\t\tcb.await();\t\n\t\t\t\t\t\tThread.sleep((long)(Math.random()*10000));\t\n\t\t\t\t\t\tSystem.out.println(\"线程\" + Thread.currentThread().getName() + \n\t\t\t\t\t\t\t\t\"即将到达集合地点3，当前已有\" + (cb.getNumberWaiting() + 1) + \"个已经到达，\" + (cb.getNumberWaiting()==2?\"都到齐了，继续走啊\":\"正在等候\"));\t\t\t\t\t\t\n\t\t\t\t\t\tcb.await();\t\t\t\t\t\t\n\t\t\t\t\t} catch (Exception e) {\n\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t}\t\t\t\t\n\t\t\t\t}\n\t\t\t};\n\t\t\tservice.execute(runnable);\n\t\t}\n\t\tservice.shutdown();\n\t}\n}\n\n```\n\n**<img src=\"/images/517519-20170109152318822-667703062.png\" alt=\"\" />**\n\n<!--more-->\n&nbsp;\n\n**倒计时器CountDownLatch**\n\n犹如倒计时计数器，**调用CountDownLatch对象的countDown方法就将计数器减1，当计数到达0时，则所有等待者或单个等待者开始执行**。这直接通过代码来说明CountDownLatch的作用，这样学员的理解效果更直接。 可以实现一个人（也可以是多个人）等待其他所有人都来通知他，这犹如一个计划需要多个领导都签字后才能继续向下实施。还可以实现一个人通知多个人的效果，类似裁判一声口令，运动员同时开始奔跑。用这个功能做百米赛跑的游戏程序不错哦！\n\n```\npackage java_thread;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.CyclicBarrier;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\n\npublic class CountdownLatchTest {\n\n\tpublic static void main(String[] args) {\n\t\tExecutorService service = Executors.newCachedThreadPool();\n\t\tfinal CountDownLatch cdOrder = new CountDownLatch(1);\n\t\tfinal CountDownLatch cdAnswer = new CountDownLatch(3);\t\t\n\t\tfor(int i=0;i<3;i++){\n\t\t\tRunnable runnable = new Runnable(){\n\t\t\t\t\tpublic void run(){\n\t\t\t\t\ttry {\n\t\t\t\t\t\tSystem.out.println(\"线程\" + Thread.currentThread().getName() + \n\t\t\t\t\t\t\t\t\"正准备接受命令\");\t\t\t\t\t\t\n\t\t\t\t\t\tcdOrder.await();\t\t\t//等待计数器归零\n\t\t\t\t\t\tSystem.out.println(\"线程\" + Thread.currentThread().getName() + \n\t\t\t\t\t\t\"已接受命令\");\t\t\t\t\t\t\t\t\n\t\t\t\t\t\tThread.sleep((long)(Math.random()*10000));\t\n\t\t\t\t\t\tSystem.out.println(\"线程\" + Thread.currentThread().getName() + \n\t\t\t\t\t\t\t\t\"回应命令处理结果\");\t\t\t\t\t\t\n\t\t\t\t\t\tcdAnswer.countDown();\t\t//减小主线程计数值\t\t\t\n\t\t\t\t\t} catch (Exception e) {\n\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t}\t\t\t\t\n\t\t\t\t}\n\t\t\t};\n\t\t\tservice.execute(runnable);\n\t\t}\t\t\n\t\ttry {\t\t\t//主线程\n\t\t\tThread.sleep((long)(Math.random()*10000));\n\t\t\n\t\t\tSystem.out.println(\"线程\" + Thread.currentThread().getName() + \n\t\t\t\t\t\"即将发布命令\");\t\t\t\t\t\t\n\t\t\tcdOrder.countDown();\t\t//减小计数值\n\t\t\tSystem.out.println(\"线程\" + Thread.currentThread().getName() + \n\t\t\t\"已发送命令，正在等待结果\");\t\n\t\t\tcdAnswer.await();\t\t\t\t\t//等待计数器归零\n\t\t\tSystem.out.println(\"线程\" + Thread.currentThread().getName() + \n\t\t\t\"已收到所有响应结果\");\t\n\t\t} catch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t}\t\t\t\t\n\t\tservice.shutdown();\n\n\t}\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20170109154044775-1670717658.png\" alt=\"\" />\n\n&nbsp;\n\n**Exchanger**\n\n用于实现两个人之间的数据交换，每个人在完成一定的事务后想与对方交换数据，第一个先拿出数据的人将一直等待第二个人拿着数据到来时，才能彼此交换数据。\n\n```\npackage java_thread;\nimport java.util.concurrent.Exchanger;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\n\npublic class ExchangerTest {\n\n\tpublic static void main(String[] args) {\n\t\tExecutorService service = Executors.newCachedThreadPool();\n\t\tfinal Exchanger exchanger = new Exchanger();\n\t\tservice.execute(new Runnable(){\n\t\t\tpublic void run() {\n\t\t\t\ttry {\t\t\t\t\n\n\t\t\t\t\tString data1 = \"zxx\";\n\t\t\t\t\tSystem.out.println(\"线程\" + Thread.currentThread().getName() + \n\t\t\t\t\t\"正在把数据\" + data1 +\"换出去\");\n\t\t\t\t\tThread.sleep((long)(Math.random()*10000));\n\t\t\t\t\tString data2 = (String)exchanger.exchange(data1);\n\t\t\t\t\tSystem.out.println(\"线程\" + Thread.currentThread().getName() + \n\t\t\t\t\t\"换回的数据为\" + data2);\n\t\t\t\t}catch(Exception e){\n\t\t\t\t\t\n\t\t\t\t}\n\t\t\t}\t\n\t\t});\n\t\tservice.execute(new Runnable(){\n\t\t\tpublic void run() {\n\t\t\t\ttry {\t\t\t\t\n\n\t\t\t\t\tString data1 = \"lhm\";\n\t\t\t\t\tSystem.out.println(\"线程\" + Thread.currentThread().getName() + \n\t\t\t\t\t\"正在把数据\" + data1 +\"换出去\");\n\t\t\t\t\tThread.sleep((long)(Math.random()*10000));\t\t\t\t\t\n\t\t\t\t\tString data2 = (String)exchanger.exchange(data1);\n\t\t\t\t\tSystem.out.println(\"线程\" + Thread.currentThread().getName() + \n\t\t\t\t\t\"换回的数据为\" + data2);\n\t\t\t\t}catch(Exception e){\n\t\t\t\t\t\n\t\t\t\t}\t\t\t\t\n\t\t\t}\t\n\t\t});\t\t\n\t}\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20170109153917197-522046844.png\" alt=\"\" />\n","tags":["多线程"]},{"title":"Java多线程——可阻塞的队列BlockingQueue","url":"/Java多线程——可阻塞的队列BlockingQueue.html","content":"**阻塞队列**与**Semaphore**有些相似，但也不同，**阻塞队列**是**一方存放数据，另一方释放数据**，Semaphore通常则是由同一方设置和释放信号量。\n\n**ArrayBlockingQueue**\n\n　　只有put方法和take方法才具有阻塞功能\n\n用3个空间的队列来演示阻塞队列的功能和效果。\n\n用两个具有1个空间的队列来实现同步通知的功能。\n\n```\npackage java_thread;\nimport java.util.concurrent.ArrayBlockingQueue;\nimport java.util.concurrent.BlockingQueue;\n\npublic class BlockingQueueTest {\n\tpublic static void main(String[] args) {\n\t\tfinal BlockingQueue queue = new ArrayBlockingQueue(3);\n\t\tfor(int i=0;i<2;i++){\t\t\t\t//2个线程放数据\n\t\t\tnew Thread(){\n\t\t\t\tpublic void run(){\n\t\t\t\t\twhile(true){\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tThread.sleep((long)(Math.random()*1000));\n\t\t\t\t\t\t\tSystem.out.println(Thread.currentThread().getName() + \"准备放数据!\");\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tqueue.put(1);\t\t\t//如果队列满的话，将在这里阻塞\n\t\t\t\t\t\t\tSystem.out.println(Thread.currentThread().getName() + \"已经放了数据，\" + \t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\"队列目前有\" + queue.size() + \"个数据\");\n\t\t\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t\t}\n\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t\n\t\t\t}.start();\n\t\t}\n\t\t\n\t\tnew Thread(){\t\t\t//1个线程取数据\n\t\t\tpublic void run(){\n\t\t\t\twhile(true){\n\t\t\t\t\ttry {\n\t\t\t\t\t\t//将此处的睡眠时间分别改为100和1000，观察运行结果\n\t\t\t\t\t\tThread.sleep(1000);\n\t\t\t\t\t\tSystem.out.println(Thread.currentThread().getName() + \"准备取数据!\");\n\t\t\t\t\t\tqueue.take();\t\t//如果队列空的话，将在这里阻塞\n\t\t\t\t\t\tSystem.out.println(Thread.currentThread().getName() + \"已经取走数据，\" + \t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\"队列目前有\" + queue.size() + \"个数据\");\t\t\t\t\t\n\t\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t\n\t\t}.start();\t\t\t\n\t}\n}\n\n```\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20170109160520385-1625575301.png\" alt=\"\" />\n","tags":["多线程"]},{"title":"Java排序算法——堆排序","url":"/Java排序算法——堆排序.html","content":"堆排序\n\n```\npackage sort;\n\npublic class Heap_Sort {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tHeap_Sort qs = new Heap_Sort();\n\t\tint[] Arr = {10,9,8,7,6,5,4,3,2,1};\n\t\tqs.heapSort(Arr);\n\t\tfor(int i=0;i<Arr.length;i++){\n\t\t\tSystem.out.println(Arr[i]);\n\t\t}\n\t}\n\t\n    //调整函数\n\tpublic void  headAdjust(int[] elements,int pos,int len){\n      //将当前节点值进行保存\n      int swap = elements[pos];\n\n      //定位到当前节点的左边的子节点\n      int child = pos * 2 + 1;\n\n      //递归，直至没有子节点为止\n      while(child < len){\n        //如果当前节点有右边的子节点，并且右子节点较大的场合，采用右子节点和当前节点进行比较\n        if(child + 1 < len &amp;&amp; elements[child] < elements[child + 1]){\n          child += 1;\n        }\n\n        //比较当前节点和最大的子节点，小于则进行值交换，交换后将当前节点定位于子节点上\n        if(elements[pos] < elements[child]){\n          elements[pos] = elements[child];\n          pos = child;\n          child = pos * 2 + 1;\n        }\n        else{\n          break;\n        }\n\n        elements[pos] = swap;\n      }\n    }\n\n    //构建堆\n    public void buildHeap(int[] elements){\n      //从最后一个拥有子节点的节点开始，将该节点连同其子节点进行比较，\n      //将最大的数交换与该节点,交换后，再依次向前节点进行相同交换处理，\n      //直至构建出大顶堆（升序为大顶，降序为小顶）\n      for(int i=elements.length/2; i>=0; i--){\n        headAdjust(elements, i, elements.length);\n      }\n    }\n\n    public void heapSort(int[] elements){\n      //构建堆\n      buildHeap(elements);\n\n      //从数列的尾部开始进行调整\n      for(int i=elements.length-1; i>0; i--){\n        //堆顶永远是最大元素，故，将堆顶和尾部元素交换，将\n        //最大元素保存于尾部，并且不参与后面的调整\n        //alert(elements);\n        int swap = elements[i];\n        elements[i] = elements[0];\n        elements[0] = swap;\n        //alert(elements);\n        //进行调整，将最大的元素调整至堆顶\n        headAdjust(elements, 0, i);\n        //alert(elements);\n      }\n    }\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["算法"]},{"title":"Java多线程——Executors和线程池","url":"/Java多线程——Executors和线程池.html","content":"**线程池的概念与Executors类的应用**\n\n**　　1.创建固定大小的线程池**\n\n```\npackage java_thread;\n\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.TimeUnit;\n\npublic class ThreadPoolTest {\n\n\t/**\n\t * @param args\n\t */\n\tpublic static void main(String[] args) {\n\t\tExecutorService threadPool = Executors.newFixedThreadPool(3);\n//\t\tExecutorService threadPool = Executors.newCachedThreadPool();\n//\t\tExecutorService threadPool = Executors.newSingleThreadExecutor();\n\t\tfor(int i=1;i<=10;i++){\n\t\t\tfinal int task = i;\n\t\t\tthreadPool.execute(new Runnable(){\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\tfor(int j=1;j<=4;j++){\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tThread.sleep(20);\n\t\t\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\t\t\t// TODO Auto-generated catch block\n\t\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t\t}\n\t\t\t\t\t\tSystem.out.println(Thread.currentThread().getName() + \" is looping of \" + j + \" for  task of \" + task);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t});\n\t\t}\n\t\tSystem.out.println(\"all of 10 tasks have committed! \");\n\t\t//threadPool.shutdownNow();\n\t\t\n//\t\tExecutors.newScheduledThreadPool(3).scheduleAtFixedRate(\n//\t\t\t\tnew Runnable(){\n//\t\t\t\t\t@Override\n//\t\t\t\tpublic void run() {\n//\t\t\t\t\tSystem.out.println(\"bombing!\");\n//\t\t\t\t\t\n//\t\t\t\t}},\n//\t\t\t\t6,\n//\t\t\t\t2,\n//\t\t\t\tTimeUnit.SECONDS);\n\t}\n\n}\n\n```\n\n<img src=\"/images/517519-20170108201807566-141895150.png\" alt=\"\" />\n\n**　　2.创建缓存线程池**\n\n```\nExecutorService threadPool = Executors.newCachedThreadPool();\n\n```\n\n<img src=\"/images/517519-20170108201658769-1385681205.png\" alt=\"\" />\n\n**　3.创建单一线程池**\n\n```\nExecutorService threadPool = Executors.newSingleThreadExecutor();\n\n```\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20170108201606503-860804670.png\" alt=\"\" />\n\n&nbsp;\n\n**关闭线程池 **\n\n　　shutdown与shutdownNow的比较\n\n```\nthreadPool.shutdownNow();\n\n```\n\n&nbsp;\n\n**用线程池启动定时器 **\n\n　　调用ScheduledExecutorService的schedule方法，返回的ScheduleFuture对象可以取消任务。\n\n　　支持间隔重复任务的定时方式，不直接支持绝对定时方式，需要转换成相对时间方式。\n\n```\n\t\tExecutors.newScheduledThreadPool(3).scheduleAtFixedRate(\t//多久气候执行，每隔多久执行\n\t\t\t\tnew Runnable(){\n\t\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\tSystem.out.println(\"bombing!\");\n\t\t\t\t\t\n\t\t\t\t}},\n\t\t\t\t6,\n\t\t\t\t2,\n\t\t\t\tTimeUnit.SECONDS);\n\n```\n\n&nbsp;\n","tags":["多线程"]},{"title":"Java多线程——线程范围内共享变量和ThreadLocal","url":"/Java多线程——线程范围内共享变量和ThreadLocal.html","content":"**多个线程访问共享对象和数据的方式** \n\n　　**1.如果每个线程执行的代码相同**，可以使用同一个Runnable对象，这个Runnable对象中有那个共享数据，例如，买票系统就可以这么做。\n\n```\npackage java_thread;\n//=================================================\n// File Name       :\tRunnable_demo2\n//------------------------------------------------------------------------------\n// Author          :\tCommon\n\n\nclass MyThread_2 implements Runnable{\n\tprivate int ticket = 5;\t\n\t\n\t@Override\n\tpublic void run() {\t\t\t\t\t\t\t\t//覆写Thread类中的run()方法\n\t\t// TODO 自动生成的方法存根\n\t\tfor (int i=0;i<10;i++){\n\t\t\tsynchronized (this) {\t\t\t\t\t//设置需要同步的操作\n\t\t\t\tif(ticket>0){\n\t\t\t\t\ttry{\n\t\t\t\t\t\tThread.sleep(300);\n\t\t\t\t\t}catch(InterruptedException e){\n\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t}\n\t\t\t\t\tSystem.out.println(\"卖票：ticket=\"+ticket--);\n\t\t\t\t}\n\t\t\t}\n//\t\t\tthis.sale();\t\t\t\t\t\t\t\t\t\t\t\t//调用同步方法\n\t\t}\n\t}\n\t\n//\tpublic synchronized void sale(){\t\t\t//声明同步方法\n//\t\tif(ticket>0){\n//\t\t\ttry{\n//\t\t\t\tThread.sleep(300);\n//\t\t\t}catch(InterruptedException e){\n//\t\t\t\te.printStackTrace();\n//\t\t\t}\n//\t\t\tSystem.out.println(\"卖票：ticket=\"+ticket--);\n//\t\t}\n//\t}\n\t\n}\n\n\n\n//主类\n//Function        : \tThread_demo2\npublic class Runnable_demo2 {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tMyThread_2 mt = new MyThread_2();\t//实例化Runnable子类对象\n\t\tThread t1 = new Thread(mt);\t\t\t\t\t\t\t//实例化Thread类对象\n\t\tThread t2 = new Thread(mt);\t\t\t\t\t\t\t//实例化Thread类对象\n\t\tThread t3 = new Thread(mt);\t\t\t\t\t\t\t//实例化Thread类对象\n\t\tt1.start();\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//启动线程\n\t\tt2.start();\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//启动线程\n\t\tt3.start();\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//启动线程\n\t}\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n\n　　**2.如果每个线程执行的代码不同**，这时候需要用不同的Runnable对象，有如下两种方式来实现这些Runnable对象之间的数据共享： \n\n　　　　**方法1：将共享数据封装在另外一个对象中**，然后将这个对象逐一传递给各个Runnable对象。每个线程对共享数据的操作方法也分配到那个对象身上去完成，这样容易实现针对该数据进行的各个操作的互斥和通信。 \n\n&nbsp;\n\n```\npackage java_thread;\n\npublic class MultiThreadShareData {\t\t\t//多线程卖票，一个加，一个减\n\n\t//private static ShareData1 data1 = new ShareData1();\n\t\n\tpublic static void main(String[] args) {\n\t\tShareData1 data2 = new ShareData1();\n\t\tnew Thread(new MyRunnable1(data2)).start();\n\t\tnew Thread(new MyRunnable2(data2)).start();\n\t\t\n\t\tfinal ShareData1 data1 = new ShareData1();\n\t\tnew Thread(new Runnable(){\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\tdata1.decrement();\n\t\t\t\t\n\t\t\t}\n\t\t}).start();\n\t\tnew Thread(new Runnable(){\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\tdata1.increment();\n\t\t\t\t\n\t\t\t}\n\t\t}).start();\n\n\t}\n\n}\n\t\n\tclass MyRunnable1 implements Runnable{\t\t//线程1\n\t\tprivate ShareData1 data1;\n\t\tpublic MyRunnable1(ShareData1 data1){\n\t\t\tthis.data1 = data1;\n\t\t}\n\t\tpublic void run() {\n\t\t\tdata1.decrement();\n\t\t\t\n\t\t}\n\t}\n\t\n\tclass MyRunnable2 implements Runnable{\t\t//线程2\n\t\tprivate ShareData1 data1;\n\t\tpublic MyRunnable2(ShareData1 data1){\n\t\t\tthis.data1 = data1;\n\t\t}\n\t\tpublic void run() {\n\t\t\tdata1.increment();\n\t\t}\n\t}\n\n\tclass ShareData1 /*implements Runnable*/{　　//共享对象\n/*\t\tprivate int count = 100;\n\t\t@Override\n\t\tpublic void run() {\n\t\t\t// TODO Auto-generated method stub\n\t\t\twhile(true){\n\t\t\t\tcount--;\n\t\t\t}\n\t\t}*/\n\t\t\n\t\tprivate int j = 0;\n\t\tpublic synchronized void increment(){\n\t\t\tj++;\n\t\t}\n\t\t\n\t\tpublic synchronized void decrement(){\n\t\t\tj--;\n\t\t}\n\t}\n\n```\n\n&nbsp;\n\n　　　　**方法2：**将这些Runnable对象作为某一个类中的内部类，共享数据作为这个外部类中的成员变量，每个线程对共享数据的操作方法也分配给外部类，以便实现对共享数据进行的各个操作的互斥和通信，作为内部类的各个Runnable对象调用外部类的这些方法。 \n\n&nbsp;\n\n```\npackage java_thread;\n\n//设计 4 个线程,其中两个线程每次对 j 增加 1,另外两个线程对 j 每次减少 1\npublic class ThreadTest1{\n\t\n\tprivate int j;\n\tpublic static void main(String args[]){\n\t\tThreadTest1 tt=new ThreadTest1();\n\t\tInc inc=tt.new Inc();\n\t\tDec dec=tt.new Dec();\n\t\tfor(int i=0;i<2;i++){\n\t\t\tThread t=new Thread(inc);\n\t\t\tt.start();\n\t\t\tt=new Thread(dec);\n\t\t\tt.start();\n\t\t}\n\t}\n\t\n\tprivate synchronized void inc(){\n\t\tj++;\n\t\tSystem.out.println(Thread.currentThread().getName()+\"-inc:\"+j);\n\t}\n\t\n\tprivate synchronized void dec(){\n\t\tj--;\n\t\tSystem.out.println(Thread.currentThread().getName()+\"-dec:\"+j);\n\t}\n\t\n\tclass Inc implements Runnable{\t\t//线程1\n\t\tpublic void run(){\n\t\t\tfor(int i=0;i<5;i++){\n\t\t\t\tinc();\n\t\t\t}\n\t\t}\n\t}\n\t\n\tclass Dec implements Runnable{\t\t//线程2\n\t\tpublic void run(){\n\t\t\tfor(int i=0;i<5;i++){\n\t\t\t\tdec();\n\t\t\t}\n\t\t}\n\t}\n\t\n}\n\n```\n\n&nbsp;\n\n　　　　上面两种方式的组合：将共享数据封装在另外一个对象中，每个线程对共享数据的操作方法也分配到那个对象身上去完成，对象作为这个外部类中的成员变量或方法中的局部变量，每个线程的Runnable对象作为外部类中的成员内部类或局部内部类。 \n\n　　　　总之，要同步互斥的几段代码最好是分别放在几个独立的方法中，这些方法再放在同一个类中，这样比较容易实现它们之间的同步互斥和通信。\n\n　　**3.**极端且简单的方式，即在任意一个类中定义一个static的变量，这将被所有线程共享。 \n\n&nbsp;\n\n```\npackage java_thread;\n\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.Random;\n\npublic class ThreadScopeShareData {\n\n\tprivate static int data = 0;\n\tprivate static Map<Thread, Integer> threadData = new HashMap<Thread, Integer>();\n\tpublic static void main(String[] args) {\n\t\tfor(int i=0;i<2;i++){\n\t\t\tnew Thread(new Runnable(){\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\tint data = new Random().nextInt();\n\t\t\t\t\tSystem.out.println(Thread.currentThread().getName() + \" has put data :\" + data);\n\t\t\t\t\tthreadData.put(Thread.currentThread(), data);\n\t\t\t\t\tnew A().get();\n\t\t\t\t\tnew B().get();\n\t\t\t\t}\n\t\t\t}).start();\n\t\t}\n\t}\n\t\n\tstatic class A{\n\t\tpublic void get(){\n\t\t\tint data = threadData.get(Thread.currentThread());\n\t\t\tSystem.out.println(\"A from \" + Thread.currentThread().getName() + \" get data :\" + data);\n\t\t}\n\t}\n\t\n\tstatic class B{\n\t\tpublic void get(){\n\t\t\tint data = threadData.get(Thread.currentThread());\t\t\t\n\t\t\tSystem.out.println(\"B from \" + Thread.currentThread().getName() + \" get data :\" + data);\n\t\t}\t\t\n\t}\n}\n\n```\n\n<img src=\"/images/517519-20170107223958300-1808046515.png\" alt=\"\" />&nbsp;\n\n&nbsp;\n\n**ThreadLocal实现线程范围的共享变量，<strong>ThreadLocal**类就相当于一个Map</strong>\n\n见下页的示意图和辅助代码解释**ThreadLocal的作用和目的**：\n\n　　**1.用于实现线程内的数据共享**，即对于相同的程序代码，多个模块在同一个线程中运行时要共享一份数据，而在另外线程中运行时又共享另外一份数据。\n\n**　　2.每个线程调用全局ThreadLocal对象的set方法，就相当于往其内部的map中增加一条记录，key分别是各自的线程，value是各自的set方法传进去的值。**在线程结束时可以调用**ThreadLocal.clear()方法**，这样会**更快释放内存**，不调用也可以，因为线程结束后也可以自动释放相关的ThreadLocal变量。\n\n<img src=\"/images/517519-20170107221206237-106608567.png\" alt=\"\" width=\"625\" height=\"429\" />\n\n**ThreadLocal的应用场景： **\n\n　　1、订单处理包含一系列操作：减少库存量、增加一条流水台账、修改总账，这几个操作要在同一个事务中完成，通常也即同一个线程中进行处理，如果累加公司应收款的操作失败了，则应该把前面的操作回滚，否则，提交所有操作，这要求这些操作使用相同的数据库连接对象，而这些操作的代码分别位于不同的模块类中。&nbsp;\n\n　　2、银行转账包含一系列操作： 把转出帐户的余额减少，把转入帐户的余额增加，这两个操作要在同一个事务中完成，它们必须使用相同的数据库连接对象，转入和转出操作的代码分别是两个不同的帐户对象的方法。\n\n　　3、例如Strut2的ActionContext，同一段代码被不同的线程调用运行时，该代码操作的数据是每个线程各自的状态和数据，对于不同的线程来说，getContext方法拿到的对象都不相同，对同一个线程来说，不管调用getContext方法多少次和在哪个模块中getContext方法，拿到的都是同一个。\n\n&nbsp;\n\n实验案例：定义一个**全局共享的ThreadLocal变量**，然后**启动多个线程**向该ThreadLocal变量中存储一个随机值，接着各个线程调用另外其他多个类的方法，这多个类的方法中读取这个ThreadLocal变量的值，就可以看到**多个类在同一个线程中共享同一份数据**。\n\n实现对ThreadLocal变量的封装，让外界不要直接操作ThreadLocal变量。\n\n　　对基本类型的数据的封装，这种应用相对很少见。\n\n　　对对象类型的数据的封装，比较常见，即让某个类针对不同线程分别创建一个独立的实例对象。\n\n```\npackage java_thread;\n\nimport java.util.Random;\n\npublic class ThreadLocalTest {\n\n\tprivate static ThreadLocal<Integer> x = new ThreadLocal<Integer>();\n\tpublic static void main(String[] args) {\n\t\tfor(int i=0;i<2;i++){\n\t\t\tnew Thread(new Runnable(){\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\tint data = new Random().nextInt();\n\t\t\t\t\tSystem.out.println(Thread.currentThread().getName() \n\t\t\t\t\t\t\t+ \" has put data :\" + data);\n\t\t\t\t\tx.set(data);     //存放与当前线程有关的数据\n\t\t\t\t\tnew A().get();\n\t\t\t\t\tnew B().get();\n\t\t\t\t}\n\t\t\t}).start();\n\t\t}\n\t}\n\t\n\tstatic class A{\n\t\tpublic void get(){\n\t\t\tint data = x.get();\n\t\t\tSystem.out.println(\"A from \" + Thread.currentThread().getName() \n\t\t\t\t\t+ \" get data :\" + data);\n\t\t}\n\t}\n\t\n\tstatic class B{\n\t\tpublic void get(){\n\t\t\tint data = x.get();\t\t\t\n\t\t\tSystem.out.println(\"B from \" + Thread.currentThread().getName() \n\t\t\t\t\t+ \" get data :\" + data);\t\t\n\t\t}\t\t\n\t}\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20170107223932034-1742769195.png\" alt=\"\" />\n\n&nbsp;\n\n**使用ThreadLocal实现在线程范围内共享变量**\n\n```\npackage java_thread;\n\nimport java.util.Random;\n\npublic class ThreadLocalTest {\n\n\tprivate static ThreadLocal<Integer> x = new ThreadLocal<Integer>();\n\t//private static ThreadLocal<MyThreadScopeData> myThreadScopeData = new ThreadLocal<MyThreadScopeData>();\n\tpublic static void main(String[] args) {\n\t\tfor(int i=0;i<2;i++){\n\t\t\tnew Thread(new Runnable(){\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\tint data = new Random().nextInt();\n\t\t\t\t\tSystem.out.println(Thread.currentThread().getName() \n\t\t\t\t\t\t\t+ \" has put data :\" + data);\n\t\t\t\t\tx.set(data);\t\t\t\t//存放与当前线程有关的数据\n/*\t\t\t\t\tMyThreadScopeData myData = new MyThreadScopeData();\n\t\t\t\t\tmyData.setName(\"name\" + data);\n\t\t\t\t\tmyData.setAge(data);\n\t\t\t\t\tmyThreadScopeData.set(myData);*/\n\t\t\t\t\tMyThreadScopeData.getThreadInstance().setName(\"name\" + data);\n\t\t\t\t\tMyThreadScopeData.getThreadInstance().setAge(data);\n\t\t\t\t\tnew A().get();\n\t\t\t\t\tnew B().get();\n\t\t\t\t}\n\t\t\t}).start();\n\t\t}\n\t}\n\t\n\tstatic class A{\n\t\tpublic void get(){\n\t\t\tint data = x.get();\n\t\t\tSystem.out.println(\"A from \" + Thread.currentThread().getName() \n\t\t\t\t\t+ \" get data :\" + data);\n/*\t\t\tMyThreadScopeData myData = myThreadScopeData.get();;\n\t\t\tSystem.out.println(\"A from \" + Thread.currentThread().getName() \n\t\t\t\t\t+ \" getMyData: \" + myData.getName() + \",\" +\n\t\t\t\t\tmyData.getAge());*/\n\t\t\tMyThreadScopeData myData = MyThreadScopeData.getThreadInstance();\n\t\t\tSystem.out.println(\"A from \" + Thread.currentThread().getName() \n\t\t\t\t\t+ \" getMyData: \" + myData.getName() + \",\" +\n\t\t\t\t\tmyData.getAge());\n\t\t}\n\t}\n\t\n\tstatic class B{\n\t\tpublic void get(){\n\t\t\tint data = x.get();\t\t\t\n\t\t\tSystem.out.println(\"B from \" + Thread.currentThread().getName() + \" get data :\" + data);\n\t\t\tMyThreadScopeData myData = MyThreadScopeData.getThreadInstance();\n\t\t\tSystem.out.println(\"B from \" + Thread.currentThread().getName() \n\t\t\t\t\t+ \" getMyData: \" + myData.getName() + \",\" +\n\t\t\t\t\tmyData.getAge());\t\t\t\n\t\t}\t\t\n\t}\n}\n\nclass MyThreadScopeData{\n\tprivate MyThreadScopeData(){}\t\t\t\t//构造方法私有化\n\tpublic static /*synchronized*/ MyThreadScopeData getThreadInstance(){\n\t\tMyThreadScopeData instance = map.get();\t\t//通过map来判断有没有其他线程生成实例对象，如果没有就创建，所以不需要加入synchronized\n\t\tif(instance == null){\n\t\t\tinstance = new MyThreadScopeData();\n\t\t\tmap.set(instance);\n\t\t}\n\t\treturn instance;\n\t}\n\t//private static MyThreadScopeData instance = null;//new MyThreadScopeData();\n\tprivate static ThreadLocal<MyThreadScopeData> map = new ThreadLocal<MyThreadScopeData>();//把ThreadLocal封装在一个类的内部\n\t\n\tprivate String name;\n\tprivate int age;\n\tpublic String getName() {\n\t\treturn name;\n\t}\n\tpublic void setName(String name) {\n\t\tthis.name = name;\n\t}\n\tpublic int getAge() {\n\t\treturn age;\n\t}\n\tpublic void setAge(int age) {\n\t\tthis.age = age;\n\t}\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20170108134637097-410349656.png\" alt=\"\" />\n","tags":["多线程"]},{"title":"Spring MVC学习笔记——SiteMesh的使用（转）","url":"/Spring MVC学习笔记——SiteMesh的使用（转）.html","content":"转自 [SiteMesh的使用](http://www.cnblogs.com/china-li/archive/2013/05/15/3080154.html)\n\n<!--more-->\n&nbsp;\n\n**SiteMesh**的介绍就不多说了，主要是用来统一页面风格，减少重复编码的。\n\n它定义了一个过滤器，然后把页面都加上统一的头部和底部。\n\n需要先在WEB-INF/lib下引入sitemesh的jar包：[http://wiki.sitemesh.org/display/sitemesh/Download](http://wiki.sitemesh.org/display/sitemesh/Download)&nbsp;。这里使用2.4版本。\n\n&nbsp;\n\n**过滤器定义：**\n\n在web.xml中\n\n```\n    <filter>\n        <filter-name>sitemesh</filter-name>\n        <filter-class>com.opensymphony.module.sitemesh.filter.PageFilter</filter-class>\n    </filter>\n    <filter-mapping>\n        <filter-name>sitemesh</filter-name>\n        <url-pattern>/*</url-pattern>\n    </filter-mapping>\n\n```\n\n&nbsp;\n\n**decorators.xml文件：**\n\nWEB-INF下新建decorators.xml文件：\n\n```\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<decorators defaultdir=\"/WEB-INF/layouts/\">\n    <!-- 此处用来定义不需要过滤的页面 -->\n    <excludes>\n        <pattern>/static/*</pattern>\n    </excludes>\n\n    <!-- 用来定义装饰器要过滤的页面 -->\n    <decorator name=\"default\" page=\"default.jsp\">\n        <pattern>/*</pattern>\n    </decorator>\n</decorators>\n\n```\n\n不用过滤/static/目录下的文件，然后指定了装饰器：/WEB-INF/layouts/default.jsp。\n\n我用的是Spring MVC，目录结构大致：\n\n<img src=\"/images/517519-20200507230902663-328580796.png\" alt=\"\" width=\"300\" height=\"330\" />\n\n&nbsp;\n\n&nbsp;\n\n&nbsp; \n\n**default.jsp：**\n\n&nbsp;\n\n```\n<%@ page contentType=\"text/html;charset=UTF-8\"%>\n<%@ taglib prefix=\"sitemesh\" uri=\"http://www.opensymphony.com/sitemesh/decorator\" %>  \n<%@ taglib prefix=\"c\" uri=\"http://java.sun.com/jsp/jstl/core\"%>\n\n<c:set var=\"ctx\" value=\"${pageContext.request.contextPath}\" />\n\n<!DOCTYPE html>\n<html>\n<head>\n<title>QuickStart示例:<sitemesh:title/></title>\n<meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\" />\n<meta http-equiv=\"Cache-Control\" content=\"no-store\" />\n<meta http-equiv=\"Pragma\" content=\"no-cache\" />\n<meta http-equiv=\"Expires\" content=\"0\" />\n\n<link type=\"image/x-icon\" href=\"${ctx}/static/images/favicon.ico\" rel=\"shortcut icon\">\n<link href=\"${ctx}/sc/bootstrap/2.3.0/css/bootstrap.min.css\" type=\"text/css\" rel=\"stylesheet\" />\n<link href=\"${ctx}/sc/jquery-validation/1.11.0/validate.css\" type=\"text/css\" rel=\"stylesheet\" />\n<link href=\"${ctx}/css/base/default.css\" type=\"text/css\" rel=\"stylesheet\" />\n<script src=\"${ctx}/sc/jquery/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"${ctx}/sc/jquery-validation/1.11.0/jquery.validate.min.js\" type=\"text/javascript\"></script>\n<script src=\"${ctx}/sc/jquery-validation/1.11.0/messages_bs_zh.js\" type=\"text/javascript\"></script>\n\n<sitemesh:head/>\n</head>\n\n<body>\n    \n        <%@ include file=\"/WEB-INF/layouts/header.jsp\"%>\n        \n            <sitemesh:body/>\n    \n        <%@ include file=\"/WEB-INF/layouts/footer.jsp\"%>\n    \n    <script src=\"${ctx}/sc/bootstrap/2.3.0/js/bootstrap.min.js\" type=\"text/javascript\"></script>\n</body>\n</html>\n\n```\n\n&nbsp;\n\n首先引入了SiteMesh标签。\n\n<sitemesh:title/> 会自动替换为被过滤页面的title。\n\n<sitemesh:head/> 会把被过滤页面head里面的东西（除了title）放在这个地方。\n\n<sitemesh:body/> 被过滤的页面body里面的内容放在这里。\n\n在content的上下引入了header和footer。\n\n我们在头部引入了js和css，就可以重用了。\n\n&nbsp;\n\n**使用：**\n\n使用的过程中，几乎感受不到SiteMesh的存在。例如下面的页面： <br />\n\n&nbsp;\n\n```\n<%@ page language=\"java\" contentType=\"text/html; charset=utf-8\"\n    pageEncoding=\"utf-8\"%>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<html>\n<!-- 第一个被装饰(目标)页面  -->\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n<title>被装饰（目标）页面title</title>\n<script type=\"text/javascript\" src=\"/js/hello.js\"></script>\n</head>\n\n<body>\n    <h4>被装饰（目标）页面body标签内内容。</h4>\n    <h3>使用SiteMesh的好处?</h3>\n    <ul>\n        - 被装饰（目标）页面和装饰页面完全分离。\n        - 做到真正的页面复用，一个装饰页面装饰多个被装饰（目标）页面。\n        - 更容易实现统一的网站风格。\n        - 还有。。。\n    </ul>\n</body>\n</html>\n\n```\n\n&nbsp;\n\n这就是一个普通的页面，但是被SiteMesh装饰之后，就会自动去掉<html> <body> <head>等元素，然后把相应的东西放在模板对应位置上。\n\n我们来看一下，被SiteMesh装饰过的页面源代码：\n\n```\n<!DOCTYPE html>\n<html>\n<head>\n<title>QuickStart示例:被装饰（目标）页面title</title>\n<meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\" />\n<meta http-equiv=\"Cache-Control\" content=\"no-store\" />\n<meta http-equiv=\"Pragma\" content=\"no-cache\" />\n<meta http-equiv=\"Expires\" content=\"0\" />\n\n<link type=\"image/x-icon\" href=\"/SpringMVC/static/images/favicon.ico\" rel=\"shortcut icon\">\n<link href=\"/SpringMVC/sc/bootstrap/2.3.0/css/bootstrap.min.css\" type=\"text/css\" rel=\"stylesheet\" />\n<link href=\"/SpringMVC/sc/jquery-validation/1.11.0/validate.css\" type=\"text/css\" rel=\"stylesheet\" />\n<link href=\"/SpringMVC/css/base/default.css\" type=\"text/css\" rel=\"stylesheet\" />\n<script src=\"/SpringMVC/sc/jquery/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/SpringMVC/sc/jquery-validation/1.11.0/jquery.validate.min.js\" type=\"text/javascript\"></script>\n<script src=\"/SpringMVC/sc/jquery-validation/1.11.0/messages_bs_zh.js\" type=\"text/javascript\"></script>\n\n\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n\n<script type=\"text/javascript\" src=\"/js/hello.js\"></script>\n\n</head>\n\n<body>\n    \n        \n\n\n\n\n        \n            \n    <h4>被装饰（目标）页面body标签内内容。</h4>\n    <h3>使用SiteMesh的好处?</h3>\n    <ul>\n        - 被装饰（目标）页面和装饰页面完全分离。\n        - 做到真正的页面复用，一个装饰页面装饰多个被装饰（目标）页面。\n        - 更容易实现统一的网站风格。\n        - 还有。。。\n    </ul>\n\n        \n        \n\n    Copyright &copy; 2005-2012 [spring.org.cn]()\n\n\n\n    \n    <script src=\"/SpringMVC/sc/bootstrap/2.3.0/js/bootstrap.min.js\" type=\"text/javascript\"></script>\n</body>\n</html>\n\n```\n\n#### 被装饰（目标）页面body标签内内容。\n\n&nbsp;\n\n**SiteMesh**查看**文档** **[使用sitemesh建立复合视图 - 2.装饰器](http://docs.huihoo.com/java/sitemesh/2.html)**\n\n&nbsp;\n\n**SiteMesh**中有一个**decorator标签**，可以轻松解决**页面布局**的问题\n\n之前**解决页面重复布局**的时候，使用的是**<include>标签**，但是需要在每个页面都用他**引入其他JSP文件**，\n\n而使用decorator标签只需要在**配置文件decorators.xml**进行相应的配置**再加上一个装饰器**(其实就是一个JSP页面)即可。\n\n&nbsp;\n\n在**web.xml文件**中加入过滤器定义\n\n```\n\t<!-- 过滤器定义 -->\n\t<filter>\n\t\t<filter-name>sitemesh</filter-name>\n\t\t<filter-class>com.opensymphony.sitemesh.webapp.SiteMeshFilter</filter-class>\n\t</filter>\n\t<filter-mapping>\n\t\t<filter-name>sitemesh</filter-name>\n\t\t<url-pattern>/*</url-pattern>\n\t</filter-mapping>\n\n```\n\n&nbsp;\n\n**加入的配置文件decorators.xml**\n\n<img src=\"/images/517519-20170104114509472-859692147.png\" alt=\"\" />\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<decorators defaultdir=\"/WEB-INF/decorators\">\n    <!-- 此处用来定义不需要过滤的页面 -->\n    <excludes>\n        <pattern>/exclude.jsp</pattern>\n        <pattern>/exclude/*</pattern>\n    </excludes>\n\t<!-- 用来定义装饰器要过滤的页面 -->\n    <decorator name=\"main\" page=\"main.jsp\">\n        <pattern>/*</pattern>\n    </decorator>\n\n</decorators>\n\n```\n\n&nbsp;\n\n**在WebContent/WEB-INF/目录下创建/decorators目录，在这个目录下写main.jsp文件**\n\n**<img src=\"/images/517519-20170104114604722-1859335036.png\" alt=\"\" />**\n\n```\n<%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\"\n    pageEncoding=\"UTF-8\"%>\n<%@ taglib uri=\"http://www.opensymphony.com/sitemesh/decorator\" prefix=\"decorator\" %>\n<%@taglib prefix=\"c\" uri=\"http://java.sun.com/jsp/jstl/core\" %>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<html>\n\t<head>\n\t\t<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n\t\t<link rel=\"stylesheet\" type=\"text/css\" href=\"<%=request.getContextPath()%>/css/main.css\"/>\n\t\t<title>欢迎使用用户管理系统<decorator:title default=\"欢迎使用用户管理系统\"/></title>\n\t\t<decorator:head/>\t\t\t\t\t\t\t<!-- 取出被装饰页面的head标签中的内容(除了head标签本身) -->\n\t</head>\n\n\t<body>\n\t\t<h1><decorator:title/></h1>\t\t<!-- 取出被装饰页面的title标签中的内容 -->\n\t\t<c:if test=\"${not empty loginUser}\">\n\t\t\t[用户添加](<%=request.getContextPath() %>/user/add)\n\t\t\t[用户列表](<%=request.getContextPath() %>/user/users)\n\t\t\t[退出系统](<%=request.getContextPath() %>/logout)\n\t\t\t当前用户:${loginUser.nickname }\n\t\t</c:if>\n\t\t<hr/>\n\t\t<decorator:body/>\t\t\t\t\t\t\t<!-- 取出被装饰页面的body标签中的内容 -->\n\t\t\n\t\t\tCopyRight@2012-2015<br/>\n\t\t\t用户管理系统\n\t\t\n\t</body>\n</html>\n\n```\n\n&nbsp;\n","tags":["SpringMVC"]},{"title":"clickhouse学习笔记——Go客户端连接clickhouse","url":"/clickhouse学习笔记——Go客户端连接clickhouse.html","content":"## 1.创建clickhouse环境\n\n安装clickhouse\n\n参考：[ubuntu16.04安装clickhouse](https://www.cnblogs.com/tonglin0325/p/5576803.html)\n\n或者使用docker\n\n参考：[https://hub.docker.com/r/clickhouse/clickhouse-server](https://hub.docker.com/r/clickhouse/clickhouse-server)\n\n```\ndocker run -d -p 18123:8123 -p 19000:9000 --name some-clickhouse-server --ulimit nofile=262144:262144 clickhouse/clickhouse-server:23.8\n\n```\n\n使用datagrip连接\n\n<img src=\"/images/517519-20240525202735281-1790572630.png\" width=\"600\" height=\"396\" loading=\"lazy\" />\n\n创建表和测试数据\n\n```\nCREATE TABLE default.my_first_table\n(\n    user_id UInt32,\n    message String,\n    timestamp DateTime,\n    metric Float32\n)\nENGINE = MergeTree()\nPRIMARY KEY (user_id, timestamp);\n\nINSERT INTO default.my_first_table (user_id, message, timestamp, metric) VALUES\n    (101, 'Hello, ClickHouse!',                                 now(),       -1.0    ),\n    (102, 'Insert a lot of rows per batch',                     yesterday(), 1.41421 ),\n    (102, 'Sort your data based on your commonly-used queries', today(),     2.718   ),\n    (101, 'Granules are the smallest chunks of data read',      now() + 5,   3.14159 )\n\n```\n\n## 2.使用client连接clickhouse\n\ngolang客户端连接clickhouse，可以使用 [clickhouse-go](https://github.com/ClickHouse/clickhouse-go) 这个库\n\n参考：[一文教你Go语言如何轻松使用ClickHouse](https://blog.csdn.net/weixin_59801183/article/details/130575624)\n\n引入依赖\n\n```\ngo get github.com/ClickHouse/clickhouse-go\n\n```\n\n导入clickhouse的driver和database/sql，也可以使用github.com/jmoiron/sqlx，参考：[golang操作clickhouse使用入门](https://segmentfault.com/a/1190000042083211)\n\n```\nimport \"database/sql\"\nimport _ \"github.com/ClickHouse/clickhouse-go\"\n\n```\n\n否则会报\n\n```\nsql: unknown driver \"clickhouse\" (forgotten import?)\n\n```\n\n创建连接\n\n```\nsource := \"tcp://localhost:19000?username=default&amp;password=&amp;database=default&amp;block_size=4096\"\ndb, err := sql.Open(\"clickhouse\", source)\nif err != nil {\n\tfmt.Println(err)\n}\ndefer func() {\n\t_ = db.Close()\n}()\n\n```\n\n记得添加defer用于关闭连接\n\n创建clickhouse表\n\n```\nquery := `\n\tCREATE TABLE default.my_first_table\n\t(\n\t\tuser_id UInt32,\n\t\tmessage String,\n\t\ttimestamp DateTime,\n\t\tmetric Float32\n\t)\n\tENGINE = MergeTree()\n\tPRIMARY KEY (user_id, timestamp)\n `\n_, err = db.Exec(query)\nif err != nil {\n\tfmt.Println(err)\n}\n\n```\n\n插入数据\n\n```\nvar arr [][]any\narr = append(arr, []any{101, \"Hello, ClickHouse!\", time.Now(), -1.0})\narr = append(arr, []any{102, \"Insert a lot of rows per batch\", time.Now().Add(-1), 1.41421})\narr = append(arr, []any{102, \"Sort your data based on your commonly-used queries\", time.Now().Add(1), 2.718})\narr = append(arr, []any{101, \"Granules are the smallest chunks of data read\", time.Now().Add(5), 3.14159})\ntx, err := db.Begin()\nif err != nil {\n\tlog.Fatal(err)\n}\nquery := `INSERT INTO default.my_first_table (user_id, message, timestamp, metric) VALUES (?, ?, ?, ?)`\nstmt, err := tx.Prepare(query)\nif err != nil {\n\tlog.Fatal(err)\n}\nfor _, data := range arr {\n\t_, err = stmt.Exec(data...)\n\tif err != nil {\n\t\tfmt.Println(err)\n\t}\n}\n_ = tx.Commit()\n\n```\n\n查询数据\n\n```\n// 读取数据\ntype Data struct {\n\tUserId    int64     `db:\"user_id\"`\n\tMessage   string    `db:\"message\"`\n\tTimestamp time.Time `db:\"timestamp\"`\n\tMetric    float64   `db:\"metric\"`\n}\n\nquery := \"SELECT * FROM default.my_first_table\"\nrows, err := db.Query(query)\nif err != nil {\n\tfmt.Println(err)\n}\nvar result []Data\nfor rows.Next() {\n\tvar data Data\n\terr = rows.Scan(&amp;data.UserId, &amp;data.Message, &amp;data.Timestamp, &amp;data.Metric)\n\tif err != nil {\n\t\tfmt.Println(err)\n\t}\n\tresult = append(result, data)\n}\nfmt.Println(result)\n\n```\n\n输出\n\n```\n[{101 Hello, ClickHouse! 2024-05-25 15:54:00 +0000 UTC -1} {101 Granules are the smallest chunks of data read 2024-05-25 15:54:00 +0000 UTC 3.14159} {102 Insert a lot of rows per batch 2024-05-25 15:54:00 +0000 UTC 1.41421} {102 Sort your data based on your commonly-used queries 2024-05-25 15:54:00 +0000 UTC 2.718}]\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["clickhouse"]},{"title":"Amazon S3限流机制","url":"/Amazon S3限流机制.html","content":"当使用S3作为Amazon EMR的存储的时候，当写入的流量比较大的时候，有时会遇到性能瓶颈，报错如下\n\n```\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Please reduce your request rate.\n\n```\n\n在如下的AWS文章中介绍，S3的性能不是按照**bucket**定义的，而是按照bucket的**prefix，**对于每个prefix，3500的PUT/COPY/POST/DELETE能力和5000的GET/HEAD能力，如果超过这个限制的话，就会被限流\n\n关于prefix定义，参考文档：[aws s3原理和常用命令](https://www.cnblogs.com/tonglin0325/p/13683861.html)\n\n<img src=\"/images/517519-20221110104634263-245477808.png\" alt=\"\" loading=\"lazy\" />\n\n参考：[从 Amazon EMR 和 AWS Glue 访问 Amazon S3 中数据的性能优化最佳实践](https://aws.amazon.com/cn/blogs/china/best-practices-to-optimize-data-access-performance-from-amazon-emr-and-aws-glue-to-amazon-s3/)\n\n在咨询了AWS的工程师后，说上面的文档不是很准确，更具体的回答是：\n\n1.对于一个刚创建的bucket，默认每个bucket有3500写+5000读的能力\n\n2.当S3 bucket的流量增加后，S3会对应进行bucket的split partition，根据的规则是**按照prefix逐位向后分割**\n\n意思是对于S3://bucket_name/[a-z,0-9][a-z,0-9]..../object的s3存储结构，prefix的第1个字符有a-z+0-9（26+10=36）种可能性，如果这36个字符出现的可能性是一样的话，则当整个bucket的流量很大的时候，bucket就会扩容到36个partition，从而获得36*（3500/5000）的能力\n\n这也就是为什么当遇到S3性能问题的能力，AWS的工程师经常会建议使用**增加随机前缀**，或者**分区倒排**的方案，其他AWS文章也有提到：[Amazon EMR实战心得浅谈](https://aws.amazon.com/cn/blogs/china/brief-introduction-to-emr-practical-experience/)\n\n**增加随机前缀（最建议，但不适用于数仓）**：对于S3://bucket_name/xxx/2022/10/01/object_name的路径，如果bucket的并发超过了3500/5000的限制后，则S3会进行扩容partition，这时S3://bucket_name/xxx/2022/10/01/和S3://bucket_name/xxx/2022/10/02/分split成2个partition，到了第二天02分区又会split成02和03分区，但是S3扩容是需要一定时间的，所以每天流量峰值的时候有可能会出现限流。\n\n如果增加了随机前缀，路径则会变成S3://bucket_name/[a-z,0-9][a-z,0-9]..../xxx/2022/10/01/object_name，这来一是流量被打散成36份，不会在每天的prefix上形成单独的热点；二来在遇到每年/每月/每日路径更新的时候，也不会进行S3扩容，因为扩容只会发生在第一个字符[a-z,0-9]\n\n**分区倒排**：对于S3://bucket_name/xxx/2022/10/01/object_name的路径，分区倒排之后，路径变成S3://bucket_name/xxx/01/10/2022/object_name，即变成日/月/年，如果S3://bucket_name/xxx/路径下的并发超过3500/5000，第一个月过后，整个bucket会split成30或者31份（每天扩容一次），到了第二个月的时候，流量就会继续循环打到这30或者31个partition上，从而不会每天进行扩容\n\n上面介绍的S3**自动split**的原理，如果是**手动split**的话，就没有以上限制，就是给AWS对接的工程师提support case让他们后台调整，人工指定从第几个字符开始split，类似于预分区\n\n<!--more-->\n&nbsp;\n\n其他优化方法，参考：[从 Amazon EMR 和 AWS Glue 访问 Amazon S3 中数据的性能优化最佳实践](https://aws.amazon.com/cn/blogs/china/best-practices-to-optimize-data-access-performance-from-amazon-emr-and-aws-glue-to-amazon-s3/)\n","tags":["AWS"]},{"title":"Spring MVC学习笔记——文件上传","url":"/Spring MVC学习笔记——文件上传.html","content":"**1.实现文件上传首先需要导入Apache的包，commons-fileupload-1.2.2.jar和commons-io-2.1.jar**\n\n　　实现上传就在**add.jsp文件中修改表单**\n\n```\nenctype=\"multipart/form-data\"\n和\n<tr>\n　　|附件:|<input type=\"file\" name=\"attach\"/>\n</tr>\n\n```\n\n<!--more-->\n&nbsp;完整的add.jsp文件\n\n```\n<%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\"\n    pageEncoding=\"UTF-8\"%>\n<%@ taglib prefix=\"sf\" uri=\"http://www.springframework.org/tags/form\" %>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<html>\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n<title>Insert title here</title>\n</head>\n<body>\n\t<sf:form method=\"post\" modelAttribute=\"user\" enctype=\"multipart/form-data\">\n\t\t<table width=\"700\" align=\"center\" border=\"1\">\n\t\t\t<tr>\n\t\t\t\t|用户名:|<sf:input path=\"username\"/><sf:errors path=\"username\"/>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t|用户密码:|<sf:password path=\"password\"/><sf:errors path=\"password\"/>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t|用户昵称:|<sf:input path=\"nickname\"/>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t|用户邮箱:|<sf:input path=\"email\"/><sf:errors path=\"email\"/>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t|附件:|<input type=\"file\" name=\"attach\"/>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td colspan=\"2\">\n\t\t\t\t<input type=\"submit\" value=\"用户添加\"/>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t</table>\n\t</sf:form>\n\t\n</body>\n</html>\n\n```\n\n&nbsp;\n\n**2.在user-servlet.xml中配置上传文件**\n\n```\n    <!-- 配置上传文件CommonsMultipartResolver -->\n    <bean id=\"multipartResolver\" class=\"org.springframework.web.multipart.commons.CommonsMultipartResolver\">\n    \t<property name=\"maxUploadSize\" value=\"5000000\"></property>\n    </bean>\n\n```\n\n&nbsp;\n\n**3.在控制器中修改add()方法**\n\n```\n\t//在具体添加用户的时候，是POST请求，就访问以下代码\n\t@RequestMapping(value=\"/add\",method=RequestMethod.POST)\n\tpublic String add(@Validated User user,BindingResult br,MultipartFile attach,HttpServletRequest req) throws IOException{//一定要紧跟@Validated之后写验证结果类\n\t\tif(br.hasErrors()){\n\t\t\t//如果有错误，直接跳转到add视图\n\t\t\treturn \"user/add\";\n\t\t}\n\t\tString realpath = req.getSession().getServletContext().getRealPath(\"/resources/upload\"); \t//取得会话对象的路径\n\t\tSystem.out.println(realpath);\n\t\tFile f = new File(realpath+\"/\"+attach.getOriginalFilename());\n\t\tFileUtils.copyInputStreamToFile(attach.getInputStream(), f);\n\t\tSystem.out.println(attach.getName()+\",\"+attach.getOriginalFilename()+\",\"+attach.getContentType());\n\t\tusers.put(user.getUsername(),user);\t//把key和user对象放进Map中\n\t\treturn \"redirect:/user/users\";\n\t}\n\n```\n\n&nbsp;\n\n&nbsp;还需要在resources文件夹下面添加upload文件夹\n\n<img src=\"/images/517519-20170102104027472-229900029.png\" alt=\"\" />\n\n在表单中添加文件上传后如下图\n\n<img src=\"/images/517519-20170102104112269-1135233549.png\" alt=\"\" />\n\n&nbsp;\n\n&nbsp;\n\n如果要**上传多个文件**的话，**修改add.jsp中的表单，注意是attachs和控制器中的attachs对应**\n\n```\n\t\t\t<tr>\n\t\t\t\t|附件:<td><input type=\"file\" name=\"attachs\"/>\n\t\t\t\t<input type=\"file\" name=\"attachs\"/>\n\t\t\t\t<input type=\"file\" name=\"attachs\"/></td>\n\t\t\t</tr>\n\n```\n\n&nbsp;\n\n**修改控制器中的add()方法，把MultipartFile改为数组，attachs对应，@RequestParam(\"attachs\")必不可少**\n\n```\n\t//在具体添加用户的时候，是POST请求，就访问以下代码\n\t@RequestMapping(value=\"/add\",method=RequestMethod.POST)\n\tpublic String add(@Validated User user,BindingResult br,@RequestParam(\"attachs\")MultipartFile[] attachs,HttpServletRequest req) throws IOException{//一定要紧跟@Validated之后写验证结果类\n\t\tif(br.hasErrors()){\n\t\t\t//如果有错误，直接跳转到add视图\n\t\t\treturn \"user/add\";\n\t\t}\n\t\tString realpath = req.getSession().getServletContext().getRealPath(\"/resources/upload\"); \t//取得会话对象的路径\n\t\tSystem.out.println(realpath);\n\t\tfor(MultipartFile attach:attachs){\n\t\t\tif(attach.isEmpty()){\t\t//检查上传多个文件的时候，每个文件是否为空，否则会在copy的时候出错\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tFile f = new File(realpath+\"/\"+attach.getOriginalFilename());\n\t\t\tFileUtils.copyInputStreamToFile(attach.getInputStream(), f);\n\t\t\tSystem.out.println(attach.getName()+\",\"+attach.getOriginalFilename()+\",\"+attach.getContentType());\n\t\t}\n\t\tusers.put(user.getUsername(),user);\t//把key和user对象放进Map中\n\t\treturn \"redirect:/user/users\";\n\t}\n\n```\n\n&nbsp;\n\n&nbsp;\n\n<img src=\"/images/517519-20170102110647706-1411575719.png\" alt=\"\" />\n\n<img src=\"/images/517519-20170102110745081-2112571453.png\" alt=\"\" />\n\n上面添加多个文件的时候，还**检测了文件是否为空**，为空的话就跳过\n\n```\n\t\t\tif(attach.isEmpty()){\n\t\t\t\tcontinue;\n\t\t\t}\n\n```\n\n**&nbsp;注意：在这个简单的上传文件的例子中，如果上传的文件和已经存在的文件同名的话，会进行覆盖**\n","tags":["SpringMVC"]},{"title":"Spring MVC学习笔记——完整的用户登录","url":"/Spring MVC学习笔记——完整的用户登录.html","content":"**1.搭建环境的第一步是导包，把下面这些包都导入工程中<br />**\n\n/media/common/工作/Ubuntu软件/SpringMVC_jar包整理/aop<br />/media/common/工作/Ubuntu软件/SpringMVC_jar包整理/apache-commons-logging<br />/media/common/工作/Ubuntu软件/SpringMVC_jar包整理/apache-log4j<br />/media/common/工作/Ubuntu软件/SpringMVC_jar包整理/bean-validator<br />/media/common/工作/Ubuntu软件/SpringMVC_jar包整理/dbcp<br />/media/common/工作/Ubuntu软件/SpringMVC_jar包整理/hibernate-3.6.8.<br />/media/common/工作/Ubuntu软件/SpringMVC_jar包整理/JSTL<br />/media/common/工作/Ubuntu软件/SpringMVC_jar包整理/mysql<br />/media/common/工作/Ubuntu软件/SpringMVC_jar包整理/pager<br />/media/common/工作/Ubuntu软件/SpringMVC_jar包整理/sitemesh<br />/media/common/工作/Ubuntu软件/SpringMVC_jar包整理/spring\n\n手动导包也可以，不过不是很方便，推荐学习使用maven的pom.xml文件来导入jar包\n\n**整个系统的结构**\n\n**<img src=\"/images/517519-20170119211452140-1950357950.png\" alt=\"\" />**\n\n**表示层（JSP页面），一般包名是view**\n\n**　<!--more-->\n&nbsp;&nbsp; ▼**\n\n**控制层，一般包名是action或者web，控制层也会操作实体层**\n\n**　　<strong>▼**</strong>\n\n**业务逻辑层，一般包名是service**\n\n**　　<strong><strong>▼**</strong></strong>\n\n**数据持久层，一般包名是dao**\n\n**　　<strong><strong><strong>▼**</strong></strong></strong>\n\n**实体层（JavaBean），一般包名是model或者entity**\n\n&nbsp;\n\n**写成的过程和上面的方向相反，从下往上写**\n\n**实体Entity层**\n\n**　　1.先写User类**\n\n**　　　　Id，username，nickname，password，email**\n\n**　　　　其中还包括注入**\n\n**　　2.再写Page类**\n\n**　　　　public class Pager<T>**\n\n**　　　　List<T> datas、offset、size、total**\n\n**　　3.写SystemContext类**\n\n**　　7.写UserException异常类**\n\n&nbsp;\n\n**数据持久层dao层，主要是操作Hibernate，还要写beans.xml**\n\n**　　4.写IUserDao接口**\n\n**　　　　增、更新、删除、根据ID查用户load、查所用用户List<User> list、查分页Pager<User> find、根据username查用户loadByUsername**\n\n**　　5.实现IUserDao接口**\n\n**　　　　分页find()中取得SystemContext类**\n\n&nbsp;\n\n**<strong>业务逻辑层service层，主要是写验证**</strong>\n\n**　　6.写IUserService接口**\n\n**　　　　增、更新、删除、根据ID查用户load、查所用用户List<User> list、查分页Pager<User> find、根据username查用户loadByUsername**\n\n**　　8.实现<strong>IUserService接口**</strong>\n\n**<strong>　　　　密码登录验证login、添加用户、修改用户、删除用户、查询用户、列出所有用户、分页find()**</strong>\n\n&nbsp;\n\n**<strong><strong><strong>控制层action层**</strong></strong></strong>\n\n**<strong><strong><strong>　　9.LoginFilter.java登录权限，实现Filter接口，doFilter()方法**</strong></strong></strong>\n\n**<strong><strong><strong>　　　　在请求是/user/*的时候拦截验证权限，没有权限重定向/login，有权限放行**</strong></strong></strong>\n\n**<strong><strong><strong>　　10.SystemContext.java分页过滤，实现Filter接口，doFilter()方法**</strong></strong></strong>\n\n&nbsp;\n\n**<strong><strong><strong>　　　　在请求是/*中，如果参数为Pager.offset的时候，拦截取得offset，设置SystemContext中的offset和size**</strong></strong></strong>\n\n**<strong><strong><strong>　　11.IndexController.java，Session共享数据**</strong></strong></strong>\n\n**<strong><strong><strong>　　　　在请求是/login的时候，将ModelMap中的属性放入Session中，实现多窗口共享数据**</strong></strong></strong>\n\n**<strong><strong><strong>　　12.UserController.java，总的请求为/user，这也就是MVC模型中的RequestMapping**</strong></strong></strong>\n\n**<strong><strong><strong>　　　　在请求是/user和/的时候，向model模型中添加&mdash;&mdash;userService.find()**</strong></strong></strong>\n\n**<strong><strong><strong><strong><strong><strong><strong>　　　　在请求是/add的时候（分GET和POST），向model模型中添加&mdash;&mdash;new User()<br />**</strong></strong></strong></strong></strong></strong></strong>\n\n**<strong><strong><strong>　　　　在请求是/{id}的时候，向model模型中添加&mdash;&mdash;userService.load(id)**</strong></strong></strong>\n\n**<strong><strong><strong>　　　　在请求是/<strong><strong><strong><strong>{id}**</strong></strong></strong>/update的时候**<strong><strong><strong><strong><strong><strong><strong>（分GET和POST）**</strong></strong></strong></strong></strong></strong></strong>...</strong></strong></strong></strong>\n\n**<strong><strong><strong><strong><strong><strong><strong>　　　　在请求是/<strong><strong><strong><strong>{id}**</strong></strong></strong>/delete的时候...</strong></strong></strong></strong></strong></strong></strong></strong>\n\n&nbsp;\n\n**<strong><strong><strong><strong><strong><strong><strong>　　　　最后再传给DispatchServlet，使用model从Controller给视图传值**</strong></strong></strong></strong></strong></strong></strong>\n\n**<strong><strong><strong><strong><strong><strong><strong>　　　　在jsp中通过 ${ } 取得属性**</strong></strong></strong></strong></strong></strong></strong>\n\n**<strong><strong><strong><strong><strong><strong><strong>　　　　记得加上@Controller，通过Annotation来配置控制器**</strong></strong></strong></strong></strong></strong></strong>\n\n&nbsp;\n\n**<strong><strong><strong><strong><strong><strong><strong>注意：在持久层、业务层、控制层中，分别采用@Repository、@Service、@Controller对分层中的类进行注释**</strong></strong></strong></strong></strong></strong></strong>\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n\n**2.在工程的src/目录下加入beans.xml、jdbc.properties和log4j.properties这三个文件**\n\n&nbsp;\n\n**3.新建一个数据库文件**\n\n　　新建一个数据库spring_user\n\n```\nmysql> create database spring_user;\n\n```\n\n&nbsp;<img src=\"/images/517519-20170102144503441-1554973233.png\" alt=\"\" />\n\n　　并把字符编码改成UTF-8，可以参考 [Ubuntu下的MySQL安装](http://www.cnblogs.com/tonglin0325/p/5299031.html)\n\n```\ncreate database spring_user default character set utf8 collate utf8_general_ci;\n\n```\n\n&nbsp;　　**添加t_user表**\n\n```\nCREATE TABLE IF NOT EXISTS `t_user` (`username` varchar(64) NOT NULL,`password` varchar(11) NOT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\n```\n\n<img src=\"/images/517519-20170102210212566-1085857715.png\" alt=\"\" />&nbsp;\n\n**　　插入用户和密码**\n\n```\nINSERT INTO t_user (username,password) VALUES('admin','admin');\n\n```\n\n**&nbsp;4.接下来在页面就可以用admin来登录**\n\n**<img src=\"/images/517519-20170102210412941-2049982831.png\" alt=\"\" />**\n\n**　　而且中文也正常显示**\n\n**<img src=\"/images/517519-20170102210510581-861273239.png\" alt=\"\" width=\"748\" height=\"150\" />**\n\n&nbsp;\n\n&nbsp;\n\n**整个项目的结构**\n\n**　　1.JSP页面：登录、添加、列表、详情、更新、错误**\n\n**<img src=\"/images/517519-20170104093154019-1126319254.png\" alt=\"\" />**\n\n**　　2.实体层(Bean或者model)：分页类、分页查询结果类、用户类、Exception类**\n\n<img src=\"/images/517519-20170104093343816-691916201.png\" alt=\"\" />\n\n**　　3.控制层(Action)：页面控制器、登录过滤器、分页查询结果过滤器、用户控制器**\n\n<img src=\"/images/517519-20170104094817316-1235115312.png\" alt=\"\" />\n\n**　　4.业务逻辑层（Service）：一个接口和一个实现类（包括注入userDAO、登入验证、添删该查用户等等）**\n\n<img src=\"/images/517519-20170104095141878-337613640.png\" alt=\"\" />\n\n**　　5.数据持久层（DAO）：****一个接口和一个实现类（和HibernateDaoSupport相关，用户增删改查等等）**\n\n<img src=\"/images/517519-20170104095421941-16196795.png\" alt=\"\" />\n\n&nbsp;\n\n&nbsp;\n\n**1.JSP页面**\n\n　　**<strong>login.jsp**</strong>\n\n**<strong><img src=\"/images/517519-20170104155810972-1033305760.png\" alt=\"\" width=\"305\" height=\"231\" />**</strong>\n\n```\n<%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\"\n    pageEncoding=\"UTF-8\"%>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<html>\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n<title>用户登录</title>\n</head>\n<body>\n<form method=\"post\">\n\t用户名:<input type=\"text\" name=\"username\"/><br/>\n\t用户密码:<input type=\"password\" name=\"password\"/><br/>\n\t<input type=\"submit\" value=\"用户登录\"/>\n</form>\n</body>\n</html>\n\n```\n\n&nbsp;\n\n　　**error<strong>.jsp**</strong>\n\n**<strong><img src=\"/images/517519-20170104155717831-772715256.png\" alt=\"\" width=\"290\" height=\"226\" />**</strong>\n\n```\n<%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\"\n    pageEncoding=\"UTF-8\"%>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<html>\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n<title>发现异常</title>\n</head>\n<body>\n<h1>${exception.message }</h1>\n</body>\n</html>\n\n```\n\n**　**\n\n**　add.jsp**\n\n**<img src=\"/images/517519-20170104101137769-838839641.png\" alt=\"\" width=\"394\" height=\"190\" />**\n\n```\n<%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\"\n    pageEncoding=\"UTF-8\"%>\n<%@ taglib prefix=\"sf\" uri=\"http://www.springframework.org/tags/form\" %>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<html>\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n<title>用户添加</title>\n</head>\n<body>\n<sf:form method=\"post\" modelAttribute=\"user\">\n<table width=\"700\" align=\"center\" border=\"1\">\n\t<tr>\n\t|用户名:|<sf:input path=\"username\"/><sf:errors path=\"username\"/>\n\t</tr>\n\t<tr>\n\t|用户密码:|<sf:password path=\"password\"/><sf:errors path=\"password\"/>\n\t</tr>\n\t<tr>\n\t|用户昵称:|<sf:input path=\"nickname\"/>\n\t</tr>\n\t<tr>\n\t|用户邮箱:|<sf:input path=\"email\"/><sf:errors path=\"email\"/>\n\t</tr>\n\t<tr>\n\t<td colspan=\"2\">\n\t\t<input type=\"submit\" value=\"用户添加\"/>\n\t</td>\n\t</tr>\n</table>\n</sf:form>\n</body>\n</html>\n\n```\n\n&nbsp;\n\n**<strong>　　list.jsp**　</strong>\n\n**<img src=\"/images/517519-20170104101327441-1439878387.png\" alt=\"\" width=\"645\" height=\"240\" />**\n\n```\n<%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\"\n    pageEncoding=\"UTF-8\"%>\n<%@taglib prefix=\"c\" uri=\"http://java.sun.com/jsp/jstl/core\" %>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<html>\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n<title>用户列表</title>\n</head>\n<body>\n<table width=\"700\" align=\"center\" border=\"1\">\n\t<tr>\n\t|用户标识:${pagers.total }|用户名|用户昵称|用户密码|用户邮箱\n\t|操作\n\t</tr>\n\t<c:if test=\"${pagers.total le 0 }\">\n\t\t<tr>\n\t\t<td colspan=\"6\">目前还没有用户数据</td>\n\t\t</tr>\n\t</c:if>\n\t<c:if test=\"${pagers.total gt 0}\">\n\t\t<c:forEach items=\"${pagers.datas }\" var=\"u\">\n\t\t<tr>\n\t\t|${u.id }|${u.username }\n\t\t|[${u.nickname }](${u.id })\n\t\t|${u.password }|${u.email }\n\t\t|[更新](${u.id }/update)&nbsp;[删除](${u.id }/delete)\n\t\t</tr>\n\t\t</c:forEach>\n\t\t<tr>\n\t\t<td colspan=\"6\">\n\t\t\t<jsp:include page=\"/inc/pager.jsp\">\n\t\t\t\t<jsp:param value=\"users\" name=\"url\"/>\n\t\t\t\t<jsp:param value=\"${pagers.total}\" name=\"items\"/>\n\t\t\t</jsp:include>\n\t\t</td>\n\t\t</tr>\t\n\t</c:if>\n</table>\n</body>\n</html>\n\n```\n\n&nbsp;\n\n**&nbsp;　　show<strong>.jsp**　</strong>\n\n**<img src=\"/images/517519-20170104155620769-1325644927.png\" alt=\"\" width=\"661\" height=\"261\" />**\n\n```\n<%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\"\n    pageEncoding=\"UTF-8\"%>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<html>\n\t<head>\n\t\t<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n\t\t<title>用户[${user.nickname }]详细信息</title>\n\t</head>\n\t\n\t<body>\n\t\t<table width=\"700\" align=\"center\" border=\"1\">\n\t\t\t<tr>\n\t\t\t\t|用户标识:|${user.id }\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t|用户名:|${user.username }\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t|用户密码:|${user.password }\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t|用户昵称:|${user.nickname }\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t|用户邮箱:|${user.email }\n\t\t\t</tr>\n\t\t</table>\n\t</body>\n</html>\n\n```\n\n&nbsp;\n\n　　**<strong>update.jsp**</strong>\n\n**<strong><img src=\"/images/517519-20170104155646878-2036002214.png\" alt=\"\" width=\"493\" height=\"252\" />**</strong>\n\n```\n<%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\"\n    pageEncoding=\"UTF-8\"%>\n<%@ taglib prefix=\"sf\" uri=\"http://www.springframework.org/tags/form\" %>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<html>\n\t<head>\n\t\t<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n\t\t<title>用户更新</title>\n\t</head>\n\t<body>\n\t\t<sf:form method=\"post\" modelAttribute=\"user\">\n\t\t\t<table width=\"700\" align=\"center\" border=\"1\">\n\t\t\t\t<tr>\n\t\t\t\t\t|用户名:|${user.username }<sf:hidden path=\"username\"/>\n\t\t\t\t</tr>\n\t\t\t\t<tr>\n\t\t\t\t\t|用户密码:|<sf:password path=\"password\"/><sf:errors path=\"password\"/>\n\t\t\t\t</tr>\n\t\t\t\t<tr>\n\t\t\t\t\t|用户昵称:|<sf:input path=\"nickname\"/>\n\t\t\t\t</tr>\n\t\t\t\t<tr>\n\t\t\t\t\t|用户邮箱:|<sf:input path=\"email\"/><sf:errors path=\"email\"/>\n\t\t\t\t</tr>\n\t\t\t\t<tr>\n\t\t\t\t\t<td colspan=\"2\">\n\t\t\t\t\t\t<input type=\"submit\" value=\"用户更新\"/>\n\t\t\t\t\t</td>\n\t\t\t\t</tr>\n\t\t\t</table>\n\t\t</sf:form>\n\t</body>\n</html>\n\n```\n\n&nbsp;\n\n**分页结构**\n\n<img src=\"/images/517519-20170103232405128-1943345749.png\" alt=\"\" width=\"796\" height=\"414\" />\n\n&nbsp;\n\n**5.在src/目录下建立包org.common.model**\n\n　　&nbsp;****model包**中一般放的是实体类,这些类定义了一些基本的属性以及简单的get/set方法,这些类和数据库中的表存在对应关系**\n\n**　　一般都是javabean对象，例如与数据库的某个表相关联。**\n\n**　　<img src=\"/images/517519-20170103114337300-282491817.png\" alt=\"\" />**\n\n&nbsp;　　先写**User类**，文件名字为**User.java，其中就是简单的get方法和set方法**\n\n```\npackage org.common.model;\n\nimport javax.persistence.Entity;\nimport javax.persistence.GeneratedValue;\nimport javax.persistence.Id;\nimport javax.persistence.Table;\n\n@Entity\t\t\t\t\t\t\t\t\t\t\t//如果我们当前这个bean要设置成实体对象，就需要加上Entity这个注解\n@Table(name=\"t_user\")\t\t\t//设置数据库的表名\npublic class User {\n\tprivate int id;\n\tprivate String username;\n\tprivate String nickname;\n\tprivate String password;\n\tprivate String email;\n\t\n\t//(建议不要在属性上引入注解，因为属性是private的，如果引入注解会破坏其封装特性，所以建议在getter方法上加入注解)\n\t@Id\t\t\t\t\t\t\t\t\t\t\t//定义为数据库的主键ID　　\n\t@GeneratedValue\t\t\t\t//ID的生成策略为自动生成\n\tpublic int getId() {\n\t\treturn id;\n\t}\n\tpublic void setId(int id) {\n\t\tthis.id = id;\n\t}\n\tpublic String getUsername() {\n\t\treturn username;\n\t}\n\tpublic void setUsername(String username) {\n\t\tthis.username = username;\n\t}\n\tpublic String getNickname() {\n\t\treturn nickname;\n\t}\n\tpublic void setNickname(String nickname) {\n\t\tthis.nickname = nickname;\n\t}\n\tpublic String getPassword() {\n\t\treturn password;\n\t}\n\tpublic void setPassword(String password) {\n\t\tthis.password = password;\n\t}\n\tpublic String getEmail() {\n\t\treturn email;\n\t}\n\tpublic void setEmail(String email) {\n\t\tthis.email = email;\n\t}\n\t\n}\n\n```\n\n&nbsp;　　接下来只需要在hibernate.cfg.xml文件里面将该实体类加进去即可：\n\n```\n<!-- 基于annotation的配置 -->\n        <mapping class=\"com.xiaoluo.bean.User\"/>\n<!-- 基于hbm.xml配置文件 -->\n        <mapping resource=\"com/xiaoluo/bean/User.hbm.xml\"/>\n\n```\n\n　　但是，我们采取的方法是使用Spring配置数据源，即**在Spring容器中定义数据源，指定映射文件、设置hibernate控制属性等信息**，完成集成组装的工作，完全抛开hibernate.cfg.xml配置文件，具体的方法就是在**beans.xml文件**中加入\n\n<img src=\"/images/517519-20170103131009409-1510960412.png\" alt=\"\" />\n\n&nbsp;\n\n<img src=\"/images/517519-20170103130416800-481526536.png\" alt=\"\" />\n\n<img src=\"/images/517519-20170103130511659-1897958410.png\" alt=\"\" />\n\n其中的Spring注解hibernate实体方法\n\n```\n<property name=\"annotatedClasses\">\n            <list>\n                <value>com.sise.domain.Admin</value>\n                <value>com.sise.domain.Remind</value>\n                <value>com.sise.domain.User</value>\n            </list>\n</property>\n\n```\n\n&nbsp;可以使用下面的来替代\n\n```\n<!-- 设置Spring取那个包中查找相应的实体类，指定hibernate实体类映射文件 -->\n\t\t<property name=\"packagesToScan\">\n\t\t\t<value>org.common.model</value>\n\t\t</property>\n\n```\n\n&nbsp;\n\n```\n\t<!-- 导入Src目录下的jdbc.properties文件 -->\n\t<context:property-placeholder location=\"classpath:jdbc.properties\" />\n\t<bean id=\"dataSource\" class=\"org.apache.commons.dbcp.BasicDataSource\"\n\t\tdestroy-method=\"close\">\n\t\t<property name=\"driverClassName\" value=\"${jdbc.driverClassName}\" />\n\t\t<property name=\"url\" value=\"${jdbc.url}\" />\n\t\t<property name=\"username\" value=\"${jdbc.username}\" />\n\t\t<property name=\"password\" value=\"${jdbc.password}\" />\n\t\t<!-- 配置连接池的初始值 -->\n\t\t<property name=\"initialSize\" value=\"1\" />\n\t\t<!-- 连接池的最大值 -->\n\t\t<!-- <property name=\"maxActive\" value=\"500\"/> -->\n\t\t<!-- 最大空闲时，当经过一个高峰之后，连接池可以将一些用不到的连接释放，一直减少到maxIdle为止 -->\n\t\t<!-- <property name=\"maxIdle\" value=\"2\"/> -->\n\t\t<!-- 当最小空闲时，当连接少于minIdle时会自动去申请一些连接 -->\n\t\t<property name=\"minIdle\" value=\"1\" />\n\t\t<property name=\"maxActive\" value=\"100\" />\n\t\t<property name=\"maxIdle\" value=\"20\" />\n\t\t<property name=\"maxWait\" value=\"1000\" />\n\t</bean>\n\t\n\t<!--创建Spring的SessionFactory工厂 -->\n\t<!-- 如果使用的是Annotation的方式，不能使用LocalSessionFactoryBean,而应该使用 org.springframework.orm.hibernate3.annotation.AnnotationSessionFactoryBean -->\n\t<bean id=\"sessionFactory\"\n\t\tclass=\"org.springframework.orm.hibernate3.annotation.AnnotationSessionFactoryBean\">\n\t\t\n\t\t<!-- 注入数据源 -->\n\t\t<property name=\"dataSource\" ref=\"dataSource\" />\n\t\t\n\t\t<!-- 设置Spring取那个包中查找相应的实体类，指定hibernate实体类映射文件 -->\n\t\t<property name=\"packagesToScan\">\n\t\t\t<value>org.common.model</value>\n\t\t</property>\n\t\t\n\t\t<!-- 指定hibernate配置属性-->\n\t\t<property name=\"hibernateProperties\">\n\t\t\t<!-- <value> hibernate.dialect=org.hibernate.dialect.HSQLDialect </value> -->\n\t\t\t<props>\n\t\t\t\t<prop key=\"hibernate.dialect\">org.hibernate.dialect.MySQLDialect</prop>\n\t\t\t\t<prop key=\"hibernate.show_sql\">true</prop>\n\t\t\t\t<prop key=\"hibernate.hbm2ddl.auto\">update</prop>\n\t\t\t\t<prop key=\"hibernate.format_sql\">false</prop>\n\t\t\t</props>\n\t\t</property>\n\t</bean>\n\n```\n\n&nbsp;\n\n**6.在src/目录下建立包org.common.dao**\n\n**　　在dao中，写与数据库的操作，增删改查等方法<br />**\n\n**<img src=\"/images/517519-20170103134144550-1581032618.png\" alt=\"\" />**\n\n**　　首先写IUserDao接口，文件名为IUserDao.java**\n\n```\npackage org.common.dao;\n\nimport java.util.List;\n\nimport org.common.model.Pager;\nimport org.common.model.User;\n\npublic interface IUserDao {\t\t\t//IUserDao接口\n\tpublic void add(User user);\n\tpublic void update(User user);\n\tpublic void delete(int id);\n\tpublic User load(int id);\n\tpublic List<User> list();\n\tpublic Pager<User> find();\n\tpublic User loadByUsername(String username);\n}\n\n```\n\n&nbsp;　　**同时，还要在包org.common.model下加上分页的类，文件名是Page.java**\n\n```\npackage org.common.model;\n\nimport java.util.List;\n\npublic class Pager<T> {\t\t//分页\n\tprivate List<T> datas;\n\tprivate int offset;\n\tprivate int size;\n\tprivate long total;\n\t\n\tpublic List<T> getDatas() {\n\t\treturn datas;\n\t}\n\tpublic void setDatas(List<T> datas) {\n\t\tthis.datas = datas;\n\t}\n\tpublic int getOffset() {\n\t\treturn offset;\n\t}\n\tpublic void setOffset(int offset) {\n\t\tthis.offset = offset;\n\t}\n\tpublic int getSize() {\n\t\treturn size;\n\t}\n\tpublic void setSize(int size) {\n\t\tthis.size = size;\n\t}\n\tpublic long getTotal() {\n\t\treturn total;\n\t}\n\tpublic void setTotal(long total) {\n\t\tthis.total = total;\n\t}\n}\n\n```\n\n&nbsp;　　在写完分页的类之后，接下来写DAO返回给service的东西，文件名是SystemContext.java\n\n```\npackage org.common.model;\n\npublic class SystemContext {\t\t\t//传分页需要把当前页和每页显示多少条\n\tprivate static ThreadLocal<Integer> offset = new ThreadLocal<Integer>();\n\tprivate static ThreadLocal<Integer> size = new ThreadLocal<Integer>();\n\t\n\tpublic static Integer getOffset() {\n\t\treturn offset.get();\n\t}\n\tpublic static void setOffset(Integer _offset) {\n\t\toffset.set(_offset);\n\t}\n\tpublic static void removeOffset() {\n\t\toffset.remove();\n\t}\n\t\n\tpublic static Integer getSize() {\n\t\treturn size.get();\n\t}\n\tpublic static void setSize(Integer _size) {\n\t\tsize.set(_size);\n\t}\n\tpublic static void removeSize() {\n\t\tsize.remove();\n\t}\n\t\n}\n\n```\n\n&nbsp;\n\n　　**接下来实现IUserDao接口**，**写UserDao类，文件名是<strong>UserDao.java**</strong>\n\n**<strong>　　在实现的同时，还要继承<strong><strong>HibernateDaoSupport**</strong>，extends HibernateDaoSupport</strong></strong>\n\n**<strong>　　但是在spring 的HibernateDaoSupport中，setSessionFactory是使用final 修饰的，无法重写，沒有办法使用注解的方式注入sessionFactroy**</strong>\n\n**<strong>　　所以可以自己定义一个方法，这个方法去调用hibernateDaoSupport 中的setSessionFacotry方法，达到注入sessionFactory 的目的。**</strong>\n\n　　因此我定义如下的类：\n\n&nbsp;\n\n```\npackage org.common.dao;\n\nimport java.util.List;\n\nimport javax.annotation.Resource;\n\nimport org.common.model.Pager;\nimport org.common.model.SystemContext;\nimport org.common.model.User;\nimport org.hibernate.Query;\nimport org.hibernate.SessionFactory;\nimport org.springframework.orm.hibernate3.support.HibernateDaoSupport;\nimport org.springframework.stereotype.Repository;\n\n@Repository(\"userDao\")\t\t\t//申明一个DAO\npublic class UserDao extends HibernateDaoSupport implements IUserDao {\n\t\n\t//但是在spring 的HibernateDaoSupport中，setSessionFactory是使用final 修饰的，无法重写，沒有办法使用注解的方式注入sessionFactroy\n\t//所以可以自己定义一个方法，这个方法去调用hibernateDaoSupport 中的setSessionFacotry方法，达到注入sessionFactory 的目的。\n\t@Resource\n\tpublic void setSuperSessionFactory(SessionFactory sessionFactory) {\t\n\t\tthis.setSessionFactory(sessionFactory);\n\t}\n\t\n\t@Override\n\tpublic void add(User user) {\n\t\tthis.getHibernateTemplate().save(user);\n\t}\n\n\t@Override\n\tpublic void update(User user) {\n\t\tthis.getHibernateTemplate().update(user);\n\t}\n\n\t@Override\n\tpublic void delete(int id) {\n\t\tUser user = this.load(id);\n\t\tthis.getHibernateTemplate().delete(user);\n\t}\n\n\t@Override\n\tpublic User load(int id) {\n\t\treturn this.getHibernateTemplate().load(User.class, id);\n\t}\n\n\t@SuppressWarnings(\"unchecked\")\n\t@Override\n\tpublic List<User> list() {\n\t\treturn this.getSession().createQuery(\"from User\").list();\n\t}\n\n\t@SuppressWarnings(\"unchecked\")\n\t@Override\n\tpublic Pager<User> find() {\n\t\tint size = SystemContext.getSize();\n\t\tint offset = SystemContext.getOffset();\n\t\tQuery query = this.getSession().createQuery(\"from User\");\n\t\tquery.setFirstResult(offset).setMaxResults(size);\n\t\tList<User> datas = query.list();\n\t\tPager<User> us = new Pager<User>();\n\t\tus.setDatas(datas);\n\t\tus.setOffset(offset);\n\t\tus.setSize(size);\n\t\tlong total = (Long)this.getSession()\n\t\t\t\t\t.createQuery(\"select count(*) from User\")\n\t\t\t\t\t.uniqueResult();\n\t\tus.setTotal(total);\n\t\treturn us;\n\t}\n\n\t@Override\n\tpublic User loadByUsername(String username) {\n\t\treturn (User)this.getSession().createQuery(\"from User where username=?\")\n\t\t\t\t\t.setParameter(0, username).uniqueResult();\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["SpringMVC"]},{"title":"Spring MVC学习笔记——引入静态文件","url":"/Spring MVC学习笔记——引入静态文件.html","content":"**1.在user-servlet.xml中加入以下代码，才能使得对静态文件的请求不被Controller捕获，而映射到一个固定的地址**\n\n```\n\t<!-- 将静态文件指定到某个特殊的文件夹中统一处理 -->\n\t<mvc:resources location=\"/resources/\" mapping=\"/resources/**\"></mvc:resources>\n\n```\n\n<!--more-->\n&nbsp;\n\n**2.在WebContent文件下面，添加resources文件夹和css/main.css文件**\n\n<img src=\"/images/517519-20170101233034695-55639760.png\" alt=\"\" />\n\n**　　mian.css文件，文字的大小和颜色**\n\n```\n*{\n\tfont-size:14px;\n\tcolor:#f00;\n}\n\n```\n\n&nbsp;\n\n**3.在list.jsp文件中，加入css样式**\n\n```\n<%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\"\n    pageEncoding=\"UTF-8\"%>\n<%@taglib prefix=\"c\" uri=\"http://java.sun.com/jsp/jstl/core\" %>\t<!-- 加上标签库 -->\n<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<html>\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n<title>用户列表</title>\n<link rel=\"stylesheet\" href=\"<%=request.getContextPath()%>/resources/css/main.css\" type=\"text/css\">\n</head>\n<body>\n[Add](add)-->${loginUser.nickname }--${tttt }<br/>\n<c:forEach items=\"${users }\" var=\"um\">\n\t[${um.value.username }](${um.value.username })--------\n\t${um.value.nickname }-------\n\t${um.value.password }----${um.value.email }\n\t[更新](${um.value.username }/update)\n\t[删除](${um.value.username }/delete)\n\t<br/>\n</c:forEach>\n</body>\n</html>\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20170101233314898-853969074.png\" alt=\"\" />\n\n&nbsp;\n","tags":["SpringMVC"]},{"title":"Spring MVC学习笔记——登录和异常处理","url":"/Spring MVC学习笔记——登录和异常处理.html","content":"**1.在WEN-INF文件夹下面，添加一个login.jsp文件**\n\n```\n<%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\"\n    pageEncoding=\"UTF-8\"%>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<html>\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n<title>用户登录</title>\n</head>\n<body>\n<form action=\"user/login\" method=\"post\">\n\t用户名:<input type=\"text\" name=\"username\"/><br/>\n\t用户密码:<input type=\"password\" name=\"password\"/><br/>\n\t<input type=\"submit\" value=\"用户登录\"/>\n</form>\n</body>\n</html>\n\n```\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20170101232649132-1663674734.png\" alt=\"\" />\n\n**2.在UserController.java中加入login**\n\n```\n\t@RequestMapping(value=\"/login\",method=RequestMethod.POST)\n\tpublic String login(String username,String password,HttpSession session){\n\t\tif(!users.containsKey(username)){\n\t\t\tthrow new UserException(\"用户名不存在\");\n\t\t}\n\t\tUser u = users.get(username);\n\t\tif(!u.getPassword().equals(password)){\n\t\t\tthrow new UserException(\"用户密码不正确\");\n\t\t}\n\t\tsession.setAttribute(\"loginUser\", u);\n\t\treturn \"redirect:/user/users\";\n\t}\n\n```\n\n&nbsp;\n\n**3.其中需要new UserException，再创建UserException.java**\n\n**<img src=\"/images/517519-20170101230440414-2072664887.png\" alt=\"\" />**\n\n```\npackage org.common.model;\n\npublic class UserException extends RuntimeException {\n\n\t/**\n\t * \n\t */\n\tprivate static final long serialVersionUID = 1L;\n\n\tpublic UserException() {\n\t\tsuper();\n\t\t// TODO Auto-generated constructor stub\n\t}\n\n\tpublic UserException(String message, Throwable cause) {\n\t\tsuper(message, cause);\n\t\t// TODO Auto-generated constructor stub\n\t}\n\n\tpublic UserException(String message) {\n\t\tsuper(message);\n\t\t// TODO Auto-generated constructor stub\n\t}\n\n\tpublic UserException(Throwable cause) {\n\t\tsuper(cause);\n\t\t// TODO Auto-generated constructor stub\n\t}\n\n\t\n}\n\n```\n\n&nbsp;如果只是做到这些的话，当输出的用户名和密码错误的时候，报错如下图\n\n<img src=\"/images/517519-20170101231227351-790025835.png\" alt=\"\" width=\"428\" height=\"281\" />\n\n&nbsp;\n\n**4.在UserController.java中加入局部异常处理，并在jsp文件夹下面添加error.jsp**\n\n&nbsp;\n\n```\n\t//局部的异常处理，仅仅只能处理这个控制器中的异常\n\t@ExceptionHandler(value={UserException.class})\n\tpublic String handlerException(UserException e,HttpServletRequest req) {\n\t\treq.setAttribute(\"exception\",e);\n\t\treturn \"error\";\n\t}\n\n```\n\n&nbsp;\n\n&nbsp;\n\n```\n<%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\"\n    pageEncoding=\"UTF-8\"%>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<html>\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n<title>Insert title here</title>\n</head>\n<body>\n发现错误:\n<h1>${exception.message}</h1>\n</body>\n</html>\n\n```\n\n&nbsp;这时候如果输入用户名或者密码输入，如下图\n\n<img src=\"/images/517519-20170101231837773-894771663.png\" alt=\"\" width=\"338\" height=\"159\" />\n\n&nbsp;\n\n**另外一种异常处理方法，全局异常**\n\n需要把Controller中的局部异常注释掉\n\n然后在user-servlet.xml中加入\n\n```\n    <!-- 全局异常处理 -->\n    <bean id=\"exceptionResolver\" class=\"org.springframework.web.servlet.handler.SimpleMappingExceptionResolver\">\n\t\t<property name=\"exceptionMappings\">\n\t\t\t<props>\n\t\t\t\t<prop key=\"org.common.model.UserException\">error</prop>\n\t\t\t\t<prop key=\"java.lang.nullpointerexception\">exception</prop>\n\t\t\t</props>\n\t\t</property>\n\t</bean>\n\n```\n\n&nbsp;<img src=\"/images/517519-20170101232541226-1933575437.png\" alt=\"\" />\n","tags":["SpringMVC"]},{"title":"Hive任务如何计算生成的map和reduce任务","url":"/Hive任务如何计算生成的map和reduce任务.html","content":"在使用hive时候，需要关注hive任务所消耗的资源，否则可能会出现hive任务过于低效，或者把所查询的数据源拉胯的情况\n\n## 1.查看当前hive所使用的引擎和配置\n\n使用set语句可以查看当前hive的配置\n\n```\nset;\n\n```\n\n<img src=\"/images/517519-20221012111003968-1090236265.png\" width=\"600\" height=\"863\" loading=\"lazy\" />\n\n查看hive当前使用的engine\n\n```\nset hive.execution.engine;\n\n```\n\n<img src=\"/images/517519-20221012112815292-1326849658.png\" alt=\"\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n查看hive.input.format和mapreduce.input.fileinputformat.split.maxsize\n\n```\nhive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat\nmapreduce.input.fileinputformat.split.maxsize=256000000\n\n```\n\n## 2.explain当前的SQL语句\n\n```\nexplain INSERT OVERWRITE TABLE xx.xx PARTITION (pdate='2022-10-11')\nselect * from xx.xx where pdate  =\"2022-10-10\";\n\n```\n\n<img src=\"/images/517519-20221012134421032-208354309.png\" alt=\"\" loading=\"lazy\" />\n\n对于input format是HDFS或者s3的hive表，explain的时候可以看到在stage-1读取文件的时候，输入表的行数和数据大小\n\n## 3.调整map和reduce任务数量\n\n首先查看SQL运行时候的日志，确定map和reduce task的数量\n\n```\nINFO  : Hadoop job information for Stage-1: number of mappers: 1727; number of reducers: 0\n\n```\n\n### 1.如果input format是HDFS或者s3的hive表\n\n可以参考文档：[【Hive任务优化】&mdash;&mdash; Map、Reduce数量调整](https://blog.csdn.net/u013332124/article/details/97373278)\n\n&nbsp;\n\n### 2.如果hive查询的是其他的数据源\n\n比如使用了mongo serde来查询mongo，那么则需要去找到该serde里面和split相关的参数，比如mongo.input.split_size\n\n```\nhttps://github.com/mongodb/mongo-hadoop/blob/r2.0.2/core/src/main/java/com/mongodb/hadoop/util/MongoConfigUtil.java#L155\n\n```\n\n<img src=\"/images/517519-20221012142739981-322674714.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;\n\n　　\n","tags":["Hive"]},{"title":"Linux学习笔记——Shell部分","url":"/Linux学习笔记——Shell部分.html","content":"## 1.shell命令\n\nshell是连接linux内核和linux命令的模块\n\n　　命令名称　　/bin/sh　　/bin/csh　　/bin/ksh\n\n　　chsh -s　　输入新的shell，即修改shell\n\n可以使用env命令查看当前的环境变量，可以查看当前使用的是什么shell命令\n\n```\nenv | grep SHELL\nSHELL=/bin/zsh\n```\n\n## **2.使用vi编辑器**\n\n\n\n|键盘输入|功能\n| ---- | ---- \n|i|进入插入模式\n|esc|进入命令模式\n|w|将目前文件内容加以保存\n|q|退出，如果文件有修改会出现错误信息\n|q!|强制离开vi，并且不保存文件\n|wq|将修改过的内容保存，并离开vi\n|set nu|给文件中每一行添加行号\n|set nonu|取消行号\n|输入数字|直接输入数字再按esc，可将光标移至该行行首\n|/字符串|从头查找特定字符串\n|?字符串|从尾查找特定字符串<br /><br />\n\n## **3.管道命令：pipe |**\n\n1.把一个命令的输出作为另一个命令的输入\n\n```\nls -al /etc | less\n\n```\n\n## **4.选取命令：cut和grep**\n\n**cut命令**可以将一段消息的某段切出来。\n\n-d接分隔符，-f是取出第几段的意思，-c是以字符串为单位取出固定的字符串范围\n\n```\nls -F | grep '/' -v | cut -d'.' -f 1　　#在取出不是文件夹的文件后，通过分隔符&lsquo;.&rsquo;来分割，取出不包括文件后缀的文件名\nls -F | grep '/' -v | cut -c 1-　　#取出第一个字符（包括第一个）之后范围的字符，也可以指定范围1-5\n\n```\n\n　　\n\n**grep命令**分析一行消息，如果其中有所需要的信息，就将该行取出。\n\n```\nls -F | grep '/'　　#取出文件夹的名字\nls -F | grep '/' -v　　#反向，取出不是文件夹的文件\nls -l | grep ^d    #只显示文件夹\nls -l | grep ^-    #只显示文件\n\n```\n\n　　\n\n**fgrep命令**用于匹配文件内容\n\n```\nfgrep 'test' ./xxx.py\nis_contain=`fgrep 'test' ./xxx.py`\nif [ ! $is_contain ]; then\n    echo \"IS NULL\"\nelse\n    echo \"NOT NULL\"\nfi\n\n```\n\n　　\n\n## **5.排序命令：sort、wc和uniq**\n\n（a.txt保存的是ls -al的输出）\n\n<!--more-->\n&nbsp;使用**sort命令**进行排序\n\n-f：表示忽略大小写\n\n-b：忽略最前面的空格\n\n-M：表示以月份的名字来排序，JAN、DEC等\n\n-n：使用&ldquo;纯数字&rdquo;进行排序，10在9之后，而不会在1之后（默认是以文字类型来排序）\n\n```\ncat 下载/a.txt | sort -t' ' -k 6 -n\n\n```\n\n-r：表示反向排序\n\n-u：表示uniq，相同的数据中，仅出现一行表示，（如果用第k栏来排序，第k栏相同的只出现一个）\n\n-t：是分隔符，-k：是按某个字段来进行排序\n\n```\ncat 下载/a.txt | sort -t' ' -k 6    #-t表示分割，-k表示用第k栏来排序\n\n```\n\n&nbsp;\n\n&nbsp;使用**uniq命令**来将重读的数据仅显示一次\n\n```\ncat 下载/a.txt | cut -d' ' -f 1,2 | sort -t' ' -k 2 -n | uniq　　#取出权限和文件数，然后按文件数排序，再去重\n\n```\n\n&nbsp;\n\n&nbsp;使用**wc命令**来统计文件的字数、行数、字符数\n\n-l：仅显示多少行，-w：仅显示多少字（英文单词），-m：多少字符\n\n```\ncat 下载/a.txt | wc　　#分别显示&ldquo;行数、字数、字符数&rdquo;\n\n```\n\n&nbsp;去掉第一行的&ldquo;总用量&rdquo;，再统计行数\n\n```\ncommon@common-Aspire-4750:~$ cat 下载/a.txt | grep -v '总用量' | wc -l\n81\ncommon@common-Aspire-4750:~$ cat 下载/a.txt | wc -l\n82\n\n```\n\n&nbsp;\n\n## **6.双向重导向：tee**\n\n　　同时将数据流分送到文件和屏幕\n\n```\ncat 下载/a.txt | sort | tee 下载/a.txt　　#把txt的内容排序后，双向重导向，不能使用>来把一个文件的内容修改后，再重导向回本身，该操作会清空这个文件\n\n```\n\n&nbsp;\n\n## **7.字符转换命令：tr、col、join、paste、expand**\n\n**tr**可以用来删除一段消息中的文字，或者是进行文字消息的替换\n\n-d：删除一段消息中的文字，-s：替换重复的字符\n\n```\ncat a.txt | sed 's/[ ][ ]*/ /g'    #先把文件中连续的空格变成一个空格，方便使用字段来进行切分或者排序\n\n```\n\n```\ncat a.txt | awk '{$1=$1;print}'　　#去掉连续空格的其他方法\n\n```\n\n```\ncat a.txt | tr -s ' '　　　　　　　　#去掉连续空格的其他方法\n\n```\n\n&nbsp;\n\n```\ncat 下载/a.txt | tr '[a-z]' '[A-Z]'    #把输出的所有小写字母替换成大写字母\n\n```\n\n```\ncat 下载/a.txt | tr -d ' '    #删除所有的空格\n\n```\n\n&nbsp;\n\n**col**可以用来将【tab】按键替换成为空格键，在cat -A中，【tab】会以^I来表示，【换行符】会以^M\n\n```\ncat regTrees.py | col -x | cat -A    #将tab键转换成对等的空格键\n\n```\n\n&nbsp;\n\n**join**可以用来处理两个文件之间的数据\n\n-t：join默认以空格符号分割数据，并且比较&ldquo;第一个字段&rdquo;的数据，如果两个文件相同，则将两个数据联成一行，且第一个字段放在第一个。\n\n-i：忽略大小写的差异\n\n-1：表示第一个文件要用哪个字段来分析\n\n-2：表示第二个文件要用哪个字段来分析\n\n比如第一个文件a.txt是\n\n```\n1 2 3\n4 5 6\n\n```\n\n第二个文件b.txt是&nbsp;\n\n```\n1 4 5\n4 7 8\n\n```\n\n&nbsp;\n\n```\ncommon@common-Aspire-4750:~/下载$ join -t ' ' -1 1 a.txt -2 1 b.txt\n1 2 3 4 5\n4 5 6 7 8\n\n```\n\n&nbsp;\n\n**paste**直接将两行贴在一起，且中间以[tab]键隔开\n\n-d：后面可以接分隔符，默认以[tab]来分割\n\n-：如果file部分写成-，表示来自标准输入的数据\n\n```\ncommon@common-Aspire-4750:~/下载$ paste a.txt b.txt\n1 2 3\t1 4 5\n4 5 6\t4 7 8\n\n```\n\n```\ncommon@common-Aspire-4750:~/下载$ cat a.txt | paste a.txt b.txt - | head -n 3    #把要显示的文件和后面的输出贴在一起，且只取出前三行\n1 2 3&nbsp;&nbsp; &nbsp;1 4 5&nbsp;&nbsp; &nbsp;1 2 3<br />4 5 6&nbsp;&nbsp; &nbsp;4 7 8&nbsp;&nbsp; &nbsp;4 5 6\n\n```\n\n&nbsp;\n\n**expand**将【tab】按键转换成空格键\n\n-t：后面可以接数字，一般一个tab键可以用8个空格键来替换\n\n**unexpand**将空格键转换成【tab】\n\n```\nexpand -1 treeExplore.py    #-t表示一个[tab]按键表示多少个字符\n\n```\n\n&nbsp;\n\n## **8.拆分命令：split**\n\n**split命令**可以用来拆分文件\n\n-b：后接要拆分的文件大小，可加单位，例如b,k,m等\n\n```\nsplit -b 1k a_copy.txt a_split    #拆分之后在前缀后面加上aa、ab、ac\ncat a_split* >> a_back.txt    #把拆分后的文件合并\n\n```\n\n-l：按行数进行拆分\n\n```\nls -al | split -l 10 - lsxiazai    #输出的信息，每10行记录成一个文件，必须要&ldquo;-&rdquo;符号\n\n```\n\n　　\n\n## **9.参数代换：xargs**\n\n**<strong>xargs命令**</strong>可以读入stdin的数据，并且以空格符或换行符作为标示，将stdin的数据分隔成为参数，作为某个命令的参数\n\n-0：如果输入的stdin含有特殊字符，例如`，\\，空格键等字符，这个-0可以还原成一般字符。这个参数可以用于特殊状态\n\n-e：这时EOF的意思。后面可以接一个字符串，当xargs分析到这个字符串的时候，就会停止继续工作\n\n-p：在执行每个命令的参数的时候，都会询问用户是否执行，输入y或者n\n\n-n：后面接次数\n\n```\ncommon@common-Aspire-4750:~$ cut -d':' -f 1 < /etc/passwd | xargs\nroot daemon bin sys ... common\n\n```\n\n```\ncut -d':' -f 1 < /etc/passwd | xargs finger    #将输入的参数作为finger命令的输出，finger命令用于查找并显示用户信息\n\n```\n\n```\ncut -d':' -f 1 < /etc/passwd | xargs -p -n 5 finger    #每次仅查看5个账号，且询问用户\n\n```\n\n```\ncut -d':' -f 1 < /etc/passwd | xargs -p -e'games' finger    #当参数遇到games的时候，就包括后面的舍弃掉\n\n```\n\n&nbsp;\n\n## **<strong><strong>10.减号（-）的用途**</strong></strong>\n\n当用到一个命令的stdout作为这次的stdin的时候，stdin和stdout可以利用&ldquo;-&rdquo;来替代\n\n```\ntar -cvf - /home | tar -xvf -    #后面的-取用前一个命令的stdout\n\n```\n\n&nbsp;\n\n## **11.通配符与特殊符号**\n\n```\n*    #代表多个字母或者数字\n？    #代表一个字母或者数字\n#    #注释\n\\    #转义符号\n|    #分隔两个管道命令\n;    #连续命令的分隔\n~    #用户的home目录\n$    #将命令变成后台工作\n!    #逻辑&ldquo;非&rdquo;\n/    #路径分隔符号\n>、>>    #输出导向\n&lsquo;    #单引号，不具有变量置换的功能\n&ldquo;    #具有变量置换的功能\n&rsquo; &lsquo;    #两个&lsquo;中间为可以先执行的命令\n()    #中间为子shell的起始与结束\n[]    #中间为字符的组合\n{}    #中间为命令区块的组合\n\n```\n\n&nbsp;\n\n```\nctrl+C    #终止当前面命令\nctrl+D    #输入结束（EOF），例如邮件结束的时候\nctrl+M    #就是Enter键\nctrl+S    #暂停屏幕的输出\nctrl+Q    #恢复屏幕的输出\nctrl+U    #在提示符下，删除整行命令\nctrl+Z    #&rdquo;暂停&ldquo;当前的命令\n\n```\n\n&nbsp;\n\n## **12.基础正则表达式**\n\n**grep命令**\n\n**-a：**在二进制文件中，以文本文件的方式搜索数据\n\n**-c：**计算找到&rdquo;搜索字符串&ldquo;的次数\n\n**-i：**忽略大小写\n\n**-n：**输出行号\n\n**-v：**反向选择\n\n搜索特定字符和利用[]来搜索集合字符\n\n```\ngrep -n 'the' regular-express.txt    #搜索特定字符串，输出行号\ngrep -n -i 'the' regular-express.txt    #忽略大小写\ngrep -n 't[ae]st' regular-express.txt    #利用[]来搜索集合字符\ngrep -n '[^g]oo' regular-express.txt    #使用反向选择^来选择不是goo的oo单词\n\n```\n\n&nbsp;行首字符^与行尾字符$\n\n```\ngrep -n '^[A-Z]' regular-express.txt    #行首字符^，选择行首是[A-Z]的行\ngrep -n '\\.$' regular-express.txt    #行尾字符$，选择行尾是.的行\ngrep -n '^[^a-zA-Z]' regular-express.txt    #选取行首不是英文字符的行\n\n```\n\n&nbsp;\n\n```\ngrep -n '\\.$' a.txt    #把以.结尾的行输出，注意window的换行是^M$，Linux的换行是$\n\n```\n\n&nbsp;任意一个字符（.）与重复字符（*）\n\n```\ngrep -n 'g..d' regular-express.txt    #找到有g??d字符的行\ngrep -n 'o*' regular-express.txt    #拥有空字符或者一个o以上的字符的行\ngrep -n 'oo*' regular-express.txt    #拥有一个o的字符的行\ngrep -n 'ooo*' regular-express.txt    #拥有两个o的字符的行\ngrep -n '[0-9][0-9]*' regular-express.txt    #找到有任意数字的行\n\n```\n\n&nbsp;限定连续重复字符范围{}，使用的时候需要使用转义字符\\{\\}\n\n```\ngrep -n 'o\\{2\\}' regular-express.txt    #找到两个o的字符串的行\ngrep -n 'go\\{2,5\\}g' regular-express.txt    #找到g后面接2~5个o，然后再接一个g的字符串的行\n\n```\n\n&nbsp;\n\n## **13.格式化显示：printf**\n\n**\\n　　输出新的一行**\n\n**\\t　　水平的tab按键**\n\n**<strong>\\v　　垂直的tab按键**</strong>\n\n**<strong>\\xNN　　NN为两位数的数字，可以转换数字为字符**</strong>\n\n```\nprintf '%s\\t %s\\t %s\\t \\n' `cat a.txt`    #``中的命令先执行，然后按格式输出，用tab分割\nprintf '\\x45\\n'    #输出E\n\n```\n\n&nbsp;\n\n## **<strong>14.变量**</strong>\n\n```\necho $HOME    #输出环境变量\ncommon@common-Aspire-4750:~$ myname=123    #定义变量并输出\ncommon@common-Aspire-4750:~$ echo $myname \n123\n\nunset myname    #取消变量\n\nname=my\\ name    #空格等特殊字符用\\或者&ldquo; &rdquo;    \nname=\"my name\"\n\ncommon@common-Aspire-4750:~$ echo $name \nmy name\ncommon@common-Aspire-4750:~$ name=$name\\ 123    #累加变量\ncommon@common-Aspire-4750:~$ echo $name \nmy name 123\n\ncommon@common-Aspire-4750:~$ name=my\\ name    #通过export让子程序可以使用父程序的变量\ncommon@common-Aspire-4750:~$ echo $name\nmy name\ncommon@common-Aspire-4750:~$ bash\ncommon@common-Aspire-4750:~$ echo $name\n\ncommon@common-Aspire-4750:~$ exit\nexit\ncommon@common-Aspire-4750:~$ export name\ncommon@common-Aspire-4750:~$ bash\ncommon@common-Aspire-4750:~$ echo $name\nmy name&nbsp;\n```\n\n参考\n\n```\nhttps://www.huweihuang.com/linux-notes/shell/shell-var.html\n\n```\n\n&nbsp;\n\n## 15.如何判断一个变量为空\n\n参考：[在shell中如何判断一个变量是否为空](https://blog.csdn.net/varyall/article/details/79140753)\n\n&nbsp;\n\n## **16.变量的有效范围**\n\n变量能否被引用是与**export**有关的，被export后的变量，可以称之为&ldquo;环境变量&rdquo;，环境变量可以被子程序所引用，但是其他的自定义变量就不会存在于子程序中。\n\n在scripts2.sh会去引用scripts1.sh中的变量，那么scripts1.sh就应该设置export\n\n&nbsp;\n\n## **17.变量键盘读取、数组与申明：read、array、declare**\n\n**1、read**，用来读取来自键盘输入的变量\n\n-p：后面可以接提示符\n\n-t：后面可以接等待的秒数，不会一直等待用户\n\n```\ncommon@common-Aspire-4750:~$ read -p \"请输入name:\" -t 30 name    #提示符和等待\n请输入name:my name\ncommon@common-Aspire-4750:~$ echo $name\nmy name\n\n```\n\n&nbsp;\n\n**2、declare和typeset**，用来申明变量的属性\n\n-a：申明数组（array）\n\n-i：申明整数数字（integer）\n\n-x：和export一样，申明环境变量\n\n-r：将一个变量申明为只读，不可更改内容，也不能unset\n\n```\ncommon@common-Aspire-4750:~$ sum=100+300\ncommon@common-Aspire-4750:~$ echo $sum\n100+300\ncommon@common-Aspire-4750:~$ declare -i sum=100+300    #申明为整数才能求和\ncommon@common-Aspire-4750:~$ echo $sum\n400\n\n```\n\n&nbsp;\n\n**3.数组属性array**\n\n```\ncommon@common-Aspire-4750:~$ name[1]=123\ncommon@common-Aspire-4750:~$ name[2]=456\ncommon@common-Aspire-4750:~$ name[3]=789\ncommon@common-Aspire-4750:~$ echo ${name[1]}　　#使用$()来读取\n123\n\n```\n\n&nbsp;\n\n## **18. shell中的$，比如**$@和$#\n\n```\n$0 Shell本身的文件名\n$1～$n 添加到Shell的各参数值。$1是第1参数、$2是第2参数&hellip;\n$$ Shell本身的PID（ProcessID）\n$! Shell最后运行的后台Process的PID\n$? 最后运行的命令的结束代码（返回值）\n$- 使用Set命令设定的Flag一览\n$* 所有参数列表。如\"$*\"用「\"」括起来的情况、以\"$1 $2 &hellip; $n\"的形式输出所有数\n$@ 所有参数列表。如\"$@\"用「\"」括起来的情况、以\"$1\" \"$2\" &hellip; \"$n\" 的形式输出所有参数\n$# 添加到Shell的参数个数\n\n```\n\n$# 参数的数量\n\n$@ 参数的内容\n\n比如test.sh\n\n```\n#!/bin/sh\n\necho \"number:$#\"\necho \"argume:$@\"\n\n```\n\n运行 ./test a b，输出\n\n```\nnumber:2\nargume:a b\n\n```\n\n　　\n\n## **19. shell中的/dev/null，代表空设备文件**\n\n**参考：**[Linux Shell 1>/dev/null 2>&amp;1 含义](https://blog.csdn.net/ithomer/article/details/9288353)\n\n```\n> &nbsp;：代表重定向到哪里，例如：echo \"123\" > /home/123.txt\n1 &nbsp;：表示stdout标准输出，系统默认值是1，所以\">/dev/null\"等同于\"1>/dev/null\"\n2 &nbsp;：表示stderr标准错误\n&amp; &nbsp;：表示等同于的意思，2>&amp;1，表示2的输出重定向等同于1\n\n1 > /dev/null 2>&amp;1&nbsp;语句含义：\n1 > /dev/null ： 首先表示标准输出重定向到空设备文件，也就是不输出任何信息到终端，说白了就是不显示任何信息。\n2>&amp;1 ：接着，标准错误输出重定向（等同于）标准输出，因为之前标准输出已经重定向到了空设备文件，所以标准错误输出也重定向到空设备文件\n\n```\n\n&nbsp;\n\n## 20. 获得文本中的最短行和最长行\n\n```\n最短行：awk '(NR==1||length(min)>length()){min=$0}END{print min}' data.txt\n最长行：awk '{if (length(max)<length()) max=$0}END{print max}' data.txt\n\n```\n\n　　\n\n## 21.Shift命令\n\nShift命令运行一次，将会销毁一个输入参数，后面的参数前移，比如下面脚本\n\n```\n#!/bin/bash\n\nwhile [ $# != 0 ]\n    do\n    echo \"prama is $1,prama size is $#\"\n    shift\ndone\n\n```\n\n输入参数是 a b c，运行一次shift，参数 a b c将会减少一个\n\n```\n./shift_test.sh a b c\nprama is a,prama size is 3\nprama is b,prama size is 2\nprama is c,prama size is 1\n\n```\n\n　　\n\n&nbsp;\n","tags":["Linux"]},{"title":"Ubuntu16.04安装Ranger2.1.0","url":"/Ubuntu16.04安装Ranger2.1.0.html","content":"## 1.编译ranger项目\n\n```\ngit clone https://github.com/apache/ranger.git\ncd ranger\ngit checkout -b release-ranger-2.1.0 release-ranger-2.1.0\nmvn clean package -DskipTests -Drat.skip=true\n\n```\n\n需要注意的是，ranger2.1.0编译的时候，maven的版本需要大于3.6.0，否则会出现下面报错\n\n```\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:1.4.1:enforce (enforce-versions) on project ranger: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -> [Help 1]\n[ERROR]\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR]\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\n\n```\n\n编译成功\n\n<img src=\"/images/517519-20220917162229651-1793794421.png\" width=\"800\" height=\"858\" loading=\"lazy\" />\n\n编译后可以看出target目录下的文件如下\n\n<img src=\"/images/517519-20220917162311021-131024174.png\" width=\"800\" height=\"190\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n## 2.安装ranger\n\n创建linux ranger用户和组\n\n```\nsudo groupadd ranger\nsudo useradd ranger -g ranger -r --no-log-init -d /var/lib/ranger\nsudo mkdir /var/lib/ranger\nsudo mkdir /var/run/ranger\nsudo chown -R ranger:ranger /var/lib/ranger\nsudo chown -R ranger:ranger /var/run/ranger\n\n```\n\n解压ranger-2.1.0-admin.tar.gz\n\n```\ntar -zxvf ranger-2.1.0-admin.tar.gz -C ~/software\n\n```\n\n由于ranger依赖Mysql数据库，所以需要有一个mysql环境\n\n在准备好mysql之后，编译配置install.properties，添加如下内容\n\n```\ndb_root_user=root\ndb_root_password=xxxx\ndb_host=localhost\n\ndb_name=ranger\ndb_user=ranger\ndb_password=ranger\n\n#audit_store=solr\n\n```\n\n然后使用root用户运行，该脚本将会初始化mysql里面的用户和表，以及系统os上的ranger用户，命令等\n\n```\nroot@master:~/software/ranger-2.1.0-admin# ./setup.sh\n\n```\n\n启动ranger，需要使用ranger用户，否则会报 -bash: ./ews/ranger-admin-services.sh: 权限不够\n\n```\nsudo -iu ranger\nranger-admin start\n\n```\n\n停止\n\n```\nranger-admin stop\n\n```\n\n之后访问 6080 端口，默认的账号密码都是admin\n\n<img src=\"/images/517519-20220917183216965-926248565.png\" width=\"1000\" height=\"343\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["ranger"]},{"title":"机器学习——利用K-均值聚类算法对未标注数据分组","url":"/机器学习——利用K-均值聚类算法对未标注数据分组.html","content":"**聚类**是一种**无监督**的学习，它将相似的对象归到同一簇中。它有点像全自动分类。聚类方法几乎可以应用到所有对象，簇内的对象越相似，聚类的效果越好。\n\n**K-均值（K-means）聚类算法**，之所以称之为K-均值是因为它可以发现k个不同的簇，且每个簇的中心采用簇中所含值的均值计算而成。\n\n**簇识别（cluster identification）**给出簇类结果的含义。假定有一些数据，现在将相似数据归到一起，簇识别会告诉我们这些簇到底都是些什么。\n\n<!--more-->\n&nbsp;\n\n**K-均值聚类算法**\n\n优点：容易实现\n\n缺点：可能收敛到局部最小值，在大规模数据集上收敛较慢\n\n使用数据类型：数值型数据\n\n&nbsp;\n\n**K-均值**是发现给定数据集的k个簇的算法。簇个数k是用户给定的，每一个簇通过其质心（centroid），即簇中所有点的中心来描述。\n\n**K-均值算法**的工作流程是：\n\n1.随机确定k个初始点作为质心\n\n2.然后将数据集中的每个点分配到一个簇中，具体来讲，为每个点找到其最近的质心，并将其分配给该质心所对应的簇。\n\n3.完成之后，每个簇的质心更新为该簇所有点的平均值。\n\n&nbsp;\n\n<img src=\"/images/517519-20161226111404882-1657248190.png\" alt=\"\" />\n\n**&nbsp;原数据**\n\n**<img src=\"/images/517519-20161226232519992-198733522.png\" alt=\"\" width=\"450\" height=\"384\" />**\n\n```\nfrom numpy import *\n\ndef loadDataSet(fileName):      #general function to parse tab -delimited floats\n\tdataMat = []                #assume last column is target value\n\tfr = open(fileName)\n\tfor line in fr.readlines():\n\t\tcurLine = line.strip().split('\\t')\n\t\tfltLine = map(float,curLine) #map all elements to float()\n\t\tdataMat.append(fltLine)\n\treturn dataMat\n\ndef plotBestFit(file,clusterAssment):\t#画出数据集\n\timport matplotlib.pyplot as plt\n\tdataMat=loadDataSet(file)\t\t#数据矩阵和标签向量\n\tdataArr = array(dataMat)\t\t#转换成数组\n\tn = shape(dataArr)[0] \n\txcord1 = []; ycord1 = []\t\t#声明两个不同颜色的点的坐标\n\txcord2 = []; ycord2 = []\n\tfor i in range(n):\t\t\t\t#绘出两个簇的聚类图\n\t\tif (clusterAssment[i,0] == 0):\n\t\t\txcord1.append(dataArr[i,0]); ycord1.append(dataArr[i,1])\n\t\telif (clusterAssment[i,0] == 1):\n\t\t\txcord2.append(dataArr[i,0]); ycord2.append(dataArr[i,1])\n\tfig = plt.figure()\n\tax = fig.add_subplot(111)\n\tax.scatter(xcord1, ycord1, s=30, c='green', marker='s')\n\tax.scatter(xcord2, ycord2, s=30, c='red')\n\tplt.xlabel('X1'); plt.ylabel('X2');\n\tplt.show()\n\ndef distEclud(vecA, vecB):\t\t\t\t#计算两个向量的欧氏距离\n\treturn sqrt(sum(power(vecA - vecB, 2))) #la.norm(vecA-vecB)\n\ndef randCent(dataSet, k):\t\t\t\t#为给定数据集构建一个包含k个随机质心的集合\n\tn = shape(dataSet)[1]\n\tcentroids = mat(zeros((k,n)))\t\n\tfor j in range(n):\t\t\t\t\t#在n维向量中选出k个随机质心\n\t\tminJ = min(dataSet[:,j]) \n\t\trangeJ = float(max(dataSet[:,j]) - minJ)\t#分别求出X轴和Y轴的最大最小值之差\n\t\tcentroids[:,j] = mat(minJ + rangeJ * random.rand(k,1))\n\treturn centroids\t\t\t\t\t#返回k&times;n矩阵\t\t\n    \ndef kMeans(dataSet, k, distMeas=distEclud, createCent=randCent):\t#K-均值聚类算法\n\tm = shape(dataSet)[0]\t\t\t\t#取得数据集的数量\n\tclusterAssment = mat(zeros((m,2)))\t#簇分配结果矩阵，第一列记录簇索引值，第二列存储误差\n\tcentroids = createCent(dataSet, k)\t#生成初始质心\n\tclusterChanged = True\n\twhile clusterChanged:\n\t\tclusterChanged = False\t\t\t#停止条件\n\t\tfor i in range(m):\t\t\t\t#把每一个点分配到最近的簇中\n\t\t\tminDist = inf; minIndex = -1\n\t\t\tfor j in range(k):\t\t\t#比较每一个点到两个簇的距离，把所有的点分成两个簇\n\t\t\t\tdistJI = distMeas(centroids[j,:],dataSet[i,:])\n\t\t\t\tif distJI < minDist:\n\t\t\t\t\tminDist = distJI; minIndex = j\n\t\t\tif clusterAssment[i,0] != minIndex:\t\t#只要有一个点被分配到另一个簇中，就重新计算质心后再次分配\n\t\t\t\tclusterChanged = True\n\t\t\tclusterAssment[i,:] = minIndex,minDist**2\n\t\t#print centroids\n\t\tfor cent in range(k):\t\t\t#重新计算质心\n\t\t\tptsInClust = dataSet[nonzero(clusterAssment[:,0].A==cent)[0]]\t#取得给定簇的所有点\n\t\t\tcentroids[cent,:] = mean(ptsInClust, axis=0)\t\t\t\t\t#计算每个簇的均值，axis=0表示沿矩阵列方向进行均值计算\n\treturn centroids, clusterAssment\t#返回两个簇的质心、K-聚类结果以及误差\n\n```\n\n**&nbsp;k=2，k-均值算法**\n\n<img src=\"/images/517519-20161226233233742-1411091143.png\" alt=\"\" width=\"477\" height=\"403\" />\n\n**&nbsp;k=4，k-均值算法**\n\n**<img src=\"/images/517519-20161226233722179-1805899472.png\" alt=\"\" width=\"487\" height=\"411\" />**\n\n**聚类的过程，较大的点表示质心**\n\n<img src=\"/images/517519-20161227094904414-1422079854.png\" alt=\"\" width=\"347\" height=\"295\" /> <img src=\"/images/517519-20161227094933257-951044591.png\" alt=\"\" width=\"349\" height=\"296\" />&nbsp;<img src=\"/images/517519-20161227095004336-859340350.png\" alt=\"\" width=\"348\" height=\"295\" />\n\n<img src=\"/images/517519-20161227095035664-7465856.png\" alt=\"\" width=\"347\" height=\"294\" />\n\n&nbsp;\n\n**使用后处理来提高聚类性能**\n\nK-均值聚类中**簇的数目k**是一个用户预先定义的参数，用户需要知道怎么选取k值是正确的。\n\n在包含簇分配结果的矩阵中保存这每个点的误差，即**该点到簇质心的距离平方值**，可以利用这个误差来**评价聚类的质量**。\n\n一种用于度量聚类效果的指标是**SSE（Sum of Squared Error，误差平方和）**。SSE值**越小**表示数据点**越接近于它们的质心**，聚类效果也越好。因为对误差取了平方，因此桁架重视那些远离中心的点。\n\n聚类的目标是在**保持簇数不变**的情况下**提高簇的质量**。\n\n1.可以对生成的簇进行后处理，一种方式**将具有最大SSE值的簇划分成两个簇**。具体实现的时候，可以将最大簇包含的点过滤出来并在这些点上运行K-均值算法，其中的k设为2。\n\n2.为了保持簇总数不变，可以**将某两个簇进行合并**。合并的方法是合并最近的质心，或者合并两个使得SSE增幅最小的质心。量化的方法有两种：\n\n　　第一种思路是通过计算所有质心之间的距离，然后合并距离最近的两个点来实现。\n\n　　第二种方法需要合并两个簇然后计算总SSE值。\n\n必须在所有可能的两个簇上重复上述处理过程，知道找到合并最佳的两个簇为止。\n\n&nbsp;\n\n**二分K-均值算法**\n\n　　为克服K-均值算法收敛于局部最小值的问题，有人提出了另一个称为**二分K-均值（bisecting K-mean）**的算法。该算法**首先将所有点作为一个簇**，然后将该簇**一分为二**。之后选择其中一个簇继续进行划分，选择哪一个簇进行划分**取决于对其划分是否可以最大程度降低SSE的值**。上述基于SSE的划分过程不断重复，直达得到用户指定的簇数目为止。\n\n　　另一种做法是选择SSE最大的簇进行划分，知道簇数目达到用户指定的数目为止。\n\n```\ndef biKmeans(dataSet, k, distMeas=distEclud):\t\t#二分K-均值聚类算法\n\tm = shape(dataSet)[0]\n\tclusterAssment = mat(zeros((m,2)))\n\tcentroid0 = mean(dataSet, axis=0).tolist()[0]\t#计算每个簇的均值，axis=0表示沿矩阵列方向进行均值计算，并矩阵转换成列表\n\tcentList =[centroid0] \t\t\t\t\t\t\t#生成一个列表，存放初始质心（所有点的均值）\n\tfor j in range(m):\t\t\t\t\t\t\t\t#计算初始平方误差和\n\t\tclusterAssment[j,1] = distMeas(mat(centroid0), dataSet[j,:])**2\n\twhile (len(centList) < k):\t\t\t\t\t\t#当质心的数量还未达到用户设定的数量时\n\t\tlowestSSE = inf\n\t\tfor i in range(len(centList)):\t\t\t\t#循环所有的簇，选取其中SSE最大的簇继续进行划分\n\t\t\tptsInCurrCluster = dataSet[nonzero(clusterAssment[:,0].A==i)[0],:]\t#取得每一簇，初始只有一簇\n\t\t\tcentroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas)\t#对每一簇进行2划分，centroidMat是质心，splitClustAss是划分索引\n\t\t\tsseSplit = sum(splitClustAss[:,1])\t\t\t\t\t\t\t\t\t\t\t#求SSE最大的簇进行2划分后的SSE和\n\t\t\tsseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:,0].A!=i)[0],1])\t#求除SSE最大的簇之外的所有簇的SSE和\n\t\t\tprint \"sseSplit, and notSplit: \",sseSplit,sseNotSplit\n\t\t\tif (sseSplit + sseNotSplit) < lowestSSE:\t\t\t\t\t\t\t#把SSE最大的簇2划分之后的SSE总和与原来的SSE和进行比较\n\t\t\t\tbestCentToSplit = i\n\t\t\t\tbestNewCents = centroidMat\n\t\t\t\tbestClustAss = splitClustAss.copy()\n\t\t\t\tlowestSSE = sseSplit + sseNotSplit\n\t\tbestClustAss[nonzero(bestClustAss[:,0].A == 1)[0],0] = len(centList)\t#取出等于1的索引，令其等于新的簇,change 1 to 3,4, or whatever\n\t\tbestClustAss[nonzero(bestClustAss[:,0].A == 0)[0],0] = bestCentToSplit\t#取出等于0的索引，令其等于SSE最大的簇\n\t\tprint 'the bestCentToSplit is: ',bestCentToSplit\n\t\tprint 'the len of bestClustAss is: ', len(bestClustAss)\n\t\tprint clusterAssment.T\n\t\traw_input()\n\t\tcentList[bestCentToSplit] = bestNewCents[0,:].tolist()[0]\t\t\t#replace a centroid with two best centroids \n\t\tcentList.append(bestNewCents[1,:].tolist()[0])\n\t\tclusterAssment[nonzero(clusterAssment[:,0].A == bestCentToSplit)[0],:]= bestClustAss#reassign new clusters, and SSE\n\treturn mat(centList), clusterAssment\n\n```\n\n&nbsp;\n","tags":["ML"]},{"title":"机器学习——模型树","url":"/机器学习——模型树.html","content":"和回归树（在每个叶节点上使用各自的均值做预测）不同，**模型树算法**需要在每个叶节点上都构建出一个线性模型，这就是把叶节点设定为分段线性函数，这个所谓的**分段线性（piecewise linear）**是指模型由多个线性片段组成。\n\n<!--more-->\n&nbsp;\n\n```\n#####################模型树#####################\ndef linearSolve(dataSet):   \t#模型树的叶节点生成函数\n\tm,n = shape(dataSet)\n\tX = mat(ones((m,n))); Y = mat(ones((m,1)))\t\t#建立两个全部元素为1的(m,n)矩阵和(m,1)矩阵\n\tX[:,1:n] = dataSet[:,0:n-1]; Y = dataSet[:,-1]\t#X存放所有的特征，Y存放   \n\txTx = X.T*X\n\tif linalg.det(xTx) == 0.0:\n\t\traise NameError('This matrix is singular, cannot do inverse,\\n\\\n\t\ttry increasing the second value of ops')\n\tws = xTx.I * (X.T * Y)\t\t\t\t\t\t\t#求线性回归的回归系数\n\treturn ws,X,Y\n\ndef modelLeaf(dataSet):\t\t\t#建立模型树叶节点函数\n    ws,X,Y = linearSolve(dataSet)\n    return ws\n\ndef modelErr(dataSet):\t\t\t#模型树平方误差计算函数\n    ws,X,Y = linearSolve(dataSet)\n    yHat = X * ws\n    return sum(power(Y - yHat,2))\n\n```\n\nmain.py\n\n```\n# coding:utf-8\n# !/usr/bin/env python\n\nimport regTrees\nimport matplotlib.pyplot as plt\nfrom numpy import *\n\nif __name__ == '__main__':\n\tmyDat = regTrees.loadDataSet('exp2.txt')\n\tmyMat = mat(myDat)\n\tmyTree = regTrees.createTree(myMat,regTrees.modelLeaf,regTrees.modelErr,(1,10))\n\tprint myTree\n\tregTrees.plotBestFit('exp2.txt')\n\n```\n\n&nbsp;<img src=\"/images/517519-20161225212600636-1198793340.png\" alt=\"\" />\n\n得到两段函数，以0.28为分界\n\n分别为y=3.46877+1.1852x和y=0.001698+11.96477x\n\n而生成该数据的真实模型是y=3.5+1.0x和y=0+12x再加上高斯噪声生成\n\n<img src=\"/images/517519-20161225212841011-2109309933.png\" alt=\"\" width=\"489\" height=\"413\" />\n","tags":["ML"]},{"title":"机器学习——回归树","url":"/机器学习——回归树.html","content":"**　　线性回归**创建模型需要**拟合所有的样本点**（局部加权线性回归除外）。当数据拥有众多特征并且特征之间关系十分复杂的时候，构建全局模型的想法就显得太难了，也略显笨拙。而且，实际生活中很多问题都是**非线性**的，不可能使用全局限性模型来拟合任何数据。\n\n　　一种可行的方法是将数据集切分成很多份易建模的数据，然后再利用线性回归技术来建模。如果首次切分之后仍然难以拟合线性模型就继续切分。\n\n　　决策树是一种**贪心算法**，它要在给定时间内做出最佳选择，但是**并不关心能否达到全局最优**。\n\n<!--more-->\n&nbsp;\n\n**CART（classification and regression trees，分类回归树）**\n\n之前使用过的**分类树**构建算法是**ID3**，**ID3决策树**学习算法是以**信息增益**为准则来选择划分属性。**ID3的做法**是每次选取当前**最佳的特征**来分割数据，并按照该特征的所有可能取值来切分。也就是说，如果一个特征有4种取值，那么数据将被切成4份。一旦按某特征切分后，该特征在之后的算法执行过程中将不会再起作用，所以所以有观点认为这种切分方式过于迅速。另外一种方法是**二元切分法**，即每次把数据集切成两份。如果数据的某特征值等于切分所要求的值，那么这些数据就进入树的左子树，反之则进入树的右子树。\n\n　　**ID3算法**还存在另一个问题，它**不能直接处理连续性数据**。只有事先将连续特征转换成离散型，才能在ID3算法中使用。\n\n　　**CART算法**使用**二元切分**来处理连续型变量。对CART稍作修改就可以处理回归问题。**CART决策树**使用&ldquo;**基尼指数**&rdquo;来选择划分属性，基尼值是用来度量**数据集的纯度**。\n\n&nbsp;\n\n<img src=\"/images/517519-20161224214938839-606433143.png\" alt=\"\" />\n\n&nbsp;\n\n```\nfrom numpy import *\n\ndef loadDataSet(fileName):      #general function to parse tab -delimited floats\n\tdataMat = []                #assume last column is target value\n\tfr = open(fileName)\n\tfor line in fr.readlines():\n\t\tcurLine = line.strip().split('\\t')\n\t\tfltLine = map(float,curLine) #map all elements to float()\n\t\tdataMat.append(fltLine)\n\treturn dataMat\n\t\ndef plotBestFit(file):\t\t\t\t#画出数据集\n\timport matplotlib.pyplot as plt\n\tdataMat=loadDataSet(file)\t\t#数据矩阵和标签向量\n\tdataArr = array(dataMat)\t\t#转换成数组\n\tn = shape(dataArr)[0] \n\txcord1 = []; ycord1 = []\t\t#声明两个不同颜色的点的坐标\n\t#xcord2 = []; ycord2 = []\n\tfor i in range(n):\n\t\txcord1.append(dataArr[i,0]); ycord1.append(dataArr[i,1])\n\tfig = plt.figure()\n\tax = fig.add_subplot(111)\n\tax.scatter(xcord1, ycord1, s=30, c='green', marker='s')\n\t#ax.scatter(xcord2, ycord2, s=30, c='green')\n\tplt.xlabel('X1'); plt.ylabel('X2');\n\tplt.show()\n\ndef binSplitDataSet(dataSet, feature, value):\t#该函数通过数组过滤方式将数据集合切分得到两个子集并返回\n\tmat0 = dataSet[nonzero(dataSet[:,feature] > value)[0],:][0]\n\tmat1 = dataSet[nonzero(dataSet[:,feature] <= value)[0],:][0]\n\treturn mat0,mat1\n\ndef regLeaf(dataSet):\t\t\t#建立叶节点函数，value为所有y的均值\n\treturn mean(dataSet[:,-1])\n\ndef regErr(dataSet):\t\t\t#平方误差计算函数\n\treturn var(dataSet[:,-1]) * shape(dataSet)[0]\t#y的方差&times;y的数量=平方误差\n\ndef chooseBestSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1,4)):\t#最佳二元切分方式\n\ttolS = ops[0]; tolN = ops[1]\t\t#tolS是容许的误差下降值，tolN是切分的最少样本数\n\t#如果剩余特征值的数量等于1，不需要再切分直接返回，（退出条件1）\n\tif len(set(dataSet[:,-1].T.tolist()[0])) == 1:\t\t\n\t\treturn None, leafType(dataSet)\n\tm,n = shape(dataSet)\n\t#the choice of the best feature is driven by Reduction in RSS error from mean\n\tS = errType(dataSet)\t\t#计算平方误差\n\tbestS = inf; bestIndex = 0; bestValue = 0\n\tfor featIndex in range(n-1):\n\t\t#循环整个集合\n\t\tfor splitVal in set(dataSet[:,featIndex]):\t#每次返回的集合中，元素的顺序都将不一样\n\t\t\tmat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal)\t\t#将数据集合切分得到两个子集\n\t\t\t#如果划分的集合的大小小于切分的最少样本数，重新划分\n\t\t\tif (shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN): continue\n\t\t\tnewS = errType(mat0) + errType(mat1)\t#计算两个集合的平方误差和\n\t\t\t#平方误差和newS小于bestS，进行更新\n\t\t\tif newS < bestS: \n\t\t\t\tbestIndex = featIndex\n\t\t\t\tbestValue = splitVal\n\t\t\t\tbestS = newS\n\t#在循环了整个集合后，如果误差减少量(S - bestS)小于容许的误差下降值，则退出，（退出条件2）\n\tif (S - bestS) < tolS: \n\t\treturn None, leafType(dataSet)\n\tmat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue)\t#按照保存的最佳分割来划分集合\n\t#如果切分出的数据集小于切分的最少样本数，则退出，（退出条件3）\n\tif (shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN):\n\t\treturn None, leafType(dataSet)\n\t#返回最佳二元切割的bestIndex和bestValue\n\treturn bestIndex,bestValue\n\ndef createTree(dataSet, leafType=regLeaf, errType=regErr, ops=(1,4)):#assume dataSet is NumPy Mat so we can array filtering\n\tfeat, val = chooseBestSplit(dataSet, leafType, errType, ops)\t#采用最佳分割，将数据集分成两个部分\n\tif feat == None: return val \t#递归结束条件\n\tretTree = {}\t\t\t\t\t#建立返回的字典\n\tretTree['spInd'] = feat\n\tretTree['spVal'] = val\n\tlSet, rSet = binSplitDataSet(dataSet, feat, val)\t#得到左子树集合和右子树集合\n\tretTree['left'] = createTree(lSet, leafType, errType, ops)\t\t#递归左子树\n\tretTree['right'] = createTree(rSet, leafType, errType, ops)\t\t#递归右子树\n\treturn retTree\n\n```\n\n&nbsp;\n\nmian.py\n\n```\n# coding:utf-8\n# !/usr/bin/env python\n\nimport regTrees\nimport matplotlib.pyplot as plt\nfrom numpy import *\n\nif __name__ == '__main__':\n\tmyDat = regTrees.loadDataSet('ex00.txt')\n\tmyMat = mat(myDat)\n\tprint myMat.T\n\tTree = regTrees.createTree(myMat)\n\tprint Tree\n\tregTrees.plotBestFit('ex00.txt')\n\n```\n\n&nbsp;\n\n**结果只是切分成两个子树**\n\n<img src=\"/images/517519-20161225150146432-17780233.png\" alt=\"\" width=\"779\" height=\"256\" />\n\n**再查看原来的数据集的分布**\n\n**<img src=\"/images/517519-20161225150301792-652767793.png\" alt=\"\" width=\"529\" height=\"449\" />**\n\n**如果换一个数据集的话**\n\n<img src=\"/images/517519-20161225150509589-566600557.png\" alt=\"\" width=\"1024\" height=\"280\" />\n\n**则子树的数量变多，再查看原来数据集的分布**\n\n**<img src=\"/images/517519-20161225150558292-1983897010.png\" alt=\"\" width=\"526\" height=\"446\" />**\n\n&nbsp;\n\n&nbsp;\n\n一棵树如果**节点过多**，表示该模型可能对数据进行了&ldquo;**过拟合**&rdquo;。通过**降低决策树的复杂度来避免过拟合**的过程称为**剪枝（pruning）**。\n\n剪枝分为**预剪枝（prepruning）和后剪枝（postpruning）**。\n\n**预剪枝**是指在决策树生成过程中，对每个节点在划分前先进行估计，若当前节点的划分**不能带来决策树泛化性能的提升**，则**停止划分**并将当前节点记为叶节点（上面的程序已经使用了预剪枝）；\n\n**后剪枝**则是先在训练集生成一棵完整的决策树，然后自底向上地对非叶节点进行考察，若**将该节点对应的子树替换为叶节点能带来决策树泛化性能提升**，则**将该子树替换为叶节点**。\n\n使用**后剪枝**方法需要**将数据集分成测试集和训练集**。首先指定参数，使得构建出的树足够大、足够复杂，便于剪枝。接下来从上而上找到叶节点，**用测试集来判断将这些叶节点合并是够能降低测试误差**。如果是的话就进行合并。\n\n&nbsp;\n\n```\n#####################回归树剪枝函数#####################\ndef isTree(obj):\t\t\t\t#该函数用于判断当前处理的是否是叶节点\n\treturn (type(obj).__name__=='dict')\n\ndef getMean(tree):\t\t\t\t#从上往下遍历树，寻找叶节点，并进行塌陷处理（用两个孩子节点的平均值代替父节点的值）\n\tif isTree(tree['right']): tree['right'] = getMean(tree['right'])\n\tif isTree(tree['left']): tree['left'] = getMean(tree['left'])\n\treturn (tree['left']+tree['right'])/2.0\n    \ndef prune(tree, testData):\t\t\t#后剪枝，tree是待剪枝的树，testData是用于剪枝的测试参数\n\t#如果测试集为空，进行塌陷处理，最后将会剩下两个叶节点\n\tif shape(testData)[0] == 0:\n\t\treturn getMean(tree)\n\t#如果测试集非空，按照保存的回归树对测试集进行切分\n\tif (isTree(tree['right']) or isTree(tree['left'])):\n\t\tlSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal'])\n\tif isTree(tree['left']):\n\t\ttree['left'] = prune(tree['left'], lSet)\n\tif isTree(tree['right']):\n\t\ttree['right'] =  prune(tree['right'], rSet)\n    #左右子树都是叶子节点，对合并前后的误差进行比较，如果合并后的误差比不合并的误差小就进行合并操作，否则直接返回\n\tif not isTree(tree['left']) and not isTree(tree['right']):\n\t\tlSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal'])\n\t\terrorNoMerge = sum(power(lSet[:,-1] - tree['left'],2)) + sum(power(rSet[:,-1] - tree['right'],2))\t#不合并的误差\n\t\ttreeMean = (tree['left']+tree['right'])/2.0\n\t\terrorMerge = sum(power(testData[:,-1] - treeMean,2))\t#合并的误差\n\t\tif errorMerge < errorNoMerge:\t\t\t\t\t\t\t#比较\n\t\t\tprint \"merging\"\n\t\t\treturn treeMean\n\t\telse: return tree\n\telse: return tree\n\n```\n\n&nbsp;大量的节点已经被剪枝掉了，但是并没有像预期的那样剪枝成两个部分，说明后剪枝可能不如预剪枝有效。一般地，为了寻求最佳模型可以同时使用两种剪枝技术。\n\n<img src=\"/images/517519-20161225201309057-1608153088.png\" alt=\"\" />\n","tags":["ML"]},{"title":"系统设计——登录系统","url":"/系统设计——登录系统.html","content":"## 1.认证方式\n\n系统的常用的认证方式如下：账号密码（Basic Auth），OAuth2.0，SAML，OIDC，LDAP等\n\n**1.OAuth2.0 **是一种授权协议，旨在允许应用程序安全访问资源，而**不是**用来验证用户的身份。\n\n它通常用于授权流程，以获得对受保护资源的访问权限。不兼容oauth1.0.允许第三方应用代表用户获得访问权限。\n\n可以作为web应用、桌面应用和手机等设备提供专门的认证流程。例如，用qq账号登录豆瓣、美团、大众点评；用支付宝账号登录淘宝、天猫等。参考：[各开放平台账号登录API对接文档](https://www.cnblogs.com/tonglin0325/p/4663297.html)\n\n**2.SAML**的全称是Security Assertion Markup Language， 是由OASIS制定的一套基于XML格式的开放标准，用在身份提供者（IdP）和服务提供者 (SP)之间交换身份验证和授权数据。\n\nSAML的一个非常重要的应用就是基于Web的单点登录（SSO）。\n\n**3.OpenID Connect (OIDC)** 是在 OAuth 2.0 的基础上构建的身份验证协议。\n\nOIDC 添加了身份验证层，允许客户端验证用户的身份，并获取有关用户的附加信息。\n\n还有其他的一些认证方式我们可以在Postman的authorization中查看\n\n<img src=\"/images/517519-20240418232329303-644567982.png\" width=\"200\" height=\"520\" />\n\n## 2.验证\n\n### 1.人机校验\n\n常用的人机校验有cloudflare，reCAPTCHA等\n\n<img src=\"/images/517519-20240427141202981-678845482.png\" width=\"400\" height=\"98\" loading=\"lazy\" />\n\n<img src=\"/images/517519-20240427141110006-1423470608.png\" width=\"400\" height=\"110\" loading=\"lazy\" />\n\n<img src=\"/images/517519-20240427141308876-360819219.png\" width=\"300\" height=\"303\" loading=\"lazy\" />\n\n### 2.两步验证\n\n二步验证、双重验证，简称2FA（Two-factor authentication）\n\n<img src=\"/images/517519-20240427143321559-1498202664.png\" width=\"300\" height=\"212\" loading=\"lazy\" />\n\n## 3.Token\n\n### Token的类型\n\n常用的Token种类有很多种，比如JWT（JSON web token），**Bearer Token**，\n\n**1.JWT**\n\n一个通常你看到的jwt，由以下三部分组成，它们分别是：\n\n1. <!--more-->\n&nbsp;header：主要声明了JWT的签名算法；\n1. &nbsp;payload：主要承载了各种声明并传递明文数据；\n1. &nbsp;signture：拥有该部分的JWT被称为JWS，也就是签了名的JWS；没有该部分的JWT被称为nonsecure JWT 也就是不安全的JWT，此时header中声明的签名算法为none。\n\n三个部分用&middot;分割。形如 xxxxx.yyyyy.zzzzz的样式。\n\n我们可以使用 [https://jwt.io/](https://jwt.io/) 网站来验证jwt格式是否正确，如下\n\n<img src=\"/images/517519-20240419000559165-760849834.png\" width=\"600\" height=\"340\" loading=\"lazy\" />\n\n2.**Bearer Token**\n\n&nbsp;\n\n### Token的无感续期\n\n由于安全的考虑，我们通常会要求tokenyou一个比较短的有效期，从而避免token泄露后会长时间造成风险，但是又不能要求用户在使用了很短的时间后，就重新进行认证，所以就需要token的自动续期\n\n参考：[token 过期后，如何自动续期？](https://cloud.tencent.com/developer/article/2001607)\n","tags":["系统设计"]},{"title":"特征预处理——特征缩放","url":"/特征预处理——特征缩放.html","content":"**特征缩放****（Feature Scaling）**是一种将数据的不同变量或特征的方位进行标准化的方法。\n\n在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲（数量级）的纯数值，便于不同单位或量级的指标能够进行比较和加权。\n\n## 特征缩放的好处：\n\n参考：[标准化的好处及常见处理方法](https://zhuanlan.zhihu.com/p/88348005)\n\n1. 提升模型的收敛速度\n\n2.提升模型的精度\n\n3.深度学习中数据归一化可以防止模型梯度爆炸。\n\n## 需要特征缩放的模型：\n\n参考：[哪些机器学习模型需要归一化](https://blog.csdn.net/weixin_43469047/article/details/116605053)\n\n概率模型（树形模型）不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、RF（随机森林）、朴素贝叶斯、XGBoost、lightGBM、GBDT\n\n而像Adaboost、SVM（支持向量机）、LR（线性回归、逻辑回归）、KNN、KMeans、神经网络（DNN、CNN和RNN）、LSTM之类的最优化问题就需要归一化\n\n## 特征缩放的方法：\n\n常用的特征缩放的方法有归一化、标准化、正态化等。参考：[2(1).数据预处理方法](https://www.cnblogs.com/nxf-rabbit75/p/11141944.html)\n\n选择建议：\n\n参考：[标准化和归一化什么区别？](https://www.zhihu.com/question/20467170) 和 [机器学习 | 数据缩放与转换方法（1）](https://xie.infoq.cn/article/1a72c74d073a807c4a738284f)\n\n1.特征是正态分布的，使用**z-score标准化**\n\n2.特征不是正态分布的，可以尝试使用**正态化**（**幂变换**）\n\n3.特征是正态分布的，如果**有离群值**，可以使用RobustScaler；没有离群值且是稀疏数据，可以使用归一化（如果数据标准差很小，min-max归一化会比z-score标准化好）\n\n4.先划分训练集和测试集，然后再**使用相同的标准化公式**对训练集和测试集进行特征缩放\n\n参考：[数据的标准化](http://webdataanalysis.net/data-analysis-method/data-normalization/) 和 [数据预处理（一）：标准化，中心化，正态化](https://www.cnblogs.com/liuxiangyan/p/14284417.html)\n\n### 1.归一化（s`caler`）\n\n#### 1.min-max归一化<br />\n\n也叫离差标准化，是对原始数据的**线性变换**，将数据统一映射到**[0,1]区间**上，转换函数如下：\n\n<img src=\"/images/517519-20221028221843157-1074129357.png\" width=\"150\" height=\"44\" loading=\"lazy\" />\n\n其中max为样本数据的最大值，min为样本数据的最小值。这种方法有一个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。\n\n**min-max归一化的特点：**\n\n- 缩放到0和1之间\n- 目的是使各个特征维度对目标函数的影响权重是一致的\n- **不改变其数据分布**的一种线性特征变换\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n**min-max归一化的适用场景：**\n\n- 如果对输出结果范围有要求，用归一化\n- 如果数据较为稳定，不存在极端的最大最小值，用归一化\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\nfrom sklearn.preprocessing import MinMaxScaler\n\ntransfer = MinMaxScaler(feature_range=[0, 1])  # 范围可改变，若不写，默认为0-1\ndata_minmax = transfer.fit_transform(train_data[[\"SalePrice\"]])\nprint(data_minmax)\n\n[[0.24107763]\n [0.20358284]\n [0.26190807]\n ...\n [0.321622  ]\n [0.14890293]\n [0.15636717]]\n\n```\n\n图片转自：https://www.zhihu.com/people/sun_shuai_\n\n<img src=\"/images/517519-20221112205223034-1498039690.png\" width=\"300\" height=\"217\" loading=\"lazy\" /><img src=\"/images/517519-20221112205329933-1901282466.png\" width=\"300\" height=\"219\" loading=\"lazy\" />\n\n#### 2.MaxAbs归一化\n\n最大值绝对值标准化(MaxAbs)即根据最大值的绝对值进行标准化，假设原转换的数据为x，新数据为x'，那么x'=x/|max|，其中max为x所在列的最大值\n\n**MaxAbs归一化的特点：**\n\n- MaxAbs方法跟Max-Min用法类似，也是将数据落入一定区间，但该方法的数据区间为[-1,1]\n- **不改变其数据分布**的一种**线性特征变换**\n\n**MaxAbs归一化的适用场景：**\n\n- MaxAbs也具有不破坏原有数据分布结构的特点，因此也可以用于稀疏数据、稀疏的CSR或CSC矩阵。\n\n```\nfrom sklearn.preprocessing import MaxAbsScaler\n\ntransfer = MaxAbsScaler() \ndata_minmax = transfer.fit_transform(train_data[[\"SalePrice\"]])\nprint(data_minmax)\n\n```\n\n图片转自：https://www.zhihu.com/people/sun_shuai_\n\n<img src=\"/images/517519-20221112205223034-1498039690.png\" width=\"300\" height=\"217\" /><img src=\"/images/517519-20221112205412817-489439559.png\" width=\"300\" height=\"221\" loading=\"lazy\" />\n\n### **2.<strong>标准化（s<strong>tandardization**）</strong></strong>\n\n#### **1.<strong><strong><strong>z-score**</strong>标准化</strong></strong>\n\n也叫**标准差标准化**，经过处理的数据符合**均值为0，标准差为1**，其转化函数为：\n\n<img src=\"/images/517519-20221028222140329-1037263907.png\" width=\"120\" height=\"50\" />\n\n其中&mu;为所有样本数据的均值，&sigma;为所有样本数据的标准差。\n\n**z-score标准化的特点：**\n\n- **假设数据是正态分布**\n- 将数值范围缩放到0附近，数据变成**均值为0，标准差为1**的正态分布\n- **不改变原始数据的分布**\n\n**<strong><strong>z-score标准化**的适用场景：</strong></strong>\n\n- 这种标准化方法适合大多数类型的数据，也是很多工具的默认标准化方法。如果对数据无从下手可以直接使用标准化；\n- 如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响\n- 需要使用距离来度量相似性的时候：比如k近邻、kmeans聚类、感知机和SVM，或者使用PCA降维的时候，标准化表现更好\n- Z-Score方法是一种中心化方法，会改变稀疏数据的结构，不适合用于对稀疏数据做处理。（稀疏数据是指绝大部分的数据都是0，仅有少部分数据为1）。在很多时候，数据集会存在稀疏性特征，表现为**标准差小**。并有很多元素的值为0.最常见的稀疏数据集是用来做协同过滤的数据集，绝大部分的数据都是0，仅有少部分数据为1。**对稀疏数据做标准化，不能采用中心化的方式**，否则会破坏稀疏数据的结构。参考：[Python数据标准化](https://blog.csdn.net/weixin_60200880/article/details/127214706)\n\n可以使用sklearn的StandardScaler函数对特征进行z-score标准化，注意fit_transform函数的输入需要时2D array\n\n```\nfrom sklearn.preprocessing import StandardScaler\n\n# z-score标准化\ntransfer = StandardScaler()\ndata_standard=transfer.fit_transform(train_data[[\"SalePrice\"]])\nprint(data_standard)\n\n[[ 0.34727322]\n [ 0.00728832]\n [ 0.53615372]\n ...\n [ 1.07761115]\n [-0.48852299]\n [-0.42084081]]\n\n# 描述性统计\nprint(pd.DataFrame(data_standard).describe())\n\ncount  1.460000e+03\nmean   1.362685e-16\nstd    1.000343e+00\nmin   -1.838704e+00\n25%   -6.415162e-01\n50%   -2.256643e-01\n75%    4.165294e-01\nmax    7.228819e+00\n\n```\n\n可以看到使用z-score标准化后的数据，均值接近0，标准差接近1\n\n<img src=\"/images/517519-20221110003259894-1848731269.png\" width=\"300\" height=\"243\" loading=\"lazy\" />\n\n#### 2.**RobustScaler**\n\n**<strong><strong>RobustScaler**的适用场景：</strong></strong>\n\n- 某种情况下，假如数据集中有离群点，我们可以使用Z-Score进行标准化，但是标准化之后的数据并不理想，因为异常点的特征往往在标准化之后便容易失去离群特征。此时可以使用RobustScaler针对离群点做标准化处理，该方法对数据中心化和数据的缩放健壮性有更强的参数控制能力。\n- 如果要最大限度保留数据集中的异常，使用RobustScaler方法。\n\n如果数据集包含较多的异常值，可以使用**RobustScaler方法**进行处理，它可以对数据集的中心和范围进行更具有鲁棒性的评估，如下\n\n<img src=\"/images/517519-20221110003843781-1680125069.png\" width=\"300\" height=\"265\" loading=\"lazy\" />\n\n```\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import RobustScaler\n\n# Robust\ntransfer = RobustScaler()\ndata_robust = transfer.fit_transform(train_data[[\"SalePrice\"]])\nsns.distplot(pd.DataFrame(data_robust), fit=norm)\nprint(pd.DataFrame(data_robust).describe())\nplt.show()\n\n```\n\n### **3.幂变换（Power Transform）**\n\n常用的幂变换包括：对数变换，box-cox变换，指数变换等，属于**非线性变换**\n\nPowerTransformer 目前提供两个这样的幂变换,<!--more-->\n&nbsp;Yeo-Johnson transform 和 the Box-Cox transform。其中&nbsp;Box-Cox 仅能应用于严格的正数，Yeo-Johnson 可以是正数也可以是负数<br />\n\n#### **<strong>1.Yeo-Johnson标准化**：</strong>\n\n其转化函数为：\n\n<img src=\"/images/517519-20221112212822868-1449265647.png\" width=\"300\" height=\"91\" />\n\n```\ndata_yeojohnson = preprocessing.PowerTransformer(method='yeo-johnson', standardize=False)\n\n```\n\n#### **<strong>2.box-cox标准化**：</strong>\n\nBox-Cox转换法是幂变换中最受欢迎的方法，用于连续的响应变量不满足正态分布的情况，其通过对原始分布改变其lambda (&lambda;)，将非正态数据转换为正态数据，lambda (&lambda;)是我们自己决定的参数，用于得到最适合的转换效果。\n\n#### **<img src=\"/images/517519-20221107225530763-1624210928.png\" width=\"200\" height=\"73\" />**\n\n在进行Box-Cox转换之前，原始数据Y需要全部为**正数**。如果不是正数，那么添加一个常数C，再进行转换 <msup><mi>Y</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><mi>Y</mi><mo>+</mo><mi>C</mi><msup><mo stretchy=&quot;false&quot;>)</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03BB;</mi></mrow></msup></math>\"> 。\n\n```\n# box-cox, 返回2个值，第二个值是最佳_lambda\ndata_boxcox, _lambda = stats.boxcox(train_data[\"SalePrice\"])\nprint(_lambda)\nsns.distplot(data_boxcox, fit=norm)\nprint(pd.DataFrame(data_boxcox).describe())\nplt.show()\n\n```\n\n或者\n\n```\ndata_boxcox = preprocessing.PowerTransformer(method='box-cox', standardize=False)\n\n```\n\n#### 3.log对数函数变换\n\n通过log函数转换的方法同样可以实现正态化，具体方法如下：Y(x)=ln(x)\n\n```\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\n\n# log\nsns.distplot(np.log(train_data[[\"SalePrice\"]]), fit=norm)\nprint(np.log(train_data[[\"SalePrice\"]]).describe())\nplt.show()\n\n```\n\n### **4.分位数转化（Quantile Transform）**\n\nQuantileTransformer 是一种非参数的数据转化技术，可以将数据转化到特定的分布（一般是高斯分布或者均匀分布），通过分位数函数来实现。\n\n这是一种**非线性变换**。QuantileTransformer类将每个特征缩放在同样的范围或分布情况下。但是，通过执行一个秩转换能够使**异常的分布平滑化**，并且能够比缩放更少地受到离群值的影响。但是它的确使特征间及特征内的**关联和距离失真**了。参考：参考：[sklearn中常用的特征预处理方法（scaler）](https://zhuanlan.zhihu.com/p/415705923)\n\n```\nfrom sklearn.preprocessing import QuantileTransformer\n\n# Quantile Transform\nquantile_transformer = QuantileTransformer(random_state=0,  output_distribution='normal')\n\n```\n\n### **5.正则化（**normalization**）**\n\n#### 1.normalizer\n\n参考：[数据转化](https://wutaoblog.com.cn/2021/08/20/data_preprocess/) 和 [机器学习 | 数据缩放与转换方法（1）](https://xie.infoq.cn/article/1a72c74d073a807c4a738284f)\n\n正则化的过程是将每个样本缩放到单位范数（每个样本的范数为1），如果后面要使用如二次型（点积）或者其它核方法计算两个样本之间的相似性这个方法会很有用。\n\nNormalization主要思想是对每个样本计算其p-范数，然后对该样本中每个元素除以该范数，这样处理的结果是使得每个处理后样本的p-范数（l1-norm,l2-norm）等于1。p-范数的定义为：\n\n<img src=\"/images/517519-20221112213053746-2135268993.png\" width=\"250\" height=\"50\" />\n\n**normalizer归一化的特点：**\n\n- 缩放到0和1之间，保留原始数据的分布\n\n**normalizer归一化的使用场景：**\n\n- 该方法主要应用于文本分类和聚类中。例如，对于两个TF-IDF向量的l2-norm进行点积，就可以得到这两个向量的余弦相似性。\n\nsklearn 中有两种方法可以进行 Normalization：normalize 函数和&nbsp;Normalizer 类，可以通过&nbsp;norm 参数指定使用的范数类型（l1,l2,max）\n\n```\nfrom sklearn.preprocessing import Normalizer\n\ntransfer = Normalizer() \ndata_normal = transfer.fit_transform(train_data[[\"SalePrice\"]], norm='max')\nprint(data_normal)\n\n```\n\n图片转自：https://www.zhihu.com/people/sun_shuai_\n\n<img src=\"/images/517519-20221112205223034-1498039690.png\" width=\"300\" height=\"217\" /><img src=\"/images/517519-20221112205753436-677132225.png\" width=\"300\" height=\"216\" />\n","tags":["Python"]},{"title":"特征预处理——特征选择和特征理解","url":"/特征预处理——特征选择和特征理解.html","content":"## 1.数据可视化\n\n### 1.单变量可视化\n\n参考：[从kaggle房价预测看探索性数据分析的一般规律](https://zhuanlan.zhihu.com/p/69853423)\n\n查看pandas某列的统计指标\n\n```\n# 描述性统计\nprint(train_data['SalePrice'].describe())\n\ncount      1460.000000　　# 行数\nmean     180921.195890　　# 平均值\nstd       79442.502883　　# 标准差\nmin       34900.000000　　# 最小值\n25%      129975.000000　　# 第1四分位数，即第25百分位数\n50%      163000.000000　　# 第2四分位数，即第50百分位数\n75%      214000.000000　　# 第3四分位数，即第75百分位数\nmax      755000.000000　　# 最大值\nName: SalePrice, dtype: float64\n\n```\n\n使用**displot函数**可以绘制**直方图**，bins越大，横坐标的精度越大\n\n```\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.displot(train_data['SalePrice'], bins=100)\nplt.show()\n\n```\n\n可以看到数据呈现**偏态分布**\n\n<img src=\"/images/517519-20221031000109338-716973522.png\" width=\"600\" height=\"306\" loading=\"lazy\" />\n\n### 2.双变量关系可视化\n\n使用**scatterplot函数**绘制**散点图**，查看**2个数值型（numerical）变量的关系**\n\n```\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 地上居住面积/房价散点图\nsns.scatterplot(y=train_data['SalePrice'], x=train_data['GrLivArea'])\nplt.show()\n# 或者\ndata = pd.concat([train_data['SalePrice'], train_data['GrLivArea']], axis=1) \ndata.plot.scatter(x='GrLivArea', y='SalePrice', ylim=(0, 800000)) \nplt.show()\n\n```\n\n可以看出面积越大的房子，价格越高\n\n<img src=\"/images/517519-20221105161824528-299028084.png\" width=\"400\" height=\"333\" loading=\"lazy\" />\n\n使用**stripplot函数**绘制**散点图**，适用于某一变量的取值是有限的情况，查看**1个数值型变量和1个标称型（categorical）变量之间的关系**\n\n```\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 房屋质量等级/房价散点图\nsns.stripplot(x=train_data[\"OverallQual\"] , y=train_data[\"SalePrice\"])\nplt.show()\n\n```\n\n可以看出质量等级越高的房子，价格越高\n\n<img src=\"/images/517519-20221105174603144-1138969034.png\" width=\"400\" height=\"335\" loading=\"lazy\" />\n\n此外，对于标称值，也可以**boxplot函数**来绘制**箱型图**进行分析\n\n```\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n# 房屋质量等级/房价箱型图\nsns.boxplot(x=train_data[\"OverallQual\"], y=train_data[\"SalePrice\"])\nplt.show()\n\n```\n\n<img src=\"/images/517519-20221105174910520-1845302283.png\" width=\"400\" height=\"335\" loading=\"lazy\" />\n\n### 3.多变量关系可视化\n\n参考：[seaborn单变量/双变量/多变量绘图](https://blog.csdn.net/weixin_35757704/article/details/89885069)\n\n可以使用**pairplot函数**来绘制多变量的**散点图**，来查看pandas数据集中**每对变量之间的关系**\n\n```\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train_data[cols], height=1.5)\nplt.show()\n\n```\n\n<img src=\"/images/517519-20221105170324096-1282133364.png\" width=\"800\" height=\"805\" loading=\"lazy\" />\n\n如果看一个应变量Y和多个自变量X之间的关系，可以指定Y轴的变量\n\n```\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train_data[cols], y_vars=\"SalePrice\", height=1.5)\nplt.show()\n\n```\n\n<img src=\"/images/517519-20221105172548726-795844603.png\" width=\"1000\" height=\"174\" loading=\"lazy\" />\n\n### 4.变量的分布\n\n如果要添加**正态分布**曲线和原数据分布进行比较，可以使用`distplot`函数，该函数会给原数据添加拟合曲线\n\n```\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\n\n# 偏度和峰度\nprint(\"Skewness: %f\" % train_data['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train_data['SalePrice'].kurt())\n\n# 绘制数据分布曲线\nsns.distplot(train_data['SalePrice'], fit=norm)\n# 绘制P-P曲线\nfig = plt.figure()\nstats.probplot(train_data['SalePrice'], plot=plt)\nplt.show()\n\n```\n\n<img src=\"/images/517519-20221105142649082-335225594.png\" width=\"800\" height=\"335\" />\n\n查看偏度和峰度\n\n```\nSkewness: 1.882876\nKurtosis: 6.536282\n\n```\n\n使用**log函数**对数据来取对数，从而进行数据归一化，参考：[特征预处理&mdash;&mdash;特征标准化](https://www.cnblogs.com/tonglin0325/p/6214808.html)\n\n```\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\n\n# 取log对数\ntrain_data['SalePrice'] = np.log(train_data['SalePrice'])\n\n# 偏度和峰度\nprint(\"Skewness: %f\" % train_data['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train_data['SalePrice'].kurt())\n\n# 绘制数据分布曲线\nsns.distplot(train_data['SalePrice'], fit=norm)\n# 绘制P-P曲线\nfig = plt.figure()\nstats.probplot(train_data['SalePrice'], plot=plt)\nplt.show()\n\n```\n\n<img src=\"/images/517519-20221105140858677-777000938.png\" width=\"800\" height=\"334\" />\n\n使用log函数进行正则化后的分布曲线的偏度和峰度\n\n```\nSkewness: 0.121335\nKurtosis: 0.809532\n\n```\n\n## 2.特征选择\n\n特征选择(排序)对于数据科学家、机器学习从业者来说非常重要。好的特征选择能够提升模型的性能，更能帮助我们理解数据的特点、底层结构，这对进一步改善模型、算法都有着重要作用。\n\n特征选择主要有两个功能：\n\n1. 减少特征数量、降维，使模型泛化能力更强，减少过拟合\n1. 增强对特征和特征值之间的理解\n\n### 1.异常值处理\n\n#### 1.空值处理\n\n如果一个特征有大量的空值，就可以考虑剔除该特征，比如当空值的比例大于15%的时候。[](https://www.cnblogs.com/tonglin0325/p/6298290.html)\n\n参考：[特征预处理&mdash;&mdash;异常值处理](https://www.cnblogs.com/tonglin0325/p/6298290.html)\n\n#### 2.离群值处理\n\n<!--more-->\n&nbsp;\n\n#### 3.去掉取值变化小的特征&nbsp;Removing features with low variance \n\n这应该是最简单的特征选择方法了：假设某特征的特征值只有0和1，并且在所有输入样本中，95%的实例的该特征取值都是1，那就可以认为这个特征作用不大。如果100%都是1，那这个特征就没意义了。当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用，而且实际当中，一般不太会有95%以上都取某个值的特征存在，所以这种方法虽然简单但是不太好用。可以把它作为特征选择的预处理，先去掉那些取值变化小的特征，然后再从接下来提到的的特征选择方法中选择合适的进行进一步的特征选择。\n\n### 2.特征处理\n\n参考：\n\n### 3.相关性分析\n\n在对数据进行清理和特征提取之后，就可以对特征进行相关性分析\n\n#### 1.使用热力图查看多元数据特征的相关性\n\n使用pandas的**dataframe.corr**`**函数**来计算`dataframe``中的两个变量之间的**相关性**，取值范围为[-1,1]，取值接近**-1**，表示**负****相关**，取值接近**1**，表示**正相关**。\n\n```\n# 所有变量的相关性矩阵\ncorrmat = train_data.corr()\nprint(corrmat)\n# 查看热力图\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True)\nplt.tight_layout()\nplt.show()\n\n```\n\n图中**浅色**的表示**正相关**，**深色**的表示**负相关**，所以图中浅色块和深色块区域的变量就具有线性关系。\n\n<img src=\"/images/517519-20221105212528305-360610715.png\" width=\"600\" height=\"478\" />\n\ncorr()默认使用的是**皮尔逊Pearson相关系数**，可以反映两个变量变化时是同向还是反向，如果同向变化就为正，反向变化就为负。由于它是标准化后的协方差，因此更重要的特性来了，它消除了两个变量变化幅度的影响，而只是单纯反应两个变量每单位变化时的相似程度，即\n\n```\ncorrmat = data.corr(method='pearson')\n\n```\n\n**相关系数分类：**0.8-1.0 极强相关；0.6-0.8 强相关；0.4-0.6 中等程度相关；0.2-0.4 弱相关；0.0-0.2 极弱相关或无相关\n\n参考：[3(1).特征选择---过滤法（特征相关性分析）](https://www.cnblogs.com/nxf-rabbit75/p/11122415.html)\n\n查看和应变量**相关性最高的10个自变量**\n\n```\n# 所有变量的相关性矩阵\ncorrmat = train_data.corr()\n\n# 与Y的相关性矩阵top10热图\nk = 10  # 热力图变量数目\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train_data[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()  # 显示图片\n\n```\n\n<img src=\"/images/517519-20221105212910194-1426743606.png\" width=\"500\" height=\"498\" />\n\n#### 2.特征共线性问题\n\n在线性模型中，若自变量之间具有线性关系会对模型的性能产生影响，因为它们包含重复的信息，应当舍去。\n\n因为在热力图中浅色块和深色块区域的变量就具有线性关系，比如上图中的TotalBsmtSF（`地下室面积`）和1stFlrSF（`1楼面积`）,GarageCars（`车库的汽车容量`）和GarageArea（车库面积），以及GrLivArea（地上居住面积）和TotRmsAbvGrd（地上房间总数）等，只需要保留和SalePrice相关性高的变量即可。\n\n参考：[多重共线性问题，如何解决？](https://zhuanlan.zhihu.com/p/72722146) 和 [如何消除多重共线性](https://cloud.tencent.com/developer/article/1840752)\n\n&nbsp;\n\n#### 3.使用斯皮尔曼等级查看多元数据特征的相关性\n\n**Pearson相关检验**是针对`**正态分布**数据`而言的，其他的还有：**Spearman相关检验**和**Kendall相关检验**，这2种检验属于**`秩检验`**\n\n参考：[多元数据的相关性检验&mdash;&mdash;基于R](https://blog.csdn.net/qq_54423921/article/details/126322760)\n\n指定corr()函数使用**斯皮尔曼spearman相关系数**，一列是特征的名字，一列是spearman相关系数，排序后使用**barplot函数**显示**条形图**\n\n```\n# spearman\ndf = pd.DataFrame()\nfeatures = train_data.columns.tolist()\ndf['feature'] = features\ndf['spearman'] = [train_data[f].corr(other=train_data['SalePrice'], method='spearman') for f in features]\ndf = df.sort_values('spearman')\nplt.figure(figsize=(8, 0.1*len(features)))\nplt.yticks(fontsize=5)\nsns.barplot(data=df, y='feature', x='spearman', orient='h')\nplt.show()\n\n```\n\n可以看出和SalePrice相关性最高的自变量和使用热力图分析出来的结果是一样的\n\n&nbsp;<img src=\"/images/517519-20221105221203155-2143422491.png\" width=\"600\" height=\"651\" loading=\"lazy\" />\n\n&nbsp;\n\n作者：[Edwin Jarvis](http://www.chaoslog.com/author/edwin-jarvis.html)\n\n拿到数据集，一个特征选择方法，往往很难同时完成这两个目的。通常情况下，我们经常不管三七二十一，选择一种自己最熟悉或者最方便的特征选择方法（往往目的是降维，而忽略了对特征和数据理解的目的）。\n\n在许多机器学习相关的书里，很难找到关于特征选择的内容，因为特征选择要解决的问题往往被视为机器学习的一种副作用，一般不会单独拿出来讨论。\n\n本文将结合[Scikit-learn提供的例子](http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)介绍几种常用的特征选择方法，它们各自的优缺点和问题。\n\n## 3.单变量特征选择 Univariate feature selection\n\n单变量特征选择能够对每一个特征进行测试，衡量该特征和响应变量之间的关系，根据得分扔掉不好的特征。对于回归和分类问题可以采用卡方检验等方式对特征进行测试。\n\n这种方法比较简单，易于运行，易于理解，通常对于理解数据有较好的效果（但对特征优化、提高泛化能力来说不一定有效）；这种方法有许多改进的版本、变种。\n\n### 3.1 Pearson相关系数 Pearson Correlation\n\n皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性，结果的取值区间为[-1，1]，-1表示完全的负相关(这个变量下降，那个就会上升)，+1表示完全的正相关，0表示没有线性相关。\n\nPearson Correlation速度快、易于计算，经常在拿到数据(经过清洗和特征提取之后的)之后第一时间就执行。Scipy的[pearsonr](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.pearsonr.html)方法能够同时计算相关系数和p-value，\n\n```\nimport numpy as np\nfrom scipy.stats import pearsonr\nnp.random.seed(0)\nsize = 300\nx = np.random.normal(0, 1, size)\nprint \"Lower noise\", pearsonr(x, x + np.random.normal(0, 1, size))\nprint \"Higher noise\", pearsonr(x, x + np.random.normal(0, 10, size))\n\n```\n\nLower noise (0.71824836862138386, 7.3240173129992273e-49)<br />\nHigher noise (0.057964292079338148, 0.31700993885324746)\n\n这个例子中，我们比较了变量在加入噪音之前和之后的差异。当噪音比较小的时候，相关性很强，p-value很低。\n\nScikit-learn提供的[f_regrssion](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html)方法能够批量计算特征的p-value，非常方便，参考sklearn的[pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n\nPearson相关系数的一个明显缺陷是，作为特征排序机制，他只对线性关系敏感。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0。\n\n```\nx = np.random.uniform(-1, 1, 100000)\nprint pearsonr(x, x**2)[0]\n\n```\n\n-0.00230804707612\n\n更多类似的例子参考[sample plots](http://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/506px-Correlation_examples2.svg.png)。另外，如果仅仅根据相关系数这个值来判断的话，有时候会具有很强的误导性，如[Anscombe&rsquo;s quartet](http://en.wikipedia.org/wiki/Anscombe%27s_quartet)，最好把数据可视化出来，以免得出错误的结论。\n\n### 3.2 互信息和最大信息系数 Mutual information and maximal information coefficient (MIC)\n\n最大信息系数克服了这两个问题。它首先寻找一种最优的离散化方式，然后把互信息取值转换成一种度量方式，取值区间在[0，1]。[minepy](http://minepy.sourceforge.net/)提供了MIC功能。\n\n反过头来看y=x^2这个例子，MIC算出来的互信息值为1(最大的取值)。\n\n```\nfrom minepy import MINE\nm = MINE()\nx = np.random.uniform(-1, 1, 10000)\nm.compute_score(x, x**2)\nprint m.mic()\n\n```\n\n1.0\n\nMIC的统计能力遭到了[一些质疑](http://statweb.stanford.edu/%7Etibs/reshef/comment.pdf)，当零假设不成立时，MIC的统计就会受到影响。在有的数据集上不存在这个问题，但有的数据集上就存在这个问题\n\n### 3.3 距离相关系数 (Distance correlation)\n\n距离相关系数是为了克服Pearson相关系数的弱点而生的。在x和x^2这个例子中，即便Pearson相关系数是0，我们也不能断定这两个变量是独立的（有可能是非线性相关）；但如果距离相关系数是0，那么我们就可以说这两个变量是独立的。\n\nR的[energy](http://cran.r-project.org/web/packages/energy/index.html)包里提供了距离相关系数的实现，另外这是[Python gist](https://gist.github.com/josef-pkt/2938402)的实现。\n\n```\n#R-code\n> x = runif (1000, -1, 1)\n> dcor(x, x**2)\n[1] 0.4943864\n\n```\n\n尽管有MIC和距离相关系数在了，但当变量之间的关系接近线性相关的时候，Pearson相关系数仍然是不可替代的。第一、Pearson相关系数计算速度快，这在处理大规模数据的时候很重要。第二、Pearson相关系数的取值区间是[-1，1]，而MIC和距离相关系数都是[0，1]。这个特点使得Pearson相关系数能够表征更丰富的关系，符号表示关系的正负，绝对值能够表示强度。当然，Pearson相关性有效的前提是两个变量的变化关系是单调的\n\n### 2.4 基于学习模型的特征排序 (Model based ranking)\n\n这种方法的思路是直接使用你要用的机器学习算法，针对每个单独的特征和响应变量建立预测模型。其实Pearson相关系数等价于线性回归里的标准化回归系数。假如某个特征和响应变量之间的关系是非线性的，可以用基于树的方法（决策树、随机森林）、或者扩展的线性模型等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是运用交叉验证。\n\n在[波士顿房价数据集](https://archive.ics.uci.edu/ml/datasets/Housing)上使用sklearn的[随机森林回归](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)给出一个单变量选择的例子：\n\n```\nfrom sklearn.cross_validation import cross_val_score, ShuffleSplit\nfrom sklearn.datasets import load_boston\nfrom sklearn.ensemble import RandomForestRegressor\n\n#Load boston housing dataset as an example\nboston = load_boston()\nX = boston[\"data\"]\nY = boston[\"target\"]\nnames = boston[\"feature_names\"]\n\nrf = RandomForestRegressor(n_estimators=20, max_depth=4)\nscores = []\nfor i in range(X.shape[1]):\n     score = cross_val_score(rf, X[:, i:i+1], Y, scoring=\"r2\",\n                              cv=ShuffleSplit(len(X), 3, .3))\n     scores.append((round(np.mean(score), 3), names[i]))\nprint sorted(scores, reverse=True)\n\n```\n\n[(0.636, &lsquo;LSTAT&rsquo;), (0.59, &lsquo;RM&rsquo;), (0.472, &lsquo;NOX&rsquo;), (0.369, &lsquo;INDUS&rsquo;), (0.311, &lsquo;PTRATIO&rsquo;), (0.24, &lsquo;TAX&rsquo;), (0.24, &lsquo;CRIM&rsquo;), (0.185, &lsquo;RAD&rsquo;), (0.16, &lsquo;ZN&rsquo;), (0.087, &lsquo;B&rsquo;), (0.062, &lsquo;DIS&rsquo;), (0.036, &lsquo;CHAS&rsquo;), (0.027, &lsquo;AGE&rsquo;)]\n\n## 4.线性模型和正则化\n\n单变量特征选择方法独立的衡量每个特征与响应变量之间的关系，另一种主流的特征选择方法是基于机器学习模型的方法。有些机器学习方法本身就具有对特征进行打分的机制，或者很容易将其运用到特征选择任务中，例如回归模型，SVM，决策树，随机森林等等。说句题外话，这种方法好像在一些地方叫做wrapper类型，大概意思是说，特征排序模型和机器学习模型是耦盒在一起的，对应的非wrapper类型的特征选择方法叫做filter类型。\n\n下面将介绍如何用回归模型的系数来选择特征。越是重要的特征在模型中对应的系数就会越大，而跟输出变量越是无关的特征对应的系数就会越接近于0。在噪音不多的数据上，或者是数据量远远大于特征数的数据上，如果特征之间相对来说是比较独立的，那么即便是运用最简单的线性回归模型也一样能取得非常好的效果。\n\n```\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\nnp.random.seed(0)\nsize = 5000\n\n#A dataset with 3 features\nX = np.random.normal(0, 1, (size, 3))\n#Y = X0 + 2*X1 + noise\nY = X[:,0] + 2*X[:,1] + np.random.normal(0, 2, size)\nlr = LinearRegression()\nlr.fit(X, Y)\n\n#A helper method for pretty-printing linear models\ndef pretty_print_linear(coefs, names = None, sort = False):\n    if names == None:\n        names = [\"X%s\" % x for x in range(len(coefs))]\n    lst = zip(coefs, names)\n    if sort:\n        lst = sorted(lst,  key = lambda x:-np.abs(x[0]))\n    return \" + \".join(\"%s * %s\" % (round(coef, 3), name)\n                                   for coef, name in lst)\n\nprint \"Linear model:\", pretty_print_linear(lr.coef_)\n\n```\n\nLinear model: 0.984 * X0 + 1.995 * X1 + -0.041 * X2\n\n在这个例子当中，尽管数据中存在一些噪音，但这种特征选择模型仍然能够很好的体现出数据的底层结构。当然这也是因为例子中的这个问题非常适合用线性模型来解：特征和响应变量之间全都是线性关系，并且特征之间均是独立的。\n\n在很多实际的数据当中，往往存在多个互相关联的特征，这时候模型就会变得不稳定，数据中细微的变化就可能导致模型的巨大变化（模型的变化本质上是系数，或者叫参数，可以理解成W），这会让模型的预测变得困难，这种现象也称为多重共线性。例如，假设我们有个数据集，它的真实模型应该是Y=X1+X2，当我们观察的时候，发现Y&rsquo;=X1+X2+e，e是噪音。如果X1和X2之间存在线性关系，例如X1约等于X2，这个时候由于噪音e的存在，我们学到的模型可能就不是Y=X1+X2了，有可能是Y=2X1，或者Y=-X1+3X2。\n\n下边这个例子当中，在同一个数据上加入了一些噪音，用随机森林算法进行特征选择。\n\n```\nfrom sklearn.linear_model import LinearRegression\n\nsize = 100\nnp.random.seed(seed=5)\n\nX_seed = np.random.normal(0, 1, size)\nX1 = X_seed + np.random.normal(0, .1, size)\nX2 = X_seed + np.random.normal(0, .1, size)\nX3 = X_seed + np.random.normal(0, .1, size)\n\nY = X1 + X2 + X3 + np.random.normal(0,1, size)\nX = np.array([X1, X2, X3]).T\n\nlr = LinearRegression()\nlr.fit(X,Y)\nprint \"Linear model:\", pretty_print_linear(lr.coef_)\n\n```\n\nLinear model: -1.291 * X0 + 1.591 * X1 + 2.747 * X2\n\n系数之和接近3，基本上和上上个例子的结果一致，应该说学到的模型对于预测来说还是不错的。但是，如果从系数的字面意思上去解释特征的重要性的话，X3对于输出变量来说具有很强的正面影响，而X1具有负面影响，而实际上所有特征与输出变量之间的影响是均等的。\n\n同样的方法和套路可以用到类似的线性模型上，比如逻辑回归。\n\n### 4.1 正则化模型\n\n正则化就是把额外的约束或者惩罚项加到已有模型（损失函数）上，以防止过拟合并提高泛化能力。损失函数由原来的E(X,Y)变为E(X,Y)+alpha||w||，w是模型系数组成的向量（有些地方也叫参数parameter，coefficients），||&middot;||一般是L1或者L2范数，alpha是一个可调的参数，控制着正则化的强度。当用在线性模型上时，L1正则化和L2正则化也称为Lasso和Ridge。\n\n### 4.2 L1正则化/Lasso\n\nL1正则化将系数w的l1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成0。因此L1正则化往往会使学到的模型很稀疏（系数w经常为0），这个特性使得L1正则化成为一种很好的特征选择方法。\n\nScikit-learn为线性回归提供了Lasso，为分类提供了L1逻辑回归。\n\n下面的例子在波士顿房价数据上运行了Lasso，其中参数alpha是通过grid search进行优化的。\n\n```\nfrom sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\nscaler = StandardScaler()\nX = scaler.fit_transform(boston[\"data\"])\nY = boston[\"target\"]\nnames = boston[\"feature_names\"]\n\nlasso = Lasso(alpha=.3)\nlasso.fit(X, Y)\n\nprint \"Lasso model: \", pretty_print_linear(lasso.coef_, names, sort = True)\n\n```\n\nLasso model: -3.707 * LSTAT + 2.992 * RM + -1.757 * PTRATIO + -1.081 * DIS + -0.7 * NOX + 0.631 * B + 0.54 * CHAS + -0.236 * CRIM + 0.081 * ZN + -0.0 * INDUS + -0.0 * AGE + 0.0 * RAD + -0.0 * TAX\n\n可以看到，很多特征的系数都是0。如果继续增加alpha的值，得到的模型就会越来越稀疏，即越来越多的特征系数会变成0。\n\n然而，L1正则化像非正则化线性模型一样也是不稳定的，如果特征集合中具有相关联的特征，当数据发生细微变化时也有可能导致很大的模型差异\n\n### 4.3 L2正则化/Ridge regression\n\nL2正则化将系数向量的L2范数添加到了损失函数中。由于L2惩罚项中系数是二次方的，这使得L2和L1有着诸多差异，最明显的一点就是，L2正则化会让系数的取值变得平均。对于关联特征，这意味着他们能够获得更相近的对应系数。还是以Y=X1+X2为例，假设X1和X2具有很强的关联，如果用L1正则化，不论学到的模型是Y=X1+X2还是Y=2X1，惩罚都是一样的，都是2alpha。但是对于L2来说，第一个模型的惩罚项是2alpha，但第二个模型的是4*alpha。可以看出，系数之和为常数时，各系数相等时惩罚是最小的，所以才有了L2会让各个系数趋于相同的特点。\n\n可以看出，L2正则化对于特征选择来说一种稳定的模型，不像L1正则化那样，系数会因为细微的数据变化而波动。所以L2正则化和L1正则化提供的价值是不同的，L2正则化对于特征理解来说更加有用：表示能力强的特征对应的系数是非零。\n\n回过头来看看3个互相关联的特征的例子，分别以10个不同的种子随机初始化运行10次，来观察L1和L2正则化的稳定性。\n\n```\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import r2_score\nsize = 100\n\n#We run the method 10 times with different random seeds\nfor i in range(10):\n    print \"Random seed %s\" % i\n    np.random.seed(seed=i)\n    X_seed = np.random.normal(0, 1, size)\n    X1 = X_seed + np.random.normal(0, .1, size)\n    X2 = X_seed + np.random.normal(0, .1, size)\n    X3 = X_seed + np.random.normal(0, .1, size)\n    Y = X1 + X2 + X3 + np.random.normal(0, 1, size)\n    X = np.array([X1, X2, X3]).T\n\n\n    lr = LinearRegression()\n    lr.fit(X,Y)\n    print \"Linear model:\", pretty_print_linear(lr.coef_)\n\n    ridge = Ridge(alpha=10)\n    ridge.fit(X,Y)\n    print \"Ridge model:\", pretty_print_linear(ridge.coef_)\n    print\n\n```\n\nRandom seed 0 Linear model: 0.728 * X0 + 2.309 * X1 + -0.082 * X2 Ridge model: 0.938 * X0 + 1.059 * X1 + 0.877 * X2\n\nRandom seed 1 Linear model: 1.152 * X0 + 2.366 * X1 + -0.599 * X2 Ridge model: 0.984 * X0 + 1.068 * X1 + 0.759 * X2\n\nRandom seed 2 Linear model: 0.697 * X0 + 0.322 * X1 + 2.086 * X2 Ridge model: 0.972 * X0 + 0.943 * X1 + 1.085 * X2\n\nRandom seed 3 Linear model: 0.287 * X0 + 1.254 * X1 + 1.491 * X2 Ridge model: 0.919 * X0 + 1.005 * X1 + 1.033 * X2\n\nRandom seed 4 Linear model: 0.187 * X0 + 0.772 * X1 + 2.189 * X2 Ridge model: 0.964 * X0 + 0.982 * X1 + 1.098 * X2\n\nRandom seed 5 Linear model: -1.291 * X0 + 1.591 * X1 + 2.747 * X2 Ridge model: 0.758 * X0 + 1.011 * X1 + 1.139 * X2\n\nRandom seed 6 Linear model: 1.199 * X0 + -0.031 * X1 + 1.915 * X2 Ridge model: 1.016 * X0 + 0.89 * X1 + 1.091 * X2\n\nRandom seed 7 Linear model: 1.474 * X0 + 1.762 * X1 + -0.151 * X2 Ridge model: 1.018 * X0 + 1.039 * X1 + 0.901 * X2\n\nRandom seed 8 Linear model: 0.084 * X0 + 1.88 * X1 + 1.107 * X2 Ridge model: 0.907 * X0 + 1.071 * X1 + 1.008 * X2\n\nRandom seed 9 Linear model: 0.714 * X0 + 0.776 * X1 + 1.364 * X2 Ridge model: 0.896 * X0 + 0.903 * X1 + 0.98 * X2\n\n可以看出，不同的数据上线性回归得到的模型（系数）相差甚远，但对于L2正则化模型来说，结果中的系数非常的稳定，差别较小，都比较接近于1，能够反映出数据的内在结构\n\n## 5.随机森林\n\n随机森林具有准确率高、鲁棒性好、易于使用等优点，这使得它成为了目前最流行的机器学习算法之一。随机森林提供了两种特征选择的方法：mean decrease impurity和mean decrease accuracy。\n\n### 5.1 平均不纯度减少 mean decrease impurity\n\n随机森林由多个决策树构成。决策树中的每一个节点都是关于某个特征的条件，为的是将数据集按照不同的响应变量一分为二。利用不纯度可以确定节点（最优条件），对于分类问题，通常采用[基尼不纯度](http://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity)或者[信息增益](http://en.wikipedia.org/wiki/Information_gain_in_decision_trees)，对于回归问题，通常采用的是[方差](http://en.wikipedia.org/wiki/Variance)或者最小二乘拟合。当训练决策树的时候，可以计算出每个特征减少了多少树的不纯度。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的值。\n\n下边的例子是sklearn中基于随机森林的特征重要度度量方法：\n\n```\nfrom sklearn.datasets import load_boston\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\n#Load boston housing dataset as an example\nboston = load_boston()\nX = boston[\"data\"]\nY = boston[\"target\"]\nnames = boston[\"feature_names\"]\nrf = RandomForestRegressor()\nrf.fit(X, Y)\nprint \"Features sorted by their score:\"\nprint sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), names), \n             reverse=True)\n\n```\n\nFeatures sorted by their score: [(0.5298, &lsquo;LSTAT&rsquo;), (0.4116, &lsquo;RM&rsquo;), (0.0252, &lsquo;DIS&rsquo;), (0.0172, &lsquo;CRIM&rsquo;), (0.0065, &lsquo;NOX&rsquo;), (0.0035, &lsquo;PTRATIO&rsquo;), (0.0021, &lsquo;TAX&rsquo;), (0.0017, &lsquo;AGE&rsquo;), (0.0012, &lsquo;B&rsquo;), (0.0008, &lsquo;INDUS&rsquo;), (0.0004, &lsquo;RAD&rsquo;), (0.0001, &lsquo;CHAS&rsquo;), (0.0, &lsquo;ZN&rsquo;)]\n\n这里特征得分实际上采用的是[Gini Importance](http://www.stat.berkeley.edu/%7Ebreiman/RandomForests/cc_home.htm#giniimp)。使用基于不纯度的方法的时候，要记住：1、这种方法存在[偏向](http://link.springer.com/article/10.1186%2F1471-2105-8-25)，对具有更多类别的变量会更有利；2、对于存在关联的多个特征，其中任意一个都可以作为指示器（优秀的特征），并且一旦某个特征被选择之后，其他特征的重要度就会急剧下降，因为不纯度已经被选中的那个特征降下来了，其他的特征就很难再降低那么多不纯度了，这样一来，只有先被选中的那个特征重要度很高，其他的关联特征重要度往往较低。在理解数据时，这就会造成误解，导致错误的认为先被选中的特征是很重要的，而其余的特征是不重要的，但实际上这些特征对响应变量的作用确实非常接近的（这跟Lasso是很像的）。\n\n[特征随机选择](http://en.wikipedia.org/wiki/Random_subspace_method)方法稍微缓解了这个问题，但总的来说并没有完全解决。下面的例子中，X0、X1、X2是三个互相关联的变量，在没有噪音的情况下，输出变量是三者之和。\n\n```\nsize = 10000\nnp.random.seed(seed=10)\nX_seed = np.random.normal(0, 1, size)\nX0 = X_seed + np.random.normal(0, .1, size)\nX1 = X_seed + np.random.normal(0, .1, size)\nX2 = X_seed + np.random.normal(0, .1, size)\nX = np.array([X0, X1, X2]).T\nY = X0 + X1 + X2\n\nrf = RandomForestRegressor(n_estimators=20, max_features=2)\nrf.fit(X, Y);\nprint \"Scores for X0, X1, X2:\", map(lambda x:round (x,3),\n                                    rf.feature_importances_)\n\n```\n\nScores for X0, X1, X2: [0.278, 0.66, 0.062]\n\n当计算特征重要性时，可以看到X1的重要度比X2的重要度要高出10倍，但实际上他们真正的重要度是一样的。尽管数据量已经很大且没有噪音，且用了20棵树来做随机选择，但这个问题还是会存在。\n\n需要注意的一点是，关联特征的打分存在不稳定的现象，这不仅仅是随机森林特有的，大多数基于模型的特征选择方法都存在这个问题\n\n### 5.2 平均精确率减少 Mean decrease accuracy\n\n另一种常用的特征选择方法就是直接度量每个特征对模型精确率的影响。主要思路是打乱每个特征的特征值顺序，并且度量顺序变动对模型的精确率的影响。很明显，对于不重要的变量来说，打乱顺序对模型的精确率影响不会太大，但是对于重要的变量来说，打乱顺序就会降低模型的精确率。\n\n这个方法sklearn中没有直接提供，但是很容易实现，下面继续在波士顿房价数据集上进行实现。\n\n```\nfrom sklearn.cross_validation import ShuffleSplit\nfrom sklearn.metrics import r2_score\nfrom collections import defaultdict\n\nX = boston[\"data\"]\nY = boston[\"target\"]\n\nrf = RandomForestRegressor()\nscores = defaultdict(list)\n\n#crossvalidate the scores on a number of different random splits of the data\nfor train_idx, test_idx in ShuffleSplit(len(X), 100, .3):\n    X_train, X_test = X[train_idx], X[test_idx]\n    Y_train, Y_test = Y[train_idx], Y[test_idx]\n    r = rf.fit(X_train, Y_train)\n    acc = r2_score(Y_test, rf.predict(X_test))\n    for i in range(X.shape[1]):\n        X_t = X_test.copy()\n        np.random.shuffle(X_t[:, i])\n        shuff_acc = r2_score(Y_test, rf.predict(X_t))\n        scores[names[i]].append((acc-shuff_acc)/acc)\nprint \"Features sorted by their score:\"\nprint sorted([(round(np.mean(score), 4), feat) for\n              feat, score in scores.items()], reverse=True)\n\n```\n\nFeatures sorted by their score: [(0.7276, &lsquo;LSTAT&rsquo;), (0.5675, &lsquo;RM&rsquo;), (0.0867, &lsquo;DIS&rsquo;), (0.0407, &lsquo;NOX&rsquo;), (0.0351, &lsquo;CRIM&rsquo;), (0.0233, &lsquo;PTRATIO&rsquo;), (0.0168, &lsquo;TAX&rsquo;), (0.0122, &lsquo;AGE&rsquo;), (0.005, &lsquo;B&rsquo;), (0.0048, &lsquo;INDUS&rsquo;), (0.0043, &lsquo;RAD&rsquo;), (0.0004, &lsquo;ZN&rsquo;), (0.0001, &lsquo;CHAS&rsquo;)]\n\n在这个例子当中，LSTAT和RM这两个特征对模型的性能有着很大的影响，打乱这两个特征的特征值使得模型的性能下降了73%和57%。注意，尽管这些我们是在所有特征上进行了训练得到了模型，然后才得到了每个特征的重要性测试，这并不意味着我们扔掉某个或者某些重要特征后模型的性能就一定会下降很多，因为即便某个特征删掉之后，其关联特征一样可以发挥作用，让模型性能基本上不变\n\n## 6.两种顶层特征选择算法\n\n之所以叫做顶层，是因为他们都是建立在基于模型的特征选择方法基础之上的，例如回归和SVM，在不同的子集上建立模型，然后汇总最终确定特征得分。\n\n### 6.1 稳定性选择 Stability selection\n\n稳定性选择是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。\n\nsklearn在[随机lasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RandomizedLasso.html)和[随机逻辑回归](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RandomizedLogisticRegression.html)中有对稳定性选择的实现。\n\n```\nfrom sklearn.linear_model import RandomizedLasso\nfrom sklearn.datasets import load_boston\nboston = load_boston()\n\n#using the Boston housing data. \n#Data gets scaled automatically by sklearn's implementation\nX = boston[\"data\"]\nY = boston[\"target\"]\nnames = boston[\"feature_names\"]\n\nrlasso = RandomizedLasso(alpha=0.025)\nrlasso.fit(X, Y)\n\nprint \"Features sorted by their score:\"\nprint sorted(zip(map(lambda x: round(x, 4), rlasso.scores_), \n                 names), reverse=True)\n\n```\n\nFeatures sorted by their score: [(1.0, &lsquo;RM&rsquo;), (1.0, &lsquo;PTRATIO&rsquo;), (1.0, &lsquo;LSTAT&rsquo;), (0.62, &lsquo;CHAS&rsquo;), (0.595, &lsquo;B&rsquo;), (0.39, &lsquo;TAX&rsquo;), (0.385, &lsquo;CRIM&rsquo;), (0.25, &lsquo;DIS&rsquo;), (0.22, &lsquo;NOX&rsquo;), (0.125, &lsquo;INDUS&rsquo;), (0.045, &lsquo;ZN&rsquo;), (0.02, &lsquo;RAD&rsquo;), (0.015, &lsquo;AGE&rsquo;)]\n\n在上边这个例子当中，最高的3个特征得分是1.0，这表示他们总会被选作有用的特征（当然，得分会收到正则化参数alpha的影响，但是sklearn的随机lasso能够自动选择最优的alpha）。接下来的几个特征得分就开始下降，但是下降的不是特别急剧，这跟纯lasso的方法和随机森林的结果不一样。能够看出稳定性选择对于克服过拟合和对数据理解来说都是有帮助的：总的来说，好的特征不会因为有相似的特征、关联特征而得分为0，这跟Lasso是不同的。对于特征选择任务，在许多数据集和环境下，稳定性选择往往是性能最好的方法之一\n\n### 6.2 递归特征消除 Recursive feature elimination (RFE)\n\n递归特征消除的主要思想是反复的构建模型（如SVM或者回归模型）然后选出最好的（或者最差的）的特征（可以根据系数来选），把选出来的特征放到一遍，然后在剩余的特征上重复这个过程，直到所有特征都遍历了。这个过程中特征被消除的次序就是特征的排序。因此，这是一种寻找最优特征子集的贪心算法。\n\nRFE的稳定性很大程度上取决于在迭代的时候底层用哪种模型。例如，假如RFE采用的普通的回归，没有经过正则化的回归是不稳定的，那么RFE就是不稳定的；假如采用的是Ridge，而用Ridge正则化的回归是稳定的，那么RFE就是稳定的。\n\nSklearn提供了[RFE](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)包，可以用于特征消除，还提供了[RFECV](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html)，可以通过交叉验证来对的特征进行排序。\n\n```\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nboston = load_boston()\nX = boston[\"data\"]\nY = boston[\"target\"]\nnames = boston[\"feature_names\"]\n\n#use linear regression as the model\nlr = LinearRegression()\n#rank all features, i.e continue the elimination until the last one\nrfe = RFE(lr, n_features_to_select=1)\nrfe.fit(X,Y)\n\nprint \"Features sorted by their rank:\"\nprint sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), names))\n\n```\n\nFeatures sorted by their rank: [(1.0, &lsquo;NOX&rsquo;), (2.0, &lsquo;RM&rsquo;), (3.0, &lsquo;CHAS&rsquo;), (4.0, &lsquo;PTRATIO&rsquo;), (5.0, &lsquo;DIS&rsquo;), (6.0, &lsquo;LSTAT&rsquo;), (7.0, &lsquo;RAD&rsquo;), (8.0, &lsquo;CRIM&rsquo;), (9.0, &lsquo;INDUS&rsquo;), (10.0, &lsquo;ZN&rsquo;), (11.0, &lsquo;TAX&rsquo;), (12.0, &lsquo;B&rsquo;), (13.0, &lsquo;AGE&rsquo;)]\n\n## 7.一个完整的例子\n\n接下来将会在上述数据上运行所有的特征选择方法，并且将每种方法给出的得分进行归一化，让取值都落在0-1之间。对于RFE来说，由于它给出的是顺序而不是得分，我们将最好的5个的得分定为1，其他的特征的得分均匀的分布在0-1之间。\n\n```\nfrom sklearn.datasets import load_boston\nfrom sklearn.linear_model import (LinearRegression, Ridge, \n                                  Lasso, RandomizedLasso)\nfrom sklearn.feature_selection import RFE, f_regression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nfrom minepy import MINE\n\nnp.random.seed(0)\n\nsize = 750\nX = np.random.uniform(0, 1, (size, 14))\n\n#\"Friedamn #1&rdquo; regression problem\nY = (10 * np.sin(np.pi*X[:,0]*X[:,1]) + 20*(X[:,2] - .5)**2 +\n     10*X[:,3] + 5*X[:,4] + np.random.normal(0,1))\n#Add 3 additional correlated variables (correlated with X1-X3)\nX[:,10:] = X[:,:4] + np.random.normal(0, .025, (size,4))\n\nnames = [\"x%s\" % i for i in range(1,15)]\n\nranks = {}\n\ndef rank_to_dict(ranks, names, order=1):\n    minmax = MinMaxScaler()\n    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n    ranks = map(lambda x: round(x, 2), ranks)\n    return dict(zip(names, ranks ))\n\nlr = LinearRegression(normalize=True)\nlr.fit(X, Y)\nranks[\"Linear reg\"] = rank_to_dict(np.abs(lr.coef_), names)\n\nridge = Ridge(alpha=7)\nridge.fit(X, Y)\nranks[\"Ridge\"] = rank_to_dict(np.abs(ridge.coef_), names)\n\n\nlasso = Lasso(alpha=.05)\nlasso.fit(X, Y)\nranks[\"Lasso\"] = rank_to_dict(np.abs(lasso.coef_), names)\n\n\nrlasso = RandomizedLasso(alpha=0.04)\nrlasso.fit(X, Y)\nranks[\"Stability\"] = rank_to_dict(np.abs(rlasso.scores_), names)\n\n#stop the search when 5 features are left (they will get equal scores)\nrfe = RFE(lr, n_features_to_select=5)\nrfe.fit(X,Y)\nranks[\"RFE\"] = rank_to_dict(map(float, rfe.ranking_), names, order=-1)\n\nrf = RandomForestRegressor()\nrf.fit(X,Y)\nranks[\"RF\"] = rank_to_dict(rf.feature_importances_, names)\n\n\nf, pval  = f_regression(X, Y, center=True)\nranks[\"Corr.\"] = rank_to_dict(f, names)\n\nmine = MINE()\nmic_scores = []\nfor i in range(X.shape[1]):\n    mine.compute_score(X[:,i], Y)\n    m = mine.mic()\n    mic_scores.append(m)\n\nranks[\"MIC\"] = rank_to_dict(mic_scores, names)\n\n\nr = {}\nfor name in names:\n    r[name] = round(np.mean([ranks[method][name] \n                             for method in ranks.keys()]), 2)\n\nmethods = sorted(ranks.keys())\nranks[\"Mean\"] = r\nmethods.append(\"Mean\")\n\nprint \"\\t%s\" % \"\\t\".join(methods)\nfor name in names:\n    print \"%s\\t%s\" % (name, \"\\t\".join(map(str, \n                         [ranks[method][name] for method in methods])))\n\n```\n\n[<img src=\"/images/5ybQKSP.png\" alt=\"\" style=\"border: 0px; vertical-align: middle; max-width: 100%; height: auto;\" />](/images/5ybQKSP.png)从以上结果中可以找到一些有趣的发现：\n\n特征之间存在线性关联关系，每个特征都是独立评价的，因此X1,&hellip;X4的得分和X11,&hellip;X14的得分非常接近，而噪音特征X5,&hellip;,X10正如预期的那样和响应变量之间几乎没有关系。由于变量X3是二次的，因此X3和响应变量之间看不出有关系（除了MIC之外，其他方法都找不到关系）。这种方法能够衡量出特征和响应变量之间的线性关系，但若想选出优质特征来提升模型的泛化能力，这种方法就不是特别给力了，因为所有的优质特征都不可避免的会被挑出来两次。\n\nLasso能够挑出一些优质特征，同时让其他特征的系数趋于0。当如需要减少特征数的时候它很有用，但是对于数据理解来说不是很好用。（例如在结果表中，X11,X12,X13的得分都是0，好像他们跟输出变量之间没有很强的联系，但实际上不是这样的）\n\nMIC对特征一视同仁，这一点上和关联系数有点像，另外，它能够找出X3和响应变量之间的非线性关系。\n\n随机森林基于不纯度的排序结果非常鲜明，在得分最高的几个特征之后的特征，得分急剧的下降。从表中可以看到，得分第三的特征比第一的小4倍。而其他的特征选择算法就没有下降的这么剧烈。\n\nRidge将回归系数均匀的分摊到各个关联变量上，从表中可以看出，X11,&hellip;,X14和X1,&hellip;,X4的得分非常接近。\n\n稳定性选择常常是一种既能够有助于理解数据又能够挑出优质特征的这种选择，在结果表中就能很好的看出。像Lasso一样，它能找到那些性能比较好的特征（X1，X2，X4，X5），同时，与这些特征关联度很强的变量也得到了较高的得分\n\n### 总结\n\n1. 对于理解数据、数据的结构、特点来说，单变量特征选择是个非常好的选择。尽管可以用它对特征进行排序来优化模型，但由于它不能发现冗余（例如假如一个特征子集，其中的特征之间具有很强的关联，那么从中选择最优的特征时就很难考虑到冗余的问题）。\n1. 正则化的线性模型对于特征理解和特征选择来说是非常强大的工具。L1正则化能够生成稀疏的模型，对于选择特征子集来说非常有用；相比起L1正则化，L2正则化的表现更加稳定，由于有用的特征往往对应系数非零，因此L2正则化对于数据的理解来说很合适。由于响应变量和特征之间往往是非线性关系，可以采用basis expansion的方式将特征转换到一个更加合适的空间当中，在此基础上再考虑运用简单的线性模型。\n1. 随机森林是一种非常流行的特征选择方法，它易于使用，一般不需要feature engineering、调参等繁琐的步骤，并且很多工具包都提供了平均不纯度下降方法。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。尽管如此，这种方法仍然非常值得在你的应用中试一试。\n1. 特征选择在很多机器学习和数据挖掘场景中都是非常有用的。在使用的时候要弄清楚自己的目标是什么，然后找到哪种方法适用于自己的任务。当选择最优特征以提升模型性能的时候，可以采用交叉验证的方法来验证某种方法是否比其他方法要好。当用特征选择的方法来理解数据的时候要留心，特征选择模型的稳定性非常重要，稳定性差的模型很容易就会导致错误的结论。对数据进行二次采样然后在子集上运行特征选择算法能够有所帮助，如果在各个子集上的结果是一致的，那就可以说在这个数据集上得出来的结论是可信的，可以用这种特征选择模型的结果来理解数据\n\n### Tips\n\n什么是[卡方检验](http://en.wikipedia.org/wiki/Chi-square_test)？用方差来衡量某个观测频率和理论频率之间差异性的方法\n\n什么是[皮尔森卡方检验](http://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test)？这是一种最常用的卡方检验方法，它有两个用途：1是计算某个变量对某种分布的拟合程度，2是根据两个观测变量的[Contingency table](http://en.wikipedia.org/wiki/Contingency_table)来计算这两个变量是否是独立的。主要有三个步骤：第一步用方差和的方式来计算观测频率和理论频率之间卡方值；第二步算出卡方检验的自由度（行数-1乘以列数-1）；第三步比较卡方值和对应自由度的卡方分布，判断显著性。\n\n什么是[p-value](http://en.wikipedia.org/wiki/P-value)？简单地说，p-value就是为了验证假设和实际之间一致性的统计学意义的值，即假设检验。有些地方叫右尾概率，根据卡方值和自由度可以算出一个固定的p-value，\n\n什么是[响应变量(response value)](http://www.answers.com/Q/What_is_a_response_variable)？简单地说，模型的输入叫做explanatroy variables，模型的输出叫做response variables，其实就是要验证该特征对结果造成了什么样的影响\n\n什么是[统计能力(statistical power)](http://en.wikipedia.org/wiki/Statistical_power)?\n\n什么是[度量(metric)](http://en.wikipedia.org/wiki/Metric_%28mathematics%29)?\n\n什么是[零假设(null hypothesis)](http://zh.wikipedia.org/wiki/%E9%9B%B6%E5%81%87%E8%AE%BE)?在相关性检验中，一般会取&ldquo;两者之间无关联&rdquo;作为零假设，而在独立性检验中，一般会取&ldquo;两者之间是独立&rdquo;作为零假设。与零假设相对的是备择假设（对立假设），即希望证明是正确的另一种可能。\n","tags":["Python"]},{"title":"机器学习——预测数值型数据：回归","url":"/机器学习——预测数值型数据：回归.html","content":"**线性回归**\n\n**优点**：结果易于理解，计算上不复杂\n\n**缺点**：对非线性的数据拟合不好\n\n**适用数据类型**：数值型和标称型数据\n\n<!--more-->\n&nbsp;\n\n**回归**的目的就预测数值型的目标值。最直接的办法就是依据输入写一个目标值的计算公式。这个计算公式就是所谓的**回归方程(regression equation)**，其中的参数就是**回归系数**，求这些回归系数的过程就是**回归**。\n\n　　说道回归，一般都是指**线性回归（linear regression）**。\n\n<img src=\"/images/517519-20161221222849823-1784959469.png\" alt=\"\" />\n\n给定由**d个属性**描述的示例 <img src=\"/images/517519-20161222103443354-1906476301.gif\" alt=\"\" />，其中xi是x在第i个属性上的取值，**线性模型**试图学得一个通过属性组合来进行预测的函数，即\n\n**　　<img src=\"/images/517519-20161222100919667-1830444903.gif\" alt=\"\" />**\n\n&nbsp;\n\n一般用**向量形式**写成\n\n**　　<img src=\"/images/517519-20161222101054589-1373333619.gif\" alt=\"\" />**\n\n其中w={w1;w2;...;wd}，w和b学得之后，模型就得以确定。\n\n&nbsp;\n\n**更加一般的情况**是，给定一个数据集** <img src=\"/images/517519-20161222103244667-1737939947.gif\" alt=\"\" />**，其中 <img src=\"/images/517519-20161222103358261-467512695.gif\" alt=\"\" />，&ldquo;线性回归&rdquo;试图学得一个线性模型以尽可能准确地预测实值输出标记。\n\n**用<strong>向量形式**写成</strong>\n\n**　　<img src=\"/images/517519-20161222103700620-1437669554.gif\" alt=\"\" />，这称为&ldquo;多元线性回归&rdquo;**\n\n&nbsp;\n\n因为是**d维的特性**，那么w就是一个由回归系数组成的**d&times;1维向量**，X**是<strong>n&times;（d+1）的矩阵（第一列元素都是1，其余列都是x1...xn）**</strong>，<img src=\"/images/517519-20161222154606042-1079480036.gif\" alt=\"\" />是**（d+1）&times;1的向量**\n\n　　<img src=\"/images/517519-20161222154503401-367206260.gif\" alt=\"\" />\n\n此时，**平方误差**（由于误差有正有负，所以不能直接相加）可以表示成\n\n　　<img src=\"/images/517519-20161222105334667-1798676736.gif\" alt=\"\" />，其中&nbsp;<img src=\"/images/517519-20161222154747761-229692827.gif\" alt=\"\" />\n\n需要使得**平方误差最小**，就需要对其求导，并另求导后的式子等于0，求出\n\n　　<img src=\"/images/517519-20161222105559823-1133499358.gif\" alt=\"\" />\n\n而这个使得**平方误差最小**的算法就称为**普通最小二乘法（OLS）<br />**\n\n&nbsp;\n\n&nbsp;\n\n```\nfrom numpy import *\n\ndef loadDataSet(fileName):      #general function to parse tab -delimited floats\n    numFeat = len(open(fileName).readline().split('\\t')) - 1 \t#取得特征的数量\n    dataMat = []; labelMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        lineArr =[]\n        curLine = line.strip().split('\\t')\n        for i in range(numFeat):\n            lineArr.append(float(curLine[i]))\n        dataMat.append(lineArr)\n        labelMat.append(float(curLine[-1]))\n    return dataMat,labelMat\n\ndef standRegres(xArr,yArr):\t\t\t#根据公式解出xArr的2维回归系数\n    xMat = mat(xArr); yMat = mat(yArr).T\n    xTx = xMat.T*xMat\n    if linalg.det(xTx) == 0.0:\t\t#判断方阵的行列式是否为零，如果为零则伴随矩阵将会除以零\n        print \"This matrix is singular, cannot do inverse\"\n        return\n#\tws = linalg.inv(xTx) * (xMat.T*yMat)\n    ws = xTx.I * (xMat.T*yMat)\t\t#.I求自身逆矩阵，.T返回自身的转置，.H返回自身的共轭转置，.A返回自身数据的2维数组的一个视图\n    return ws\n\n```\n\n&nbsp;\n\n&nbsp;mian.py\n\n```\n# coding:utf-8\n# !/usr/bin/env python\n\nimport regression\nimport matplotlib.pyplot as plt\nfrom numpy import *\n\nif __name__ == '__main__':\n\txArr,yArr = regression.loadDataSet(\"ex0.txt\")\n\tws = regression.standRegres(xArr,yArr)\n\tprint ws\t\t\t#输出回归系数\n\txMat = mat(xArr)\t#输入的X矩阵，n&times;(d+1)维\n\tyMat = mat(yArr)\t#真实的标记，n&times;1维\n\tyHat = xMat*ws\t\t#预测的结果\n\tfig = plt.figure()\n\tax = fig.add_subplot(111)\n\tax.scatter(xMat[:,1].flatten().A[0],yMat.T[:,0].flatten().A[0])\t#取得矩阵的第二列并转换成list\n\txCopy = xMat.copy()\n\txCopy.sort(0)\t\t#参数0表示从小到大，1表示从大到小，排序\n\tyHat = xCopy*ws\n\tax.plot(xCopy[:,1],yHat)\n\tplt.show()\n\tprint corrcoef(yHat.T,yMat)\t#计算相关系数\n\n```\n\n&nbsp;\n\n最佳拟合直线方法将数据视为直线进行建模，具有不错的表现。但是还可以根据数据来进行**局部调整预测**。\n\n**局部加权线性回归**\n\n　　线性回归的一个问题是有可能**出现欠拟合现象**，因为它求的是**具有最小均方误差的无偏估计**。显而易见，如果模型欠拟合将不能取得最好的预测效果。所以将有些方法允许在估计中**引入一些偏差**，从而**降低预测的均方误差**。\n\n&nbsp;　　其中的一个方法是**局部加权线性回归**（Locally Weighted Linear Regression,**LWLR**）。在该算法中，给待遇测点附件的每一个点**赋予一定的权重**，然后在这个子集上基于最小均方误差来进行普通的回归。与kNN一样，这种算法每次预测均需要事先选取出对应的数据子集。该算法解出回归系数w的形式如下：\n\n　　<img src=\"/images/517519-20161222171911167-540100570.gif\" alt=\"\" />\n\n其中，w是一个矩阵，用来给每个数据点**赋予权重**。\n\n　　**LWLR**使用的核类型可以自由选择，最常用的核就是**高斯核（RBF核）**，高斯核对应的权重如下：\n\n　　<img src=\"/images/517519-20161222172503589-122060246.gif\" alt=\"\" />\n\n&nbsp;\n\n```\ndef lwlr(testPoint,xArr,yArr,k=1.0):\t\t#局部加权线性回归函数\n\txMat = mat(xArr); yMat = mat(yArr).T\n\tm = shape(xMat)[0]\n\tweights = mat(eye((m)))\t\t\t\t\t#创建m维对角矩阵\n\tfor j in range(m):\n\t\tdiffMat = testPoint - xMat[j,:]\t\t#testPoint分别减去每一行xMat，testPoint也将是每一行的xMat\n\t\tweights[j,j] = exp(diffMat*diffMat.T/(-2.0*k**2))\t#权重值大小以指数级衰减，weights仍然只有对角线上有值\n\txTx = xMat.T * (weights * xMat)\n\tif linalg.det(xTx) == 0.0:\n\t\tprint \"This matrix is singular, cannot do inverse\"\n\t\treturn\n\tws = xTx.I * (xMat.T * (weights * yMat))\n\treturn testPoint * ws\t\t\t\t\t#直接返回拟合出来的值，每一个样本的回归系数都不一样\n\ndef lwlrTest(testArr,xArr,yArr,k=1.0):\t\t\t\t#循环所有的样本，对其使用lwlr函数\n    m = shape(testArr)[0]\n    yHat = zeros(m)\n    for i in range(m):\n        yHat[i] = lwlr(testArr[i],xArr,yArr,k)\n    return yHat\n\n```\n\nmain.py\n\n```\n# coding:utf-8\n# !/usr/bin/env python\n\nimport regression\nimport matplotlib.pyplot as plt\nfrom numpy import *\n\nif __name__ == '__main__':\n\txArr,yArr = regression.loadDataSet(\"ex0.txt\")\n\t\n\txMat = mat(xArr)\t#输入的X矩阵，n&times;(d+1)维\n\tyMat = mat(yArr)\t#真实的标记，n&times;1维\n\tyHat = regression.lwlrTest(xArr,xArr,yArr,k=0.01)\n\tsrtInd = xMat[:,1].argsort(0)\n\txSort = xMat[srtInd][:,0,:]\n\tfig = plt.figure()\n\tax = fig.add_subplot(111)\n\tax.scatter(xMat[:,1].flatten().A[0],yMat.T[:,0].flatten().A[0],s=5,c='red')\t#取得矩阵的第二列并转换成list\n\tax.plot(xSort[:,1],yHat[srtInd])\n\tplt.show()\n\n```\n\n&nbsp;<img src=\"/images/517519-20161222224447198-1097088193.png\" alt=\"\" width=\"445\" height=\"378\" />\n\n&nbsp;\n\n**缩减系数（shrinking）**\n\n如果**数据的特征比样本点还多**的话，那么就不能使用线性回归和之前的方法来做预测了。因为在计算 <img src=\"/images/517519-20161222225449182-2046957053.gif\" alt=\"\" /> 的时候会出错。\n\n如果特征比样本点还多（n>m），也就是说输入数据的矩阵x**不是满秩矩阵**。非满秩矩阵在**求逆**的时候会出现问题。\n\n为了解决这个问题，引入了**岭回归（ridge regression）**的概念，这是一种**缩减方法**。\n\n简单地说，**岭回归**就是在矩阵 <img src=\"/images/517519-20161222230026854-1111464627.gif\" alt=\"\" /> 上加一个 <img src=\"/images/517519-20161222230056589-539761205.gif\" alt=\"\" /> 从而使得矩阵**非奇异**，进而能对 <img src=\"/images/517519-20161222230132104-539074521.gif\" alt=\"\" /> 求逆。其中矩阵 I 是一个m&times;m的单位矩阵，对角线上元素全为1，其他元素全为0。在这种情况下，**回归系数**的计算公式将变成：\n\n　　<img src=\"/images/517519-20161222230410292-1823656465.gif\" alt=\"\" />\n\n岭回归最先用来处理**特征数多于样本数**的情况，现在也用于**在估计中加入偏差**，从而得到更好的估计。这里通过引入 <img src=\"/images/517519-20161222231354214-1156853578.gif\" alt=\"\" /> 来限制了所有 w 之和，通过引入该**惩罚项**，能够减少不重要的参数，这个技术在统计学中称为**缩减（shrinkage）**。\n\n缩减方法可以去掉不重要的参数，因此能更好地理解数据。此外，与简单的线性回归相比，缩减法能够取得更好的预测效果。\n\n&nbsp;\n\n```\ndef ridgeRegres(xMat,yMat,lam=0.2):\t\t\t#岭回归，计算回归系数，&lambda;默认等于0.2\n    xTx = xMat.T*xMat\n    denom = xTx + eye(shape(xMat)[1])*lam\n    if linalg.det(denom) == 0.0:\t\t\t#防止&lambda;等于0\n        print \"This matrix is singular, cannot do inverse\"\n        return\n    ws = denom.I * (xMat.T*yMat)\n    return ws\n    \ndef ridgeTest(xArr,yArr):\t\t\t#用于在一组不同的&lambda;上分别测试结果，返回&lambda;和8个特征值的关系曲线\n    xMat = mat(xArr); yMat=mat(yArr).T\n    yMean = mean(yMat,0)\t\t\t#求yMat的均值\n    yMat = yMat - yMean\n    #regularize X's\n    xMeans = mean(xMat,0)   \t\t#求xMat的均值\n    xVar = var(xMat,0)      \t\t#求xMat的方差\n    xMat = (xMat - xMeans)/xVar\t\t#数据归一化\n    numTestPts = 30\t\t\t\t\t#30个&lambda;的取值\n    wMat = zeros((numTestPts,shape(xMat)[1]))\t#shape(xMat)[1]表示特征的数量\n    for i in range(numTestPts):\n        ws = ridgeRegres(xMat,yMat,exp(i-10))\n        wMat[i,:]=ws.T\n    return wMat\n\n```\n\n&nbsp;mian.py\n\n```\n# coding:utf-8\n# !/usr/bin/env python\n\nimport regression\nimport matplotlib.pyplot as plt\nfrom numpy import *\n\nif __name__ == '__main__':\n\tabX,abY = regression.loadDataSet(\"abalone.txt\")\n\tridgeWeights = regression.ridgeTest(abX,abY)\n\tfig = plt.figure()\n\tax = fig.add_subplot(111)\n\tax.plot(ridgeWeights)\n\tplt.show()\n\t\n\t\n\n```\n\n&nbsp;\n\n**30个不同的log(&lambda;)**所对应的回归系数。**在最左边**，即&lambda;最小的时候，可以得到所有系数的原始值（与线性回归一致）；**而在右边**，系数全部缩减到0，缩减了一些不重要的参数；**在中间部分**的某值将可以取得最好的预测效果。为了找到最佳参数值，还需要进行交叉验证。\n\n下图是岭回归的回归系数变化图。可以在中间某处找到使得预测的结果最好的&lambda;值。\n\n<img src=\"/images/517519-20161223104536495-1593681521.png\" alt=\"\" width=\"425\" height=\"364\" />\n\n还有一些其他缩减方法，如lasso、LAR、PCA回归以及子集选择等。与岭回归一样，这样方法不仅可以提高预测精确率，而且可以解释回归系数。\n\n&nbsp;\n\n**lasso方法**\n\n在增加下面约束的条件下，**普通最小二乘法回归（OLS）**将会得到与**岭回归**一样的公式：\n\n　　<img src=\"/images/517519-20161223105526745-1319488180.gif\" alt=\"\" />\n\n在使用普通的最下二乘法回归在发那个两个或者更多的特征相关的时候，可能会得到一个很大的正系数和一个很大的负系数。正是因为上述限制条件的存在，使用岭回归可以便面这个问题。\n\n**lasso方法**中也对系数进行了限定，对应的约束条件如下：\n\n　　<img src=\"/images/517519-20161223105842667-2011833497.gif\" alt=\"\" />\n\n&nbsp;\n\n**前向逐步回归**\n\n前向逐步回归算法可以得到和lasso算法差不多的效果，但是更加简单。它属于一种贪心算法，即每一步都尽可能减少误差。一开始，所有的权重都设为1，然后每一步所做的决策是对某个权重增加或者减少一个很小的值。\n\n&nbsp;\n\n```\ndef regularize(xMat):\t\t\t#按均值为0方差为1进行标准化处理\n    inMat = xMat.copy()\n    inMeans = mean(inMat,0)   \t#calc mean then subtract it off\n    inVar = var(inMat,0)      \t#calc variance of Xi then divide by it\n    inMat = (inMat - inMeans)/inVar\n    return inMat\n\ndef stageWise(xArr,yArr,eps=0.01,numIt=100):\t#前向逐步线性回归，eps表示每次迭代需要调整的步长，numIt表示迭代次数\n    xMat = mat(xArr); yMat=mat(yArr).T\n    yMean = mean(yMat,0)\n    yMat = yMat - yMean     \t\t#can also regularize ys but will get smaller coef\n    xMat = regularize(xMat)\t\t\t#把特征进行标准化\n    m,n=shape(xMat)\t\t\t\t\t#m是数据数量，n是特征数量\n    returnMat = zeros((numIt,n)) \t#testing code remove\n    ws = zeros((n,1)); wsTest = ws.copy(); wsMax = ws.copy()\n    for i in range(numIt):\t\t\t#迭代100次，每一次迭代只以步进的长度修改一个特征值\n        #print ws.T\n        lowestError = inf; \n        for j in range(n):\t\t\t#循环所有特征，最后找到步进后均方误差最小的保存wsMax\n            for sign in [-1,1]:\t\t#步进的正负\n                wsTest = ws.copy()\n                wsTest[j] += eps*sign\n                yTest = xMat*wsTest\n                rssE = rssError(yMat.A,yTest.A)\t\t#求均方误差\n                if rssE < lowestError:\t\t\t\t#比较\n                    lowestError = rssE\n                    wsMax = wsTest\t\t\t\t\t#保存均方误差最小的wsMax\n        ws = wsMax.copy()\n        returnMat[i,:]=ws.T\t\t\t#保存回归系数ws的整个修改过程\n    return returnMat\n\n```\n\n&nbsp;\n\nmian.py\n\n```\n# coding:utf-8\n# !/usr/bin/env python\n\nimport regression\nimport Old_regression\nimport matplotlib.pyplot as plt\nfrom numpy import *\n\nif __name__ == '__main__':\n\tabX,abY = regression.loadDataSet(\"abalone.txt\")\n\treturnMat = Old_regression.stageWise(abX,abY)\n\tprint returnMat\n\n```\n\n&nbsp;<img src=\"/images/517519-20161223160645542-1873914217.png\" alt=\"\" />\n\n&nbsp;\n\n<img src=\"/images/517519-20161224202157854-1794784665.png\" alt=\"\" />\n","tags":["ML"]},{"title":"机器学习——非均衡分类问题","url":"/机器学习——非均衡分类问题.html","content":"在机器学习的分类问题中，我们都假设所有类别的**分类代价**是一样的。但是事实上，不同分类的代价是不一样的，比如我们通过一个用于检测患病的系统来检测马匹是否能继续存活，如果我们把能存活的马匹检测成患病，那么这匹马可能就会被执行安乐死；如果我们把不能存活的马匹检测成健康，那么就会继续喂养这匹马。一个代价是错杀一只昂贵的动物，一个代价是继续喂养，很明显这**两个代价是不一样的**。\n\n## **1.性能度量**\n\n衡量模型泛化能力的评价标准，就是**性能度量**。除了基于**错误率**来衡量分类器任务的成功程度的。**错误率**指的是在所有测试样例中错分的样例比例。但是，这样却掩盖了样例**如何被错分的事实**。在机器学习中，有一个普遍试用的称为**混淆矩阵（confusion matrix）**的工具，可以帮助人们更好地了解分类的错误。\n\n<img src=\"/images/517519-20161221171100307-1558715339.png\" alt=\"\" width=\"573\" height=\"131\" />\n\n利用混淆矩阵就可以更好地理解分类中的错误了。如果矩阵中的**非对角元素均为0**，就会得到一个**完美的分类器**。\n\n## **2.正确率（Precision）、召回率（Recall）**\n\n<img src=\"/images/517519-20161221171346948-1136185734.png\" alt=\"\" width=\"603\" height=\"115\" />\n\n**正确率P** = TP/（TP+FP），给出的是**预测为正例**的样本中的**真正正例**的比例。\n\n**召回率R** = TP/（TP+FN），给出的是**预测为正例的真实正例**占**所有真实正例**的比例。\n\n## 3.**ROC曲线**\n\n另一个用于度量分类中的非均衡性的工具是**ROC曲线**（ROC curve），ROC代表**接收者操作特征\"Receiver Operating Characteristic\"**\n\n**<img src=\"/images/517519-20161221200027948-1498850195.png\" alt=\"\" width=\"329\" height=\"294\" />**\n\nROC曲线的**纵轴**是&ldquo;**真正例率**&rdquo;，TPR=TP/（TP+FN）\n\n**横轴**是&ldquo;**假正例率**&rdquo;，FPR=FP/（TN+FP）\n\n在理想的情况下，最佳的分类器应该尽可能地处于左上角，这就意味着分类器在**假正例率很低**的同时，获得了**很高的真正例率**。\n\n## 4.**AUC**（**曲线下的面积）**\n\n对不同的ROC曲线进行比较的一个指标就是**曲线下的面积（AUC）**，AUC给出的是分类器的平均性能值。一个完美的分类器的AUC是1，而随机猜测的AUC则为0.5。\n\n若一个学习器的ROC曲线能把另一个学习器的ROC曲线完全包住，则这个学习器的性能比较好。\n\n**为什么要使用AUC曲线**\n\n因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。AUC对样本的比例变化有一定的容忍性。AUC的值通常在0.6-0.85之间。\n\n<!--more-->\n&nbsp;\n\n```\ndef plotROC(predStrengths, classLabels):\t\t#ROC曲线的绘制及AUC计算函数\n    import matplotlib.pyplot as plt\n    cur = (1.0,1.0) #cursor\n    ySum = 0.0 #variable to calculate AUC\n    numPosClas = sum(array(classLabels)==1.0)\n    yStep = 1/float(numPosClas); xStep = 1/float(len(classLabels)-numPosClas)\n    sortedIndicies = predStrengths.argsort()#get sorted index, it's reverse\n    fig = plt.figure()\n    fig.clf()\n    ax = plt.subplot(111)\n    #loop through all the values, drawing a line segment at each point\n    for index in sortedIndicies.tolist()[0]:\n        if classLabels[index] == 1.0:\n            delX = 0; delY = yStep;\n        else:\n            delX = xStep; delY = 0;\n            ySum += cur[1]\n        #draw line from cur to (cur[0]-delX,cur[1]-delY)\n        ax.plot([cur[0],cur[0]-delX],[cur[1],cur[1]-delY], c='b')\n        cur = (cur[0]-delX,cur[1]-delY)\n    ax.plot([0,1],[0,1],'b--')\n    plt.xlabel('False positive rate'); plt.ylabel('True positive rate')\n    plt.title('ROC curve for AdaBoost horse colic detection system')\n    ax.axis([0,1,0,1])\n    plt.show()\n    print \"the Area Under the Curve is: \",ySum*xStep\n\n```\n\n&nbsp;mian.py\n\n```\n# coding:utf-8\n# !/usr/bin/env python\n\nimport adaboost\n\nif __name__ == '__main__':\n\tdatMat,classLabels = adaboost.loadDataSet(\"horseColicTraining2.txt\")\n\tweakClassArr,aggClassEst = adaboost.adaBoostTrainDS(datMat,classLabels,40)\n#\tdatMat1,classLabels1 = adaboost.loadDataSet(\"horseColicTest2.txt\")\n\taggClassEst,sign = adaboost.adaClassify(datMat,weakClassArr)\n\tprint aggClassEst.T\n\tadaboost.plotROC(aggClassEst.T, classLabels)\n\n```\n\n&nbsp;<img src=\"/images/517519-20161221210648932-1909790489.png\" alt=\"\" width=\"402\" height=\"343\" />\n\n&nbsp;\n\n基于**代价函数**的分类器决策控制\n\n为权衡不同类型错误所造成的不同损失，可为错误赋予&ldquo;非均等代价&rdquo;。\n\n<img src=\"/images/517519-20161221211346964-1308357773.png\" alt=\"\" width=\"540\" height=\"195\" />\n\n在&ldquo;**代价矩阵**&rdquo;中，将-1错判成+1的代价（50），比把+1错判成-1的代价（1）要高。\n\n&nbsp;\n\n**处理非均衡问题的数据抽样方法**\n\n另外一种针对非均衡问题调节分类器的方法，就是对分类器的训练数据进行改造。这可以通过**欠抽样**或者**过抽样**来实现。\n\n**过抽样**意味着复制样例，而**欠抽样**意味着删除样例。\n","tags":["ML"]},{"title":"git命令","url":"/git命令.html","content":"cd到需要git的目录\n\n初始化git仓库\n\n```\ngit init\ngit remote add origin git@github.com:tonglin0325/XXX.git\n\n```\n\n新建分支\n\n```\ngit checkout -b testing\n\n```\n\n添加并转到testing分支，不要直接在master分支上操作\n\n```\ngit branch -d testing\n\n```\n\n<!--more-->\n&nbsp;\n\n撤销一次commit\n\n```\ngit reset --soft HEAD^\n\n```\n\n撤销2次commit\n\n```\ngit reset --soft HEAD~2\n\n```\n\n&nbsp;\n\n```\ngit add .　　　　//先add需要添加的文件，然后再git commit -a\ngit status -s\ngit commit -m '注释'　　//撤销commit的方法 git reset HEAD~\n\n如果要把当前版本的commi回退到上一个版本,然后这个版本所做的修改全部回到上一个版本\ngit reset --hard HEAD^\n现在HEAD就会会回到上一个版本(可以用在误删文件的时候)\n\n然后可以\n  （使用 \"git add/rm <文件>...\" 更新要提交的内容）\n  （使用 \"git checkout -- <文件>...\" 丢弃工作区的改动）\n\n1.git reset HEAD~ 回到上一个版本\n2.git checkout -- file 丢弃工作区的修改,\n　　没有add的时候,回到和库版本相同;\n　　add之后,从add之后的修改回到add时候的版本\n3.对于错误的add,使用git reset HEAD file将暂存区的修改重新放回工作区\n\n\n如果要回到刚刚的那个版本就需要使用git reflog来查看那个版本的ID,然后\ngit reset -- hard XXXXX\n\n如果有不需要添加的文件的话，就一个一个add进去，然后\ngit commit -a\n//如果想取消commit的话\ngit log查看commit的ID，然后git reset --soft commit-id\n//code review\narc diff\n//如果出现乱码直接保存退出后在网页中添加reviewer和scriber信息\n\ngit push -u origin master\n\n```\n\n&nbsp;\n\n从缓存区移除文件，保留本地文件\n\n```\ngit rm --cached XXX\n\n```\n\n&nbsp;\n\n拉取一个remote分支\n\n```\ngit checkout xxx\ngit pull origin xxx\n\n```\n\n　　\n\n&nbsp;\n\ngit忽略生成的class这些路径的方法，在.gitignore文件中添加\n\n```\n.DS_Store\n*.tgz\n*.zip\n\n.idea\ntarget\n.classpath\n.sbtserver*\ntarget\nlib_managed\n\ntestdata\n\n```\n\n&nbsp;或者\n\n```\n.idea\n*.iws\n*.iml\natlassian-ide-plugin.xml\ntarget/\nlib_managed/\nsrc_managed/\nproject/boot/\n.history\n.cache\ndependency-reduced-pom.xml\n\n```\n\n&nbsp;\n\ngit submodule\n\n初始化\n\n```\ngit submodule init \n\n```\n\n更新\n\n```\ngit submodule update --remote\n\n```\n\n&nbsp;add submodule\n\n```\ngit submodule add -b master git@github.com:xxxx/xxxx.git lib/xxxx\n\n```\n\n　　\n\n将一个现有的目录添加到一个github仓库中\n\n```\ngit remote add origin git@github.com:xxx/xxx.git\ngit branch -M main\ngit push -u origin main\n\n```\n\n　　\n\n其他文档：[Git版本控制软件结合GitHub从入门到精通常用命令学习手册](https://www.cnblogs.com/loong-hon/p/5905072.html)\n","tags":["Git"]},{"title":"网络抓包wireshark（转）","url":"/网络抓包wireshark（转）.html","content":"## ubuntu下非root用户下获得使用wireshark的权限\n\n在非root用户下不能使用wireshark用来抓包，所以需要进行以下操作：\n\n```\nsudo groupadd  wireshark\nsudo chgrp wireshark /usr/bin/dumpcap\nsudo chmod 4755 /usr/bin/dumpcap\nsudo gpasswd -a common wireshark\n\n```\n\n抓包应该是每个技术人员掌握的基础知识，无论是技术支持运维人员或者是研发，多少都会遇到要抓包的情况，用过的抓包工具有fiddle、wireshark，作为一个不是经常要抓包的人员，学会用Wireshark就够了，毕竟它是功能最全面使用者最多的抓包工具。\n\nWireshark（前称Ethereal）是一个网络封包分析软件。网络封包分析软件的功能是撷取网络封包，并尽可能显示出最为详细的网络封包资料。Wireshark使用WinPCAP作为接口，直接与网卡进行数据报文交换。\n\nwireshark的官方下载网站： [http://www.wireshark.org/](http://www.wireshark.org/)\n\nwireshark是非常流行的网络封包分析软件，功能十分强大。可以截取各种网络封包，显示网络封包的详细信息。\n\nwireshark是开源软件，可以放心使用。 可以运行在Windows和Mac OS上。\n\n## Wireshark不能做的\n\n为了安全考虑，wireshark只能查看封包，而不能修改封包的内容，或者发送封包。\n\n## Wireshark VS Fiddler\n\nwireshark能获取HTTP，也能获取HTTPS，但是不能解密HTTPS，所以wireshark看不懂HTTPS中的内容\n\n## 同类的其他工具\n\nsniffer \n\n## 什么人会用到wireshark\n\n2. 软件测试工程师使用wireshark抓包，来分析自己测试的软件\n\n3. 从事socket编程的工程师会用wireshark来调试\n\n4. 听说，华为，中兴的大部分工程师都会用到wireshark。\n\n总之跟网络相关的东西，都可能会用到wireshark.\n\n## wireshark 开始抓包\n\n开始界面\n\n<img src=\"/images/150046-20160802202001059-707610366.png\" alt=\"\" />\n\nwireshark是捕获机器上的某一块网卡的网络包，当你的机器上有多块网卡的时候，你需要选择一个网卡。\n\n点击Caputre->Interfaces.. 出现下面对话框，选择正确的网卡。然后点击\"Start\"按钮, 开始抓包\n\n<img src=\"/images/150046-20160802202001903-227653544.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n## Wireshark 窗口介绍\n\n<img src=\"/images/150046-20160802202003372-291503522.png\" alt=\"\" />\n\n&nbsp;\n\nWireShark 主要分为这几个界面\n\n1. Display Filter(显示过滤器)，&nbsp; 用于过滤\n\n2. Packet List Pane(封包列表)， 显示捕获到的封包， 有源地址和目标地址，端口号。 颜色不同，代表\n\n3. Packet Details Pane(封包详细信息), 显示封包中的字段\n\n4. Dissector Pane(16进制数据)\n\n5. Miscellanous(地址栏，杂项)\n\n&nbsp;\n\n## Wireshark 显示过滤&nbsp;\n\n<img src=\"/images/150046-20160802202004372-179986181.png\" alt=\"\" />\n\n使用过滤是非常重要的， 初学者使用wireshark时，将会得到大量的冗余信息，在几千甚至几万条记录中，以至于很难找到自己需要的部分。搞得晕头转向。\n\n过滤器会帮助我们在大量的数据中迅速找到我们需要的信息。\n\n过滤器有两种，\n\n一种是显示过滤器，就是主界面上那个，用来在捕获的记录中找到所需要的记录\n\n一种是捕获过滤器，用来过滤捕获的封包，以免捕获太多的记录。 在Capture -> Capture Filters 中设置\n\n&nbsp;\n\n## 保存过滤\n\n在Filter栏上，填好Filter的表达式后，点击Save按钮， 取个名字。比如\"Filter 102\",\n\n<img src=\"/images/150046-20160802202004840-367195657.png\" alt=\"\" />\n\nFilter栏上就多了个\"Filter 102\" 的按钮。\n\n<img src=\"/images/150046-20160802202005497-535988035.png\" alt=\"\" />\n\n## 过滤表达式的规则\n\n表达式规则\n\n&nbsp;1. 协议过滤\n\n比如TCP，只显示TCP协议。\n\n2. IP 过滤\n\n比如 ip.src ==192.168.1.102 显示源地址为192.168.1.102，\n\nip.dst==192.168.1.102, 目标地址为192.168.1.102\n\n3. 端口过滤\n\ntcp.port ==80,&nbsp; 端口为80的\n\ntcp.srcport == 80,&nbsp; 只显示TCP协议的愿端口为80的。\n\n4. Http模式过滤\n\nhttp.request.method==\"GET\",&nbsp;&nbsp; 只显示HTTP GET方法的。\n\n5. 逻辑运算符为 AND/ OR\n\n常用的过滤表达式\n|过滤表达式|用途\n|http|只查看HTTP协议的记录\n|ip.src ==192.168.1.102 or ip.dst==192.168.1.102|&nbsp;源地址或者目标地址是192.168.1.102\n|&nbsp;|&nbsp;\n|&nbsp;|&nbsp;\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n\n## 封包列表(Packet List Pane)\n\n你也可以修改这些显示颜色的规则，&nbsp; View ->Coloring Rules.\n\n<img src=\"/images/150046-20160802202006247-1266474477.png\" alt=\"\" />\n\n## 封包详细信息 (Packet Details Pane)\n\n各行信息分别为\n\nFrame:&nbsp;&nbsp; 物理层的数据帧概况\n\nEthernet II: 数据链路层以太网帧头部信息\n\nInternet Protocol Version 4: 互联网层IP包头部信息<br />\n\nTransmission Control Protocol:&nbsp; 传输层T的数据段头部信息，此处是TCP<br />\n\nHypertext Transfer Protocol:&nbsp; 应用层的信息，此处是HTTP协议<br />\n\n## wireshark与对应的OSI七层模型\n\n<img src=\"/images/150046-20160802202006918-885956603.png\" alt=\"\" />\n\n## TCP包的具体内容\n\n从下图可以看到wireshark捕获到的TCP包中的每个字段。\n\n&nbsp;\n\n<img src=\"/images/150046-20160802202008372-1811385287.png\" alt=\"\" />\n\n&nbsp;\n\n## 实例分析TCP三次握手过程\n\n看到这， 基本上对wireshak有了初步了解， 现在我们看一个TCP三次握手的实例\n\n&nbsp;\n\n&nbsp;三次握手过程为\n\n<img src=\"/images/150046-20160802202009309-1996784026.png\" alt=\"\" />\n\n&nbsp;\n\n这图我都看过很多遍了， 这次我们用wireshark实际分析下三次握手的过程。\n\n打开wireshark, 打开浏览器输入 <a>http://www.cnblogs.com/tankxiao</a>\n\n在wireshark中输入http过滤， 然后选中GET /tankxiao HTTP/1.1的那条记录，右键然后点击\"Follow TCP Stream\", \n\n这样做的目的是为了得到与浏览器打开网站相关的数据包，将得到如下图\n\n<img src=\"/images/150046-20160802202009981-1942606838.png\" alt=\"\" />\n\n图中可以看到wireshark截获到了三次握手的三个数据包。第四个包才是HTTP的， 这说明HTTP的确是使用TCP建立连接的。 \n\n&nbsp;\n\n第一次握手数据包\n\n客户端发送一个TCP，标志位为SYN，序列号为0， 代表客户端请求建立连接。 如下图\n\n<img src=\"/images/150046-20160802202010575-1736889917.png\" alt=\"\" />\n\n第二次握手的数据包\n\n服务器发回确认包, 标志位为 SYN,ACK. 将确认序号(Acknowledgement Number)设置为客户的I S N加1以.即0+1=1, 如下图\n\n<img src=\"/images/150046-20160802202011231-471497268.png\" alt=\"\" />\n\n第三次握手的数据包\n\n客户端再次发送确认包(ACK) SYN标志位为0,ACK标志位为1.并且把服务器发来ACK的序号字段+1,放在确定字段中发送给对方.并且在数据段放写ISN的+1, 如下图:\n\n<img src=\"/images/150046-20160802202011997-1580125577.png\" alt=\"\" />\n\n&nbsp;就这样通过了TCP三次握手，建立了连接\n\n[http://www.cnblogs.com/TankXiao/archive/2012/10/10/2711777.html](http://www.cnblogs.com/TankXiao/archive/2012/10/10/2711777.html)\n\n**网络中明码传输的危险性**\n\n&nbsp;&nbsp;&nbsp;&nbsp;通过明码传输的protocol和工具相当多，典型的就是telnet,ftp,http。我们拿telnet做这次实验。假设我以telnet方式登录到我的linux服务器，然后通过wireshark抓包，以抓取账号和密码信息。\n\n1、首先启动wireshark，并处于Capture状态。然后通过telnet远程登录我们的linux服务器。\n\n[<img src=\"/images/150046-20160802202015887-326862603.jpg\" alt=\"wKiom1PtxDOQimlAAACdwiZqfeE162.jpg\" width=\"650\" title=\"1.png\" style=\"padding: 0px; margin: 0px; vertical-align: top; border-width: medium; border-style: none; float: none;\" />](http://s3.51cto.com/wyfs02/M02/46/13/wKiom1PtxDOQimlAAACdwiZqfeE162.jpg)\n\n进入登录界面后，输入账号和密码登入系统。\n\n[<img src=\"/images/150046-20160802202016590-290108096.jpg\" alt=\"wKioL1PtxUvT0yksAACzkwdoI_Y013.jpg\" width=\"650\" title=\"2.png\" style=\"padding: 0px; margin: 0px; vertical-align: top; border-width: medium; border-style: none; float: none;\" />](http://s3.51cto.com/wyfs02/M01/46/15/wKioL1PtxUvT0yksAACzkwdoI_Y013.jpg)\n\n2、接下来停止wireshark的截取封包的操作，执行快捷方式的\"Stop\"即可。\n\n&nbsp;&nbsp;&nbsp;&nbsp;不过，捕获的信息非常多，这个时候可以利用Display Filter功能，过滤显示的内容，如下图所示，点击Expression,然后选择过滤表达式。这里，我们选择TELNET即可。\n\n[<img src=\"/images/150046-20160802202018090-2094601524.jpg\" alt=\"wKiom1PtxVXjVpp9AAU8cd7PEgE073.jpg\" width=\"650\" title=\"1.png\" style=\"padding: 0px; margin: 0px; vertical-align: top; border-width: medium; border-style: none;\" />](http://s3.51cto.com/wyfs02/M02/46/14/wKiom1PtxVXjVpp9AAU8cd7PEgE073.jpg)\n\n表达式确定之后，选择\"Apply\",就可以过滤出只包含TELNET的封包\n\n[<img src=\"/images/150046-20160802202020106-1808415128.jpg\" alt=\"wKioL1Ptx42CAgvXAAYLelyRK58954.jpg\" width=\"650\" title=\"1.png\" style=\"padding: 0px; margin: 0px; vertical-align: top; border-width: medium; border-style: none;\" />](http://s3.51cto.com/wyfs02/M02/46/16/wKioL1Ptx42CAgvXAAYLelyRK58954.jpg)来，我们查看一下整个telnet会话的所有记录， wireshark可以记录会话记录（就像我们聊QQ时，\"QQ聊天记录\"一样），任意找到一个telnet封包，右键找到\"Follow TCP Stream\"，wireshark就会返回整个会话记录。\n\n[<img src=\"/images/150046-20160802202022153-3188226.jpg\" alt=\"wKioL1PtyevSaTfJAASR__mwGZk771.jpg\" width=\"650\" title=\"1.png\" style=\"padding: 0px; margin: 0px; vertical-align: top; border-width: medium; border-style: none;\" />](http://s3.51cto.com/wyfs02/M02/46/17/wKioL1PtyevSaTfJAASR__mwGZk771.jpg)\n\nOK， 我们看到以下这些数据信息，红色的部分是我们发送出去的DATA，蓝色的部分是我们接收到的DATA。&nbsp;[<img src=\"/images/150046-20160802202022934-1132784557.gif\" alt=\"j_0061.gif\" style=\"padding: 0px; margin: 0px; vertical-align: top; border-width: medium; border-style: none;\" />](http://img.baidu.com/hi/jx2/j_0061.gif)， 告诉我， 你看到了什么<br style=\"padding: 0px; margin: 0px;\" />\n\n[<img src=\"/images/150046-20160802202023653-467070702.jpg\" alt=\"wKiom1PtyVriu__WAAINWJKx1S0346.jpg\" title=\"1.png\" style=\"padding: 0px; margin: 0px; vertical-align: top; border-width: medium; border-style: none;\" />](http://s3.51cto.com/wyfs02/M02/46/16/wKiom1PtyVriu__WAAINWJKx1S0346.jpg)\n\n为了更准确的看清楚，我们再次仅筛选出我们发送出去的DATA。或者仅接收到的DATA。\n\n[<img src=\"/images/150046-20160802202024872-89218706.jpg\" alt=\"wKioL1PtzA7D1_ewAAIm3rxMBZk668.jpg\" title=\"1.png\" style=\"padding: 0px; margin: 0px; vertical-align: top; border-width: medium; border-style: none;\" />](http://s3.51cto.com/wyfs02/M02/46/18/wKioL1PtzA7D1_ewAAIm3rxMBZk668.jpg)\n\n从这里，我们可以确切的抓到账号和密码信息。login:wireshark &nbsp;Password:123456，除了这些，我们还可以更进一步知道别人在看什么网站，或是私人文件，隐私将毫无保障。\n\n注：为了避免这些情况，防止有心人监测到重要信息，可以使用SSH,SSL,TSL,HTTPS等加密协议对重要数据进行加密，然后再到网络上传输，如果被人截取下来，看到的内容也是被加密的。\n","tags":["计算机网络"]},{"title":"使用openssl生成自签名ssl证书","url":"/使用openssl生成自签名ssl证书.html","content":"使用自签名的证书的网站默认不会被浏览器信任，使用浏览器带打开可能会弹出如下界面，需要在浏览器中点击继续前往或者添加例外\n\n<img src=\"/images/517519-20240323141523460-1115943310.png\" width=\"600\" height=\"451\" loading=\"lazy\" />\n\n添加的例外可以在Firefox浏览器中如下界面中设置&mdash;&mdash;隐私与安全&mdash;&mdash;证书中进行查看\n\n<img src=\"/images/517519-20240323142619913-2003308091.png\" width=\"600\" height=\"392\" loading=\"lazy\" />\n\n## 生成CA私钥\n\nca.key是证书颁发机构（Certificate Authority，CA）的私钥文件，CA私钥用于签署证书并保护证书颁发机构的安全性。\n\n```\nopenssl genrsa -out ca.key 2048\n\n```\n\n## 生成CA证书\n\nca.crt是证书颁发机构（Certificate Authority，CA）的证书文件，CA证书用于签名其他证书，验证证书的合法性。\n\n```\n# 20 年有效期\nopenssl req -x509 -new -nodes -key ca.key -sha256 -days 7300 -out ca.crt\n\n```\n\n## 生成服务器ssl证书私钥\n\nserver.key是服务器私钥，用于生成服务器证书、加密通信和验证身份。\n\n```\nopenssl genrsa -out server.key 2048\n\n```\n\n## 生成服务器ssl证书CSR\n\n**server.csr** 是指服务器证书签名请求（Certificate Signing Request，CSR）文件。\n\n在创建 SSL/TLS 证书时，CSR 是一个包含有组织或个人身份信息的加密文本块，用于向证书颁发机构（CA）请求签署数字证书。\n\n```\nopenssl req -new -sha256 -key server.key -out server.csr\n\n```\n\n## 创建域名附加配置文件\n\ncert.ext是域名附加配置文件\n\n```\nextendedKeyUsage=serverAuth\nsubjectAltName=DNS:xx.com,DNS:*.xx.com\n\n```\n\n## 使用CA签署服务器ssl证书\n\n**server.crt**：这是服务器的公钥证书文件。它包含服务器的公钥以及与该公钥相关联的其他信息，比如服务器的域名等\n\n```\n# ssl证书有效期10年\nopenssl x509 -req -in server.csr -sha256 -days 3650 \\\n  -CAcreateserial -CA ca.crt -CAkey ca.key \\\n  -CAserial serial -extfile cert.ext -out server.crt \n\n```\n\n## 查看证书的签署信息\n\n```\nopenssl x509 -in server.crt -noout -text\n\n```\n\n## 使用CA验证证书是否通过\n\n```\nopenssl verify -CAfile ca.crt server.crt\nserver.crt: OK\n\n```\n\n参考：[使用openssl制作自定义CA、自签名ssl证书](https://gist.github.com/liuguangw/4d4b87b750be8edb700ff94c783b1dd4)\n\n[https://github.com/UnblockNeteaseMusic/server/blob/enhanced/generate-cert.sh](https://github.com/UnblockNeteaseMusic/server/blob/enhanced/generate-cert.sh)\n\n<!--more-->\n&nbsp;\n","tags":["计算机网络"]},{"title":"ubuntu16.04下Jenkins使用","url":"/ubuntu16.04下Jenkins使用.html","content":"**1.安装**\n\n```\nsudo apt-get install jenkins=2.249.2\n\n```\n\n修改端口\n\n```\nsudo vim /etc/default/jenkins\n\nHTTP_PORT=10001\n\n```\n\n参考\n\n```\nhttps://www.jenkins.io/doc/book/installing/linux/#debianubuntu\n\n```\n\n以及\n\n[ubuntu安装Jenkins及修改端口](https://www.jianshu.com/p/1303b683fb73)\n\n<!--more-->\n&nbsp;\n\n**2.新建Jenkins任务**，使用参考\n\n```\nhttps://github.com/muyinchen/woker/blob/master/%E9%9B%86%E6%88%90%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E6%90%AD%E5%BB%BAJenkins%2BGithub%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%E7%8E%AF%E5%A2%83.md\n\n```\n\n创建一个item\n\n<img src=\"/images/517519-20220418150950479-1228332766.png\" width=\"800\" height=\"487\" loading=\"lazy\" />\n\n可以选择不同的模板，一般使用freestyle模板\n\nfreestyle模板\n\n&nbsp;<img src=\"/images/517519-20220418154110063-1556444814.png\" width=\"800\" height=\"651\" loading=\"lazy\" />\n\npipeline模板\n\n<img src=\"/images/517519-20220418152053858-1023869890.png\" width=\"800\" height=\"636\" loading=\"lazy\" />\n\nexternal job模板\n\n&nbsp;<img src=\"/images/517519-20220418153539237-566087607.png\" width=\"800\" height=\"541\" loading=\"lazy\" />\n\nmutile-config-project模板\n\n<img src=\"/images/517519-20220418152357484-545210030.png\" width=\"800\" height=\"496\" loading=\"lazy\" />\n\nmutiljob-project模板\n\n<img src=\"/images/517519-20220418153339026-1132531233.png\" width=\"800\" height=\"649\" loading=\"lazy\" />\n\ngit模板\n\n<img src=\"/images/517519-20220418153818711-297882537.png\" width=\"800\" height=\"548\" loading=\"lazy\" />\n\n&nbsp;\n\n**3.Jenkins REST API**\n\n勾选build triggers，填写token\n\n<img src=\"/images/517519-20220421103012476-1749255263.png\" width=\"700\" height=\"106\" loading=\"lazy\" />\n\n此外还需要在用户界面中配置API Token，用于Jenkins的basic auth\n\n<img src=\"/images/517519-20220421111114436-972721687.png\" width=\"700\" height=\"134\" loading=\"lazy\" />\n\n&nbsp;\n\n就可以使用POST请求类似下面url来触发Jenkins任务\n\n```\nhttps://${url}/job/my_jenkins_job/build?token=xxxxxx\n```\n\n<img src=\"/images/517519-20220421111321438-1443987847.png\" width=\"700\" height=\"208\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;\n\n**4.带参数的Jenkins任务**\n\n勾选parameterized选项，选择string parameter，填写\n\n<img src=\"/images/517519-20220421104438036-1870614755.png\" width=\"700\" height=\"366\" loading=\"lazy\" />\n\n然后就可以在build的shell中使用 $key 变量\n\n&nbsp;\n","tags":["jenkins"]},{"title":"机器学习-损失函数 （转）","url":"/机器学习-损失函数 （转）.html","content":"参考\n\n```\n作者：刘帝伟\n原文地址：http://www.csuldw.com/\n\n```\n\n<!--more-->\n&nbsp;\n\n**损失函数（loss function）**是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。损失函数是**经验风险函数**的核心部分，也是**结构风险函数**重要组成部分。模型的结构风险函数包括了经验风险项和正则项，通常可以表示成如下式子：\n\n　　　　<img src=\"/images/517519-20161130211118865-802969511.png\" alt=\"\" />\n\n## 一、log对数损失函数（逻辑回归）\n\n有些人可能觉得逻辑回归的损失函数就是平方损失，其实并不是。平方损失函数可以通过线性回归在假设样本是高斯分布的条件下推导得到，而逻辑回归得到的并不是平方损失。在逻辑回归的推导中，它假设样本服从**伯努利分布（0-1分布）**，然后求得满足该分布的似然函数，接着取对数求极值等等。而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：**最小化负的似然函数（即max F(y, f(x)) &mdash;-> min -F(y, f(x)))**。从损失函数的视角来看，它就成了log损失函数了。\n\n**log损失函数的标准形式**：<mi>L</mi><mo stretchy=&quot;false&quot;>(</mo><mi>Y</mi><mo>,</mo><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>Y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mo>&amp;#x2212;</mo><mi>log</mi><mo>&amp;#x2061;</mo><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>Y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo></math>\">&amp;lt;br&amp;gt;\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img src=\"/images/517519-20161130211308334-1632337962.png\" alt=\"\" /><br />刚刚说到，取对数是为了方便计算极大似然估计，因为在MLE中，直接求导比较困难，所以通常都是先取对数再求导找极值点。损失函数L(Y, P(Y|X))表达的是样本X在分类Y的情况下，使概率P(Y|X)达到最大值（换言之，**就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大**）。因为log函数是单调递增的，所以logP(Y|X)也会达到最大值，因此在前面加上负号之后，最大化P(Y|X)就等价于最小化L了。  \n\n逻辑回归的P(Y=y|x)表达式如下（为了将类别标签y统一为1和0，下面将表达式分开表示）：\n\n　　　　<img src=\"/images/517519-20200706135754708-1582215529.png\" width=\"600\" height=\"93\" loading=\"lazy\" />\n\n将它带入到上式，通过推导可以得到logistic的损失函数表达式，如下：\n\n　　　　<img src=\"/images/517519-20200510221848644-975396702.png\" alt=\"\" width=\"400\" height=\"58\" />\n\n逻辑回归最后得到的目标式子如下：\n\n　　　　<img src=\"/images/517519-20161130211342240-1584548530.png\" alt=\"\" /><mi>J</mi><mo stretchy=&quot;false&quot;>(</mo><mi>&amp;#x03B8;</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mo>&amp;#x2212;</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mo>[</mo><mrow><msup><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>i</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mi>log</mi><mo>&amp;#x2061;</mo><msub><mi>h</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03B8;</mi></mrow></msub><mo stretchy=&quot;false&quot;>(</mo><msup><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>i</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo>&amp;#x2212;</mo><msup><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>i</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo stretchy=&quot;false&quot;>)</mo><mi>log</mi><mo>&amp;#x2061;</mo><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo>&amp;#x2212;</mo><msub><mi>h</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03B8;</mi></mrow></msub><mo stretchy=&quot;false&quot;>(</mo><msup><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>i</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></mrow><mo>]</mo></mrow></math>\">\n\n上面是针对二分类而言的。这里需要解释一下：**之所以有人认为逻辑回归是平方损失，是因为在使用梯度下降来求最优解的时候，它的迭代式子与平方损失求导后的式子非常相似，从而给人一种直观上的错觉**。\n\n这里有个PDF可以参考一下：[Lecture 6: logistic regression.pdf](https://www.cs.berkeley.edu/%7Erussell/classes/cs194/f11/lectures/CS194%20Fall%202011%20Lecture%2006.pdf).\n\n## 二、平方损失函数（最小二乘法, Ordinary Least Squares ）\n\n最小二乘法是线性回归的一种，OLS将问题转化成了一个凸优化问题。在线性回归中，它假设样本和噪声都服从高斯分布（为什么假设成高斯分布呢？其实这里隐藏了一个小知识点，就是**中心极限定理**，可以参考[【central limit theorem】](https://en.wikipedia.org/wiki/Central_limit_theorem)），最后通过极大似然估计（MLE）可以推导出最小二乘式子。最小二乘的基本原则是：**最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小**。换言之，OLS是基于距离的，而这个距离就是我们用的最多的欧几里得距离。为什么它会选择使用欧式距离作为误差度量呢（即Mean squared error， MSE），主要有以下几个原因：\n\n- 简单，计算方便；\n- 欧氏距离是一种很好的相似性度量标准；\n- 在不同的表示域变换后特征性质不变。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n**平方损失（Square loss）的标准形式如下：**<mi>L</mi><mo stretchy=&quot;false&quot;>(</mo><mi>Y</mi><mo>,</mo><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><mi>Y</mi><mo>&amp;#x2212;</mo><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo><msup><mo stretchy=&quot;false&quot;>)</mo><mn>2</mn></msup></math>\"> &amp;lt;br&amp;gt;\n\n当样本个数为n时，此时的损失函数变为：\n\n<img src=\"/images/517519-20200510220616809-81580768.png\" alt=\"\" width=\"300\" height=\"88\" />\n\n`Y-f(X)`表示的是残差，整个式子表示的是**残差的平方和**，而我们的目的就是最小化这个目标函数值（注：该式子未加入正则项），也就是最小化残差的平方和（residual sum of squares，RSS）。\n\n而在实际应用中，通常会使用均方差（MSE）作为一项衡量指标，公式如下\n\n　　<img src=\"/images/517519-20161130211453881-1727406827.png\" alt=\"\" />\n\n的线性函数。在机器学习中，通常指的都是后一种情况。\n\n## 三、指数损失函数（Adaboost）\n\n学过Adaboost算法的人都知道，它是前向分步加法算法的特例，是一个加和模型，损失函数就是指数函数。在Adaboost中，经过m此迭代之后，可以得到fm(x):\n\n　　　　<img src=\"/images/517519-20200510215311531-852193725.png\" alt=\"\" width=\"250\" height=\"53\" />\n\nAdaboost每次迭代时的目的是为了找到最小化下列式子时的参数alpha和G：\n\n　　　　<img src=\"/images/517519-20200510215355332-1320712481.png\" alt=\"\" width=\"300\" height=\"60\" />\n\n**而指数损失函数(exp-loss）的标准形式如下**\n\n**　　　　<img src=\"/images/517519-20200510220830816-716307186.png\" alt=\"\" width=\"250\" height=\"54\" />**\n\n可以看出，Adaboost的目标式子就是指数损失，在给定n个样本的情况下，Adaboost的损失函数为：\n\n　　　　<img src=\"/images/517519-20200510220857711-938623529.png\" alt=\"\" width=\"300\" height=\"70\" />\n\n关于Adaboost的推导，可以参考Wikipedia：[AdaBoost](https://en.wikipedia.org/wiki/AdaBoost)或者《统计学习方法》P145.\n\n## 四、Hinge损失函数（SVM）\n\n在机器学习算法中，hinge损失函数和SVM是息息相关的。在**线性支持向量机**中，最优化问题可以等价于下列式子：<br />　　　　<img src=\"/images/517519-20200510215543464-843436473.png\" alt=\"\" width=\"300\" height=\"81\" />\n\n下面来对式子做个变形，令：\n\n　　　　<img src=\"/images/517519-20200510215616002-2016862431.png\" alt=\"\" width=\"300\" height=\"88\" />\n\n于是，原式就变成了：<br />　　　　<img src=\"/images/517519-20200510215641556-208247395.png\" alt=\"\" width=\"250\" height=\"89\" />\n\n如示成：<br />　　　　<img src=\"/images/517519-20200510215852088-865324336.png\" alt=\"\" width=\"250\" height=\"63\" />\n\n可以看出，该式子与下式非常相似：<br />　　　　\n\n　　<img src=\"/images/517519-20200510221225142-272273994.png\" alt=\"\" width=\"300\" height=\"78\" />\n\n前半部分中的l就是hinge损失函数，而后面相当于L2正则项。\n\n**Hinge 损失函数的标准形式**\n\n　　　　<img src=\"/images/517519-20161130211909318-41872097.png\" alt=\"\" /><br />可以看出，当|y|>=1时，L(y)=0。\n\n更多内容，参考[Hinge-loss](https://en.wikipedia.org/wiki/Hinge_loss)。\n\n补充一下：在libsvm中一共有4中核函数可以选择，对应的是`-t`参数分别是：\n\n- 0-线性核；\n- 1-多项式核；\n- 2-RBF核；\n- 3-sigmoid核。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## 五、其它损失函数\n\n除了以上这几种损失函数，常用的还有：\n\n**0-1损失函数**\n\n<img src=\"/images/517519-20200510220427783-312885400.png\" alt=\"\" width=\"250\" height=\"66\" />\n\n**绝对值损失函数**\n\n<img src=\"/images/517519-20200510220500845-85600133.png\" alt=\"\" width=\"250\" height=\"60\" />\n\n下面来看看几种损失函数的可视化图像，对着图看看横坐标，看看纵坐标，再看看每条线都表示什么损失函数，多看几次好好消化消化。<br />最后，需要记住的是：**参数越多，模型越复杂，而越复杂的模型越容易过拟合**。过拟合就是说模型在训练数据上的效果远远好于在测试集上的性能。此时可以考虑正则化，通过设置正则项前面的hyper parameter，来权衡损失函数和正则项，减小参数规模，达到模型简化的目的，从而使模型具有更好的泛化能力。\n\n&nbsp;\n\n## 参考文献\n\n```\nhttps://github.com/JohnLangford/vowpal_wabbit/wiki/Loss-functions\nlibrary_design/losses\nhttp://www.cs.cmu.edu/~yandongl/loss.html\nhttp://math.stackexchange.com/questions/782586/how-do-you-minimize-hinge-loss\n《统计学习方法》 李航 著.    \n\n```\n\n&nbsp;\n","tags":["ML"]},{"title":"机器学习——AdaBoost元算法","url":"/机器学习——AdaBoost元算法.html","content":"　　当做重要决定时，我们可能会考虑**吸取多个**专家而**不只是一个**人的意见。机器学习处理问题也是这样，这就是**元算法（meta-algorithm）**背后的思路。\n\n**　　元算法**是**对其他算法进行组合**的一种方式，其中最流行的一种算法就是**AdaBoost算法**。**某些人认为AdaBoost是最好的监督学习的方法**，所以该方法是机器学习工具箱中最强有力的工具之一。\n\n　　**集成学习**或者**元算法**的一般结构是：先产生一组**&ldquo;个体学习器&rdquo;**，再用某种策略将他们结合起来。**个体学习器**通常是由一个现有的学习算法从训练数据产生。\n\n　　根据**个体学习器的生成方式**，目前的集成学习方法大致可分为**两大类**，即\n\n　　1.个体学习器间存在强依赖关系、必须串行生成的**序列化方法**，典型的代表是**Boosting**，其中**AdaBoost**就是**Boosting**的最流行的一个版本\n\n　　2.个体学习器间不存在强依赖关系、可同时生成的**并行化方法**，典型的代表是**Bagging和&ldquo;随机森林&rdquo;（Random Forest）**\n\n<!--more-->\n&nbsp;\n\n**AdaBoost**\n\n**优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整**\n\n**缺点：对离群点敏感**\n\n**使用数据类型：数值型和标称型数据**\n\n&nbsp;\n\n**bagging：基于数据随机重抽样的分类器构建方法**\n\n**自举汇聚法（bootstrap aggregating）**，也称为**bagging方法**，它直接基于自助采样法（bootstrap samping）。\n\n给定包含**m个样本的数据集**，我们先**随机取出一个样本**放入采样集中，再**把该样本放回初始数据集**，使得下次采样时该样本仍有可能被选中，这样，经过**m次随机采样操作**，我们**得到了含m个样本的采样集。**这样从原始数据集**选择T次**后得到T**个新数据集**，且每个新数据集的大小和原数据集的大小相等。在T个新数据集建好之后，将某个学习算法**分别作用于每个数据集**就得到了**T个分类器**。当我们要对新数据集进行分类时，就可以应用这T个分类器进行分类。与此同时，选择分类器**投票结果中最多**的类别作为最后的分类结果（权重相等）。\n\n&nbsp;\n\n**Boosting**\n\nboosting是一种和bagging很类似的技术。其使用的多个分类器的类型都是一致的。\n\n**在boosting中**，不同的分类器是通过**串行训练**而获得的，每个新分类器都**根据已训练出的分类器的性能来进行训练**。boosting是通过**集中关注被已有分类器错分的那些数据**来获得新的分类器。\n\n**boosting分类的结果是基于所有分类器的加权求和结果**的，在bagging中的分类器权重是相等的，而boosting中的分类器**权重并不相等**，每个权重代表的是其对应分类器在上一轮迭代中的成功度。\n\n&nbsp;\n\n现在介绍其中的**AdaBoost**\n\n**<img src=\"/images/517519-20161127224516753-1518470281.png\" alt=\"\" />**\n\n&nbsp;\n\n**弱分类器**的&ldquo;弱&rdquo;意味着分类器的性能比随机猜测要略好，但是也不会好太多。这就是说，在二分类情况下，弱分类器的错误率会高于50%，而强分类器的错误率会低很多。\n\n**AdaBoost**是**adaptive boosting（自适应boosting）**的缩写，其运行过程如下：\n\n假设一个二类分类的**训练数据集**\n\n**<img src=\"/images/517519-20161216210816511-1819415414.gif\" alt=\"\" />**\n\n<1>训练数据中的每个样本，并赋予其一个权重，这些权重构成了**初始向量D**。一开始，这些权重都初始化成相等值。\n\n<img src=\"/images/517519-20161216211155276-864361291.gif\" alt=\"\" />\n\n&nbsp;\n\nAdaBoost算法多种推导方式，比较容易理解的是基于&ldquo;**加性模型**&rdquo;，即**基学习器的线性组合**\n\n　　<img src=\"/images/517519-20161216215245886-1741463451.gif\" alt=\"\" />，其中 <img src=\"/images/517519-20161130210322177-1513831152.gif\" alt=\"\" /> 为基学习器，<img src=\"/images/517519-20161130210401146-1546111983.gif\" alt=\"\" /> 为系数\n\n来最小化**指数损失函数（exponential loss function），损失函数见 [机器学习-损失函数 （转）](http://www.cnblogs.com/tonglin0325/p/6109470.html)<br />**\n\n**　　 <img src=\"/images/517519-20161216221341667-1659955469.gif\" alt=\"\" /><strong>，其中f(x)是正确的分类，等于-1或者1，H(x)是分类器的分类结果，等于-1或者1**</strong>\n\n**　　<img src=\"/images/517519-20161216222906339-1896357023.gif\" alt=\"\" />**\n\n　　　　　　　　　　<img src=\"/images/517519-20161216222912151-1229615814.gif\" alt=\"\" />　　\n\n**，所以对该式子求<img src=\"/images/517519-20161216223025245-1601671422.gif\" alt=\"\" />的偏导，得&nbsp;<img src=\"/images/517519-20161216223057401-1572250285.gif\" alt=\"\" />，并令其等于0，得**\n\n**　　<img src=\"/images/517519-20161216223149401-17227164.gif\" alt=\"\" />　**\n\n<2>首先在训练分类器上**训练出一个弱分类器并计算该分类的错误率**，然后在同一数据集上再次训练弱分类器。\n\n**在分类器的第二次训练中，将会重新调整每个样本的权重，其中第一次分对的样本的权重将会降低，而第一次分错的样本的权重将会提高。**为了从所有弱分类器中得到最终的分类结果，**AdaBoost为每个分类器都分配了一个权重值alpha，这些alpha值是基于每个弱分类器的错误率进行计算的。**\n\n其中，**错误率** <img src=\"/images/517519-20161128094840787-1341809821.gif\" alt=\"\" /> 的定义为\n\n　　<img src=\"/images/517519-20161128094840787-1341809821.gif\" alt=\"\" />= 为正确分类的样本数目/所有样本数目\n\n而**alpha**的计算公式如下：\n\n　　<img src=\"/images/517519-20161128095013943-1500762234.gif\" alt=\"\" />\n\n计算出alpha值之后，可以**对权重向量D进行更新**，以使得那些正确分类的样本的权重降低而错分样本的权重升高。D的计算方法如下：\n\n　　&nbsp;<img src=\"/images/517519-20161216231408401-1888397285.gif\" alt=\"\" />\n\n其中，<img src=\"/images/517519-20161216232153198-2061695919.gif\" alt=\"\" />，<img src=\"/images/517519-20161216232322714-277179853.gif\" alt=\"\" />是**规范化因子**\n\n　　<img src=\"/images/517519-20161217111119323-93392128.gif\" alt=\"\" />\n\n它使得<img src=\"/images/517519-20161216231408401-1888397285.gif\" alt=\"\" />成为一个概率分布\n\n如果某个样本被**正确分类**，那么该样本的权重更改为\n\n　　<img src=\"/images/517519-20161217111715417-844931160.gif\" alt=\"\" />\n\n如果某个样本被**错误分类**，那么该样本的权重更改为\n\n　　<img src=\"/images/517519-20161217111806386-774519091.gif\" alt=\"\" />\n\n在计算出D之后，AdaBoost算法**又开始进入下一轮迭代**。\n\nAdaBoost算法会不断地重复训练和调整权重的过程，直到**训练错误率为0**或者**弱分类器的数目达到用户的指定值**为止。\n\n&nbsp;\n\n```\nfrom numpy import *\n\ndef loadSimpData():\n    datMat = matrix([[ 1. ,  2.1],\n        [ 1.5,  1.6],\n        [ 1.3,  1. ],\n        [ 1. ,  1. ],\n        [ 2. ,  1. ]])\n    classLabels = [1.0, 1.0, -1.0, -1.0, 1.0]\n    return datMat,classLabels\n\n```\n\n&nbsp;<img src=\"/images/517519-20161219160113463-1616562119.png\" alt=\"\" width=\"433\" height=\"367\" />\n\n```\ndef plotBestFit(weakClassArr):\t\t#画出数据集和所有的基学习器\n\t#import matplotlib.pyplot as plt\n\tdataMat,labelMat=loadSimpData()\t#数据矩阵和标签向量\n\tdataArr = array(dataMat)\t\t#转换成数组\n\tn = shape(dataArr)[0] \n\txcord1 = []; ycord1 = []\t\t#声明两个不同颜色的点的坐标\n\txcord2 = []; ycord2 = []\n\tfor i in range(n):\n\t\tif int(labelMat[i])== 1:\n\t\t\txcord1.append(dataArr[i,0]); ycord1.append(dataArr[i,1])\n\t\telse:\n\t\t\txcord2.append(dataArr[i,0]); ycord2.append(dataArr[i,1])\n\tfig = plt.figure()\n\tax = fig.add_subplot(111)\n\tax.scatter(xcord1, ycord1, s=30, c='red', marker='s')\n\tax.scatter(xcord2, ycord2, s=30, c='green')\n\tprint \"weakClassArr[0]['thresh']\",weakClassArr[0]['dim']\n\tfor j in range(len(weakClassArr)):\n\t\tif(weakClassArr[j]['dim'] == 1):\n\t\t\tx = arange(-0.0, 2.5, 0.1)\n\t\t\ty = x*0+weakClassArr[j]['thresh']\n\t\t\tax.plot(x, y)\n\t\telse:\n\t\t\ty = array(arange(-0.0, 2.5, 0.1))\n\t\t\tx = y*0+weakClassArr[j]['thresh']\n\t\t\tax.plot(x, y)\n\tplt.xlabel('X1'); plt.ylabel('X2');\n\tplt.show()\n\ndef stumpClassify(dataMatrix,dimen,threshVal,threshIneq):\t#通过阈值比较对数据进行分类\n    retArray = ones((shape(dataMatrix)[0],1))\t\t\t\t#首先将返回的数组的全部元素设置为1\n    if threshIneq == 'lt':\n        retArray[dataMatrix[:,dimen] <= threshVal] = -1.0\t#将满足<=不等式的元素设为-1\n    else:\n        retArray[dataMatrix[:,dimen] > threshVal] = -1.0\t#将满足>不等式的元素设为-1\n    return retArray\n    \n\ndef buildStump(dataArr,classLabels,D):\t\t#遍历stumpClassify()函数所有的可能输入值，并找到数据集上最佳的单层决策树\n    dataMatrix = mat(dataArr); labelMat = mat(classLabels).T\n    m,n = shape(dataMatrix)\t\t\t\t\t#m=5,n=2\n    numSteps = 10.0; bestStump = {}; bestClasEst = mat(zeros((m,1)))\n    minError = inf \t\t\t\t\t\t\t#初始误差总和，为无穷大\n    for i in range(n):\t\t\t\t\t\t#循环X和Y两个维度\n        rangeMin = dataMatrix[:,i].min(); rangeMax = dataMatrix[:,i].max();\t\t#取得X和Y两个维度的最大值和最小值特征\n        stepSize = (rangeMax-rangeMin)/numSteps\t\t\t\t\t\t\t\t\t#步进长度\n        for j in range(-1,int(numSteps)+1):\t\t\t\t#从-1到10步进\n            for inequal in ['lt', 'gt']: \t\t\t\t#\"lt\"为满足<=不等式，\"gt\"为满足>不等式\n                threshVal = (rangeMin + float(j) * stepSize)\t\t\t\t\t#当前阈值\n                predictedVals = stumpClassify(dataMatrix,i,threshVal,inequal)\t#根据阈值和不等式，计算预测的分类\n                errArr = mat(ones((m,1)))\n                errArr[predictedVals == labelMat] = 0\t#样本估计错误的标记为1\n                weightedError = D.T*errArr  \t\t\t#通过权重和错误标记，计算泛化误差\n                #print \"split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f\" % (i, threshVal, inequal, weightedError)\n                if weightedError < minError:\t\t\t#如果泛化误差是最小的\n                    minError = weightedError\n                    bestClasEst = predictedVals.copy()\t#保存最佳预测结果\n                    bestStump['dim'] = i\t\t\t\t#保存维度、阈值、不等式符号\n                    bestStump['thresh'] = threshVal\n                    bestStump['ineq'] = inequal\n    return bestStump,minError,bestClasEst\n\n\ndef adaBoostTrainDS(dataArr,classLabels,numIt=40):\t#基于单层决策树的AdaBoost训练\n    weakClassArr = []\n    m = shape(dataArr)[0]\t\t\t\t\t\t\t#需要分类的数据量，m=5\n    D = mat(ones((m,1))/m)   \t\t\t\t\t\t#D为权重向量，初始D1...D5的和等于1\n    aggClassEst = mat(zeros((m,1)))\t\t\t\t\t#基分类器的线性组合\n    for i in range(numIt):\n\t    #建立单层决策树，bestStump包括维度，不等式，阈值，error泛化误差，classEst是每个基分类器\n        bestStump,error,classEst = buildStump(dataArr,classLabels,D)\t\n        print \"最佳决策树=\",bestStump,\"泛化误差=\",error,\"更新前的分类器预测结果=\",classEst.T \n        #print \"D:\",D.T\n        alpha = float(0.5*log((1.0-error)/max(error,1e-16)))\t#根据泛化误差，计算基分类器的权重&alpha;值\n        bestStump['alpha'] = alpha  \t\t\t\t\t\t\t#把权重&alpha;值添加到最佳决策树的列表中\n        print \"最佳决策树=\",bestStump\n        weakClassArr.append(bestStump)                  \t\t#保存单层最佳决策树参数到数组中\n        print \"预测分类: \",classEst.T\n        expon = multiply(-1*alpha*mat(classLabels).T,classEst) \t#权重&alpha;&times;真实分类&times;预测分类，multiply为对应元素相乘，不是矩阵相乘\n        D = multiply(D,exp(expon))                              #Calc New D for next iteration\n        D = D/D.sum()\t\t\t\t\t\t\t\t\t\t\t#更新D,D.sum()为规范化因子\n        #calc training error of all classifiers, if this is 0 quit for loop early (use break)\n        aggClassEst += alpha*classEst\n        print \"更新后的分类器预测结果: \",aggClassEst.T\n        aggErrors = multiply(sign(aggClassEst) != mat(classLabels).T,ones((m,1)))\t#boolean值和1相乘\n        errorRate = aggErrors.sum()/m\t\t\t\t\t\t\t\t\t\t\t\t#计算错误率\n        print \"total error: \",errorRate\n        if errorRate == 0.0: break\n    return weakClassArr,aggClassEst\n\ndef adaClassify(datToClass,classifierArr):\t\t\t#AdaBoost分类函数\n    dataMatrix = mat(datToClass)\t\t\t\t\t#输入[0,0]转换成[[0,0]]矩阵\n    m = shape(dataMatrix)[0]\n    aggClassEst = mat(zeros((m,1)))\n    for i in range(len(classifierArr)):\n        classEst = stumpClassify(dataMatrix,classifierArr[i]['dim'],\\\n                                 classifierArr[i]['thresh'],\\\n                                 classifierArr[i]['ineq'])\t\t\t\t#维度、阈值、不等式符号\n        aggClassEst += classifierArr[i]['alpha']*classEst\t\t\t\t#计算在每一个基分类器上的预测值的累加和\n        print \"aggClassEst=\",aggClassEst\n    return sign(aggClassEst)\n\n```\n\n&nbsp;\n\n**main.py**\n\n```\n# coding:utf-8\n# !/usr/bin/env python\n\nimport adaboost\n\nif __name__ == '__main__':\n\tdatMat,classLabels = adaboost.loadSimpData()\n\tweakClassArr,aggClassEst = adaboost.adaBoostTrainDS(datMat,classLabels)\n\tprint \"弱分类器组合:\",weakClassArr\n\tprint adaboost.adaClassify([[0,0],[5,5]],weakClassArr)\n\tadaboost.plotBestFit(weakClassArr)\n\n```\n\n&nbsp;\n\n**1个分类器&mdash;&mdash;&mdash;&mdash;　　　　　　　　　　　　2个分类器&mdash;&mdash;&mdash;&mdash;　　　　　　　　　　　　3个分类器&mdash;&mdash;&mdash;&mdash;**\n\n<img src=\"/images/517519-20161219165612260-1041210981.png\" alt=\"\" width=\"301\" height=\"255\" /> <img src=\"/images/517519-20161219165633275-873247263.png\" alt=\"\" width=\"296\" height=\"255\" /> <img src=\"/images/517519-20161219165647213-1007997440.png\" alt=\"\" width=\"301\" height=\"256\" />\n\n&nbsp;\n","tags":["ML"]},{"title":"机器学习——支持向量机(SVM)之核函数(kernel)","url":"/机器学习——支持向量机(SVM)之核函数(kernel).html","content":"对于**线性不可分**的数据集，可以利用**核函数（kernel）**将数据转换成易于分类器理解的形式。\n\n　　如下图，如果在x轴和y轴构成的坐标系中插入直线进行分类的话， 不能得到理想的结果，或许我们可以对圆中的数据进行某种形式的转换，从而得到某些新的变量来表示数据。在这种表示情况下，我们就更容易得到大于0或者小于0的测试结果。在这个例子中，我们将数据从一个特征空间转换到另一个特征空间，在新的空间下，我们可以很容易利用已有的工具对数据进行处理，将这个过程称之为**从一个特征空间到另一个特征空间的映射**。在通常情况下，这种映射会**将低维特征空间映射到高维空间**。\n\n<img src=\"/images/517519-20161127190536393-290442956.png\" alt=\"\" />\n\n　　这种从某个特征空间到另一个特征空间的映射是通过核函数来。\n\n　　SVM优化中一个特别好的地方就是，所有的运算都可以**写成内积（inner product）的形式**。向量的内积指的就是两个向量相乘，之后得到单个标量或者数值。我们可以**把内积运算替换成核函数**，而并不必做简化处理。将内积替换成核函数的方法被称之为**核技巧（kernel trick）**或者**核&ldquo;变电&rdquo;（kernel substation）**。\n\n<!--more-->\n&nbsp;\n\n**径向基核函数**\n\n径向基核函数是SVM中常用的一个**核函数**。**径向基函数**是一个**采用向量作为自变量的函数**，能够**基于向量距离运算**输出一个**标量**。\n\n&nbsp;\n\n```\n'''#######********************************\n以下是有核函数的版本\n'''#######********************************\nclass optStruct:\n    def __init__(self,dataMatIn, classLabels, C, toler, kTup):  # Initialize the structure with the parameters \n        self.X = dataMatIn\n        self.labelMat = classLabels\n        self.C = C\n        self.tol = toler\n        self.m = shape(dataMatIn)[0]\n        self.alphas = mat(zeros((self.m,1)))\n        self.b = 0\n        self.eCache = mat(zeros((self.m,2))) #first column is valid flag\n        self.K = mat(zeros((self.m,self.m)))\n        for i in range(self.m):\n            self.K[:,i] = kernelTrans(self.X, self.X[i,:], kTup)\n        \ndef calcEk(oS, k):\t\t\t#计算误差\n    fXk = float(multiply(oS.alphas,oS.labelMat).T*oS.K[:,k] + oS.b)\n    Ek = fXk - float(oS.labelMat[k])\n    return Ek\n        \ndef selectJ(i, oS, Ei):         \t#用于选择第2个循环（内循环）的alpha值，内循环中的启发式方法\n    maxK = -1; maxDeltaE = 0; Ej = 0\n    oS.eCache[i] = [1,Ei]  \t\t#set valid #choose the alpha that gives the maximum delta E\n    validEcacheList = nonzero(oS.eCache[:,0].A)[0]\n    if (len(validEcacheList)) > 1:\n        for k in validEcacheList:   \t#loop through valid Ecache values and find the one that maximizes delta E\n            if k == i: continue \t#跳过本身\n            Ek = calcEk(oS, k)\n            deltaE = abs(Ei - Ek)\n            if (deltaE > maxDeltaE):\t#选取具有最大步长的j\n                maxK = k; maxDeltaE = deltaE; Ej = Ek\n        return maxK, Ej\n    else:   #in this case (first time around) we don't have any valid eCache values\n        j = selectJrand(i, oS.m)\n        Ej = calcEk(oS, j)\n    return j, Ej\n\ndef updateEk(oS, k):\t\t\t#alpha改变后，更新缓存\n    Ek = calcEk(oS, k)\n    oS.eCache[k] = [1,Ek]\n        \n#内部循环的代码和简版的SMO代码很相似\ndef innerL(i, oS):\t\t\n    Ei = calcEk(oS, i)\n    #判断每一个alpha是否被优化过，如果误差很大，就对该alpha值进行优化，toler是容错率\n    if ((oS.labelMat[i]*Ei < -oS.tol) and (oS.alphas[i] < oS.C)) or ((oS.labelMat[i]*Ei > oS.tol) and (oS.alphas[i] > 0)):\n        j,Ej = selectJ(i, oS, Ei) \t\t\t#使用启发式方法选取第2个alpha，选取使得误差最大的alpha\n        alphaIold = oS.alphas[i].copy(); alphaJold = oS.alphas[j].copy();\n        #保证alpha在0与C之间\n        if (oS.labelMat[i] != oS.labelMat[j]):\t\t#当y1和y2异号，计算alpha的取值范围 \n            L = max(0, oS.alphas[j] - oS.alphas[i])\n            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])\n        else:\t\t\t\t\t\t#当y1和y2同号，计算alpha的取值范围\n            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)\n            H = min(oS.C, oS.alphas[j] + oS.alphas[i])\n        if L==H: print \"L==H\"; return 0\n\t#eta是alpha[j]的最优修改量，eta=K11+K22-2*K12,也是f(x)的二阶导数，K表示核函数\n        eta = 2.0 * oS.K[i,j] - oS.K[i,i] - oS.K[j,j] \t#changed for kernel\n        #如果二阶导数-eta <= 0，说明一阶导数没有最小值，就不做任何改变，本次循环结束直接运行下一次for循环\n        if eta >= 0: print \"eta>=0\"; return 0\n        oS.alphas[j] -= oS.labelMat[j]*(Ei - Ej)/eta\t#利用公式更新alpha[j]，alpha2new=alpha2-yj(Ei-Ej)/eta\n        oS.alphas[j] = clipAlpha(oS.alphas[j],H,L)\t#判断alpha的范围是否在0和C之间\n        updateEk(oS, j) \t\t\t\t#在alpha改变的时候更新Ecache\n        print \"j=\",j\n        print oS.alphas.A[j]\n        #如果alphas[j]没有调整，就忽略下面语句，本次循环结束直接运行下一次for循环\n        if (abs(oS.alphas[j] - alphaJold) < 0.00001): print \"j not moving enough\"; return 0\n        oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j])#update i by the same amount as j\n        updateEk(oS, i) \t\t\t\t#在alpha改变的时候更新Ecache                   \n        print \"i=\",i\n        print oS.alphas.A[i]     \n        #已经计算出了alpha，接下来根据模型的公式计算b   \n        b1 = oS.b - Ei- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,i] - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[i,j]\n        b2 = oS.b - Ej- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,j]- oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[j,j]\n        #根据公式确定偏移量b，理论上可选取任意支持向量来求解，但是现实任务中通常使用所有支持向量求解的平均值，这样更加鲁棒\n        if (0 < oS.alphas[i]) and (oS.C > oS.alphas[i]): oS.b = b1\n        elif (0 < oS.alphas[j]) and (oS.C > oS.alphas[j]): oS.b = b2\n        else: oS.b = (b1 + b2)/2.0\n        return 1\t\t#如果有任意一对alpha发生改变，返回1\n    else: return 0\n\n#完整版Platt SMO的外循环\ndef smoP(dataMatIn, classLabels, C, toler, maxIter,kTup=('lin', 0)):    \n    oS = optStruct(mat(dataMatIn),mat(classLabels).transpose(),C,toler, kTup)\n    iter = 0\n    entireSet = True; alphaPairsChanged = 0\n    while (iter < maxIter) and ((alphaPairsChanged > 0) or (entireSet)):\t#有alpha改变同时遍历次数小于最大次数，或者需要遍历整个集合\n        alphaPairsChanged = 0\n        #首先进行完整遍历，过程和简化版的SMO一样\n        if entireSet:   \t\n            for i in range(oS.m):        \n                alphaPairsChanged += innerL(i,oS)\t\t#i是第1个alpha的下标\n                print \"完整遍历, 迭代次数: %d i:%d, 成对改变的次数 %d\" % (iter,i,alphaPairsChanged)\n            iter += 1\n        #非边界遍历，挑选其中alpha值在0和C之间非边界alpha进行优化 \n        else:\n            nonBoundIs = nonzero((oS.alphas.A > 0) * (oS.alphas.A < C))[0]\t#然后挑选其中值在0和C之间的非边界alpha进行遍历\n            for i in nonBoundIs:\n                alphaPairsChanged += innerL(i,oS)\n                print \"非边界, 迭代次数: %d i:%d, 成对改变的次数 %d\" % (iter,i,alphaPairsChanged)\n            iter += 1\n        #如果这次是完整遍历的话，下次不用进行完整遍历\n        if entireSet: entireSet = False \t\t\t#终止完整循环\n        elif (alphaPairsChanged == 0): entireSet = True  \t#如果alpha的改变数量为0的话，再次遍历所有的集合一次\n        print \"iteration number: %d\" % iter\n    return oS.b,oS.alphas\n\ndef calcWs(alphas,dataArr,classLabels):\t\t#计算模型的参数w，即alpha*y*x转置的累加\n    X = mat(dataArr); labelMat = mat(classLabels).transpose()\n    m,n = shape(X)\n    w = zeros((n,1))\n    for i in range(m):\n        w += multiply(alphas[i]*labelMat[i],X[i,:].T)\n    return w\n\ndef testRbf(k1=1.3):\n    dataArr,labelArr = loadDataSet('testSetRBF.txt')\n    b,alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, ('rbf', k1)) #C=200 important\n    datMat=mat(dataArr); labelMat = mat(labelArr).transpose()\n    svInd=nonzero(alphas.A>0)[0]\n    sVs=datMat[svInd] #get matrix of only support vectors\n    labelSV = labelMat[svInd];\n    print \"there are %d Support Vectors\" % shape(sVs)[0]\n    m,n = shape(datMat)\n    errorCount = 0\n    for i in range(m):\n        kernelEval = kernelTrans(sVs,datMat[i,:],('rbf', k1))\n        predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b\n        if sign(predict)!=sign(labelArr[i]): errorCount += 1\n    print \"the training error rate is: %f\" % (float(errorCount)/m)\n    dataArr,labelArr = loadDataSet('testSetRBF2.txt')\n    errorCount = 0\n    datMat=mat(dataArr); labelMat = mat(labelArr).transpose()\n    m,n = shape(datMat)\n    for i in range(m):\n        kernelEval = kernelTrans(sVs,datMat[i,:],('rbf', k1))\n        predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b\n        if sign(predict)!=sign(labelArr[i]): errorCount += 1    \n    print \"the test error rate is: %f\" % (float(errorCount)/m)    \n    \ndef img2vector(filename):\n    returnVect = zeros((1,1024))\n    fr = open(filename)\n    for i in range(32):\n        lineStr = fr.readline()\n        for j in range(32):\n            returnVect[0,32*i+j] = int(lineStr[j])\n    return returnVect\n\ndef loadImages(dirName):\n    from os import listdir\n    hwLabels = []\n    trainingFileList = listdir(dirName)           #load the training set\n    m = len(trainingFileList)\n    trainingMat = zeros((m,1024))\n    for i in range(m):\n        fileNameStr = trainingFileList[i]\n        fileStr = fileNameStr.split('.')[0]     #take off .txt\n        classNumStr = int(fileStr.split('_')[0])\n        if classNumStr == 9: hwLabels.append(-1)\n        else: hwLabels.append(1)\n        trainingMat[i,:] = img2vector('%s/%s' % (dirName, fileNameStr))\n    return trainingMat, hwLabels    \n\ndef testDigits(kTup=('rbf', 10)):\n    dataArr,labelArr = loadImages('trainingDigits')\n    b,alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, kTup)\n    datMat=mat(dataArr); labelMat = mat(labelArr).transpose()\n    svInd=nonzero(alphas.A>0)[0]\n    sVs=datMat[svInd] \n    labelSV = labelMat[svInd];\n    print \"there are %d Support Vectors\" % shape(sVs)[0]\n    m,n = shape(datMat)\n    errorCount = 0\n    for i in range(m):\n        kernelEval = kernelTrans(sVs,datMat[i,:],kTup)\n        predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b\n        if sign(predict)!=sign(labelArr[i]): errorCount += 1\n    print \"the training error rate is: %f\" % (float(errorCount)/m)\n    dataArr,labelArr = loadImages('testDigits')\n    errorCount = 0\n    datMat=mat(dataArr); labelMat = mat(labelArr).transpose()\n    m,n = shape(datMat)\n    for i in range(m):\n        kernelEval = kernelTrans(sVs,datMat[i,:],kTup)\n        predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b\n        if sign(predict)!=sign(labelArr[i]): errorCount += 1    \n    print \"the test error rate is: %f\" % (float(errorCount)/m) \n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20161127213106690-1242215380.png\" alt=\"\" />\n","tags":["ML"]},{"title":"机器学习——支持向量机(SVM)之Platt SMO算法","url":"/机器学习——支持向量机(SVM)之Platt SMO算法.html","content":"**　　Platt SMO算法**是通过一个**外循环**来选择**第一个alpha值**的，并且其选择过程会在两种方式之间进行交替：\n\n一种方式是**在所有数据集上进行单遍扫描**，另一种方式则是**在非边界alpha中实现单遍扫描**。　　\n\n　　所谓**非边界alpha**指的就是那些**不等于边界0或者C的alpha值**。对整个数据集的扫描相当容易，而实现非边界alpha值的扫描时，首先需要建立这些alpha值的列表，然后再对这个表进行遍历。同时，该步骤会跳过那些已知的不会改变的alpha值，即。\n\n　　在选择第一个alpha值后，算法会通过一个**内循环**来选择**第二个alpha值**。在优化过程中，会通过**最大化步长**的方式来获得第二个alpha值。在简化版SMO算发放中，我们会在选择j之后计算错误率Ej。但在这里，我们会建立一个全局的缓存用于保存误差值，并从中**选择使得步长或者说Ei-Ej最大的alpha值**。\n\n<!--more-->\n&nbsp;\n\n**完整版的SMO代码及其注释：**\n\n```\n'''#######********************************\n以下是没有核函数的版本\n'''#######********************************\n\nclass optStructK:\n    def __init__(self,dataMatIn, classLabels, C, toler):  # Initialize the structure with the parameters \n        self.X = dataMatIn\n        self.labelMat = classLabels\n        self.C = C\n        self.tol = toler\n        self.m = shape(dataMatIn)[0]\n        self.alphas = mat(zeros((self.m,1)))\n        self.b = 0\n        self.eCache = mat(zeros((self.m,2))) #first column is valid flag\n        \ndef calcEkK(oS, k):\t\t\t#计算误差\n    fXk = float(multiply(oS.alphas,oS.labelMat).T*(oS.X*oS.X[k,:].T)) + oS.b\n    Ek = fXk - float(oS.labelMat[k])\n    return Ek\n        \ndef selectJK(i, oS, Ei):        \t#用于选择第2个循环（内循环）的alpha值，内循环中的启发式方法\n    maxK = -1; maxDeltaE = 0; Ej = 0\n    oS.eCache[i] = [1,Ei]  #set valid #choose the alpha that gives the maximum delta E\n    validEcacheList = nonzero(oS.eCache[:,0].A)[0]\n    if (len(validEcacheList)) > 1:\n        for k in validEcacheList:   #loop through valid Ecache values and find the one that maximizes delta E\n            if k == i: continue \t#跳过本身\n            Ek = calcEk(oS, k)\n            deltaE = abs(Ei - Ek)\n            if (deltaE > maxDeltaE):\t#选取具有最大步长的j\n                maxK = k; maxDeltaE = deltaE; Ej = Ek\n        return maxK, Ej\n    else:   #in this case (first time around) we don't have any valid eCache values\n        j = selectJrand(i, oS.m)\n        Ej = calcEk(oS, j)\n    return j, Ej\n\ndef updateEkK(oS, k):\t\t#alpha改变后，更新缓存\n    Ek = calcEk(oS, k)\n    oS.eCache[k] = [1,Ek]\n\n#内部循环的代码\ndef innerLK(i, oS):\t\t\n    Ei = calcEk(oS, i)\n    #判断每一个alpha是否被优化过，如果误差很大，就对该alpha值进行优化，toler是容错率\n    if ((oS.labelMat[i]*Ei < -oS.tol) and (oS.alphas[i] < oS.C)) or ((oS.labelMat[i]*Ei > oS.tol) and (oS.alphas[i] > 0)):\n        j,Ej = selectJ(i, oS, Ei) \t\t\t#使用启发式方法选取第2个alpha，选取使得误差最大的alpha\n        alphaIold = oS.alphas[i].copy(); alphaJold = oS.alphas[j].copy();\n        #保证alpha在0与C之间\n        if (oS.labelMat[i] != oS.labelMat[j]):\t\t#当y1和y2异号，计算alpha的取值范围 \n            L = max(0, oS.alphas[j] - oS.alphas[i])\n            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])\n        else:\t\t\t\t\t\t#当y1和y2同号，计算alpha的取值范围\n            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)\n            H = min(oS.C, oS.alphas[j] + oS.alphas[i])\n        if L==H: print \"L==H\"; return 0\n        #eta是alpha[j]的最优修改量，eta=K11+K22-2*K12,也是f(x)的二阶导数，K表示内积\n        eta = 2.0 * oS.X[i,:]*oS.X[j,:].T - oS.X[i,:]*oS.X[i,:].T - oS.X[j,:]*oS.X[j,:].T\n        #如果二阶导数-eta <= 0，说明一阶导数没有最小值，就不做任何改变，本次循环结束直接运行下一次for循环\n        if eta >= 0: print \"eta>=0\"; return 0\n        oS.alphas[j] -= oS.labelMat[j]*(Ei - Ej)/eta\t#利用公式更新alpha[j]，alpha2new=alpha2-yj(Ei-Ej)/eta\n        oS.alphas[j] = clipAlpha(oS.alphas[j],H,L)\t#判断alpha的范围是否在0和C之间\n        updateEk(oS, j) \t\t\t\t#在alpha改变的时候更新Ecache\n        #如果alphas[j]没有调整，就忽略下面语句，本次循环结束直接运行下一次for循环\n        if (abs(oS.alphas[j] - alphaJold) < 0.00001): print \"j not moving enough\"; return 0\n        oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j])#update i by the same amount as j\n        updateEk(oS, i) \t\t\t\t#在alpha改变的时候更新Ecache\n        #已经计算出了alpha，接下来根据模型的公式计算b  \n        b1 = oS.b - Ei- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.X[i,:]*oS.X[i,:].T - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.X[i,:]*oS.X[j,:].T\n        b2 = oS.b - Ej- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.X[i,:]*oS.X[j,:].T - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.X[j,:]*oS.X[j,:].T\n        #根据公式确定偏移量b，理论上可选取任意支持向量来求解，但是现实任务中通常使用所有支持向量求解的平均值，这样更加鲁棒\n        if (0 < oS.alphas[i]) and (oS.C > oS.alphas[i]): oS.b = b1\n        elif (0 < oS.alphas[j]) and (oS.C > oS.alphas[j]): oS.b = b2\n        else: oS.b = (b1 + b2)/2.0\n        return 1\t\t\t#如果有任意一对alpha发生改变，返回1\n    else: return 0\n\n#无核函数 Platt SMO 的外循环\ndef smoPK(dataMatIn, classLabels, C, toler, maxIter):    #full Platt SMO\n    oS = optStruct(mat(dataMatIn),mat(classLabels).transpose(),C,toler)\n    iter = 0\n    entireSet = True; alphaPairsChanged = 0\n    #有alpha改变同时遍历次数小于最大次数，或者需要遍历整个集合\n    while (iter < maxIter) and ((alphaPairsChanged > 0) or (entireSet)):\t\n        alphaPairsChanged = 0\n        #首先进行完整遍历，过程和简化版的SMO一样\n        if entireSet:   #go over all\n            for i in range(oS.m):        \n                alphaPairsChanged += innerL(i,oS)\t\t#i是第1个alpha的下标\n                print \"fullSet, iter: %d i:%d, pairs changed %d\" % (iter,i,alphaPairsChanged)\n            iter += 1\n        #非边界遍历，挑选其中alpha值在0和C之间非边界alpha进行优化 \n        else:#go over non-bound (railed) alphas\n            nonBoundIs = nonzero((oS.alphas.A > 0) * (oS.alphas.A < C))[0]\t#然后挑选其中值在0和C之间的非边界alpha进行遍历\n            for i in nonBoundIs:\n                alphaPairsChanged += innerL(i,oS)\n                print \"non-bound, iter: %d i:%d, pairs changed %d\" % (iter,i,alphaPairsChanged)\n            iter += 1\n        #如果这次是完整遍历的话，下次不用进行完整遍历\n        if entireSet: entireSet = False \t\t\t#终止完整循环\n        elif (alphaPairsChanged == 0): entireSet = True  \t#如果alpha的改变数量为0的话，再次遍历所有的集合一次\n        print \"iteration number: %d\" % iter\n    return oS.b,oS.alphas\n\n```\n\n&nbsp;\n","tags":["ML"]},{"title":"ubuntu重启搜狗输入法","url":"/ubuntu重启搜狗输入法.html","content":"```\nfcitx | xargs kill\nsogou-qimpanel | xargs kill\n\n```\n\n<!--more-->\n&nbsp;或者编写Shell脚本restart_sougou.sh，放到`/usr/bin`目录下，不要忘记chmod修改运行权限，然后就能在终端输入restart_sougou命令来执行重启搜狗输入法\n\n```\n#!/bin/sh\npidof fcitx | xargs kill\npidof sogou-qimpanel | xargs kill\nnohup fcitx  1>/dev/null 2>/dev/null &amp;\nnohup sogou-qimpanel  1>/dev/null 2>/dev/null &amp;\n\n```\n\n&nbsp;\n","tags":["Linux"]},{"title":"广告系统——搜索广告","url":"/广告系统——搜索广告.html","content":"**搜索广告**是指广告主根据自己的产品或服务的内容、特点等，确定相关的关键词，撰写广告内容并自主定价投放的广告。\n\n<!--more-->\n&nbsp;\n\n&nbsp;\n\n参考：[互联网搜索广告介绍(一)](https://www.cnblogs.com/Jack47/p/intro_to_sponsored_search.html)\n\n[互联网搜索广告介绍(二)](https://www.cnblogs.com/Jack47/p/intro_to_sponsored_search-1.html)\n","tags":["广告系统"]},{"title":"Mongo节点类型","url":"/Mongo节点类型.html","content":"## 1.Mongo集群类型\n\n1.Master/Slave 已经不推荐\n\n2.Replica Set\n\n3.Sharded Cluster，其包含3个组件：mongos，config server和mongod\n\n<img src=\"/images/517519-20230301111532643-344925570.png\" alt=\"\" width=\"593\" height=\"408\" loading=\"lazy\" />\n\n参考：[mongodb 三种集群的区别（Replica Set / Sharding / Master-Slaver）](https://segmentfault.com/q/1010000012592436)\n\n[Mongodb高级篇-Replication &amp; Sharding](https://www.jianshu.com/p/ddcc3643aec9)\n\n[https://www.mongodb.com/docs/manual/sharding/#sharded-cluster](https://www.mongodb.com/docs/manual/sharding/#sharded-cluster)\n\n<!--more-->\n&nbsp;\n\n## 2.Mongo节点类型\n\n1.primary节点\n\n2.secondary节点\n\n3.SHARD_ROUTER节点\n\n4.STANDALONE节点\n\n5.OTHER节点\n\n&nbsp;\n","tags":["mongo"]},{"title":"机器学习——支持向量机(SVM)之拉格朗日乘子法，KKT条件以及简化版SMO算法分析","url":"/机器学习——支持向量机(SVM)之拉格朗日乘子法，KKT条件以及简化版SMO算法分析.html","content":"**SVM**有很多实现，现在只关注其中最流行的一种实现，即**序列最小优化（Sequential Minimal Optimization，SMO）算法**，然后介绍如何使用一种**核函数（kernel）**的方式将SVM扩展到更多的数据集上。\n\n<!--more-->\n&nbsp;\n\n**1.基于最大间隔分隔数据**\n\n几个概念：\n\n**1.线性可分（linearly separable）**：对于图6-1中的圆形点和方形点，如果很容易就可以在图中画出一条直线将两组数据点分开，就称这组数据为**线性可分**数据\n\n**2.分隔超平面（separating hyperplane）**：将数据集分隔开来的直线称为**分隔超平面**\n\n3.如果数据集是1024维的，那么就需要一个1023维的超平面来对数据进行分隔\n\n**4.间隔（margin）**：数据点到分隔面的距离称为间隔\n\n**5.支持向量（support vector）**:离分隔超平面最近的那些点\n\n&nbsp;\n\n支持向量机的**优点**：泛化错误率低，计算开销不大，结果易解释\n\n支持向量机的**缺点**：对**参数调节**和**核函数**的选择敏感，原始分类器不加修改仅适用于处理二类问题\n\n**适用数据类型**：数值型和标称型数据**<br />**\n\n<img src=\"/images/517519-20161119205818060-72763193.png\" alt=\"\" /><img src=\"/images/517519-20161119210352779-372502914.png\" alt=\"\" />\n\n&nbsp;\n\n**2.寻找最大间隔**\n\n如何求解数据集的**最佳分隔直线**？\n\n**分隔超平面**的形式可以写成&nbsp;&nbsp;<img src=\"/images/517519-20161119212810873-1095052075.gif\" alt=\"\" />\n\n其中 w = (w1,w2,w3...wd)为**法向量**，决定了超平面的方向，其中d等于数据的维度，\n\n这很好理解，假设二维的(x1,x2)点可以被 ax+b=0 分隔，这里面直线 ax+b=0 是一维的，但是这里面a和x都是二维的\n\nb为**位移项**，决定了超平面与原点之间的距离\n\n对于图6-3中A点到分隔直线的**距离**为&nbsp; <img src=\"/images/517519-20161119212215998-1385030410.gif\" alt=\"\" />\n\n<img src=\"/images/517519-20161121154505018-995625440.gif\" alt=\"\" />表示向量的模，<img src=\"/images/517519-20161121154710346-1816491730.gif\" alt=\"\" />，w与w共轭的内积再开方\n\n<img src=\"/images/517519-20161119210929357-604622496.png\" alt=\"\" />\n\n假设超平面（w,b）能将训练样本正确分类，即对于 <img src=\"/images/517519-20161120153201951-1025233280.gif\" alt=\"\" />,\n\n有&nbsp; <img src=\"/images/517519-20161120153921513-1576248773.gif\" alt=\"\" />\n\n则两个异类**支持向量**到**超平面**的**距离之和**为 <img src=\"/images/517519-20161120154524779-2087020272.gif\" alt=\"\" />\n\n欲找到**具有&ldquo;最大间隔（maximum margin）&rdquo;的划分超平面**，也就是要找到能**满足 **<img src=\"/images/517519-20161120153921513-1576248773.gif\" alt=\"\" />** 中约束的参数w和b，使得<img src=\"/images/517519-20161120154811545-811234539.gif\" alt=\"\" />最大**，即\n\n&nbsp; 　　<img src=\"/images/517519-20161120160323029-749909102.gif\" alt=\"\" />&nbsp; ，\n\n**其中约束条件为**　s.t.&nbsp; <img src=\"/images/517519-20161120160530873-1090029417.gif\" alt=\"\" /> ，其实这个约束条件就是把两个不等式合并成了一个\n\n显然，为了**最大化间隔**，仅需**最大化** <img src=\"/images/517519-20161120160658310-1157447378.gif\" alt=\"\" />，这等价于**最小化 <img src=\"/images/517519-20161120160753701-1935394994.gif\" alt=\"\" />** ，于是上式可重写为\n\n　　<img src=\"/images/517519-20161120160938498-586563504.gif\" alt=\"\" />，\n\n**其中约束条件为**　s.t.&nbsp; <img src=\"/images/517519-20161120160530873-1090029417.gif\" alt=\"\" />\n\n这就是**支持向量机（Support Vector Machine，简称SVM）的基本型**\n\n&nbsp;\n\n对于这类**带有不等式约束的最优化问题**，可以使用**拉格朗日乘子法（Lagrange Multiplier）**对其进行求解。\n\n首先先了解一下**最优化问题**的分类，最优化问题可以分为一下三类：\n\n**<1>无约束的优化问题**，可以写成：\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img src=\"/images/517519-20161120205243842-1733982077.gif\" alt=\"\" />\n\n对于第<1>类的优化问题，常常使用的方法就是**Fermat定理**，即使用**求取f(x)的导数**，然后令其为零，可以求得候选最优值，再在这些候选值中验证；如果是凸函数，可以保证是最优解。\n\n**<2>有等式约束的优化问题**，可以写成：\n\n　　<img src=\"/images/517519-20161120205243842-1733982077.gif\" alt=\"\" />\n\n　　约束条件&nbsp; <img src=\"/images/517519-20161120205619482-1487489645.gif\" alt=\"\" />\n\n对于第<2>类的优化问题，常常使用的方法就是**拉格朗日乘子法（Lagrange Multiplier)** ，即把**等式约束** <img src=\"/images/517519-20161120205844029-1149195359.gif\" alt=\"\" /> 用一个系数与**目标函数f(x)**写为一个式子，称为**拉格朗日函数**，而系数称为**拉格朗日乘子**。**拉格朗日函数**的形式如下，u即**拉格朗日乘子**：\n\n　　<img src=\"/images/517519-20161120210557232-191696071.gif\" alt=\"\" />\n\n**通过拉格朗日函数对各个变量求导**，令其为零，可以求得候选值集合，然后验证求得最优值。\n\n**<3>有不等式约束的优化问题**，可以写成：\n\n　　<img src=\"/images/517519-20161120205243842-1733982077.gif\" alt=\"\" />\n\n　　约束条件&nbsp;&nbsp;<img src=\"/images/517519-20161120211011576-1631829227.gif\" alt=\"\" />\n\n　　　　　　　　　<img src=\"/images/517519-20161120211050935-1029180450.gif\" alt=\"\" />\n\n对于第<3>类的优化问题，常常使用的方法就是**KKT条件**。同样地，我们把所有的**等式、不等式约束**与**f(x)**写为一个式子，也叫**拉格朗日函数**，系数也称**拉格朗日乘子**，通过一些条件，可以求出最优值的必要条件，这个条件称为**KKT条件**。\n\n&nbsp;\n\n**拉格朗日乘子法（Lagrange Multiplier）：**\n\n&nbsp;拉格朗日乘子法可以应用于**有等式约束的优化问题**和**有不等式约束的优化问题**，\n\n对于**有等式约束的优化问题**， <img src=\"/images/517519-20161120205243842-1733982077.gif\" alt=\"\" />，约束条件&nbsp; <img src=\"/images/517519-20161120205619482-1487489645.gif\" alt=\"\" /> ，其中 f(x) 被称为**目标函数**\n\n<1>当**没有约束条件**的时候，我们通过**求导**的方法，来寻找最优点\n\n设 <img src=\"/images/517519-20161120212116888-516015937.gif\" alt=\"\" /> 是这个最优点，即此时有\n\n　　<img src=\"/images/517519-20161120212520467-1303579209.gif\" alt=\"\" />&nbsp;\n\n此外，如果 f(x) 是一个实值函数，<img src=\"/images/517519-20161120212719935-139427790.gif\" alt=\"\" /> 是一个n维向量的话，那么 f(x) 对向量 <img src=\"/images/517519-20161120212719935-139427790.gif\" alt=\"\" /> 的导数被定义为\n\n　　<img src=\"/images/517519-20161120213037013-417143938.gif\" alt=\"\" />\n\n&nbsp;\n\n<2>当**有一个等式约束条件** <img src=\"/images/517519-20161120205619482-1487489645.gif\" alt=\"\" /> 的时候，举例设\n\n　　<img src=\"/images/517519-20161120214500357-822366773.gif\" alt=\"\" />\n\n　　<img src=\"/images/517519-20161120214558404-1224820432.gif\" alt=\"\" />\n\n从几何的角度看，可以看成是在一个曲面 <img src=\"/images/517519-20161120214935513-2016752650.gif\" alt=\"\" /> 上寻找函数 <img src=\"/images/517519-20161120215020592-1596341636.gif\" alt=\"\" /> 的最小值\n\n设目标函数&nbsp; <img src=\"/images/517519-20161120223250826-648340093.gif\" alt=\"\" />，当 z 取不同的值的时候，相当于可以投影在曲面 <img src=\"/images/517519-20161120214935513-2016752650.gif\" alt=\"\" />上，即形成**等高线**，如下图\n\n<img src=\"/images/517519-20161120225534435-468977411.jpg\" alt=\"\" />\n\n当 <img src=\"/images/517519-20161120223250826-648340093.gif\" alt=\"\" /> 取d1或者d2的时候，形成了**虚线的等高线**，此时我们**要求的是最小的z**\n\n现在我们约束 <img src=\"/images/517519-20161120223749310-539711456.gif\" alt=\"\" /> 的时候，在曲面 <img src=\"/images/517519-20161120214935513-2016752650.gif\" alt=\"\" />上形成一条曲线，即图中**红色的曲线**\n\n假设形成的**红色曲线**与**等高线相交**，**交点就是同时满足等式约束条件和目标函数的可行域的值**，但肯定不是最优值，因为相交意味着肯定还存在其它的等高线在该条等高线的内部或者外部，使得新的等高线与目标函数的交点的值更大或者更小。\n\n只有到**等高线与目标函数的曲线相切**的时候，**可能取得最优值**，如上图中的情况，即**等高线和目标函数的曲线在该点的法向量必须有相同方向**，所以最优值必须满足：f(x)的梯度 = <img src=\"/images/517519-20161120224928060-1442965907.gif\" alt=\"\" /> * h(x)的梯度， <img src=\"/images/517519-20161120224928060-1442965907.gif\" alt=\"\" /> 是常数，表示左右两边同向。这个等式就是**拉格朗日函数** <img src=\"/images/517519-20161120210557232-191696071.gif\" alt=\"\" /> **对参数x求导**后，**令其等于0**的结果，即<img src=\"/images/517519-20161120230110310-1484857061.gif\" alt=\"\" />，最后求得**最优解**是 <img src=\"/images/517519-20161120220720248-1305229386.gif\" alt=\"\" /> 。\n\n&nbsp;\n\n以上的约束条件只是 <img src=\"/images/517519-20161120223749310-539711456.gif\" alt=\"\" />，这时约束条件是**一个等式**。\n\n<3>那么当**约束条件是多个等式**的时候，**拉格朗日函数**应该改写成\n\n<img src=\"/images/517519-20161121094925565-1122116578.gif\" alt=\"\" /> ，<img src=\"/images/517519-20161121103214612-1720539992.gif\" alt=\"\" />是需要求其最小值的**目标函数**，<img src=\"/images/517519-20161121095905393-884156279.gif\" alt=\"\" />是**拉格朗日乘子**，<img src=\"/images/517519-20161121100012940-2140803854.gif\" alt=\"\" />是**约束条件**\n\n&nbsp;\n\n<4>如果**约束条件是等式和不等式，且有多个**的时候，就需要若干个**KKT<strong>(Karush-Kuhn-Tucker)**条件</strong>，即对于\n\n　　<img src=\"/images/517519-20161120205243842-1733982077.gif\" alt=\"\" />\n\n　　约束条件&nbsp;&nbsp;<img src=\"/images/517519-20161120211011576-1631829227.gif\" alt=\"\" />\n\n　　　　　　　　　<img src=\"/images/517519-20161120211050935-1029180450.gif\" alt=\"\" />\n\n想取得**最优解**，要满足以下条件（**KKT条件**）：\n\n　　1. L(x, a, b)对x求导为零；\n\n　　2. h(x) =0;\n\n　　3. a*g(x) = 0;\n\n&nbsp;\n\n一步步来说明，此时我们要求f(x)的最小值，可以构建**拉格朗日函数**\n\n**　　<img src=\"/images/517519-20161121105148971-1001425650.gif\" alt=\"\" />**\n\n为什么这么构建呢？因为当 g(x)<=0，h(x)=0，a>=0 的时候，（**注意此处的h(x)=0是KKT条件的第2个**）\n\n**a*g(x)<=0**，所以 <img src=\"/images/517519-20161121141245081-1548633830.gif\" alt=\"\" /> 在**取得最大值**的时候，即a*g(x)=0的时候，**就等于f(x)**，（**注意此处的a*g(x)=0是KKT条件的第3个**）\n\n所以需要求解的**目标函数的最小值**写成表达式是\n\n　　<img src=\"/images/517519-20161121141806925-417367662.gif\" alt=\"\" />\n\n上式的**对偶表达式**是\n\n　　<img src=\"/images/517519-20161121141920284-1677401358.gif\" alt=\"\" />\n\n由于我们要求的最优化是满足**强对偶**的，即**对偶式子的最优值等于原式子的最优解**\n\n设当 <img src=\"/images/517519-20161121142201018-1649113246.gif\" alt=\"\" /> 的时候有最优解，此时\n\n　　<img src=\"/images/517519-20161121142602800-728735603.gif\" alt=\"\" />\n\n　　　　　<img src=\"/images/517519-20161121142825175-188007659.gif\" alt=\"\" />\n\n　　　　　<img src=\"/images/517519-20161121143630175-576926988.gif\" alt=\"\" />\n\n即函数 <img src=\"/images/517519-20161121143330128-767842863.gif\" alt=\"\" /> 在 <img src=\"/images/517519-20161121143351440-1539851469.gif\" alt=\"\" /> 处取得最小值\n\n用**fermat定理**，对于函数 <img src=\"/images/517519-20161121143330128-767842863.gif\" alt=\"\" /> **求取导数**后令其等于0后的<img src=\"/images/517519-20161121143351440-1539851469.gif\" alt=\"\" />结果，（**注意此处的导数=0是KKT条件的第1个**）\n\n即f(x)的梯度 + a*g(x)的梯度 + b*h(x)的梯度 = 0\n\n&nbsp;\n\n所以对于**多个不等式和等式混合的约束条件**的时候，**拉格朗日函数**可以写成\n\n**　　<img src=\"/images/517519-20161121105148971-1001425650.gif\" alt=\"\" />**\n\n　　约束条件&nbsp;&nbsp;<img src=\"/images/517519-20161120211011576-1631829227.gif\" alt=\"\" />\n\n　　　　　　　　　<img src=\"/images/517519-20161120211050935-1029180450.gif\" alt=\"\" />\n\n**从而上面提到的支持向量机（Support Vector Machine，简称SVM）的基本型**\n\n**　　<img src=\"/images/517519-20161120160938498-586563504.gif\" alt=\"\" />，其中约束条件为**　s.t.&nbsp; <img src=\"/images/517519-20161120160530873-1090029417.gif\" alt=\"\" />\n\n就可以重新写成\n\n　　<img src=\"/images/517519-20161121145658268-1680724641.gif\" alt=\"\" />\n\n其中 <img src=\"/images/517519-20161121152720487-1409045968.gif\" alt=\"\" />，<img src=\"/images/517519-20161121152757628-307838811.gif\" alt=\"\" />\n\n接下来由**第1个****KKT条件（导数等于0）**，令 <img src=\"/images/517519-20161121151222159-783829004.gif\" alt=\"\" /> 对 <img src=\"/images/517519-20161121151424471-1545026850.gif\" alt=\"\" /> 和 b 的**偏导等于0**，可得\n\n　　<img src=\"/images/517519-20161121213035581-417058637.gif\" alt=\"\" />\n\n把上面两个等式带入 <img src=\"/images/517519-20161121153502159-1503735332.gif\" alt=\"\" /> 中，得\n\n　　&nbsp;<img src=\"/images/517519-20161121172429362-1609252710.gif\" alt=\"\" /> 的对偶表达式等于\n\n　　<img src=\"/images/517519-20161121220759471-388980748.gif\" alt=\"\" />\n\n　　<img src=\"/images/517519-20161121213432253-1826291133.gif\" alt=\"\" />\n\n约束条件 <img src=\"/images/517519-20161121221108159-2120964952.gif\" alt=\"\" />\n\n在最后推出的公式中，**y表示的是标签向量**，**u表示的是拉格朗日乘子**，**x表示的是待分类的点的坐标**，也是一个向量\n\n现在**约束条件变成了等式**，所以**我们的任务变成了**\n\n　　<img src=\"/images/517519-20161123203049315-515256614.gif\" alt=\"\" />\n\n约束条件 <img src=\"/images/517519-20161121221108159-2120964952.gif\" alt=\"\" />\n\n&nbsp;\n\n以上的推导有一个**假设条件**，就是数据是**线性可分**的，而实际上，大多数情况是**线性不可分**的，对于这种情况：\n\n我们把约束条件从 <img src=\"/images/517519-20161120160530873-1090029417.gif\" alt=\"\" /> 改成 <img src=\"/images/517519-20161123203510378-42258974.gif\" alt=\"\" />，允许出现例外\n\n所以**支持向量机（Support Vector Machine，简称SVM）的基本型**就需要改写成，加上了**松弛变量**（这里面有泛函的数学原理，参考《神经网络与机器学习 第3版》）\n\n　　<img src=\"/images/517519-20161123205516300-676129216.gif\" alt=\"\" />，其中约束条件为 <img src=\"/images/517519-20161123205658237-582783719.gif\" alt=\"\" />\n\n通过和上面一样的对 w,b和<img src=\"/images/517519-20161123205745878-203257725.gif\" alt=\"\" />求偏导的过程，进行简化之后，也是得到\n\n　　<img src=\"/images/517519-20161123203049315-515256614.gif\" alt=\"\" />\n\n但是**约束条件**变成了 <img src=\"/images/517519-20161123210017737-2125389590.gif\" alt=\"\" />\n\n&nbsp;\n\n现在可以应用上面的**等式约束条件**的方法，求出 u 后，在求出w与b即可得到模型\n\n　　<img src=\"/images/517519-20161121222624534-239668829.gif\" alt=\"\" />&nbsp;\n\n&nbsp;\n\n**2.SMO高效优化算法**\n\n接下来分析**SMO算法**，因为有**约束条件** <img src=\"/images/517519-20161123210017737-2125389590.gif\" alt=\"\" />，所以**一次不能只更新一个 <img src=\"/images/517519-20161123210223550-906147236.gif\" alt=\"\" width=\"15\" height=\"12\" />**，因为如果一次只更新一个 <img src=\"/images/517519-20161123210223550-906147236.gif\" alt=\"\" width=\"15\" height=\"12\" /> 的话，那么累加之后的结果就不能等于0，这就**不满足约束条件**了，所以SMO算法采取的是**挑选一对 <img src=\"/images/517519-20161123210223550-906147236.gif\" alt=\"\" width=\"15\" height=\"12\" /> 进行更新**，这样就能满足约束条件。\n\n举例，当更新** <img src=\"/images/517519-20161123210223550-906147236.gif\" alt=\"\" width=\"15\" height=\"12\" />** 的时候，一次同时更新 <img src=\"/images/517519-20161123211258893-246859444.gif\" alt=\"\" /> 和 <img src=\"/images/517519-20161123211307643-553413937.gif\" alt=\"\" />，根据约束条件 <img src=\"/images/517519-20161123210017737-2125389590.gif\" alt=\"\" />，\n\n那么 <img src=\"/images/517519-20161123213443550-1815768005.gif\" alt=\"\" />，即1和2相加，等于3到m相加的负数。\n\n<img src=\"/images/517519-20161123213807925-678109614.png\" alt=\"\" />\n\n图来自网上\n\n**当y1和y2异号的时候，即y1=1,y2=-1或者y1=-1,y2=1的时候**，我们先固定一个 <img src=\"/images/517519-20161123211258893-246859444.gif\" alt=\"\" /> 的值，然后求 <img src=\"/images/517519-20161123211307643-553413937.gif\" alt=\"\" /> 的范围\n\n**<1>设y1=1,y2=-1**\n\n则如上图所示，横坐标轴是 <img src=\"/images/517519-20161123211258893-246859444.gif\" alt=\"\" /> ，纵坐标轴是 <img src=\"/images/517519-20161123211307643-553413937.gif\" alt=\"\" />，因为 <img src=\"/images/517519-20161123213443550-1815768005.gif\" alt=\"\" />，且&nbsp;**<img src=\"/images/517519-20161123210223550-906147236.gif\" alt=\"\" width=\"15\" height=\"12\" />** 是大于等于0的，\n\n所以当 <img src=\"/images/517519-20161123211258893-246859444.gif\" alt=\"\" /> 和 <img src=\"/images/517519-20161123211307643-553413937.gif\" alt=\"\" /> 有可能满足**红线**的情况，即y1=1,y2=-1时，<img src=\"/images/517519-20161125112838096-1953986228.gif\" alt=\"\" />，那么 <img src=\"/images/517519-20161123211307643-553413937.gif\" alt=\"\" /> 的范围是 <img src=\"/images/517519-20161123215800393-116823641.gif\" alt=\"\" />，即 <img src=\"/images/517519-20161123215833815-949974384.gif\" alt=\"\" />\n\n所以当 <img src=\"/images/517519-20161123211258893-246859444.gif\" alt=\"\" /> 和 <img src=\"/images/517519-20161123211307643-553413937.gif\" alt=\"\" /> 也有可能满足**蓝线**的情况，即y1=1,y2=-1时，<img src=\"/images/517519-20161125112838096-1953986228.gif\" alt=\"\" />，那么 <img src=\"/images/517519-20161123211307643-553413937.gif\" alt=\"\" /> 的范围是 <img src=\"/images/517519-20161123215951362-973799435.gif\" alt=\"\" />，即&nbsp;<img src=\"/images/517519-20161125142513284-319227777.gif\" alt=\"\" />\n\n由于**不知道是红线还是蓝线**，所以 <img src=\"/images/517519-20161123211307643-553413937.gif\" alt=\"\" /> **的取值范围为：<br />**\n\n　　<img src=\"/images/517519-20161125142648440-1467544045.gif\" alt=\"\" />\n\n&nbsp;**<2>设y1=-1,y2=1**\n\n同样可能通过画图来说明，计算出来的 <img src=\"/images/517519-20161123211307643-553413937.gif\" alt=\"\" /> **的取值范围仍然是：**\n\n**　　<img src=\"/images/517519-20161125142648440-1467544045.gif\" alt=\"\" />**\n\n综上所述，**当y1和y2异号的时候，<img src=\"/images/517519-20161123211307643-553413937.gif\" alt=\"\" /> <strong>的取值范围为**：</strong>\n\n　　**<img src=\"/images/517519-20161125142648440-1467544045.gif\" alt=\"\" />**\n\n&nbsp;\n\n**同理，当y1和y2同号的时候，<img src=\"/images/517519-20161123211307643-553413937.gif\" alt=\"\" /> <strong>的取值范围**为：</strong>\n\n　　<img src=\"/images/517519-20161125145157753-1791760819.gif\" alt=\"\" />\n\n&nbsp;\n\n以上内容的参考文献：\n\n[支持向量机入门系列-2：等式约束极小的最优性条件 ](http://blog.csdn.net/vivihe0/article/details/7037945)\n\n[深入理解拉格朗日乘子法（Lagrange Multiplier) 和KKT条件 ](http://blog.csdn.net/xianlingmao/article/details/7919597)\n\n《机器学习》&mdash;&mdash;周志华\n\n《神经网络与机器学习（第3版）》\n\n《机器学习实战》\n\n&nbsp;\n\n以下内容参考的网址：\n\n[支持向量机（SVM），SMO算法原理及源代码剖析 ](http://www.tuicool.com/articles/RRZvYb)\n\n《机器学习实战》\n\n&nbsp;\n\n我们需要**求解的问题**是\n\n<img src=\"/images/517519-20161123203049315-515256614.gif\" alt=\"\" />\n\n**约束条件为&nbsp;** <img src=\"/images/517519-20161123210017737-2125389590.gif\" alt=\"\" />\n\n现在把求最大值转换成求最小值，其实是一样的，现在问题变成了\n\n<img src=\"/images/517519-20161125150609878-5966626.gif\" alt=\"\" />\n\n**约束条件为&nbsp;** <img src=\"/images/517519-20161123210017737-2125389590.gif\" alt=\"\" />\n\n&nbsp;\n\n因为 <img src=\"/images/517519-20161123213443550-1815768005.gif\" alt=\"\" />\n\n所以 <img src=\"/images/517519-20161125145909487-1372911272.gif\" alt=\"\" />\n\n接下来将 <img src=\"/images/517519-20161123211258893-246859444.gif\" alt=\"\" /> 和 <img src=\"/images/517519-20161123211307643-553413937.gif\" alt=\"\" /> 代入 <img src=\"/images/517519-20161125150609878-5966626.gif\" alt=\"\" />\n\n得 <img src=\"/images/517519-20161125150609878-5966626.gif\" alt=\"\" />\n\n&nbsp;\n\n**以下推导内容是CSDN上一篇博文的内容，直接贴图片，符号表示有些许区别**\n\n**<img src=\"/images/517519-20161123230013362-1224223527.png\" alt=\"\" />**\n\n<img src=\"/images/517519-20161123230038550-433546900.png\" alt=\"\" />\n\n<img src=\"/images/517519-20161123230107300-1640299292.png\" alt=\"\" />\n\n<img src=\"/images/517519-20161123230132456-270446226.png\" alt=\"\" />\n\n&nbsp;\n\n对**最小化的目标函数**和**约束条件**进行优化，以下是Python代码对应的**伪代码流程**\n\n&nbsp;<img src=\"/images/517519-20161120143914795-1219132426.png\" alt=\"\" />\n\n&nbsp;<img src=\"/images/517519-20161120143937498-2075072408.png\" alt=\"\" />\n\n&nbsp;\n\n**源代码及代码注释：**\n\n\n&nbsp;\n\n```\nfrom numpy import *\nfrom time import sleep\nimport time\n\ndef loadDataSet(fileName):\t#逐行解析，得到每行的类标签和整个数据矩阵\n    dataMat = []; labelMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        lineArr = line.strip().split('\\t')\n        dataMat.append([float(lineArr[0]), float(lineArr[1])])\n        labelMat.append(float(lineArr[2]))\n    return dataMat,labelMat\n\n#辅助函数，用于在某一区间范围内随机选择一个整数\n#i是第一个alpha的下标，m是所有alpha的数目，只要函数值不等于输入值i，函数就会进行随机选择\ndef selectJrand(i,m):\t\n    j=i \t\t#we want to select any J not equal to i\n    while (j==i):\n        j = int(random.uniform(0,m))\n    return j\n\ndef clipAlpha(aj,H,L):\t#辅助函数，数值太大或者太小时对其进行调整\n    if aj > H: \n        aj = H\n    if L > aj:\n        aj = L\n    return aj\n\n#Platt SMO算法中的外循环确定要优化的最佳aplha集合，而简化版却会跳过这一部分\n#首先在数据集上遍历每一个alpha，然后在剩下的alpha集合中随机选择另一个alpha，构建alpha对\n#简化版SMO算法，数据集，类别标签，常数C，容错率，取消前最大的循环次数\ndef smoSimple(dataMatIn, classLabels, C, toler, maxIter):\t\n    dataMatrix = mat(dataMatIn); labelMat = mat(classLabels).transpose()\t#把列表dataMatIn和classLabels转换成矩阵dataMatrix和labelMat\n    b = 0; m,n = shape(dataMatrix)\t#初始的b=0,m=100,n=2\n    alphas = mat(zeros((m,1)))\t\t#生成一个100&times;1的零矩阵\n    iter = 0\n    while (iter < maxIter):\t#在iter小于maxIter的时候循环\n        alphaPairsChanged = 0\t#用于记录alpha是否已经进行优化\n        for i in range(m):\t#循环所有数据集，总共有100个\n            fXi = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b\t#fXi是预测的类别，i是alpha的下标，把第i个x代入公式计算\n            Ei = fXi - float(labelMat[i])\t\t\t\t\t\t#Ei是第i数据向量的计算误差\n            #判断每一个alpha是否被优化过，如果误差很大，就对该alpha值进行优化，toler是容错率\n            if ((labelMat[i]*Ei < -toler) and (alphas[i] < C)) or ((labelMat[i]*Ei > toler) and (alphas[i] > 0)):\t\n                j = selectJrand(i,m)\t\t\t\t\t\t\t\t#随机选择另外一个数据向量j\n                print \"随机选择的j下标是\",j\n                fXj = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[j,:].T)) + b\t#同时优化这两个向量，如果都不能被优化，退出内循环\n                Ej = fXj - float(labelMat[j])\t\t\t\t\t\t\t#Ej是第j数据向量的计算误差\n                alphaIold = alphas[i].copy();alphaJold = alphas[j].copy();\n                #保证alpha在0与C之间\n                if (labelMat[i] != labelMat[j]):\t\t#当y1和y2异号，计算alpha的取值范围 \t\n                    L = max(0, alphas[j] - alphas[i])\n                    H = min(C, C + alphas[j] - alphas[i])\n                else:\t\t\t\t\t\t#当y1和y2同号，计算alpha的取值范围\n                    L = max(0, alphas[j] + alphas[i] - C)\n                    H = min(C, alphas[j] + alphas[i])\n                #如果L和H相等，就不做任何改变，本次循环结束直接运行下一次for循环\n                if L==H: print \"L==H\"; continue\t\t\t\n                #eta是alpha[j]的最优修改量，eta=K11+K22-2*K12,也是f(x)的二阶导数，K表示内积\n                eta = 2.0 * dataMatrix[i,:]*dataMatrix[j,:].T - dataMatrix[i,:]*dataMatrix[i,:].T - dataMatrix[j,:]*dataMatrix[j,:].T\n                #如果二阶导数-eta <= 0，说明一阶导数没有最小值，就不做任何改变，本次循环结束直接运行下一次for循环\n                if eta >= 0: print \"eta>=0\"; continue\n                alphas[j] -= labelMat[j]*(Ei - Ej)/eta\t\t#利用公式更新alpha[j]，alpha2new=alpha2-yj(Ei-Ej)/eta\n                alphas[j] = clipAlpha(alphas[j],H,L)\t\t#再判断一次alpha[j]的范围\n                #如果alphas[j]没有调整，就忽略下面语句，本次循环结束直接运行下一次for循环\n                if (abs(alphas[j] - alphaJold) < 0.00001): print \"j改变的不够\"; continue\n                alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])\t#调整alphas[i]，修改量与j相同，但是方向相反\n                #已经计算出了alpha，接下来根据模型的公式计算b\n                b1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T\n                b2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T\n                #根据公式确定偏移量b，理论上可选取任意支持向量来求解，但是现实任务中通常使用所有支持向量求解的平均值，这样更加鲁棒\n                if (0 < alphas[i]) and (C > alphas[i]): b = b1\n                elif (0 < alphas[j]) and (C > alphas[j]): b = b2\n                else: b = (b1 + b2)/2.0\n                alphaPairsChanged += 1\n                print \"总共100次迭代的次数: %d 当前i的下标:%d, 成对改变的alpha的数量 %d\" % (iter,i,alphaPairsChanged)\n                #print \"输出现在的alphas\",alphas.T\n                \n\t#只有满足alphaPairsChanged为0的条件，即100个alpha的误差都在范围之内，才能进入下一个递归，不然只能在100个alpha再次循环优化\n        if (alphaPairsChanged == 0): iter += 1\n        else: iter = 0\n        print \"iteration number: %d\" % iter\n        plotBestFit(alphas,dataArr,labelArr,b)\n    return b,alphas\n\n```\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n\n```\ndef calcWs(alphas,dataArr,classLabels):\n    X = mat(dataArr); labelMat = mat(classLabels).transpose()\n    m,n = shape(X)\n    w = zeros((n,1))\n    for i in range(m):\n        w += multiply(alphas[i]*labelMat[i],X[i,:].T)\n    return w\n\ndef plotBestFit(alphas,dataArr,labelArr,b):\t#画出数据集和超平面\n    import matplotlib.pyplot as plt\n    dataMat,labelMat=loadDataSet('testSet.txt')\t#数据矩阵和标签向量\n    dataArr = array(dataMat)\t\t\t#转换成数组\n    n = shape(dataArr)[0] \n    xcord1 = []; ycord1 = []\t\t\t#声明两个不同颜色的点的坐标\n    xcord2 = []; ycord2 = []\n    for i in range(n):\n        if int(labelMat[i])== 1:\n            xcord1.append(dataArr[i,0]); ycord1.append(dataArr[i,1])\n        else:\n            xcord2.append(dataArr[i,0]); ycord2.append(dataArr[i,1])\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.scatter(xcord1, ycord1, s=30, c='red')\n    ax.scatter(xcord2, ycord2, s=30, c='green', marker='s')\n    x = arange(2.0, 6.0, 0.1)\n    ws = calcWs(alphas,dataArr,labelArr)\n    #最佳拟合曲线，因为0是两个分类（0和1）的分界处（Sigmoid函数）\n    #图中y表示x2,x表示x1\n    y = [(i*ws[0]+array(b)[0])/-ws[1] for i in x]\n    ax.plot(x, y)\n    plt.xlabel('X1'); plt.ylabel('X2');\n    plt.show()\n\n```\n\n&nbsp;\n\n```\nif __name__ == '__main__':\n    dataArr,labelArr = loadDataSet('testSet.txt')\n    b,alphas = smoSimple(dataArr,labelArr,0.6,0.001,40)\n    plotBestFit(alphas,dataArr,labelArr,b)\n\n```\n\n&nbsp;\n\n**用超平面对数据集进行划分**\n\n<img src=\"/images/517519-20161125231452971-806347382.png\" alt=\"\" />\n","tags":["ML"]},{"title":"广告系统——广告归因","url":"/广告系统——广告归因.html","content":"## 1.广告归因的概念\n\n在做用户增长的时候需要对各**渠道**（广告投放平台，如腾讯广告、字节-巨量引擎、百度营销平台等）上的投放效果进行**广告归因**，归因的作用是**判断用户从何渠道下载应用（或打开落地页、小程序），通过匹配用户广告行为，分析是何原因促使用户产生转化**。 广告归因的数据结果是衡量广告效果、评估渠道质量的重要依据，可帮助广告主合理优化广告素材，高效开展拉新、促活营销推广。\n\n参考：[神策广告归因介绍](https://manual.sensorsdata.cn/sensorsadstracking/latest/%E5%B9%BF%E5%91%8A%E5%BD%92%E5%9B%A0%E4%BB%8B%E7%BB%8D-128057805.html)\n\n## 2.常用的渠道\n\n## 3.广告归因原理\n\n1.用户A在app1上点击了app2在渠道方（各种投放平台，比如巨量引擎）投放的广告，这时候渠道方就知道用户A点击了广告\n\n2.第三方归因平台（即下图中的服务端，比如字节穿山甲的增长参谋）会和很多的渠道方进行合作，渠道方支持将数据回传给第三方的归因平台，比如oaid，imei这种的用户ID，一般需要在渠道方的平台上进行配置，参考：[appsflyer-巨量引擎广告归因配置 - Bytedance Ads Configuration](https://support.appsflyer.com/hc/zh-cn/articles/115003019403-%E5%B7%A8%E9%87%8F%E5%BC%95%E6%93%8E%E5%B9%BF%E5%91%8A%E5%BD%92%E5%9B%A0%E9%85%8D%E7%BD%AE-Bytedance-Ads-Configuration)\n\n3.当用户首次打开APP的时候，会将埋点数据上报给第三方的归因平台（像穿山甲会提供SDK用于上传打点），一般也是在渠道方平台上进行配置，参考：[归因结果兼容API使用说明：穿山甲](https://www.csjplatform.com/supportcenter/26792)\n\n4.归因平台再将激活的数据回传给各个渠道方，用于优化投放的点击率（CTR）\n\n5.归因平台完成渠道归因\n\n6.上报数据到数据仓库中\n\n7.app内部打点统计次留，播放，付费的时间，上报给归因平台\n\n8.归因再将这些转化数据回传给各个渠道方，用于优化投放的转化率（CVR）\n\n9.时间回传成功\n\n10.数据存储\n\n<img src=\"/images/517519-20231030151653608-146217649.png\" width=\"800\" height=\"593\" />\n\n整个归因的流程参考：[广告归因-让你彻底弄归因架构实现 ](https://learnku.com/articles/74634)\n\n参考：[基于用户的归因模式&mdash;&mdash;通用ID方案](https://www.ichdata.com/people-based-attribution.html)\n\n### Android设备\n\n[APP来源追踪方式（归因）&mdash;&mdash;Android篇](https://www.ichdata.com/app-traffic-source-tracking-method-for-android.html)\n\n### Ios设备\n\n [APP来源追踪方式（归因）&mdash;&mdash;iOS篇](https://www.ichdata.com/app-traffic-source-tracking-method-for-ios.html)\n\n## 4.第三方归因平台\n\n1.Apps Flyer\n\n2.TalkingData\n\n3.热云\n\n4.友盟\n\n5.Adjust\n\n参考：[三方归因监测](https://www.csjplatform.com/supportcenter/5804)\n\n[https://www.ichdata.com/app-traffic-source-tracking-method-for-android.html#自归因渠道](https://www.ichdata.com/app-traffic-source-tracking-method-for-android.html#自归因渠道)\n\n[https://www.ichdata.com/people-based-attribution.html#媒体工具提供](https://www.ichdata.com/people-based-attribution.html#媒体工具提供)\n\n## 5.广告归因模型\n\n### 1.归因模型\n\n参考：[广告中的归因模型（Web端）](https://www.ichdata.com/web-attribution.html)\n\n### 2.展示归因和点击归因\n\n参考：[浏览归因/展示归因/曝光归因](https://www.ichdata.com/view-through-attribution.html)\n\n<!--more-->\n&nbsp;\n","tags":["广告系统"]},{"title":"Scylla学习笔记——Scylla原理","url":"/Scylla学习笔记——Scylla原理.html","content":"<!--more-->\n&nbsp;\n\n参考：[https://medium.com/@hansrajchoudhary_88463/scylladb-architecture-understanding-consistent-hashing-bloom-filters-memtable-and-sstable-95d95a27920f](https://medium.com/@hansrajchoudhary_88463/scylladb-architecture-understanding-consistent-hashing-bloom-filters-memtable-and-sstable-95d95a27920f)\n","tags":["scyllaDB"]},{"title":"机器学习——Logistic回归","url":"/机器学习——Logistic回归.html","content":"<img src=\"/images/517519-20161115103618154-1700308565.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n**1.基于Logistic回归和Sigmoid函数的分类**\n\n**<img src=\"/images/517519-20161115105125529-306279060.png\" alt=\"\" />**\n\n&nbsp;\n\n**2.基于最优化方法的最佳回归系数确定**\n\n**<img src=\"/images/517519-20161117100414763-1503549785.png\" alt=\"\" />**\n\n&nbsp;\n\n**2.1 梯度上升法**\n\n参考：[机器学习&mdash;&mdash;梯度下降算法](http://www.cnblogs.com/tonglin0325/p/6067379.html)\n\n&nbsp;\n\n**2.2 训练算法：使用梯度上升找到最佳参数**\n\n**<img src=\"/images/517519-20161117100604451-2011258246.png\" alt=\"\" />**\n\n&nbsp;\n\n**Logistic回归梯度上升优化算法**\n\n```\ndef loadDataSet():\n    dataMat = []; labelMat = []\n    fr = open('testSet.txt')\n    for line in fr.readlines():\n        lineArr = line.strip().split()\n        dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])\t#加上第0维特征值\n        labelMat.append(int(lineArr[2]))\n    return dataMat,labelMat\t#返回数据矩阵和标签向量\n\ndef sigmoid(inX):\n    return 1.0/(1+exp(-inX))\n\ndef gradAscent(dataMatIn, classLabels):\t\t#Logistic回归梯度上升优化算法\n    dataMatrix = mat(dataMatIn)             \t#由列表转换成NumPy矩阵数据类型，dataMatrix是一个100&times;3的矩阵\n    labelMat = mat(classLabels).transpose() \t#由列表转换成NumPy矩阵数据类型，labelMat是一个100&times;1的矩阵\n    m,n = shape(dataMatrix)\t\t    \t#shape函数取得矩阵的行数和列数，m=100,n=3\n    alpha = 0.001\t\t\t\t#向目标移动的步长\n    maxCycles = 500\t\t\t\t#迭代次数\n    weights = ones((n,1))\t\t\t#3行1列的矩阵，这个矩阵为最佳的回归系数，和原来的100&times;3相乘，可以得到100&times;1的结果\n    for k in range(maxCycles):              \n        h = sigmoid(dataMatrix*weights)     \t#矩阵相乘，得到100&times;1的矩阵，即把dataMat的每一行的所有元素相加\n        error = (labelMat - h)              \t#求出和目标向量之间的误差\n\t#梯度下降算法\n        weights = weights + alpha * dataMatrix.transpose()* error #3&times;100的矩阵乘以100&times;1的矩阵，weights是梯度算子，总是指向函数值增长最快的方向\n    return weights\t\t\t\t#返回一组回归系数，确定了不同类别数据之间的分割线\n\n```\n\n&nbsp;\n\n```\n    dataMat,labelMat = loadDataSet()\n    print gradAscent(dataMat,labelMat)\t#输出回归系数\n\n```\n\n&nbsp;\n\n```\n[[ 4.12414349]\n [ 0.48007329]\n [-0.6168482 ]]\n\n```\n\n&nbsp;\n\n**2.3 分析数据：画出决策边界**\n\n**&nbsp;画出数据集和Logistic回归最佳拟合直线的函数**\n\n```\ndef plotBestFit(wei):\t\t\t#画出数据集和Logistic回归最佳拟合直线的函数\n    import matplotlib.pyplot as plt\n    weights = wei.getA()\n    dataMat,labelMat=loadDataSet()\t#数据矩阵和标签向量\n    dataArr = array(dataMat)\t\t#转换成数组\n    n = shape(dataArr)[0] \n    xcord1 = []; ycord1 = []\t\t#声明两个不同颜色的点的坐标\n    xcord2 = []; ycord2 = []\n    for i in range(n):\n        if int(labelMat[i])== 1:\n            xcord1.append(dataArr[i,1]); ycord1.append(dataArr[i,2])\n        else:\n            xcord2.append(dataArr[i,1]); ycord2.append(dataArr[i,2])\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.scatter(xcord1, ycord1, s=30, c='red', marker='s')\n    ax.scatter(xcord2, ycord2, s=30, c='green')\n    x = arange(-3.0, 3.0, 0.1)\n    #最佳拟合曲线，这里设w0x0+w1x1+w2x2=0，因为0是两个分类（0和1）的分界处（Sigmoid函数），且此时x0=1\n    #图中y表示x2,x表示x1\n    y = (-weights[0]-weights[1]*x)/weights[2]\t\n    ax.plot(x, y)\n    plt.xlabel('X1'); plt.ylabel('X2');\n    plt.show()\n\n```\n\n<img src=\"/images/517519-20161117103330232-1008013154.png\" alt=\"\" />&nbsp;\n\n<img src=\"/images/517519-20161117101116420-1496540020.png\" alt=\"\" />\n\n```\n    dataMat,labelMat = loadDataSet()\n    #print dataMat\n    #print labelMat\n    #print gradAscent(dataMat,labelMat)\t#输出回归系数\n    plotBestFit(gradAscent(dataMat,labelMat))\n\n```\n\n&nbsp;<img src=\"/images/517519-20161117101226248-853488830.png\" alt=\"\" />\n\n&nbsp;\n\n**2.4 训练算法：随梯度上升**\n\n**<img src=\"/images/517519-20161117103631373-535541070.png\" alt=\"\" />**\n\n```\ndef stocGradAscent0(dataMatrix, classLabels):\t#随机梯度上升算法\n    m,n = shape(dataMatrix)\n    alpha = 0.01\n    weights = ones(n)   \t\t\t#3行1列的矩阵，初始最佳回归系数都为1，\n    for i in range(m):\n        h = sigmoid(sum(dataMatrix[i]*weights))\t#计算出是数值，而不是向量，dataMatrix[100&times;3]中取得[1&times;3],乘以[3&times;1]，得到数值\n        error = classLabels[i] - h\n        weights = weights + alpha * error * dataMatrix[i]\n    return weights\n\ndef plotBestFit(weights):\t\t\t#画出数据集和Logistic回归最佳拟合直线的函数\n    import matplotlib.pyplot as plt\n    #weights = wei.getA()\n    dataMat,labelMat=loadDataSet()\t#数据矩阵和标签向量\n    dataArr = array(dataMat)\t\t#转换成数组\n    n = shape(dataArr)[0] \n    xcord1 = []; ycord1 = []\t\t#声明两个不同颜色的点的坐标\n    xcord2 = []; ycord2 = []\n    for i in range(n):\n        if int(labelMat[i])== 1:\n            xcord1.append(dataArr[i,1]); ycord1.append(dataArr[i,2])\n        else:\n            xcord2.append(dataArr[i,1]); ycord2.append(dataArr[i,2])\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.scatter(xcord1, ycord1, s=30, c='red', marker='s')\n    ax.scatter(xcord2, ycord2, s=30, c='green')\n    x = arange(-3.0, 3.0, 0.1)\n    #最佳拟合曲线，这里设w0x0+w1x1+w2x2=0，因为0是两个分类（0和1）的分界处（Sigmoid函数），且此时x0=1\n    #图中y表示x2,x表示x1\n    y = (-weights[0]-weights[1]*x)/weights[2]\t\n    ax.plot(x, y)\n    plt.xlabel('X1'); plt.ylabel('X2');\n    plt.show()\n\n```\n\n&nbsp;\n\n```\n    dataMat,labelMat = loadDataSet()\n    #print dataMat\n    #print labelMat\n    #print gradAscent(dataMat,labelMat)\t#输出回归系数\n    #plotBestFit(gradAscent(dataMat,labelMat))\n    plotBestFit(stocGradAscent0(array(dataMat),labelMat))\n\n```\n\n&nbsp;<img src=\"/images/517519-20161117110424404-582108542.png\" alt=\"\" />\n\n<img src=\"/images/517519-20161117111630107-1436266181.png\" alt=\"\" />\n\n&nbsp;<img src=\"/images/517519-20161117144348779-1780458663.png\" alt=\"\" />\n\n**改进的随机梯度上升算法**\n\n```\ndef stocGradAscent1(dataMatrix, classLabels, numIter=150):\n    m,n = shape(dataMatrix)\n    weights = ones(n)   \t\t#初始化回归系数\n    for j in range(numIter):\t\t#从0到149开始循环\n        dataIndex = range(m)\n        for i in range(m):\t\t#从0到99开始循环\n            alpha = 4/(1.0+j+i)+0.0001    \t\t\t#步进alpha的值逐渐减小，j=0-150,i=1-100，使得收敛的速度加快\n            randIndex = int(random.uniform(0,len(dataIndex)))\t#样本随机选择0-99中的一个数计算回归系数，减小周期性波动的现象\n            h = sigmoid(sum(dataMatrix[randIndex]*weights))\n            error = classLabels[randIndex] - h\n            weights = weights + alpha * error * dataMatrix[randIndex]\n            del(dataIndex[randIndex])\n    return weights\n\n```\n\n&nbsp;<img src=\"/images/517519-20161117144444295-1458764983.png\" alt=\"\" width=\"619\" height=\"52\" />\n\n<img src=\"/images/517519-20161117144549951-733959553.png\" alt=\"\" width=\"640\" height=\"756\" />\n\n<img src=\"/images/517519-20161117144708029-41120941.png\" alt=\"\" width=\"624\" height=\"51\" />\n\n<img src=\"/images/517519-20161117145118513-1280301744.png\" alt=\"\" />\n\n&nbsp;\n\n**示例：从疝气病症预测病马的死亡率**\n\n**<img src=\"/images/517519-20161117161213092-844977821.png\" alt=\"\" />**\n\n&nbsp;\n\n**&nbsp;1.准备数据：处理数据中的缺失值**\n\n<img src=\"/images/517519-20161117162324420-1706852633.png\" alt=\"\" />\n\n<img src=\"/images/517519-20161117162350701-131081919.png\" alt=\"\" />\n\n&nbsp;\n\n**2.测试算法：使用Logistic回归进行分类**\n\n```\ndef classifyVector(inX, weights):\t#输入回归系数和特征向量，计算出Sigmoid值，如果大于0.5则返回1，否则返回0\n    prob = sigmoid(sum(inX*weights))\n    if prob > 0.5: return 1.0\n    else: return 0.0\n\ndef colicTest():\n    frTrain = open('horseColicTraining.txt'); frTest = open('horseColicTest.txt')\n    trainingSet = []; trainingLabels = []\n    for line in frTrain.readlines():\t\t#导入训练数据\n        currLine = line.strip().split('\\t')\n        lineArr =[]\n        for i in range(21):\t\t\t#把0-20个病症加到列表中\n            lineArr.append(float(currLine[i]))\n        trainingSet.append(lineArr)\t\t\t#把得到的每个列表加到训练集合中\n        trainingLabels.append(float(currLine[21]))\t#把标签加到训练标签中\n    trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 1000)\t#使用改进的随机梯度上升算法，递归1000次，计算回归系数\n    errorCount = 0; numTestVec = 0.0\n    for line in frTest.readlines():\t\t#导入测试数据\n        numTestVec += 1.0\t\t\t#测试数据的总数\n        currLine = line.strip().split('\\t')\n        lineArr =[]\n        for i in range(21):\t\t\t#把0-20个病症加到列表中，作为分类器的输入\n            lineArr.append(float(currLine[i]))\n        if int(classifyVector(array(lineArr), trainWeights))!= int(currLine[21]):\t#计算分类错误的次数，currLine[21]表示真正死亡与否\n            errorCount += 1\n    errorRate = (float(errorCount)/numTestVec)\t\t\t\t\t\t#计算错误率\n    print \"the error rate of this test is: %f\" % errorRate\n    return errorRate\n\ndef multiTest():\t#调用colicTest()十次并求结果的平均值\n    numTests = 10; errorSum=0.0\n    for k in range(numTests):\n        errorSum += colicTest()\n    print \"after %d iterations the average error rate is: %f\" % (numTests, errorSum/float(numTests))\n\n```\n\n&nbsp;<img src=\"/images/517519-20161117162459435-791320839.png\" alt=\"\" />\n","tags":["ML"]},{"title":"机器学习——梯度下降算法","url":"/机器学习——梯度下降算法.html","tags":["ML"]},{"title":"zk学习笔记——超级用户","url":"/zk学习笔记——超级用户.html","content":"如果遇到auth遗忘，又想删除zknode的情况，可以使用超级用户用来删除zknode\n\n超级用户只能在zkserver启动的时候启用，需要在zkserver的启动命令中添加 `-Dzookeeper.DigestAuthenticationProvider.superDigest` 参数\n\n<img src=\"/images/517519-20211020235027680-1418669248.png\" width=\"800\" height=\"174\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n```\n-Dzookeeper.DigestAuthenticationProvider.superDigest=super:xQJmxLMiHGwaqBvst5y6rkB6HQs=\n\n```\n\n&nbsp;然后就可以使用 super:admin 的auth来删除zknode\n\n```\n[zk: localhost:2181(CONNECTED) 15] addauth digest super:admin\n[zk: localhost:2181(CONNECTED) 16] get /hbase\n\ncZxid = 0x155e7a\nctime = Wed Oct 20 21:35:34 CST 2021\nmZxid = 0x155e7a\nmtime = Wed Oct 20 21:35:34 CST 2021\npZxid = 0x15639a\ncversion = 40\ndataVersion = 0\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 0\nnumChildren = 12\n[zk: localhost:2181(CONNECTED) 17] rmr /hbase\n\n```\n\n参考：[zookeeper acl认证机制及dubbo、kafka集成、zooviewer/idea zk插件配置](https://www.cnblogs.com/zhjh256/p/11679639.html)\n\n&nbsp;\n","tags":["zookeeper"]},{"title":"机器学习——基于概率论的分类方法：朴素贝叶斯","url":"/机器学习——基于概率论的分类方法：朴素贝叶斯.html","content":"**1.基于贝叶斯决策理论的分类方法**\n\n**<img src=\"/images/517519-20161113194032624-1398593119.png\" alt=\"\" />**\n\n<img src=\"/images/517519-20161113194728577-947004527.png\" alt=\"\" />\n\n**2.使用朴素贝叶斯进行文档分类**\n\n**<img src=\"/images/517519-20161113200740342-1122566074.png\" alt=\"\" />**\n\n<img src=\"/images/517519-20161113200750889-521954360.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n**使用Python进行文本分类**\n\n**1.准备数据：从文本中构建词向量**\n\n```\ndef loadDataSet():\n    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n    return postingList,classVec\t\t#第一个变量是词条切分后的文档集合，取出标点，第二个变量是类别标签的集合\n                 \ndef createVocabList(dataSet):\t\t#创建一个包含在所有文档中出现的不重复词的列表\n    vocabSet = set([])  \t\t#生成一个空的集合\n    for document in dataSet:\n        vocabSet = vocabSet | set(document) #求两个集合的并集\n    return list(vocabSet)\n\ndef setOfWords2Vec(vocabList, inputSet):#输入参数为词汇表vocabList及某个文档inputSet，输出的是文档向量，即文档中的每个词在词汇表中出现的次数\n    returnVec = [0]*len(vocabList)\n    for word in inputSet:\n        if word in vocabList:\n            returnVec[vocabList.index(word)] = 1\n        else: print \"the word: %s is not in my Vocabulary!\" % word\n    return returnVec\n\n```\n\n&nbsp;\n\n```\n    listOPosts,listClasses = loadDataSet()\n    print \"某个文档：\"\n    print listOPosts[0]\n    myVocabList = createVocabList(listOPosts)\t#由词表生成向量\n    print \"词汇表：\"\n    print myVocabList\n    print \"文档向量：\"\n    print setOfWords2Vec(myVocabList,listOPosts[0])\n\n```\n\n&nbsp;\n\n```\n某个文档：\n['my', 'dog', 'has', 'flea', 'problems', 'help', 'please']\n词汇表：\n['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my']\n文档向量：\n[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n\n```\n\n&nbsp;\n\n**2.训练算法：从词向量计算概率**\n\n&nbsp;<img src=\"/images/517519-20161113221115920-1062926066.png\" alt=\"\" />\n\n&nbsp;\n\n```\n#朴素贝叶斯分类器训练函数，trainMatrix表示由多个词条在词汇表中出现的次数组成的矩阵，trainCategory表示某个词条出不出现\ndef trainNB0(trainMatrix,trainCategory):\n    numTrainDocs = len(trainMatrix)\t#输出词条的数量\n    numWords = len(trainMatrix[0])\t#输出词汇表的数量\n    pAbusive = sum(trainCategory)/float(numTrainDocs)\n    p0Num = ones(numWords); p1Num = ones(numWords)      #change to ones() \n    p0Denom = 2.0; p1Denom = 2.0                        #change to 2.0\n    for i in range(numTrainDocs):\t#循环每一个词条，总计由这些词条组成的文档的概率\n        if trainCategory[i] == 1:\t#trainCategory表示某个词条是不是侮辱性的\n            p1Num += trainMatrix[i]\t#如果词条出现，增加该词条的计数值，p1Num是起始全为1的一维向量，如果是侮辱性的词条则加上该词条的词汇表计数\n            p1Denom += sum(trainMatrix[i])\t#增加所有词条的计数值\n        else:\n            p0Num += trainMatrix[i]\n            p0Denom += sum(trainMatrix[i])\n    p1Vect = log(p1Num/p1Denom)          #词汇表中每个词出现在侮辱性词条中的次数除以出现的总次数，再求log\n    p0Vect = log(p0Num/p0Denom)          #change to log()\n    return p0Vect,p1Vect,pAbusive\n\n```\n\n&nbsp;\n\n```\n    from numpy import *\n    listOPosts,listClasses = loadDataSet()\n    myVocabList = createVocabList(listOPosts)\t#由词表生成向量\n    print \"词汇表：\"\n    print myVocabList\n    trainMat = []\n    for posinDoc in listOPosts:\n    \ttrainMat.append(setOfWords2Vec(myVocabList,posinDoc))\n    p0V,p1V,pAb = trainNB0(trainMat,listClasses)\n    print pAb\n    print \"词汇表中的词出现在侮辱性词条中的概率：\"\n    print p1V\n\n```\n\n&nbsp;\n\n```\n词汇表：\n['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my']\n0.5\n词汇表中的词出现在侮辱性词条中的概率：\n[ 0.04761905  0.04761905  0.04761905  0.0952381   0.0952381   0.04761905\n  0.04761905  0.04761905  0.0952381   0.0952381   0.04761905  0.04761905\n  0.04761905  0.0952381   0.0952381   0.0952381   0.0952381   0.0952381\n  0.04761905  0.14285714  0.04761905  0.0952381   0.0952381   0.04761905\n  0.14285714  0.04761905  0.19047619  0.04761905  0.0952381   0.04761905\n  0.04761905  0.04761905]\n\n```\n\n&nbsp;\n\n**3.测试算法：根据现实情况修改分类器**\n\n**<img src=\"/images/517519-20161113232734467-1469253001.png\" alt=\"\" />**\n\n**朴素贝叶斯分类函数**\n\n```\ndef classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\t#需要分类的向量vec2Classify，pClass1是类别的概率\n    p1 = sum(vec2Classify * p1Vec) + log(pClass1)       #向量相乘，再加上类别的对数概率\n    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n    if p1 > p0:\n        return 1\n    else: \n        return 0\n\ndef testingNB():\t#测试贝叶斯分类器\n    listOPosts,listClasses = loadDataSet()\t#载入训练数据\n    myVocabList = createVocabList(listOPosts)\t#取得词汇表\n    trainMat=[]\n    for postinDoc in listOPosts:\n        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\t#向训练矩阵中添加训练概率向量\n    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\t#训练出的参数\n    testEntry = ['love', 'my', 'dalmation']\t#测试词条1\n    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n    print testEntry,'分类为: ',classifyNB(thisDoc,p0V,p1V,pAb)\n    testEntry = ['stupid', 'garbage']\t\t#测试词条2\n    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n    print testEntry,'分类为: ',classifyNB(thisDoc,p0V,p1V,pAb)\n\n```\n\n&nbsp;\n\n```\n['love', 'my', 'dalmation'] 分类为:  0\n['stupid', 'garbage'] 分类为:  1\n\n```\n\n&nbsp;\n\n**4.准备数据：文档词袋模型**\n\n<img src=\"/images/517519-20161114110033545-1203064995.png\" alt=\"\" />\n\n```\ndef bagOfWords2VecMN(vocabList, inputSet):\t#基于词袋模型的朴素贝叶斯，遇到一个单词时，增加词向量中的对应值\n    returnVec = [0]*len(vocabList)\n    for word in inputSet:\n        if word in vocabList:\n            returnVec[vocabList.index(word)] += 1\t#出现次数从1开始增加\n    return returnVec\n\n```\n\n&nbsp;\n\n**示例：使用朴素贝叶斯过滤垃圾邮件**\n\n**<img src=\"/images/517519-20161114110805560-1309760417.png\" alt=\"\" />**\n\n&nbsp;\n\n&nbsp;\n\n**1.准备数据：切分文本**\n\n```\ndef textParse(bigString):    #输入字符串，切分字符串成列表\n    import re\n    listOfTokens = re.split(r'\\W*', bigString)\n    return [tok.lower() for tok in listOfTokens if len(tok) > 2] #返回字符串长度大于2的小写字符串\n\n```\n\n&nbsp;\n\n```\n    mySent = 'This book is the best book on Python'\n    print textParse(mySent)\n\n```\n\n&nbsp;\n\n```\n['this', 'book', 'the', 'best', 'book', 'python']\n\n```\n\n**&nbsp;**\n\n**切分一封完整的电子邮件的实际处理结果**\n\n```\n    import re\n    regEx = re.compile('\\\\W*')\t#正则表达式\n    emailText = open('email/ham/6.txt').read()\n    listOfTokens = regEx.split(emailText)\n    print listOfTokens\n\n```\n\n&nbsp;<img src=\"/images/517519-20161114113843513-993729064.png\" alt=\"\" />\n\n&nbsp;\n\n**2.测试算法：使用朴素贝叶斯进行交叉验证**\n\n```\ndef spamTest():\n    docList=[]; classList = []; fullText =[]\n    for i in range(1,26):\n        wordList = textParse(open('email/spam/%d.txt' % i).read())\t#导入并解析25个文本文件成字符串列表\n        docList.append(wordList)\t#把多个列表分组添加到一个列表中\n        fullText.extend(wordList)\t#把多个列表添加到一个列表中\n        classList.append(1)\n        wordList = textParse(open('email/ham/%d.txt' % i).read())\n        docList.append(wordList)\n        fullText.extend(wordList)\n        classList.append(0)\n    vocabList = createVocabList(docList)\t#生成词汇表\n    trainingSet = range(50); testSet=[]         #生成测试集合，trainingSet是从0到49的列表\n    for i in range(10):\t\t\t\t#生成一个长为10的随机列表作为测试集合\n        randIndex = int(random.uniform(0,len(trainingSet)))\n        testSet.append(trainingSet[randIndex])\n        del(trainingSet[randIndex])  \n    trainMat=[]; trainClasses = []\n    for docIndex in trainingSet:\t\t#原来0-49的数据用来训练分类器\n        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n        trainClasses.append(classList[docIndex])\n    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\t#训练分类器\n    errorCount = 0\n    for docIndex in testSet:        \t\t#分类测试数据\n        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n            errorCount += 1\n            print \"classification error\",docList[docIndex]\n    print 'the error rate is: ',float(errorCount)/len(testSet)\n    #return vocabList,fullText\n\n```\n\n&nbsp;\n\n```\nclassification error ['yeah', 'ready', 'may', 'not', 'here', 'because', 'jar', 'jar', 'has', 'plane', 'tickets', 'germany', 'for']\nclassification error ['yay', 'you', 'both', 'doing', 'fine', 'working', 'mba', 'design', 'strategy', 'cca', 'top', 'art', 'school', 'new', 'program', 'focusing', 'more', 'right', 'brained', 'creative', 'and', 'strategic', 'approach', 'management', 'the', 'way', 'done', 'today']\nthe error rate is:  0.2\n\n```\n\n&nbsp;\n\n**示例：使用朴素贝叶斯分类器从个人广告中获取区域倾向**\n\n**<img src=\"/images/517519-20161114160802498-324495553.png\" alt=\"\" />**\n\n&nbsp;\n\n**1.收集数据：导入RSS源及高频次去除函数**\n\n```\ndef calcMostFreq(vocabList,fullText):\t#计算出现的频率\n    import operator\n    freqDict = {}\n    for token in vocabList:\n        freqDict[token]=fullText.count(token)\n    sortedFreq = sorted(freqDict.iteritems(), key=operator.itemgetter(1), reverse=True) \n    return sortedFreq[:30]       \n\ndef localWords(feed1,feed0):\n    import feedparser\n    docList=[]; classList = []; fullText =[]\n    minLen = min(len(feed1['entries']),len(feed0['entries']))\n    for i in range(minLen):\n        wordList = textParse(feed1['entries'][i]['summary'])\n        docList.append(wordList)\n        fullText.extend(wordList)\n        classList.append(1) #NY is class 1\n        wordList = textParse(feed0['entries'][i]['summary'])\n        docList.append(wordList)\n        fullText.extend(wordList)\n        classList.append(0)\n    vocabList = createVocabList(docList)\t\t#生成词汇表\n    top30Words = calcMostFreq(vocabList,fullText)  \t#移除出现次数前30的词\n    print \"输出出现次数排名前30的词：\"\n    print top30Words\n    for pairW in top30Words:\n        if pairW[0] in vocabList: vocabList.remove(pairW[0])\n    #生成一个训练集合和测试集合\n    trainingSet = range(2*minLen); testSet=[] \n    #长度为20的测试集合，元素随机取         \n    for i in range(20):\n        randIndex = int(random.uniform(0,len(trainingSet)))\n        testSet.append(trainingSet[randIndex])\n        del(trainingSet[randIndex])  \n    trainMat=[]; trainClasses = []\n    #训练\n    for docIndex in trainingSet:\t\t\t#添加训练矩阵和分类标签\n        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n        trainClasses.append(classList[docIndex])\n    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\t#训练分类器\n    errorCount = 0\n    #分类判别到底是SF还是NY，并计算错误率\n    for docIndex in testSet:        \t\t\t#classify the remaining items\n        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n            errorCount += 1\n    print 'the error rate is: ',float(errorCount)/len(testSet)\n    return vocabList,p0V,p1V\n\n\n```\n\n&nbsp;\n\n**2.分析数据：显示地域相关的用词**\n\n```\ndef getTopWords(ny,sf):\t\t#最具表征性的词汇显示函数\n    import operator\n    vocabList,p0V,p1V=localWords(ny,sf)\n    topNY=[]; topSF=[]\n    for i in range(len(p0V)):\n        if p0V[i] > -4.5 : topSF.append((vocabList[i],p0V[i]))\n        if p1V[i] > -4.5 : topNY.append((vocabList[i],p1V[i]))\n    sortedSF = sorted(topSF, key=lambda pair: pair[1], reverse=True)\n    print \"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\"\n    for item in sortedSF:\n        print item[0]\n    sortedNY = sorted(topNY, key=lambda pair: pair[1], reverse=True)\n    print \"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\"\n    for item in sortedNY:\n        print item[0]\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20161114205040123-1524426610.png\" alt=\"\" />\n","tags":["ML"]},{"title":"Ubuntu提示卷boot仅剩0字节的硬盘空间，解决办法","url":"/Ubuntu提示卷boot仅剩0字节的硬盘空间，解决办法.html","content":"查看当前安装的linux内核版本号\n\n```\ndpkg --get-selections |grep linux-image\n\n```\n\n<!--more-->\n&nbsp;查看当前使用的内核版本号\n\n```\nuname -a\n\n```\n\n卸载不需要的内核\n\n```\nsudo apt-get purge linux-image-3.5.0-27-generic\n\n```\n\n&nbsp;最后使用df命令查看boot的占用情况\n\n```\ndf\n\n```\n\n&nbsp;\n","tags":["Linux"]},{"title":"Python学习笔记——pandas","url":"/Python学习笔记——pandas.html","content":"官方文档：[https://pandas.pydata.org/docs/reference/index.html](https://pandas.pydata.org/docs/reference/index.html)\n\n1.loc属性，通过标签或布尔数组访问一组行和列。[pandas.DataFrame.loc](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas-dataframe-loc)\n\n```\n>>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n...      index=['cobra', 'viper', 'sidewinder'],\n...      columns=['max_speed', 'shield'])\n>>> df\n            max_speed  shield\ncobra               1       2\nviper               4       5\nsidewinder          7       8\n\n```\n\n获取单个label，返回是一个series\n\n```\n>>> df.loc['viper']\nmax_speed    4\nshield       5\nName: viper, dtype: int64\n\n```\n\n获取List of labels。注意使用 `[[]]` 返回的是一个 DataFrame\n\n```\n>>> df.loc[['viper', 'sidewinder']]\n            max_speed  shield\nviper               4       5\nsidewinder          7       8\n\n```\n\n通过行标签和列标签来获得具体值\n\n```\n>>> df.loc['cobra', 'shield']\n2\n\n```\n\n对行进行切片，对列进行单标签切片（取得部分行和部分列）\n\n```\n>>> df.loc['cobra':'viper', 'max_speed']\ncobra    1\nviper    4\nName: max_speed, dtype: int64\n\n```\n\n通过boolean值来选取部分行\n\n```\n>>> df.loc[[False, False, True]]\n            max_speed  shield\nsidewinder          7       8\n\n```\n\n条件筛选符合条件的行或者列\n\n```\n>>> df.loc[df['shield'] > 6]\n            max_speed  shield\nsidewinder          7       8\n\n```\n\n条件筛选后再指定某个列\n\n```\n>>> df.loc[df['shield'] > 6, ['max_speed']]\n            max_speed\nsidewinder          7\n\n```\n\n多个条件\n\n```\n>>> df.loc[(df['max_speed'] > 1) &amp; (df['shield'] < 8)]\n       max_speed  shield\nviper          4       5\n\n>>> df.loc[(df['max_speed'] > 4) | (df['shield'] < 5)]\n            max_speed  shield\ncobra               1       2\nsidewinder          7       8\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["Python"]},{"title":"机器学习——决策树","url":"/机器学习——决策树.html","content":"**1.决策树的构造**\n\n优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据\n\n缺点：可能会产生过度匹配问题\n\n适用数据类型：数值型和标称型\n\n<!--more-->\n&nbsp;\n\n```\n# coding:utf-8\n# !/usr/bin/env python\n\n'''\nCreated on Oct 12, 2010\nDecision Tree Source Code for Machine Learning in Action Ch. 3\n@author: Peter Harrington\n'''\nfrom math import log\nimport operator\n\n#通过是否浮出水面和是否有脚蹼，来划分鱼类和非鱼类\ndef createDataSet():\n    dataSet = [[1, 1, 'yes'],\n               [1, 1, 'yes'],\n               [1, 0, 'no'],\n               [0, 1, 'no'],\n               [0, 1, 'no']]\n    labels = ['no surfacing','flippers']\n    #change to discrete values\n    return dataSet, labels\n\ndef calcShannonEnt(dataSet):\t#计算给定数据集的香农熵\n    numEntries = len(dataSet)\t#数据集中的实例总数\n    labelCounts = {}\n    #为所有可能的分类创建字典，键是可能的特征属性，值是含有这个特征属性的总数\n    for featVec in dataSet:\n        currentLabel = featVec[-1]\n        if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0\n        labelCounts[currentLabel] += 1\n    #计算香农熵\n    shannonEnt = 0.0\n    #为所有的分类计算香农熵\n    for key in labelCounts:\n        prob = float(labelCounts[key])/numEntries\n        shannonEnt -= prob * log(prob,2) \t#以2为底求对数\n    #香农熵Ent的值越小，纯度越高，即通过这个特征属性来分类，属于同一类别的结点会比较多\n    return shannonEnt\n    \ndef splitDataSet(dataSet, axis, value):\n    retDataSet = []\n    for featVec in dataSet:\n        if featVec[axis] == value:\n            reducedFeatVec = featVec[:axis]     #chop out axis used for splitting\n            reducedFeatVec.extend(featVec[axis+1:])\n            retDataSet.append(reducedFeatVec)\n    return retDataSet\n    \ndef chooseBestFeatureToSplit(dataSet):\n    numFeatures = len(dataSet[0]) - 1      #the last column is used for the labels\n    baseEntropy = calcShannonEnt(dataSet)\n    bestInfoGain = 0.0; bestFeature = -1\n    for i in range(numFeatures):        #iterate over all the features\n        featList = [example[i] for example in dataSet]#create a list of all the examples of this feature\n        uniqueVals = set(featList)       #get a set of unique values\n        newEntropy = 0.0\n        for value in uniqueVals:\n            subDataSet = splitDataSet(dataSet, i, value)\n            prob = len(subDataSet)/float(len(dataSet))\n            newEntropy += prob * calcShannonEnt(subDataSet)     \n        infoGain = baseEntropy - newEntropy     #calculate the info gain; ie reduction in entropy\n        if (infoGain > bestInfoGain):       #compare this to the best gain so far\n            bestInfoGain = infoGain         #if better than current best, set to best\n            bestFeature = i\n    return bestFeature                      #returns an integer\n\ndef majorityCnt(classList):\n    classCount={}\n    for vote in classList:\n        if vote not in classCount.keys(): classCount[vote] = 0\n        classCount[vote] += 1\n    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)\n    return sortedClassCount[0][0]\n\ndef createTree(dataSet,labels):\n    classList = [example[-1] for example in dataSet]\n    if classList.count(classList[0]) == len(classList): \n        return classList[0]#stop splitting when all of the classes are equal\n    if len(dataSet[0]) == 1: #stop splitting when there are no more features in dataSet\n        return majorityCnt(classList)\n    bestFeat = chooseBestFeatureToSplit(dataSet)\n    bestFeatLabel = labels[bestFeat]\n    myTree = {bestFeatLabel:{}}\n    del(labels[bestFeat])\n    featValues = [example[bestFeat] for example in dataSet]\n    uniqueVals = set(featValues)\n    for value in uniqueVals:\n        subLabels = labels[:]       #copy all of labels, so trees don't mess up existing labels\n        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)\n    return myTree                            \n    \ndef classify(inputTree,featLabels,testVec):\n    firstStr = inputTree.keys()[0]\n    secondDict = inputTree[firstStr]\n    featIndex = featLabels.index(firstStr)\n    key = testVec[featIndex]\n    valueOfFeat = secondDict[key]\n    if isinstance(valueOfFeat, dict): \n        classLabel = classify(valueOfFeat, featLabels, testVec)\n    else: classLabel = valueOfFeat\n    return classLabel\n\ndef storeTree(inputTree,filename):\n    import pickle\n    fw = open(filename,'w')\n    pickle.dump(inputTree,fw)\n    fw.close()\n    \ndef grabTree(filename):\n    import pickle\n    fr = open(filename)\n    return pickle.load(fr)\n    \n    \nif __name__ == '__main__':\n    myDat,labels = createDataSet()\n    print myDat\n    print calcShannonEnt(myDat)\n\t\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20161110105603858-1755597109.png\" alt=\"\" />\n\n<img src=\"/images/517519-20161110105926045-430376645.png\" alt=\"\" />\n\n&nbsp;\n\n```\n#通过是否浮出水面和是否有脚蹼，来划分鱼类和非鱼类\ndef createDataSet():\n    dataSet = [[1, 1, 'yes'],\n               [1, 1, 'yes'],\n               [1, 0, 'no'],\n               [0, 1, 'no'],\n               [0, 1, 'no']]\n    labels = ['no surfacing','flippers']\n    #change to discrete values\n    return dataSet, labels\n\ndef calcShannonEnt(dataSet):\t#计算给定数据集的香农熵\n    numEntries = len(dataSet)\t#数据集中的实例总数\n    labelCounts = {}\n    #为所有可能的分类创建字典，键是可能的特征属性，值是含有这个特征属性的总数\n    for featVec in dataSet:\n        currentLabel = featVec[-1]\n        if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0\n        labelCounts[currentLabel] += 1\n    #计算香农熵\n    shannonEnt = 0.0\n    #为所有的分类计算香农熵\n    for key in labelCounts:\n        prob = float(labelCounts[key])/numEntries\n        shannonEnt -= prob * log(prob,2) \t#以2为底求对数\n    #香农熵Ent的值越小，纯度越高，即通过这个特征属性来分类，属于同一类别的结点会比较多\n    return shannonEnt\n\n```\n\n&nbsp;\n\n```\nmyDat,labels = createDataSet()\nprint myDat\nprint calcShannonEnt(myDat)\n\n```\n\n&nbsp;<img src=\"/images/517519-20161110114028577-599745723.png\" alt=\"\" />\n\n&nbsp;\n\n**2.划分数据集**\n\n**<img src=\"/images/517519-20161110170806452-174749686.png\" alt=\"\" />**\n\n```\ndef splitDataSet(dataSet, axis, value):\t\t#按照给定特征划分数据集，axis表示根据第几个特征，value表示特征的值\n    retDataSet = []\t\t\t\t#创建新的list对象\n    for featVec in dataSet:\n        if featVec[axis] == value:\n            reducedFeatVec = featVec[:axis]     #切片\n            reducedFeatVec.extend(featVec[axis+1:])\t#把序列添加到列表reducedFeatVec中\n            #print reducedFeatVec\n            retDataSet.append(reducedFeatVec)\t\t#把对象reducedFeatVec（是一个list）添加到列表retDataSet中\n    return retDataSet\n\n```\n\n&nbsp;\n\n```\ndef chooseBestFeatureToSplit(dataSet):\t\t#选择最好的数据集划分方式\n    numFeatures = len(dataSet[0]) - 1      \t#特征的数量，最后一列是标签，所以减去1\n    baseEntropy = calcShannonEnt(dataSet)\n    bestInfoGain = 0.0; bestFeature = -1\t#信息增益和最好的特征下标\n    for i in range(numFeatures):        \t#递归所有特征\n        featList = [example[i] for example in dataSet]\t#创建一个列表，包含第i个特征的所有值\n        uniqueVals = set(featList)       \t#创建一个集合set，由不同的元素组成\n        newEntropy = 0.0\n        for value in uniqueVals:\n            subDataSet = splitDataSet(dataSet, i, value)\t#按照所有特征的可能划分数据集\n            prob = len(subDataSet)/float(len(dataSet))\t\t#计算所有特征的可能性\n            newEntropy += prob * calcShannonEnt(subDataSet)     \n        infoGain = baseEntropy - newEntropy     #计算信息增益\n        if (infoGain > bestInfoGain):       \t#比较不同特征之间信息增益的大小\n            bestInfoGain = infoGain         \t#选取信息增益大的特征\n            bestFeature = i\n    return bestFeature                      \t#返回特征的下标\n\n```\n\n<img src=\"/images/517519-20161110201519186-1849265480.png\" alt=\"\" />\n\n&nbsp;\n\n**3.递归构建决策树**\n\n&nbsp;<img src=\"/images/517519-20161110211659967-1907219120.png\" alt=\"\" />\n\n```\ndef createTree(dataSet,labels):\t\t#创建决策树的函数，采用字典的表示形式\n    classList = [example[-1] for example in dataSet]\n    if classList.count(classList[0]) == len(classList): \t#如果类别完全相同则停止继续划分\n        return classList[0]\n    if len(dataSet[0]) == 1: \t\t\t\t\t#遍历完所有特征时返回出现次数最多的\n        return majorityCnt(classList)\n    bestFeat = chooseBestFeatureToSplit(dataSet)\t\t#选择信息增益最大的特征下标\n    bestFeatLabel = labels[bestFeat]\t\t\t\t#选择信息增益最大的特征\n    myTree = {bestFeatLabel:{}}\n    del(labels[bestFeat])\t\t\t\t\t#从标签中删除已经划分好的特征\n    featValues = [example[bestFeat] for example in dataSet]\t#取得该特征的所有可能取值\n    uniqueVals = set(featValues)\t\t\t\t#建立一个集合\n    for value in uniqueVals:\n        subLabels = labels[:]       \n        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)\t#递归createTree\n    return myTree  \n\n```\n\n&nbsp;\n\n```\nmyDat,labels = createDataSet()\nmyTree = createTree(myDat,labels)\nprint myTree\n\n{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n\n```\n\n&nbsp;<img src=\"/images/517519-20161110211831420-49413193.png\" alt=\"\" />\n\n&nbsp;\n\n**4.在Python中使用Matplotlib注解绘制树形图**\n\n```\nmyDat,labels = createDataSet()\nprint myDat\nimport treePlotter\ntreePlotter.createPlot(myTree)　　#绘制树形图\n\n```\n\n&nbsp;<img src=\"/images/517519-20161110220503233-1374927553.png\" alt=\"\" />\n\n&nbsp;\n\n**5.构造注解树**\n\n**&nbsp;获取叶节点的数目和树的层数**\n\n```\nimport matplotlib.pyplot as plt\n\ndecisionNode = dict(boxstyle=\"sawtooth\", fc=\"0.8\")\nleafNode = dict(boxstyle=\"round4\", fc=\"0.8\")\narrow_args = dict(arrowstyle=\"<-\")\n\ndef getNumLeafs(myTree):\t\t#获取叶子节点的数目\n    numLeafs = 0\n    firstStr = myTree.keys()[0]\n    secondDict = myTree[firstStr]\n    for key in secondDict.keys():\n        if type(secondDict[key]).__name__=='dict':\t#测试节点的数据类型是否为字典\n            numLeafs += getNumLeafs(secondDict[key])\t#递归\n        else:   numLeafs +=1\t\t\t\t#如果不是字典，则说明是叶子节点\n    return numLeafs\n\ndef getTreeDepth(myTree):\t\t#获取树的层数\n    maxDepth = 0\n    firstStr = myTree.keys()[0]\n    secondDict = myTree[firstStr]\n    for key in secondDict.keys():\n        if type(secondDict[key]).__name__=='dict':\t#测试节点的数据类型是否为字典，如果不是字典，则说明是叶子节点\n            thisDepth = 1 + getTreeDepth(secondDict[key])\t#递归\n        else:   thisDepth = 1\t\t\t\t\n        if thisDepth > maxDepth: maxDepth = thisDepth\n    return maxDepth\n\n```\n\n**&nbsp;绘制树形图**\n\n**<img src=\"/images/517519-20161111111451639-181697891.png\" alt=\"\" /><img src=\"/images/517519-20161111111539952-1822098321.png\" alt=\"\" />**\n\n&nbsp;\n\n&nbsp;\n\n```\ndef plotNode(nodeTxt, centerPt, parentPt, nodeType):\t#绘制带箭头的注解\n    #annotate参数：nodeTxt：标注文本，xy：所要标注的位置坐标，xytext：标注文本所在位置，arrowprops：标注箭头属性信息\n    createPlot.ax1.annotate(nodeTxt, xy=parentPt,  xycoords='axes fraction',\n             xytext=centerPt, textcoords='axes fraction',\n             va=\"center\", ha=\"center\", bbox=nodeType, arrowprops=arrow_args )\n    \ndef plotMidText(cntrPt, parentPt, txtString):\t\t#在父子节点间填充文本信息\n    xMid = (parentPt[0]-cntrPt[0])/2.0 + cntrPt[0]\n    yMid = (parentPt[1]-cntrPt[1])/2.0 + cntrPt[1]\n    createPlot.ax1.text(xMid, yMid, txtString, va=\"center\", ha=\"center\", rotation=30)\n\ndef plotTree(myTree, parentPt, nodeTxt):\t\t#if the first key tells you what feat was split on\n    numLeafs = getNumLeafs(myTree)  \t\t\t#计算宽与高\n    depth = getTreeDepth(myTree)\n    firstStr = myTree.keys()[0]     \t\t\t#the text label for this node should be this\n    print plotTree.xOff\n    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff)\n    print parentPt\n    print cntrPt\n    plotMidText(cntrPt, parentPt, nodeTxt)\t\t#标记子节点属性值\n    plotNode(firstStr, cntrPt, parentPt, decisionNode)\n    secondDict = myTree[firstStr]\n    plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD\t#减少y偏移\n    for key in secondDict.keys():\n        if type(secondDict[key]).__name__=='dict':\t#test to see if the nodes are dictonaires, if not they are leaf nodes   \n            plotTree(secondDict[key],cntrPt,str(key))        #recursion\n        else:   #it's a leaf node print the leaf node\n            plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW\n            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)\n            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))\n    plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD\n#if you do get a dictonary you know it's a tree, and the first element will be another dict\n\ndef createPlot(inTree):\t\t\t#绘制树形图，调用了plotTree()\n    fig = plt.figure(1, facecolor='white')\n    fig.clf()\n    axprops = dict(xticks=[], yticks=[])\n    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)    #no ticks\n    #createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses \n    plotTree.totalW = float(getNumLeafs(inTree))\t#存储树的宽度\n    plotTree.totalD = float(getTreeDepth(inTree))\t#存储树的深度\n    plotTree.xOff = -0.5/plotTree.totalW; plotTree.yOff = 1.0;\n    plotTree(inTree, (0.5,1.0), '')\n    plt.show()\n\n```\n\n&nbsp;\n\n&nbsp;\n\n**测试和存储分类器**\n\n**1.测试算法：使用决策树执行分类**\n\n```\ndef classify(inputTree,featLabels,testVec):\t#使用决策树的分类函数\n    firstStr = inputTree.keys()[0]\n    secondDict = inputTree[firstStr]\n    featIndex = featLabels.index(firstStr)\t#将标签字符串转换为索引\n    key = testVec[featIndex]\n    valueOfFeat = secondDict[key]\n    if isinstance(valueOfFeat, dict): \n        classLabel = classify(valueOfFeat, featLabels, testVec)\n    else: classLabel = valueOfFeat\n    return classLabel\n\n```\n\n&nbsp;\n\n```\n    myDat,labels = createDataSet()\n    Labels = labels\n    print \"myDat=\"\n    print myDat\n    print \"labels=\"\n    print labels\n\n    import treePlotter\n    myTree = treePlotter.retrieveTree(0)\t#绘制树形图\n    print myTree\n    print classify(myTree,Labels,[0,1])\n\n```\n\n&nbsp;\n\n**&nbsp;2.使用算法：决策树的存储**\n\n**<img src=\"/images/517519-20161112155409186-677545778.png\" alt=\"\" />**\n\n```\ndef storeTree(inputTree,filename):\t#使用pickle模块存储决策树\n    import pickle\n    fw = open(filename,'w')\n    pickle.dump(inputTree,fw)\n    fw.close()\n    \ndef grabTree(filename):\t\t\t#查看决策树\n    import pickle\n    fr = open(filename)\n    return pickle.load(fr)\n\n```\n\n&nbsp;\n\n```\n    myDat,labels = createDataSet()\n    Labels = labels\n    print \"myDat=\"\n    print myDat\n    print \"labels=\"\n    print labels\n    import treePlotter\n    myTree = treePlotter.retrieveTree(0)\t#绘制树形图\n    print myTree\n    storeTree(myTree,'classifierStorage.txt')\n    print grabTree('classifierStorage.txt')\n\n```\n\n&nbsp;\n\n**示例：使用决策树预测隐形眼镜类型**\n\n**<img src=\"/images/517519-20161113153533686-1056298409.png\" alt=\"\" /><img src=\"/images/517519-20161113153641670-1959801477.png\" alt=\"\" />**\n\n&nbsp;\n\n```\n    import treePlotter\n    import simplejson\n    import ch\n    ch.set_ch()\n    from matplotlib import pyplot as plt\n    fr = open('lenses.txt')\n    lenses = [inst.strip().split('\\t') for inst in fr.readlines()]\t#读取一行数据，以tab键分割并去掉空格\n    lensesLabels = [u'年龄',u'近远视',u'散光',u'眼泪等级']\t\t\t#使用unicode，不然编码会报错\n    lensesTree = createTree(lenses,lensesLabels)\n    print simplejson.dumps(lensesTree, encoding=\"UTF-8\", ensure_ascii=False)\t#使用simplejson模块输出对象中的中文\n    treePlotter.createPlot(lensesTree)\n\n```\n\n&nbsp;<img src=\"/images/517519-20161113193400608-477071066.png\" alt=\"\" />\n\n<img src=\"/images/517519-20161113193437124-1835980705.png\" alt=\"\" />\n","tags":["ML"]},{"title":"Datagrip查询开启kerberos的hive","url":"/Datagrip查询开启kerberos的hive.html","content":"## 1.添加driver\n\nhive集群的版本是1.1.0-cdh5.16.2，而datagrip自带的hive driver版本是3.1.1和3.1.2，所以需要自行添加driver\n\n参考：[kerberos-2.datagrip（jdbc）连接hive kerberos](https://blog.csdn.net/github_39319229/article/details/112692897)\n\n<img src=\"/images/517519-20220923210155212-1045539097.png\" width=\"800\" height=\"284\" loading=\"lazy\" />\n\nadd custome JARs，所需要的jar包如下\n\n<img src=\"/images/517519-20220923210342164-1360608000.png\" width=\"600\" height=\"103\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n## 2.添加data source\n\n添加hive URL\n\n```\njdbc:hive2://master:10000/default;principal=hive/master@HADOOP.COM\n\n```\n\n<img src=\"/images/517519-20220923210545204-66327933.png\" width=\"800\" height=\"523\" loading=\"lazy\" />\n\n在Advanced的VM options中添加配置\n\n<img src=\"/images/517519-20220923213152337-2000994993.png\" width=\"800\" height=\"473\" loading=\"lazy\" />\n\n```\n-Djava.security.auth.login.config=/Users/lintong/Downloads/krb5kdc/hive.login \n-Djava.security.krb5.realm=HADOOP.COM \n-Djava.security.krb5.kdc=master \n-Djavax.security.auth.useSubjectCredsOnly=false\n\n```\n\nhive.login的内容\n\n```\ncom.sun.security.jgss.initiate{\n      com.sun.security.auth.module.Krb5LoginModule required\n      useKeyTab=true\n      useTicketCache=false\n      keyTab=\"/Users/lintong/Downloads/hive.keytab\"\n      principal=\"hive/master@HADOOP.COM\"\n      doNotPrompt=true\n      debug=true\n      debugNative=true;\n   };\n\n```\n\n## 3.查询hive\n\n<img src=\"/images/517519-20220923210900246-11726628.png\" width=\"800\" height=\"522\" loading=\"lazy\" />\n\n&nbsp;\n\n<img src=\"/images/517519-20220923211204156-1632387998.png\" width=\"800\" height=\"588\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["Hive"]},{"title":"scyllaDB基本使用","url":"/scyllaDB基本使用.html","content":"## 1.scylla部署\n\n### docker单机部署\n\n可以使用docker镜像来启动scyllaDB\n\n<img src=\"/images/517519-20211105105922898-792948816.png\" alt=\"\" loading=\"lazy\" />\n\n### docker集群部署\n\n也可以使用docker镜像来部署scyllaDB集群\n\n```\ndocker run --name scylla -p 9042:9042 -p 9160:9160 -p 10000:10000 -p 9180:9180 -v /var/lib/scylla:/var/lib/scylla -d scylladb/scylla\n\ndocker run --name scylla-node2 -p 8042:9042 -p 8160:9160 -p 1000:10000 -p 8180:9180 -v /var/lib/scylladb2:/var/lib/scylla -d scylladb/scylla --seeds=\"$(docker inspect --format='{{ .NetworkSettings.IPAddress }}' scylla)\"\n\ndocker run --name scylla-node3 -p 10042:9042 -p 10160:9160 -p 1100:10000 -p 10180:9180 -v /var/lib/scylladb3:/var/lib/scylla -d scylladb/scylla --seeds=\"$(docker inspect --format='{{ .NetworkSettings.IPAddress }}' scylla)\"\n\ndocker run --name scylla-node4 -p 11042:9042 -p 11160:9160 -p 1200:10000 -p 11180:9180 -v /var/lib/scylladb4:/var/lib/scylla -d scylladb/scylla --seeds=\"$(docker inspect --format='{{ .NetworkSettings.IPAddress }}' scylla)\"\n\n```\n\n　　\n\n## 2.在cqlsh中操作scyllaDB\n\n在cqlsh中可以使用CQL (the Cassandra Query Language) 来对scyllaDB做一些基本操作\n\n```\nsh-4.2# cqlsh\nConnected to  at 172.17.0.3:9042.\n[cqlsh 5.0.1 | Cassandra 3.0.8 | CQL spec 3.3.1 | Native protocol v4]\nUse HELP for help.\ncqlsh>\n\n```\n\n参考：[CQLSh: the CQL shell](https://docs.scylladb.com/getting-started/cqlsh/)\n\n　　\n\n## 3.scyllaDB的操作\n\nscylla数据存储于table当中，而table由keyspace分组\n\n### 创建keysapce\n\n名字叫做test\n\n```\ncqlsh> CREATE KEYSPACE IF NOT EXISTS test WITH REPLICATION = {'class': 'SimpleStrategy','replication_factor':1};\ncqlsh> describe keyspaces;\n\nsystem_schema  system_auth  system  system_distributed  test  system_traces\n\n```\n\nREPLICATION参数指定了备份策略，使用了REPLICATION后必须指定class，其中class有SimpleStrategy，NetworkTopologyStrategy，在这里由于是单机测试，所以我指定副本数量是1\n\n### 创建表\n\n```\ncqlsh> use test;\ncqlsh:test>\n\nCREATE TABLE demo (\nuser_id int,\nstr text,\nmtime timestamp,\nPRIMARY KEY (user_id, mtime)\n) WITH CLUSTERING ORDER BY (mtime DESC);\n\ncqlsh:test> DESCRIBE TABLES\n\ndemo\n\ncqlsh:test>\n\n```\n\nPRIMARY KEY参数指定了主键，会按照user_id，mtime的顺序来排列key\n\nCLUSTERING参数指定了mtime按照降序排列\n\n### 插入数据到scylla表\n\n```\nINSERT INTO demo (\nuser_id,str,mtime\n) VALUES \n(6,'test','2021-10-09 07:00:00') \nusing TTL 86400;\n\n```\n\n可以看到相同的key user_id会聚合到一起，相同的user_id中mtime按照降序排列\n\n```\ncqlsh:test> select * from demo;\n\n user_id | mtime                           | str\n---------+---------------------------------+-------\n       5 | 2021-10-09 02:00:00.000000+0000 | Panda\n       1 | 2021-10-09 04:00:00.000000+0000 |   Kay\n       1 | 2021-10-01 02:00:00.000000+0000 |   Kay\n       2 | 2021-10-09 03:00:00.000000+0000 | Snail\n       2 | 2021-10-01 02:00:00.000000+0000 | Snail\n       6 | 2021-10-09 07:00:00.000000+0000 |  test\n\n(6 rows)\n\n```\n\n参考：[Data Definition](https://docs.scylladb.com/getting-started/ddl/)　\n\n### 查看scylla集群状态\n\n```\n[root@6a30e1b8fc71 /]# nodetool status\nUsing /etc/scylla/scylla.yaml as the config file\nDatacenter: datacenter1\n=======================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address     Load       Tokens       Owns    Host ID                               Rack\nUN  172.17.0.2  1.49 MB    256          ?       eaee8765-450d-4d4e-a7b5-2ed4c6b20df3  rack1\nUN  172.17.0.5  1.04 MB    256          ?       17153bb7-f4f1-4436-bc49-f1eca3409040  rack1\nUN  172.17.0.4  1.03 MB    256          ?       25fd6224-7edc-4161-bf49-ba6fe51c9f73  rack1\nUN  172.17.0.6  1.04 MB    256          ?       3d5757d2-dc5f-4ba4-8d90-e02eb9d4c255  rack1\n\nNote: Non-system keyspaces don't have the same replication settings, effective ownership information is meaningless　　\n```\n\n### 查看表状态\n\n```\nnodetool tablestats my_ks.my_tb\n\n```\n\n### 增加字段\n\n```\nALTER TABLE xx.xx ADD col_name col_type;\n\n```\n\n### 删除scylla表<!--more-->\n&nbsp;\n\n```\ndrop table xx.xx;\n\n```\n\n&nbsp;\n\n## 4.spark和scyllaDB集成\n\n### **使用spark读写scyllaDB&nbsp;**\n\n由于scyllaDB兼容cassandra API，所以可以参考：\n\n```\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/cassandra/spark-create-operations\n\n```\n\n或者可以参考我的文章：[Spark学习笔记&mdash;&mdash;读写ScyllaDB](https://www.cnblogs.com/tonglin0325/p/15531196.html)\n\n&nbsp;\n\n其他：[sizing-your-scylla-cluster](https://www.slideshare.net/ScyllaDB/sizing-your-scylla-cluster)\n\n[MySQL 亿级数据迁移 之 Cassandra概述](https://segmentfault.com/a/1190000040511710)\n\n&nbsp;\n","tags":["scyllaDB"]},{"title":"机器学习——k-近邻算法","url":"/机器学习——k-近邻算法.html","content":"**k-近邻算法（kNN）**采用**测量不同特征值之间的距离**方法进行分类。\n\n<!--more-->\n&nbsp;\n\n优点：精度高、对异常值不敏感、无数据输入假定\n\n缺点：计算复杂度高、空间复杂度高\n\n使用数据范围：数值型和标称型\n\n&nbsp;\n\n**工作原理**：存在一个样本数据集合，也称为**训练样本集**，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中的k的出处，通常k是不大于20的整数。然后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。\n\n&nbsp;\n\n<img src=\"/images/517519-20161108230327186-629442010.png\" alt=\"\" />\n\n&nbsp;\n\n**kNN.py**\n\n```\n# coding:utf-8\n# !/usr/bin/env python\n\n'''\nCreated on Sep 16, 2010\nkNN: k Nearest Neighbors\n\nInput:      inX: vector to compare to existing dataset (1xN)\n            dataSet: size m data set of known vectors (NxM)\n            labels: data set labels (1xM vector)\n            k: number of neighbors to use for comparison (should be an odd number)\n            \nOutput:     the most popular class label\n\n@author: pbharrin\n'''\n\nfrom numpy import *\nimport operator\nfrom os import listdir\n\n\ndef classify0(inX, dataSet, labels, k):\t\t#inX是用于分类的输入向量，dataSet是输入的训练样本集，labels是标签向量，k是选择最近邻居的数目\n    dataSetSize = dataSet.shape[0]\t#shape函数求数组array的大小，例如dataSet一个4行2列的数组\n    #距离计算\n    diffMat = tile(inX, (dataSetSize,1)) - dataSet\t#tile函数的功能是重复某个数组，例如把[0,0]重复4行1列，并和dataSet相减\n    sqDiffMat = diffMat**2\t\t#对数组中和横纵坐标平方\n    #print(sqDiffMat)\n    sqDistances = sqDiffMat.sum(axis=1)\t#把数组中的每一行向量相加，即求a^2+b^2\n    #print(sqDistances)\n    distances = sqDistances**0.5\t#开根号，&radic;a^2+b^2\n    #print(distances)\n    #a = array([1.4, 1.5,1.6,1.2])\n    sortedDistIndicies = distances.argsort()     #按升序排序，从小到大的下标依次是2,3,1,0\t\n    #print(sortedDistIndicies)\n    classCount={}        #字典\n    \n    #选择距离最小的k个点  \n    for i in range(k):\n        voteIlabel = labels[sortedDistIndicies[i]]\t#按下标取得标记\n        #print(voteIlabel)\n        classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1\t#在字典中计数\n        #print(classCount[voteIlabel])\n    #排序\n    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)\n    #print(sortedClassCount)\n    return sortedClassCount[0][0]\t#返回计数最多的标记\n\ndef createDataSet():\n    group = array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]])\n    labels = ['A','A','B','B']\n    return group, labels\n\ndef file2matrix(filename):\t\t#处理格式问题，输入为文件名字符串，输出为训练样本矩阵和类标签向量\n    fr = open(filename)\n    numberOfLines = len(fr.readlines())         #取得文件的行数，1000行\n    returnMat = zeros((numberOfLines,3))        #生成一个1000行3列的矩阵\n    classLabelVector = []                       #创建一个列表 \n    fr = open(filename)\n    index = 0\t\t\t\t\t#表示特征矩阵的行数\n    for line in fr.readlines():\n        line = line.strip()\n        listFromLine = line.split('\\t')\t\t#将字符串切片并转换为列表\n        returnMat[index,:] = listFromLine[0:3]\t#选取前三个元素，存储在特征矩阵中\n        #print listFromLine\n        #print returnMat[index,:]\n        classLabelVector.append(int(listFromLine[-1]))\t#将列表的最后一列存储到向量classLabelVector中\n        index += 1\n    return returnMat,classLabelVector\t\t#返回特征矩阵和类标签向量\n    \ndef autoNorm(dataSet):\t\t\t#归一化特征值\n    minVals = dataSet.min(0)\t\t#最小值\n    maxVals = dataSet.max(0)\t\t#最大值\n    ranges = maxVals - minVals\t\t#范围\n    normDataSet = zeros(shape(dataSet))\n    m = dataSet.shape[0]\n    normDataSet = dataSet - tile(minVals, (m,1))    #原来的值和最小值的差\n    normDataSet = normDataSet/tile(ranges, (m,1))   #特征值差除以范围\n    return normDataSet, ranges, minVals\n   \ndef datingClassTest():\n    hoRatio = 0.10      \t\t#测试数据的比例\n    datingDataMat,datingLabels = file2matrix('datingTestSet2.txt')       #load data setfrom file\n    normMat, ranges, minVals = autoNorm(datingDataMat)\n    m = normMat.shape[0]\n    numTestVecs = int(m*hoRatio)\n    errorCount = 0.0\n    for i in range(numTestVecs):\n    \t#inX是用于分类的输入向量，dataSet是输入的训练样本集，labels是标签向量，k是选择最近邻居的数目\n        classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:],datingLabels[numTestVecs:m],3)\n        print \"分类器的结果: %d, 真正的结果: %d\" % (classifierResult, datingLabels[i])\n        if (classifierResult != datingLabels[i]): errorCount += 1.0\n    print \"整体的错误率: %f\" % (errorCount/float(numTestVecs))\n    print errorCount\n    \ndef classifyPerson():\n    resultList = ['不喜欢','一点点','很喜欢']\n    percentTats = float(raw_input('请输入玩游戏的时间百分比：'))\n    ffMiles = float(raw_input('请输入飞行里程总数：'))\n    iceCream = float(raw_input('请输入冰淇淋的升数：'))\n    datingDateMat,datingLabels = file2matrix(\"datingTestSet2.txt\")\t#导入数据\n    normMat,ranges,minVals = autoNorm(datingDateMat)\t\t\t#归一化\n    inArr = array([ffMiles,percentTats,iceCream])\n    classifierResult = classify0((inArr-minVals)/ranges,normMat,datingLabels,3)\t#分类的结果\n    print \"喜欢的程度：\",resultList[classifierResult-1]\n    \ndef img2vector(filename):\t\t#把32&times;32的二进制图像矩阵转换为1&times;1024的向量\n    returnVect = zeros((1,1024))\n    fr = open(filename)\n    for i in range(32):\n        lineStr = fr.readline()\n        for j in range(32):\n            returnVect[0,32*i+j] = int(lineStr[j])\n    return returnVect\n\ndef handwritingClassTest():\n    #准备训练数据\n    hwLabels = []\n    trainingFileList = listdir('digits/trainingDigits')    #导入训练数据集合\n    m = len(trainingFileList)\n    trainingMat = zeros((m,1024))\n    #和m个训练样本进行对比\n    for i in range(m):\t\t\t\t\n        fileNameStr = trainingFileList[i]\n        fileStr = fileNameStr.split('.')[0]     \t#取得去掉后缀名的文件名\n        classNumStr = int(fileStr.split('_')[0])\t#取得文件名中代表的数字\n        hwLabels.append(classNumStr)\t\t\t#由文件名生成标签向量\n        trainingMat[i,:] = img2vector('digits/trainingDigits/%s' % fileNameStr)\t#输入的训练样本集\n    #准备测试数据\n    testFileList = listdir('digits/testDigits')        #iterate through the test set\n    errorCount = 0.0\n    mTest = len(testFileList)\n    #预测测试样本\n    for i in range(mTest):\n        fileNameStr = testFileList[i]\n        fileStr = fileNameStr.split('.')[0]     \t#取得去掉后缀名的文件名\n        classNumStr = int(fileStr.split('_')[0])\t#取得文件名中代表的数字\n        vectorUnderTest = img2vector('digits/testDigits/%s' % fileNameStr)\t#用于分类的输入向量\n        #inX是用于分类的输入向量，dataSet是输入的训练样本集，labels是标签向量，k是选择最近邻居的数目\n        classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3)\n        print \"分类器的结果: %d, 真实的结果: %d\" % (classifierResult, classNumStr)\n        if (classifierResult != classNumStr): errorCount += 1.0\n    print \"\\n预测的错误数是: %d\" % errorCount\n    print \"\\n预测的错误率是: %f\" % (errorCount/float(mTest))\n    \nif __name__ == '__main__':\n#\t#group,labels = createDataSet()\n#\t#classify0([0,0],group,labels,3)\n#\tdatingDateMat,datingLabels = file2matrix(\"datingTestSet2.txt\")\n#\t#print datingDateMat\n#\t#print datingLabels\n#\timport matplotlib\n#\timport matplotlib.pyplot as plt\n#\tfig = plt.figure()\n#\tax = fig.add_subplot(111)\t\t\t\t#控制位置\n#\tax.scatter(datingDateMat[:,1],datingDateMat[:,2],15.0*array(datingLabels),15.0*array(datingLabels))\t#点的横纵坐标，大小和颜色\n#\t#plt.show()\n#\t\n#\tnormMat,ranges,minVals = autoNorm(datingDateMat)\n#\tprint normMat\n#\tprint ranges\n#\tprint minVals\n\n#\tdatingClassTest()\n#\tclassifyPerson()\n#\ttestVector = img2vector(\"digits/testDigits/0_0.txt\")\n#\tprint testVector[0,0:31]\n\thandwritingClassTest()\n\n```\n\n&nbsp;\n\n&nbsp;\n\n**1.使用Python导入数据**\n\n**<strong>NumPy函数库**中存在两种不同的数据类型（矩阵matrix和数组array），都可以用于处理行列表示的数字元素</strong>\n\n```\n>>> import kNN\n>>> group,labels = kNN.createDataSet()\n>>> group\narray([[ 1. ,  1.1],\n       [ 1. ,  1. ],\n       [ 0. ,  0. ],\n       [ 0. ,  0.1]])\n>>> labels\n['A', 'A', 'B', 'B']\n\n```\n\n&nbsp;\n\n```\n>>> group\narray([[ 1. ,  1.1],\n       [ 1. ,  1. ],\n       [ 0. ,  0. ],\n       [ 0. ,  0.1]])\n>>> group.shape　　#shape函数求数组array的大小\n(4, 2)\n>>> group.shape[0]\n4\n>>> group.shape[1]\n2\n\n```\n\n&nbsp;\n\n**2.从文本文件中解析数据**\n\n**<img src=\"/images/517519-20161109104459561-1368950388.png\" alt=\"\" />**\n\n```\n>>> kNN.classify0([0,0],group,labels,3)　　#[0,0]是用于分类的输入向量，group是输入的训练样本集，labels是标签向量，3是选择最近邻居的数目\n'B'\n\n```\n\n<img src=\"/images/517519-20161109105016499-1582315111.png\" alt=\"\" />\n\n<img src=\"/images/517519-20161109105049936-1930285510.png\" alt=\"\" />\n\n&nbsp;\n\n**3.如何测试分类器**\n\n**<img src=\"/images/517519-20161109172043436-108279038.png\" alt=\"\" />**\n\n&nbsp;\n\n**例子：使用k-近邻算法改进约会网站的配对效果**\n\n**1.准备数据：从文本文件中解析数据**\n\n<img src=\"/images/517519-20161109203148874-2138540934.png\" alt=\"\" />\n\n&nbsp;\n\n**2.分析数据：使用Matplotlib创建散点图**\n\n```\ndatingDateMat,datingLabels = file2matrix(\"datingTestSet2.txt\")\n\t#print datingDateMat\n\t#print datingLabels\n\timport matplotlib\n\timport matplotlib.pyplot as plt\n\tfig = plt.figure()\n\tax = fig.add_subplot(111)\t\t\t\t#控制位置\n\tax.scatter(datingDateMat[:,1],datingDateMat[:,2],15.0*array(datingLabels),15.0*array(datingLabels))\t#点的横纵坐标，大小和颜色\n\tplt.show()\n\n```\n\n&nbsp;<img src=\"/images/517519-20161109210922624-1549684476.png\" alt=\"\" width=\"515\" height=\"437\" />\n\n**&nbsp;**\n\n**3.准备数据：归一化数值**\n\n在计算欧式距离的时候，数值差最大的属性对计算结果的影响最大。在处理这种不同取值范围的特征值的时候，我们通常的方法是将数值归一化，如将取值范围处理为0到1或者-1到1之间。\n\n<img src=\"/images/517519-20161109211628405-1685923038.png\" alt=\"\" />\n\n```\ndef autoNorm(dataSet):\t\t\t#归一化特征值\n    minVals = dataSet.min(0)\t\t#最小值\n    maxVals = dataSet.max(0)\t\t#最大值\n    ranges = maxVals - minVals\t\t#范围\n    normDataSet = zeros(shape(dataSet))\n    m = dataSet.shape[0]\n    normDataSet = dataSet - tile(minVals, (m,1))    #原来的值和最小值的差\n    normDataSet = normDataSet/tile(ranges, (m,1))   #特征值差除以范围\n    return normDataSet, ranges, minVals\n\n```\n\n&nbsp;\n\n```\nnormMat,ranges,minVals = autoNorm(datingDateMat)\nprint normMat\nprint ranges\nprint minVals\n\n```\n\n**&nbsp;**\n\n**4.测试算法：作为完整程序验证分类器**\n\n&nbsp;<img src=\"/images/517519-20161109215729670-958648379.png\" alt=\"\" />\n\n```\ndef datingClassTest():\n    hoRatio = 0.10      \t\t#测试数据的比例\n    datingDataMat,datingLabels = file2matrix('datingTestSet2.txt')       #load data setfrom file\n    normMat, ranges, minVals = autoNorm(datingDataMat)\n    m = normMat.shape[0]\n    numTestVecs = int(m*hoRatio)\n    errorCount = 0.0\n    for i in range(numTestVecs):\n    \t#inX是用于分类的输入向量，dataSet是输入的训练样本集，labels是标签向量，k是选择最近邻居的数目\n        classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:],datingLabels[numTestVecs:m],3)\n        print \"分类器的结果: %d, 真正的结果: %d\" % (classifierResult, datingLabels[i])\n        if (classifierResult != datingLabels[i]): errorCount += 1.0\n    print \"整体的错误率: %f\" % (errorCount/float(numTestVecs))\n    print errorCount\n\n```\n\n&nbsp;\n\n**5.使用算法：构建完整可用系统**\n\n```\ndef classifyPerson():\n    resultList = ['不喜欢','一点点','很喜欢']\n    percentTats = float(raw_input('请输入玩游戏的时间百分比：'))\n    ffMiles = float(raw_input('请输入飞行里程总数：'))\n    iceCream = float(raw_input('请输入冰淇淋的升数：'))\n    datingDateMat,datingLabels = file2matrix(\"datingTestSet2.txt\")\t#导入数据\n    normMat,ranges,minVals = autoNorm(datingDateMat)\t\t\t#归一化\n    inArr = array([ffMiles,percentTats,iceCream])\n    classifierResult = classify0((inArr-minVals)/ranges,normMat,datingLabels,3)\t#分类的结果\n    print \"喜欢的程度：\",resultList[classifierResult-1]\n\n```\n\n&nbsp;\n\n&nbsp;\n\n**例子：手写识别系统**\n\n**<img src=\"/images/517519-20161110091632155-1007195624.png\" alt=\"\" /><img src=\"/images/517519-20161110091659217-1329885159.png\" alt=\"\" />**\n\n&nbsp;\n\n**1.准备数据，将图像转换为测试向量**\n\n```\ndef img2vector(filename):\t\t#把32&times;32的二进制图像矩阵转换为1&times;1024的向量\n    returnVect = zeros((1,1024))\n    fr = open(filename)\n    for i in range(32):\n        lineStr = fr.readline()\n        for j in range(32):\n            returnVect[0,32*i+j] = int(lineStr[j])\n    return returnVect\n\n```\n\n&nbsp;\n\n```\ntestVector = img2vector(\"digits/testDigits/0_0.txt\")\nprint testVector[0,0:31]\n\n```\n\n&nbsp;\n\n**2.测试算法：使用k-近邻算法识别手写数字**\n\n<img src=\"/images/517519-20161110094556436-1544654665.png\" alt=\"\" />\n\n```\ndef handwritingClassTest():\n    #准备训练数据\n    hwLabels = []\n    trainingFileList = listdir('digits/trainingDigits')    #导入训练数据集合\n    m = len(trainingFileList)\n    trainingMat = zeros((m,1024))\n    #和m个训练样本进行对比\n    for i in range(m):\t\t\t\t\n        fileNameStr = trainingFileList[i]\n        fileStr = fileNameStr.split('.')[0]     \t#取得去掉后缀名的文件名\n        classNumStr = int(fileStr.split('_')[0])\t#取得文件名中代表的数字\n        hwLabels.append(classNumStr)\t\t\t#由文件名生成标签向量\n        trainingMat[i,:] = img2vector('digits/trainingDigits/%s' % fileNameStr)\t#输入的训练样本集\n    #准备测试数据\n    testFileList = listdir('digits/testDigits')        #iterate through the test set\n    errorCount = 0.0\n    mTest = len(testFileList)\n    #预测测试样本\n    for i in range(mTest):\n        fileNameStr = testFileList[i]\n        fileStr = fileNameStr.split('.')[0]     \t#取得去掉后缀名的文件名\n        classNumStr = int(fileStr.split('_')[0])\t#取得文件名中代表的数字\n        vectorUnderTest = img2vector('digits/testDigits/%s' % fileNameStr)\t#用于分类的输入向量\n        #inX是用于分类的输入向量，dataSet是输入的训练样本集，labels是标签向量，k是选择最近邻居的数目\n        classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3)\n        print \"分类器的结果: %d, 真实的结果: %d\" % (classifierResult, classNumStr)\n        if (classifierResult != classNumStr): errorCount += 1.0\n    print \"\\n预测的错误数是: %d\" % errorCount\n    print \"\\n预测的错误率是: %f\" % (errorCount/float(mTest))\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20161110101341030-1255406654.png\" alt=\"\" />\n","tags":["ML"]},{"title":"机器学习——分类和回归","url":"/机器学习——分类和回归.html","content":"**1.机器学习的主要任务：**<br />一是将实例数据划分到合适的分类中，即**分类问题**。 而是是**回归**， 它主要用于预测数值型数据，典型的回归例子：数据拟合曲线。\n\n<!--more-->\n&nbsp;\n\n**2.监督学习和无监督学习：**\n\n**分类和回归**属于**监督学习**，之所以称之为监督学习，是因为这类算法必须直到预测什么，即**目标变量的分类信息**。\n\n&nbsp;\n\n对于**无监督学习**，此时**数据没有类别信息，也不会给定目标值**。在无监督学习中，将数据集合分成由类似的对象组成的多个类的过程被成为**聚类**；将寻找描述数据统计值的过程称之为**密度估计**。此外，无监督学习还可以减少数据特征的维度，以便我们可以使用二维或者三维图形更加直观地展示数据信息。\n\n<img src=\"/images/517519-20161108221103624-1805790586.png\" alt=\"\" />\n\n&nbsp;\n\n**3.线性回归和非线性回归**\n\n　　**线性回归**需要一个线性模型。一个**线性的模型**意味着模型的**每一项**要么是一个**常数**，要么是**一个常数和一个预测变量的乘积**。一个线性等式等于每一项相加的和。等式：\n\nResponse = constant + parameter * predictor + ... + parameter * predictor　　<=>　　Y = b<sub> o</sub> + b<sub>1</sub>X<sub>1</sub> + b<sub>2</sub>X<sub>2</sub> + ... + b<sub>k</sub>X<sub>k<br /></sub>\n\n在统计学中，如果一个回归方程是线性的，那么它的参数必须是线性的。但是可以转换预测变量加上平方，来使得模型产生曲线，比如 Y = b<sub> o</sub> + b<sub>1</sub>X<sub>1</sub> + b<sub>2</sub>X<sub>1</sub><sup>2</sup>\n\n这时模型仍然是线性的，虽然预测变量带有平方。当然加上log或者反函数也是可以的。\n\n<img src=\"/images/517519-20161220231517026-126662411.png\" alt=\"\" width=\"578\" height=\"188\" />\n\n<img src=\"/images/517519-20161220231543245-965428296.png\" alt=\"\" width=\"609\" height=\"172\" />\n\n　另外可以参考的博文：[Khan公开课 - 统计学学习笔记：（九）线性回归公式，决定系数和协方差 ](http://blog.csdn.net/flowingflying/article/details/8070181)　\n\n线性回归等式有一个基本的形式，而**非线性回归**提供了许多灵活的曲线拟合方程。以下是几个MATLAB中的典型非线性方程例子。\n\n非线性函数的一个例子就是高阶多项式（即多项式阶数p>1）：G = a<sub> o</sub> + a<sub>1</sub>X + a<sub>2</sub>X^2+ ... + a<sub>k</sub>X^k。其他类型的非线性函数可通过泰勒展开用多项式逼近表示。函数G在x<sub>0</sub>处的线性近似为G(x<sub>0</sub>)+G'(x<sub>0</sub>)(x-x<sub>0</sub>)。\n\n<img src=\"/images/517519-20161220144431823-1676217192.png\" alt=\"\" width=\"354\" height=\"345\" />\n\n<img src=\"/images/517519-20161220144453292-1483914024.png\" alt=\"\" width=\"341\" height=\"160\" />\n\n&nbsp;\n","tags":["ML"]},{"title":"Hive学习笔记——beeline","url":"/Hive学习笔记——beeline.html","content":"使用beeline连接hive\n\n```\nkinit -kt xxx.keytab xxx\nbeeline -u \"jdbc:hive2://10.65.13.98:10000/default;principal=hive/_HOST@CLOUDERA.SITE\"\n\n```\n\n参考：\n\n```\nhttps://docs.cloudera.com/runtime/7.2.7/securing-hive/topics/hive_remote_data_access.html\n\n```\n\n如果要想直接运行SQL，可以\n\n```\nbeeline -u \"jdbc:hive2://10.65.13.98:10000/default;principal=hive/_HOST@CLOUDERA.SITE\" --silent=true --outputformat=tsv2 --showHeader=false -e \"select * from xxx.xxx\"\n\n```\n\n退出beeline\n\n```\n!quit\n\n```\n\n　　\n","tags":["Hive"]},{"title":"IDEA的kubernetes插件使用","url":"/IDEA的kubernetes插件使用.html","content":"## **1.在IDEA或者pycharm中安装kubernets插件**\n\n<img src=\"/images/517519-20220515205810831-1113476921.png\" width=\"600\" height=\"589\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n## **2.使用kubernetes插件**\n\n在view的Tool windows中点击service\n\n<img src=\"/images/517519-20220515221054940-1591523956.png\" width=\"500\" height=\"228\" loading=\"lazy\" />\n\n可以看到docker和kubernetes的管理界面\n\n&nbsp;<img src=\"/images/517519-20220515221427343-1294800765.png\" width=\"800\" height=\"232\" loading=\"lazy\" />\n\n&nbsp;可以切换context，从而在不同k8s集群之间切换\n\n<img src=\"/images/517519-20220518234757344-642887864.png\" width=\"600\" height=\"149\" loading=\"lazy\" />\n\n多个k8s集群的配置文件默认在\n\n```\n/Users/lintong/.kube/config\n\n```\n\n可以使用下面命令进行查看配置\n\n```\nkubectl config viewq　\n```\n\n切换k8s集群可以使用kubectx，参考：[K8S多集群切换](https://www.cnblogs.com/wjoyxt/p/14334968.html)\n\n```\nbrew install kubectx\n\nkubectx\n\n```\n\n比如切换docker-desktop\n\n<img src=\"/images/517519-20220519002607692-737722335.png\" width=\"300\" height=\"97\" loading=\"lazy\" />\n\n在使用docker-desktop作为k8s集群的时候，记得在docker配置中开启k8s选项\n\n<img src=\"/images/517519-20220519003426813-308501757.png\" width=\"800\" height=\"337\" loading=\"lazy\" />\n\n&nbsp;&nbsp;&nbsp;\n\n## **3.创建ns.yaml用于创建namespace**\n\n选择kres的k8s配置模板\n\n&nbsp;<img src=\"/images/517519-20220519002852849-644549194.png\" width=\"600\" height=\"213\" loading=\"lazy\" />\n\n```\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev-ns\n\n```\n\n在k8s集群中创建该yaml配置的namespace\n\n<img src=\"/images/517519-20220519003548962-1086044914.png\" width=\"700\" height=\"406\" loading=\"lazy\" />\n\n成功创建\n\n```\n➜  /Users/lintong $ kubectl get ns -A\nNAME              STATUS   AGE\ndefault           Active   3d2h\ndev-ns            Active   31s\nkube-node-lease   Active   3d2h\nkube-public       Active   3d2h\nkube-system       Active   3d2h\n\n```\n\n之后就可以切到这个namespace下\n\n<img src=\"/images/517519-20220519103519765-1996445872.png\" width=\"400\" height=\"88\" loading=\"lazy\" />\n\n&nbsp;\n\n## **4.创建deployment.yaml用于创建deployment**\n\n可以输入kd产生模板\n\n<img src=\"/images/517519-20220519103807289-888309468.png\" width=\"300\" height=\"385\" loading=\"lazy\" />\n\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aa-bb-cc\n  namespace: dev-ns\n  labels:\n    app: aa-bb-cc\nspec:\n  replicas: 2\n  template:\n    metadata:\n      name:aa-bb-cc\n      labels:\n        app: aa-bb-cc\n    spec:\n      containers:\n        - name: aa-bb-cc\n          image: xxxx/aa-bb-cc:latest\n          imagePullPolicy: IfNotPresent\n          ports:\n            - containerPort: 8000\n      restartPolicy: Always\n  selector:\n    matchLabels:\n      app: aa-bb-cc\n\n```\n\n创建deployment\n\n<img src=\"/images/517519-20220519213108451-722697731.png\" width=\"600\" height=\"58\" loading=\"lazy\" />\n\n&nbsp;\n\n## **5.创建service来创建一个ip**\n\n使用ks快捷键，其中几个port含义可以参考：[k8s 辨析 port、NodePort、targetPort、containerPort 区别&nbsp;](https://www.cnblogs.com/veeupup/p/13545361.html)\n\n**nodePort** 提供了集群外部客户端访问 Service 的一种方式，nodePort 提供了集群外部客户端访问 Service 的端口，通过&nbsp;`nodeIP:nodePort`&nbsp;提供了外部流量访问k8s集群中service的入口。\n\n比如外部用户要访问k8s集群中的一个Web应用，那么我们可以配置对应service的`type=NodePort`，`nodePort=30001`。其他用户就可以通过浏览器`http://node:30001`访问到该web服务。\n\n而数据库等服务可能不需要被外界访问，只需被内部服务访问即可，那么我们就不必设置service的NodePort。\n\n**port&nbsp;**是暴露在cluster ip上的端口，:port提供了集群内部客户端访问service的入口，即`clusterIP:port`。\n\nmysql容器暴露了3306端口（参考[DockerFile](https://github.com/docker-library/mysql/)），集群内其他容器通过33306端口访问mysql服务，但是外部流量不能访问mysql服务，因为mysql服务没有配置NodePort\n\n**targetPort&nbsp;**是pod上的端口，从port/nodePort上来的数据，经过kube-proxy流入到后端pod的targetPort上，最后进入容器。\n\n与制作容器时暴露的端口一致（使用DockerFile中的EXPOSE），例如官方的nginx（参考[DockerFile](https://github.com/nginxinc/docker-nginx)）暴露80端口。&nbsp;\n\n**containerPort** 是在pod控制器中定义的、pod中的容器需要暴露的端口。\n\n```\napiVersion: v1\nkind: Service\nmetadata:\n  name: aa-bb-cc\n  namespace: dev-ns\nspec:\n  selector:\n    app: aa-bb-cc\n  ports:\n    - port: 18080\n      targetPort: 8000\n      nodePort: 30009\n  type: NodePort\n\n```\n\n创建service\n\n<img src=\"/images/517519-20220519231617808-1959501207.png\" width=\"600\" height=\"50\" loading=\"lazy\" />\n\n&nbsp;\n\n## **6.查看服务状态**\n\n<img src=\"/images/517519-20220519231659363-1051273387.png\" width=\"500\" height=\"333\" loading=\"lazy\" />\n\n这是如果是docker-desktop集群的话，就可以使用localhost:nodePort，即 localhost:30009 来访问启动的服务了\n\n此时也可以查看pod的日志等\n\n<img src=\"/images/517519-20220519231947742-1727799660.png\" width=\"800\" height=\"175\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["k8s"]},{"title":"Java数据结构——字典树TRIE","url":"/Java数据结构——字典树TRIE.html","content":"又称**单词查找树**，[Trie树](http://baike.baidu.com/view/1436495.htm)，是一种[树形结构](http://baike.baidu.com/view/540464.htm)，是一种**哈希树的变种**。\n\n典型应用是用于统计，排序和保存大量的[字符](http://baike.baidu.com/view/263416.htm)串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。\n\n它的优点是：利用字符串的公共前缀来减少查询时间，最大限度地减少无谓的字符串比较，**查询效率比[哈希](http://baike.baidu.com/view/99075.htm)树高**。\n\n<!--more-->\n&nbsp;\n\n下面是字典树数据的Java实现代码，字典树中存放的是26个英文字母\n\n```\npackage coding_Interview_guide.ch5;\n\nclass trie {\n    // 根节点\n    private TrieNode root;\n\n    public trie() {\n        super();\n        this.root = new TrieNode();\n    }\n\n    public void insert(String word) {\n        if (word == null) {\n            return;\n        }\n        char[] chs = word.toCharArray();\n        TrieNode node = root;\n        int index = 0; // root的map[TrieNode]数组的下标\n        for (char ch : chs) {\n            index = ch - 'a';\n            if (node.map[index] == null) {\n                node.map[index] = new TrieNode();\n            }\n            node = node.map[index]; // 转到下一个字符的节点\n            node.path++; // 公用节点树加1\n        }\n        node.end++; // 结尾\n    }\n\n    public boolean search(String word) {\n        if (word == null) {\n            return false;\n        }\n        char[] chs = word.toCharArray();\n        TrieNode node = root;\n        int index = 0; // root的map[TrieNode]数组的下标\n        for (char ch : chs) {\n            index = ch - 'a';\n            if (node.map[index] == null) {\n                return false;\n            } else {\n                node = node.map[index];\n            }\n        }\n        return node.end != 0;\n    }\n\n    public void delete(String word) {\n        if (search(word)) {\n            char[] chs = word.toCharArray();\n            TrieNode node = root;\n            int index = 0; // root的map[TrieNode]数组的下标\n            for (char ch : chs) {\n                index = ch - 'a';\n                if (node.map[index].path-- == 1) { // 最后一个节点的公用节点数减一,同时如果只有一个公用节点的话,直接置为null\n                    node.map[index] = null;\n                    return;\n                }\n                // 否则继续下一个节点\n                node = node.map[index]; // 转到下一个字符的节点\n            }\n            node.end--; // 结尾数减一\n        }\n    }\n\n    public int prefixNumber(String pre) {\n        if (pre == null) {\n            return 0;\n        }\n        char[] chs = pre.toCharArray();\n        TrieNode node = root;\n        int index = 0; // root的map[TrieNode]数组的下标\n        for (char ch : chs) {\n            index = ch - 'a';\n            if (node.map[index] == null) {\n                return 0;\n            }\n            node = node.map[index];\n        }\n        return node.path;\n    }\n}\n\nclass TrieNode {\n    public int path; // 表示有多少个单词公用这个节点\n    public int end; // 表示有多少个单词以这个节点结尾\n    public TrieNode[] map;\n\n    public TrieNode() {\n        super();\n        this.path = 0;\n        this.end = 0;\n        this.map = new TrieNode[26];\n    }\n}\n\npublic class Trie {\n\n    public static void main(String[] args) {\n        // TODO 自动生成的方法存根\n        trie t = new trie();\n        \n    }\n\n}\n\n```\n\n&nbsp;\n","tags":["数据结构"]},{"title":"面试题目——《CC150》高等难题","url":"/面试题目——《CC150》高等难题.html","content":"**面试题18.1：编写一个函数，将两个数字相加。不得使用+或其他算数运算符。**\n\n```\npackage cc150.high;\n\npublic class Add {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t}\n\t\n\tpublic int add(int a,int b){\t//将两个数字相加，不得使用+或者其他算数运算符\n\t\tif(b == 0)\n\t\t\treturn a;\n\t\tint sum = a^b;\t\t\t\t\t//相加，但不进位\n\t\tint carry = (a&amp;b)<<1;\t\t//进位，但是不相加\n\t\treturn add(sum,carry);\n\t}\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n\n**面试题18.2：编写一个方法，洗一副牌。要求做到完美洗牌，换言之，这副牌52！中排列组合出现的概率相同。假设给定一个完美的随机数发生器。**\n\n```\npackage cc150.high;\n\npublic class ShuffleArrayInteratively {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t}\n\t\n\tpublic void shuffleArrayInteratively(int[] cards){\t//使用递归算法洗牌\n\t\tfor(int i=0;i<cards.length;i++){\n\t\t\tint k = rand(0,i);\n\t\t\tint temp = cards[k];\n\t\t\tcards[k] = cards[i];\n\t\t\tcards[i] = temp;\n\t\t}\n\t}\n\t\n\tpublic int rand(int lower,int higher){\t//生成一个lower和higher（含）之间的随机数\n\t\treturn lower+(int)(Math.random()*(higher-lower+1));\n\t}\n\t\n\tpublic int[] shuffleArrayRecursively(int[] cards,int i){\t//打乱前i个部分的次序\n\t\tif(i==0)\n\t\t\treturn cards;\n\t\tshuffleArrayRecursively(cards,i-1);\t//打乱先前部分的次序\n\t\tint k=rand(0,i);\t\t\t//在前i中随机选一个数和第i个数交换\n\t\t//交换\n\t\tint temp = cards[k];\n\t\tcards[k] = cards[i];\n\t\tcards[i] = temp;\n\t\t//返回元素次序被打乱的数组\n\t\treturn cards;\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题18.3：编写一个方法，从大小为n的数组中随机选出m个整数。要求每个元素被选中的概率相同。**\n\n&nbsp;\n\n**面试题18.4：编写一个方法，数出0到n（含）中数字2出现了几次。**\n\n```\npackage cc150.high;\n\npublic class Count2 {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tCount2 c2 = new Count2();\n\t\tSystem.out.println(c2.countNumberOf2s(21));\n\t}\n\t\n\tpublic int countNumberOf2s(int n) {\t//对比Leetcode中数1的题目\n\t\tif (n <= 1) return 0;\n\t\t \n\t    int res = 0, m;\n\t    for (m = 1;m <= n;m *= 10) {\t\t\t\t//m为10/100/1000...\n\t        int tmp1 = n/m, tmp2 = n%m;\t//tmp1是较高位，tmp2是较低位\n\t        if(tmp1 % 10 == 2){\t\t\t\t//m=1，tmp1为倒数第一位，tmp2为0；m=10，倒二倒一\n\t        \tres += (tmp1 + 7) / 10 * m + (tmp2 + 1);\t//如果十位是2，加上个位加一，求十位是2的个数\n\t        \tSystem.out.println(\"res1=\"+res);\n\t        }\n\t        else{\t\t\t\t\t\t\t\t\t\t\t//如果个位不是2，求小于这个数的个数是2的个数\n\t        \tres += (tmp1 + 7) / 10 * m;\n\t        \tSystem.out.println(\"res2=\"+res);\n\t        }\n\t    }\n\t    return res;\n\t}\n\t\n//\tpublic int countNumberOf2s(int n) {\t\t//复杂度过高，耗时过多\n//        // write code here\n//\t\tint count = 0;\n//\t\tfor(int i=2;i<=n;i++)\n//\t\t\tcount += countNumberOf2(i);\n//\t\treturn count;\n//    }\n//\t\n//\tpublic int countNumberOf2(int n) {\n//        // write code here\n//\t\tint count = 0;\n//\t\twhile(n>0){\n//\t\t\tif(n%10 == 2)\n//\t\t\t\tcount++;\n//\t\t\tn /= 10;\n//\t\t}\n//\t\treturn count;\n//    }\n\n}\n\n```\n\n&nbsp;\n\n**面试题18.5：有个内含单词的超大文本文件，给定任意两个单词，找出在这个文件中这两个单词的最短距离（也即相隔几个单词）。有办法在O(1)时间里完成搜索操作吗？解法的空间复杂度如何？**\n\n```\npackage cc150.high;\n\npublic class Distance {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\n\t}\n\t\n\tpublic int getDistance(String[] article, int n, String x, String y) {\n        // write code here\n\t\tint min = Integer.MAX_VALUE;\n\t\tint lastWord1 = -1;\n\t\tint lastWord2 = -1;\n\t\tfor(int i=0;i<article.length;i++){\n\t\t\tString currentWord = article[i];\n\t\t\tif(currentWord.equals(x)){\t\t//等于x\n\t\t\t\tlastWord1 = i;\n\t\t\t\t//如果x和y有顺序要求，注释下面三句\n\t\t\t\tint distance =  lastWord1-lastWord2;\t//不用求绝对值，因为lastWord1肯定在lastWord2后面\n\t\t\t\tif(lastWord2>=0 &amp;&amp; min>distance)\n\t\t\t\t\tmin = distance;\n\t\t\t}else if(currentWord.equals(y)){\n\t\t\t\tlastWord2 = i;\n\t\t\t\tint distance =  lastWord2-lastWord1;\n\t\t\t\tif(lastWord1>=0 &amp;&amp; min>distance)\n\t\t\t\t\tmin = distance;\n\t\t\t}\n\t\t}\n\t\treturn min;\n    }\n\t\n}\n\n```\n\n&nbsp;\n\n**面试题18.7：给定一组单词，编写一个程序，找出其中的最长单词，且该单词由这组单词中的其他单词组合而成。**\n\n```\npackage cc150.high;\n\nimport java.util.Arrays;\nimport java.util.Comparator;\nimport java.util.HashMap;\n\npublic class LongestString {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tString[] a = {\"abcd\",\"bcd\",\"abc\",\"ab\",\"bc\",\"d\"};\n\t\tLongestString ls = new LongestString();\n\t\tString b = ls.getLongest(a,6);\n\t\tSystem.out.println(b);\n\t}\n\t\n\tpublic String  getLongest(String[] str, int n) {\n        // write code here\n\t\tHashMap<String,Boolean> map = new HashMap<String,Boolean>();\n\t\tfor(String s:str)\t\t\t\t\t//把所有的字符串放进哈希表中，并标为true\n\t\t\tmap.put(s, true);\n\t\tArrays.sort(str,new Comparator<String>(){\t\t//排序\n\t          \n            @Override\n            public int compare(String str1, String str2) {\n                // TODO Auto-generated method stub        \n                    int l1 = str1.length();\n                    int l2 = str2.length();\n                    return l2-l1;\t\t\t//按字符串的长度从长到短排序\n            }\n        });\n\t\tfor(String s:str){\n\t\t\tif(canBuildWord(s, true, map)){\n\t\t\t\t//System.out.println(s);\n\t\t\t\treturn s;\n\t\t\t}\n\t\t}\n\t\treturn \"\";\n    }\n\t\n\tpublic boolean canBuildWord(String str,boolean isOriginalWord,HashMap<String,Boolean> map){\n\t\tif(map.containsKey(str) &amp;&amp; !isOriginalWord)\t//如果已经包含这个字符串，且不是原始字符\n\t\t\treturn map.get(str);\t\t\t\t\t\t//返回true\n\t\tfor(int i=1;i<str.length();i++){\n\t\t\tString left = str.substring(0,i);\n\t\t\tString right = str.substring(i);\n\t\t\t//使用递归，单词可以由任意数量的其他单词组成，即right也可能由其他单词组成\n\t\t\tif(map.containsKey(left) &amp;&amp; map.get(left) == true &amp;&amp; canBuildWord(right, false, map))\t\n\t\t\t\treturn true;\n\t\t}\n\t\tmap.put(str,false);\t\t//把这个检查过的str标记成false，避免以后遇到再重复检查\n\t\treturn false;\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题18.8：给定一个字符串s和一个包含较短字符串的数组T，设计一个方法，根据T中的每一个较短字符串，对s进行搜索。**\n\n```\npackage cc150.high;\n\npublic class Substr {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tString[] str = {\"a\",\"b\",\"c\",\"d\"};\n\t\tString s = \"abc\";\n\t\tSubstr sb = new Substr();\n\t\tboolean[] b  = sb.chkSubStr(str,4,s);\n\t\tfor(int i=0;i<b.length;i++)\n\t\t\tSystem.out.println(b[i]);\n\t}\n\t\n\tpublic boolean[] chkSubStr(String[] p, int n, String s) {\n        // write code here\n\t\tint len = p.length;\n\t\tboolean[] result = new boolean[len];\n\t\tfor(int i=0;i<len;i++){\n\t\t\tif(s.contains(p[i]))\n\t\t\t\tresult[i] = true;\n\t\t\telse\n\t\t\t\tresult[i] = false;\n\t\t}\n\t\treturn result;\n    }\n\n}\n\n```\n\n&nbsp;\n\n**面试题18.9：随机生成一些数字并传入某个方法。编写一个程序，每当收到新数字时，找出并记录中位数。**\n\n```\npackage cc150.high;\n\n\nimport java.util.Comparator;\nimport java.util.PriorityQueue;\n\npublic class Middle {\t\t//实时中位数\n\n\tprivate Comparator<Integer> minHeapComparator;\t//小堆存放大于中位数的值\n\tprivate Comparator<Integer> maxHeapComparator =  new Comparator<Integer>(){  //大堆存放小于中位数的值，队头是中位数\n\t\t@Override\n\t\tpublic int compare(Integer o1, Integer o2) {\n\t\t\t// TODO 自动生成的方法存根\n\t\t\t  if(o1 > o2)  {  \n\t                return -1;  \n\t            }  \n\t            else if(o1<o2){  \n\t                return 1;  \n\t            }  \n\t            else{  \n\t                return 0;  \n\t            }  \n\t\t}  \n    };\n    \n\tprivate PriorityQueue<Integer> maxHeap = new PriorityQueue<Integer>(maxHeapComparator);\t//编写Comparator，最大的数据项在队头\n\tprivate PriorityQueue<Integer> minHeap = new PriorityQueue<Integer>();\t//优先队列中，关键字最小的数据项总是在队头\n\t\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tMiddle md = new Middle();\n\t\tint[] a = {236312,116289,257841,40359,21993,121674,68768,288444,98015,118071,130963,221777,71589,233048,89053,20048,264772,141943,170253,299901,193849,211453,198250,280383,126656,4775,229057,119532};\n\t\tint[] re = md.getMiddle(a,28);\n\t\tfor(int i=0;i<re.length;i++)\n\t\t\tSystem.out.println(re[i]);\n\t\t\n//\t\tmd.addNewNumber(6);\n//\t\tmd.addNewNumber(8);\n//\t\tmd.addNewNumber(3);\n//\t\tmd.addNewNumber(5);\n//\t\tmd.addNewNumber(1);\n//\t\tmd.addNewNumber(2);\n//\t\tSystem.out.println(md.maxHeap.poll());\n//\t\tSystem.out.println(md.maxHeap.poll());\n//\t\tSystem.out.println(md.maxHeap.poll());\n//\t\tSystem.out.println();\n//\t\tSystem.out.println(md.minHeap.poll());\n//\t\tSystem.out.println(md.minHeap.poll());\n//\t\tSystem.out.println(md.minHeap.poll());\n\t\t\n\t}\n\t\n\tpublic int[] getMiddle(int[] A, int n) {\n        // write code here\n\t\tint[] result = new int[n];\n\t\tfor(int i=0;i<n;i++){\n\t\t\taddNewNumber(A[i]);\n\t\t\tresult[i] = getMedian();\n\t\t}\n\t\treturn result;\n    }\n\t\n\tpublic int getMedian(){\n\t\tif(maxHeap.isEmpty())\n\t\t\treturn 0;\n\t\tif(maxHeap.size() == minHeap.size())\n\t\t\t//return (maxHeap.peek()+minHeap.peek())>>1;\n\t\t\treturn maxHeap.peek();\n\t   else\n\t\t   return maxHeap.peek();\n\t}\n\t\n\tpublic void addNewNumber(int randomNumber){\n\t\tif(maxHeap.size() == minHeap.size()){\t\t//两个队列相等的情况\n\t\t\tif((minHeap.peek() != null) &amp;&amp; randomNumber > minHeap.peek()){\t//新的数大于小堆的队头\n\t\t\t\tmaxHeap.offer(minHeap.poll());\n\t\t\t\tminHeap.offer(randomNumber);\n\t\t\t}else{\t\t\t\t\t//新的数小于等于小堆的队头，为空默认放进大堆\n\t\t\t\tmaxHeap.offer(randomNumber);\n\t\t\t}\n\t\t}else{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//两个队列不相等的情况\n\t\t\tif(randomNumber < maxHeap.peek()){\t\t//新的数小于大堆的队头\n\t\t\t\tminHeap.offer(maxHeap.poll());\n\t\t\t\tmaxHeap.offer(randomNumber);\n\t\t\t}else{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//新的数大于等于大堆的队头\n\t\t\t\tminHeap.offer(randomNumber);\n\t\t\t}\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题18.10：给定两个字典里的单词，长度相等。编写一个方法，将一个单词变换成另一个单词，一次只改动一个字母。在变换过程中，每一步得到的新单词都必须是字典里存在的。**\n\n```\npackage cc150.high;\n\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.LinkedList;\nimport java.util.Map;\nimport java.util.Queue;\nimport java.util.Set;\nimport java.util.TreeMap;\nimport java.util.TreeSet;\n\npublic class Change {\t\t//字符串变换，字符串s变换到t所需要的最少步数，且变换过程中每个字符串都是字典中的\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tSet<String> s = new HashSet<String>();\n\t\ts.add(\"ABC\");\n\t\ts.add(\"ADC\");\n\t\ts.add(\"BDC\");\n\t\ts.add(\"AAA\");\n\t\tChange c = new Change();\n\t\tString[] str = {\"abc\",\"adc\",\"bdc\",\"aaa\"};\n\t\tSystem.out.println(c.countChanges(str,4,\"abc\",\"bdc\"));\n//\t\tSystem.out.println(c.countChanges(s,4,\"abc\",\"bdc\"));\n//\t\tLinkedList<String> l = c.countChanges(s,4,\"abc\",\"bdc\");\n//\t\tIterator i = l.iterator();\n//\t\twhile(i.hasNext())\n//\t\t\tSystem.out.println(i.next());\n\t}\n\t\n\t//public int countChanges(Set<String> dic, int n, String s, String t) {\n\tpublic int countChanges(String[] dictionary, int n, String s, String t) {\n        // write code here\n\t\tSet<String> dic = new HashSet<String>();\n\t\tfor(int i=0;i<n;i++){\n\t\t\tdic.add(dictionary[i].toUpperCase());\n\t\t}\n\t\ts = s.toUpperCase();\n\t\tt = t.toUpperCase();\n\t\tQueue<String> actionQueue = new LinkedList<String>();\n\t\tSet<String> visitedSet = new HashSet<String>();\n\t\tMap<String,String> backtraceMap = new TreeMap<String,String>();\n\t\t\n\t\tactionQueue.add(s);\n\t\tvisitedSet.add(s);\n\t\t\n\t\twhile(!actionQueue.isEmpty()){\t\t//非空的时候循环\n\t\t\tString w = actionQueue.poll();\n\t\t\tfor(String v : getOneEditWords(w)){\n\t\t\t\tif(v.equals(t)){\t\t\t\t\t//如果改变一次等于t的话\n\t\t\t\t\tLinkedList<String> list = new LinkedList<String>();\n\t\t\t\t\tlist.add(v);\n\t\t\t\t\twhile(w != null){\n\t\t\t\t\t\tlist.add(0,w);\t\t\t\t//把w插入到list的首部\n\t\t\t\t\t\tw = backtraceMap.get(w);\t//到最后的时候，w指向的是null，这个是结束条件，其他时候w的值赋值为v\n\t\t\t\t\t}\n\t\t\t\t\t//return list;\n\t\t\t\t\treturn list.size();\n\t\t\t\t}\n\t\t\t\tif(dic.contains(v)){\t\t\t//如果改变一次后不等于t，但是是字典中的值的话\n\t\t\t\t\tif(!visitedSet.contains(v)){\n\t\t\t\t\t\tactionQueue.add(v);\t\t//把这个新的值加入队列中，会从头开始检查\n\t\t\t\t\t\tvisitedSet.add(v);\t\t\t\t//标记为已经访问\n\t\t\t\t\t\tbacktraceMap.put(v, w);\t\t//把v指向w，BDC指向ADC指向ABC\n\t\t\t\t\t\t//System.out.println(v+\",\"+w);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t//return null;\n\t\treturn 0;\n    }\n\t\n\tSet<String> getOneEditWords(String word){\t//把s的所有字符换成其他的字符（只替换一个）的所有可能性，放到TreeSet中\n\t\tSet<String> words = new TreeSet<String>();\n\t\tfor(int i=0;i<word.length();i++){\n\t\t\tchar[] wordArray = word.toCharArray();\n\t\t\tfor(char c='A';c<='Z';c++){\t\t\t//把第i个字符替换成和原来不一样的字符，并放入TreeSet中\n\t\t\t\tif(c != word.charAt(i)){\n\t\t\t\t\twordArray[i] = c;\n\t\t\t\t\t//System.out.println(new String(wordArray));\n\t\t\t\t\twords.add(new String(wordArray));\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn words;\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["刷题"]},{"title":"面试题目——《华为机试》","url":"/面试题目——《华为机试》.html","content":"**1.[字符串最后一个单词的长度](http://www.nowcoder.com/practice/8c949ea5f36f422594b306a2300315da?tpId=37&amp;tqId=21224&amp;rp=&amp;ru=/ta/huawei&amp;qru=/ta/huawei/question-ranking)**\n\n```\npackage huawei;\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\n\npublic class TheLastWordLen {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tBufferedReader buf = null;              //声明BufferedReader对象\n        buf = new BufferedReader(new InputStreamReader(System.in)); //实例化BufferedReader对象\n        String str = null;                              //接收输入内容\n        System.out.print(\"请输入内容：\");\n        try{\n            str = buf.readLine();                       //读取输入内容\n        }catch(IOException e){                      //要进行异常处理\n            e.printStackTrace();\n        }\n        System.out.println(\"输入的内容：\"+str);\n        System.out.println(\"最后一个单词的长度是：\"+theLastLen(str)); \n\t}\n\t\n\tpublic static int theLastLen(String str){\n\t\tif(str.length() <= 0)\n\t\t\treturn 0;\n\t\tint len=str.length();\n\t\tint count=0;\n\t\tfor(int i=0;i<len;i++){\n\t\t\tif(str.charAt(i) != ' ')\n\t\t\t\tcount++;\n\t\t\tif(str.charAt(i) == ' ')\n\t\t\t\tcount=0;\n\t\t}\n\t\treturn count; \n\t}\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n\n**2.**[**计算字符个数**](http://www.nowcoder.com/practice/a35ce98431874e3a820dbe4b2d0508b1?tpId=37&amp;tqId=21225&amp;rp=&amp;ru=/ta/huawei&amp;qru=/ta/huawei/question-ranking)\n\n```\npackage huawei;\n\npublic class CountStrNum {\t// 计算字符个数 \n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tString str;\n        char c;\n        InputData input = new InputData();\n        c = input.getChar(\"请输入第一个字符：\");\n        System.out.println(c);\n        str = input.getString(\"请输入第一个字符串：\");\n        System.out.println(str);\n        System.out.println(CountNum(str,c));\n        \n\t}\n\n\tpublic static int CountNum(String str,char c){\n\t\tif(str.length() <= 0 || c == ' ')\n\t\t\t\treturn 0;\n\t\tint count=0;\n\t\tfor(int i=0;i<str.length();i++){\n\t\t\tif(str.charAt(i) == c)\n\t\t\t\tcount++;\n\t\t}\n\t\treturn count;\n\t}\n}\n\n```\n\n&nbsp;\n\n**3.[明明的随机数](http://www.nowcoder.com/practice/3245215fffb84b7b81285493eae92ff0?tpId=37&amp;tqId=21226&amp;rp=&amp;ru=/ta/huawei&amp;qru=/ta/huawei/question-ranking)**\n\n```\npackage huawei;\n\nimport java.util.Scanner;\n\npublic class MingMingRandom {\t//明明的随机数 \n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t//while(true){\n\t\tScanner sn = new Scanner(System.in);\n\t\t \n        while(sn.hasNext()) {\n            int n = sn.nextInt();\n            int[] a = new int[n];\n            for(int i = 0; i < n; i++) {\n                a[i] = sn.nextInt();\n            }\n            int[] c = qucong(a);\n \n            for(int i = 0; i < 1000; i++) {\n                if(c[i] == 1) {\n                    System.out.println(i);\n                }\n            }\n        }\n    }\n \n    public static int[] qucong(int[] a) {\n        int[] b = new int[1000];\n        for(int i = 0; i < 1000; i++) {\n            b[i] = 0;\n        }\n        for(int i = 0; i < a.length; i++) {\n            b[a[i]] = 1;\n        }\n        return b;\n    }\n\n}\n\n```\n\n&nbsp;\n\n**&nbsp;4.[字符串分隔](http://www.nowcoder.com/practice/d9162298cb5a437aad722fccccaae8a7?tpId=37&amp;tqId=21227&amp;rp=&amp;ru=/ta/huawei&amp;qru=/ta/huawei/question-ranking)**\n\n```\npackage huawei;\n\nimport java.util.ArrayList;\n\npublic class DivideStr {\n\n\tpublic static void main(String[] args) {\t//字符串分隔 \n\t\t// TODO 自动生成的方法存根\n\t\tString str1,str2;\n        InputData input = new InputData();\n        str1 = input.getString(\"请输入第一个字符串：\");\n        System.out.println(str1);\n        str2 = input.getString(\"请输入第二个字符串：\");\n        System.out.println(str2);\n        reStr(str1);\n        reStr(str2);\n        String[] str = new String[theList.size()];\n        for(int i=0;i<str.length;i++){  \n            str[i] = theList.get(i);\n            System.out.println(str[i]);\n        } \n        //System.out.println(theList.toArray());\n\t}\n\n\tpublic static ArrayList<String> theList = new ArrayList<String>();\n\t\n\tpublic static void reStr(String str){\n\t\tif(str.length() <= 0)\n\t\t\treturn;\n\t\t//ArrayList<String> theList = new ArrayList<String>();\n\t\tint len = str.length();\n\t\tfor(int i=0;i<len;i++){\n\t\t\tString temp = \"\";\n\t\t\tif(i%8 == 7 &amp;&amp; i != 0){\n\t\t\t\ttemp = str.substring(i-7,i+1);\n\t\t\t\ttheList.add(temp);\n\t\t\t}\n\t\t}\n\t\tif(len%8 != 0){\n\t\t\tString lastStr = str.substring(len-len%8,len);\n\t\t\twhile(lastStr.length()<8){\n\t\t\t\tlastStr += '0';\n\t\t\t}\n\t\t\ttheList.add(lastStr);\n\t\t}\t\n\t\t//return theList;\n\t}\n}\n\n```\n\n&nbsp;\n\n**6.[质数因子](http://www.nowcoder.com/practice/196534628ca6490ebce2e336b47b3607?tpId=37&amp;tqId=21229&amp;rp=&amp;ru=/ta/huawei&amp;qru=/ta/huawei/question-ranking)**\n\n```\npackage huawei;\n\nimport java.util.Scanner;\n\npublic class Factorization {\t//质数因子 \n\n\tpublic static void main(String[] args) {\t\t//正整数的因子分解，从小到大输出因子，最后一个因子最后也要有一个空格\n\t\t// TODO 自动生成的方法存根\n\t\tScanner in = new Scanner(System.in);\n        while(in.hasNext()){\n            long l = in.nextLong();\n            Factorization fact = new Factorization();\n            System.out.print(fact.getResult(l));\n        }\n\t}\n\t\n\tpublic String getResult(long ulDataInput){\n\t\tStringBuffer buf = new StringBuffer();\n\t\tint i = 2;\n\t\twhile(ulDataInput != 1){\n\t\t\twhile(ulDataInput % i == 0){\t\t//一个一个除以小于n的整数，直到n除以n本身等于1停止\n\t\t\t\tbuf.append(i);\n\t\t\t\tbuf.append(\" \");\n\t\t\t\tulDataInput = ulDataInput/i;\n\t\t\t}\n\t\t\ti++;\n\t\t}\n\t\treturn buf.toString().trim();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**&nbsp;7.[取近似值](http://www.nowcoder.com/practice/3ab09737afb645cc82c35d56a5ce802a?tpId=37&amp;tqId=21230&amp;rp=&amp;ru=/ta/huawei&amp;qru=/ta/huawei/question-ranking)**\n\n```\npackage huawei;\n\npublic class ApproximateVal {\t//取近似值 \n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tfloat i;\n        int j;\n        InputData input = new InputData();\n        i = input.getFloat(\"请输入第一个浮点数：\", \"输入的数据必须是数浮点数，请重新输入！\");\n        System.out.println(reInt(i));\n        \n\t}\n\t\n\tpublic static int reInt(float f){\n\t\tint temp=(int)f;\n\t\tint temp_10=(int)(f*10);\n\t\tif(temp_10%10 >=5)\n\t\t\treturn temp+1;\n\t\telse\n\t\t\treturn temp;\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["刷题"]},{"title":"Java排序算法——桶排序","url":"/Java排序算法——桶排序.html","content":"文字部分为转载：http://hxraid.iteye.com/blog/647759\n\n对N个关键字进行桶排序的时间复杂度分为两个部分：\n\n(1) 循环计算每个关键字的桶映射函数，这个时间复杂度是O(N)。\n\n(2) 利用先进的比较排序[算法](http://lib.csdn.net/base/31)对每个桶内的所有数据进行排序，其时间复杂度为<!--more-->\n&nbsp; &sum; O(Ni*logNi) 。其中Ni 为第i个桶的数据量。\n\n&nbsp;\n\n很显然，第(2)部分是桶排序性能好坏的决定因素。尽量减少桶内数据的数量是提高效率的唯一办法(因为基于比较排序的最好平均时间复杂度只能达到O(N*logN)了)。因此，我们需要尽量做到下面两点：\n\n(1) 映射函数f(k)能够将N个数据平均的分配到M个桶中，这样每个桶就有[N/M]个数据量。\n\n(2) 尽量的增大桶的数量。极限情况下每个桶只能得到一个数据，这样就完全避开了桶内数据的&ldquo;比较&rdquo;排序操作。当然，做到这一点很不容易，数据量巨大的情况下，f(k)函数会使得桶集合的数量巨大，空间浪费严重。这就是一个时间代价和空间代价的权衡问题了。\n\n&nbsp;\n\n对于N个待排数据，M个桶，平均每个桶[N/M]个数据的桶排序平均时间复杂度为：\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; O(N)+O(M*(N/M)*log(N/M))=O(N+N*(logN-logM))=O(N+N*logN-N*logM)\n\n当N=M时，即极限情况下每个桶只有一个数据时。桶排序的最好效率能够达到O(N)。\n\n&nbsp;\n\n**总结：**&nbsp;桶排序的平均时间复杂度为线性的O(N+C)，其中C=N*(logN-logM)。如果相对于同样的N，桶数量M越大，其效率越高，最好的时间复杂度达到O(N)。&nbsp;当然桶排序的空间复杂度&nbsp;为O(N+M)，如果输入数据非常庞大，而桶的数量也非常多，则空间代价无疑是昂贵的。此外，桶排序是稳定的。\n\n&nbsp;\n\n&nbsp;\n\n```\npackage sort;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\n\n//类名：Arrays_Bucket\n//属性：\n//方法：\nclass Arrays_Bucket{\n\tprivate int[] arrays;\n\tprivate int curNum;\n\n\tpublic Arrays_Bucket(int max) {\t\t\t//建立一个max长度的空数组\n\t\tsuper();\n\t\tarrays = new int[max];\n\t\tcurNum = 0;\n\t}\n\t\n\tpublic void insert(int value){\t\t\t\t\t//往空的数组里面增加元素\n\t\tarrays[curNum] = value;\n\t\tcurNum++;\n\t}\n\t\n\tpublic void display(){\t\t\t\t\t\t\t\t\t//显示数组\n\t\tSystem.out.println(Arrays.toString(arrays));\n\t}\n\t\n\tpublic static final int ARRAY_SIZE = 10000;  \t//数组中最大的数的值\n    public static final int BUCKET_SIZE=10; \t\t\t//桶的大小，桶的数量极限等于数组大小，就可以避免桶内的排序\n\tpublic void BucketSort(){\n\t\tArrayList<ArrayList<Integer>> bucket=new ArrayList<ArrayList<Integer>>();  \n        for(int i=0;i<BUCKET_SIZE;i++)  \t\t\t\t\t\t\t//建立10个桶\n        {  \n            bucket.add(new ArrayList<Integer>());  \n        }  \n         for(int i=0;i<arrays.length;i++)  \t\t\t\t\t\t\t\t//按大小把数组分配到不同的桶中\n         {  \n             int k=arrays[i]/(ARRAY_SIZE/BUCKET_SIZE);  \n            // System.out.println(\"k=\"+k);\n             bucket.get(k).add(arrays[i]);  \n         }  \n         for(ArrayList<Integer> list:bucket)  \t//对桶内的元素进行排序\n             Collections.sort(list);  \n         for(ArrayList<Integer> list:bucket)  \t//从小到大输出桶内的元素\n             System.out.println(list); \n\t}\n\t\n\t\n}\n\npublic class BucketSort {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tint maxSize = 5;\n\t\tArrays_Bucket arrays_demo = new Arrays_Bucket(maxSize);\n\t\tarrays_demo.insert(580);\n\t\tarrays_demo.insert(570);\n\t\tarrays_demo.insert(560);\n\t\tarrays_demo.insert(600);\n\t\tarrays_demo.insert(1590);\n\t\tarrays_demo.display();\n\t\tarrays_demo.BucketSort();\n\t\t//arrays_demo.display();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n","tags":["算法"]},{"title":"面试题目——《CC150》中等难题","url":"/面试题目——《CC150》中等难题.html","content":"**面试题17.1：编写一个函数，不用临时变量，直接交换两个数。**\n\n**　　思路：使用差值或者异或**\n\n```\npackage cc150.middle;\n\npublic class Exchange {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tint[] a= {1,2};\n\t\tExchange ec = new Exchange();\n\t\tint[] b= ec.exchangeAB(a);\n\t\tSystem.out.println(b[0]);\n\t\tSystem.out.println(b[1]);\n\t}\n\t\n\t public int[] exchangeAB(int[] AB) {\n\t        // write code here\n//\t\t \tAB[0] = AB[0]-AB[1];\n//\t\t \tAB[1] = AB[0]+AB[1]; \n//\t\t \tAB[0] = AB[1]-AB[0]; \n\t\t \n\t\t \tAB[0] = AB[0]^AB[1];\n\t\t \tAB[1] = AB[0]^AB[1]; \n\t\t \tAB[0] = AB[1]^AB[0]; \n\t\t \treturn AB;\n\t    }\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n\n**面试题17.2：设计一个算法，判断玩家是否赢了井字游戏。**\n\n```\npackage cc150.middle;\n\npublic class Board {\t//井字棋\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tint[][] a = {{1,0,1},{1,-1,-1},{1,-1,0}};\n\t\tBoard b = new Board();\n\t\tSystem.out.println(b.checkWon(a));\n\t}\n\t\n\tpublic boolean checkWon(int[][] board) {\n        // write code here\n\t\tint N = board.length;\n\t\tint row = 0;\n\t\tint col = 0;\n\t\t\n\t\t//检查行\n\t\tfor(row = 0;row<N;row++){\n\t\t\tfor(col = 1;col<N;col++){\n\t\t\t\tif(board[row][col] != board[row][col-1])\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif(col == N &amp;&amp; board[row][col-1] == 1)\n\t\t\t\treturn true;\n\t\t}\n\t\t\n\t\t//检查列\n\t\tfor(col = 0;col<N;col++){\n\t\t\tfor(row = 1;row<N;row++){\n\t\t\t\tif(board[row][col] != board[row-1][col])\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif(row == N &amp;&amp; board[row-1][col] == 1)\n\t\t\t\treturn true;\n\t\t}\n\t\t\n\t\t//检查对角线，从左上角开始\n\t\tif(board[0][0] != 0){\n\t\t\tfor(row = 1;row<N;row++){\n\t\t\t\tif(board[row][row] != board[row-1][row-1])\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif(row == N &amp;&amp; board[row-1][row-1] == 1)\n\t\t\t\treturn true;\n\t\t}\n\t\t\n\t\t//检查对角线，左下角开始\n\t\tif(board[N-1][0] != 0){\n\t\t\tfor(row = 1;row<N;row++){\n\t\t\t\tif(board[N-row-1][row] != board[N-row][row-1])\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif(row == N &amp;&amp; board[N-row][row-1] == 1)\n\t\t\t\treturn true;\n\t\t}\n\t\treturn false;\n    }\n\n}\n\n```\n\n&nbsp;\n\n**面试题17.3：设计一个算法，算出n阶乘有多少个尾随零。**\n\n```\npackage cc150.middle;\n\npublic class Factor {\n\n\tpublic static void main(String[] args) {\t\t//n阶乘有多少个尾随零\n\t\t// TODO 自动生成的方法存根\n\t\tFactor f = new Factor();\n\t\tSystem.out.println(f.getFactorSuffixZero(10));\n\t}\n\t\n\tpublic int getFactorSuffixZero(int n) {//统计所有相乘的数中分解出5的个数，2的个数必定大于5，只要统计5的个数即可\n        // write code here\n//\t\tint count = 0;\n//\t\tfor(int i=2;i<=n;i++)\n//\t\t\tcount += factorOf5(i);\n//\t\treturn count;\n\t\t\n\t\tint count = 0;\n\t\tfor(int i=5;n/i>0;i*=5)\t\t//如果n=10的话，有2个数在5和25之间\n\t\t\tcount += n/i;\n\t\treturn count;\n    }\n\t\n\tpublic int factorOf5(int n) {\n        // write code here\n\t\tint count = 0;\n\t\twhile(n%5 == 0){\t//求余数\n\t\t\tcount ++;\n\t\t\tn/=5;\n\t\t}\n\t\treturn count;\n    }\n\n}\n\n```\n\n&nbsp;\n\n**面试题17.4：编写一个方法，找出两个数字中最大的那一个。不得使用if-else或其他比较运算符。**\n\n```\npackage cc150.middle;\n\npublic class Max {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t}\n\t\n\tpublic int getMax(int a, int b) {\n        // write code here\n\t\tb = a-b;\t\t\t\t\t//此时b>>31为1则b小于0即a<b,若b>>31为0 则a>b\n\t    a -= b&amp;(b>>31); \t//若a<b a=a-(a-b),若a>b a=a-0 \n\t    return a;\n    }\n\n}\n\n```\n\n&nbsp;\n\n**面试题17.6：给定一个整数数组，编写一个函数，找出索引m和n，只要将m和n之间的元素排好序，整个数组就是有序的。注意：n-m越小越好，也就是说，找出符合条件的最短序列。**\n\n&nbsp;\n\n**面试题17.7：给定一个整数，打印该整数的英文描述（例如&ldquo;One Thousand,Two Hundred Thirty Four&rdquo;）。**\n\n```\npackage cc150.middle;\n\npublic class ToString {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tToString ts = new ToString();\n\t\tSystem.out.println(ts.toString(1000));\t\t//输入1234，输出\"One Thousand,Two Hundred Thirty Four\"\n\t}\n\t\n\tpublic static String[] digits = {\"One\",\"Two\",\"Three\",\"Four\",\"Five\",\"Six\",\"Seven\",\"Eight\",\"Nine\"};\n\tpublic static String[] teens = {\"Eleven\",\"Twelve\",\"Thirteen\",\"Fourteen\",\"Fifteen\",\"Sixteen\",\"Seventeen\",\"Eighteen\",\"Nineteen\"};\n\tpublic static String[] tens = {\"Ten\",\"Twenty\",\"Thirty\",\"Forty\",\"Fifty\",\"Sixty\",\"Seventy\",\"Eighty\",\"Ninety\"};\n\tpublic static String[] bigs = {\"\",\"Thousand\",\"Million\"};\n\t\n\tpublic String toString(int x) {\n        // write code here\n\t\tif(x == 0)\n\t\t\treturn \"Zero\";\n\t\telse if(x < 0)\n\t\t\treturn \"Negative \" + toString(-1*x);\n\t\tint count = 0;\n\t\tString str = \"\";\n\t\twhile(x > 0){\n\t\t\tif(x % 1000 != 0){\t\t\n\t\t\t\tif(count == 0 || str == \"\")  //One Million后面不能有逗号\n\t\t\t\t\tstr = toString100(x % 1000)+bigs[count]+str; //把count大的million放在前面，原来的str放在后面\n\t\t\t\telse\n\t\t\t\t\tstr = toString100(x % 1000)+bigs[count]+\",\"+str; //把count大的million放在前面，原来的str放在后面\n\t\t\t}\n\t\t\tx /= 1000;\n\t\t\tcount ++;\n\t\t}\n\t\treturn str.trim();\n    }\n\t\n\tpublic String toString100(int x){\n\t\tString str = \"\";\n\t\t//转换百位数的地方\n\t\tif(x >= 100){\n\t\t\tstr+=digits[x/100-1]+\" Hundred \";\n\t\t\tx%=100;\n\t\t}\n\t\t//转换十位数的地方\n\t\tif(x >= 11 &amp;&amp; x <= 19){\n\t\t\treturn str+teens[x-11]+\" \";\n\t\t}else if(x == 10 || x >= 20){\n\t\t\tstr+=tens[x/10-1]+\" \";\n\t\t\tx%=10;\n\t\t}\n\t\t//转换个位数的地方\n\t\tif(x >= 1 &amp;&amp; x<=9){\n\t\t\tstr+=digits[x-1]+\" \";\t\t\n\t\t}\n\t\treturn str;\n\t}\n\t\n\t\n\n}\n\n```\n\n&nbsp;\n\n**面试题17.8：给定一个整数数组（有正数有负数），找出总和最大的连续数列，并返回总和。（剑指Offer原题）**\n\n```\npackage jianzhiOffer;\n\npublic class FindGreatestSumOfSubArray {\t\t//连续子数组的最大和\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n//\t\tint[] Arr = {-1,-2,-3,-10,-4,-7,-2,-5};\n\t\tint[] Arr = {6,-3,-2,7,-15,1,2,2};\n\t\tSystem.out.println(findGreatestSumOfSubArray(Arr));\n\t}\n\t\n\tpublic static boolean InvalidInput = false;\n\t\n\tpublic static int findGreatestSumOfSubArray(int[] nums){\n\t\tif(nums == null || nums.length<=0){\n\t\t\tInvalidInput = true;\n\t\t\treturn 0;\n\t\t}\n\t\tInvalidInput = true;\n\t\tint curSum = 0;\n\t\tint curGreatestSum = Integer.MIN_VALUE;//数组内可能全部都是负数\n\t\tfor(int i=0;i<nums.length;i++){\n\t\t\tif(curSum < 0)//如果前n个相加小于0，那么最大的和只能从n+1个开始算\n\t\t\t\tcurSum = nums[i];\n\t\t\telse\n\t\t\t\tcurSum += nums[i];\n\t\t\tif(curSum > curGreatestSum)//数组内可能全部都是负数\n\t\t\t\tcurGreatestSum = curSum;\n\t\t}\n\t\treturn curGreatestSum;\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题17.9：设计一个方法，找出任意指定单词在一本书中的出现频率。**\n\n<img src=\"/images/517519-20161016142639030-276549718.png\" alt=\"\" />\n\n```\npackage cc150.middle;\n\nimport java.util.Hashtable;\n\npublic class Frequency {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t}\n\t\n\tpublic int getFrequency(String[] article, int n, String word) {\n        // write code here\n\t\treturn wordFrequency(setupDictionary(article),word);\n    }\n\t\n\tpublic Hashtable<String,Integer> setupDictionary(String[] book){\n\t\tHashtable<String,Integer> table = new Hashtable<String,Integer>();\n\t\tfor(String word:book){\n\t\t\tword = word.toLowerCase();\n\t\t\tif(word.trim() != \"\"){\t\t//去掉空格之后，不等于空\n\t\t\t\tif(!table.containsKey(word))\n\t\t\t\t\ttable.put(word, 0);\n\t\t\t\ttable.put(word, table.get(word)+1);\n\t\t\t}\n\t\t}\n\t\treturn table;\n\t}\n\t\n\tpublic int wordFrequency(Hashtable<String,Integer> table, String word){\n\t\tif(table == null || word == null)\n\t\t\treturn -1;\n\t\tword = word.toLowerCase();\n\t\tif(table.containsKey(word))\n\t\t\treturn table.get(word);\n\t\treturn 0;\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题17.12：设计一个算法，找出数组中两数之和为指定值的所有整数对。**\n\n```\npackage cc150.middle;\n\nimport java.util.Arrays;\n\npublic class FindPair {\n\n\tpublic static void main(String[] args) {\t\t//有重复的情况\n\t\t// TODO 自动生成的方法存根\n\t\tint[] a = {11,7,7,6,14,2,14,15,2,1,2,12,13,9,8,15,13,8,10,11,14,10,2,9,4,9,3,7,6,10,15,4,7,6,15,3,9,13,5,2,6,10,10,1,12,4,3,3,8,8,1,4,7,11,13,5,13,15,4,3,1,11,6,11,9,9,11,15,12,10,13,3,11,4,8,9,7,3,13,9,11,3,2,11,10,1,4,2,3,3,14,11,5,10,1,14,8,1,11,3,1,9,14,6,1,7,15,10,14,6,4,12,11};\n\t\tFindPair fp = new FindPair();\n\t\tSystem.out.println(fp.countPairs(a,113,16));\n\t}\n\t\n\tpublic int countPairs(int[] A, int n, int sum) {\n        // write code here\n\t\tArrays.sort(A);\n\t\tfor(int i=0;i<A.length;i++)\n\t\t\tSystem.out.println(A[i]);\n\t\tint first = 0;\n\t\tint end = n-1;\n\t\tint count=0;\n\t\tint countLeft = 1;\n\t\tint countRight = 1;\n\t\twhile(first<end){\n\t\t\tint s = A[first]+A[end];\n\t\t\tif(s == sum){\t\t\t\t\t\t\t//如果相等\n\t\t\t\tif(A[first] == A[end]){\t\t//情况1，一串连续的，且这些相加等于sum\n\t\t\t\t\tcount += ((end-first)*(end-first+1)/2);\n\t\t\t\t\treturn count;\t\t//注意要在这里结束，否则会继续计数\n\t\t\t\t}else{\t\t\t\t\t\t\t\t\t//情况2，两串连续的，两个串中各取一个相加等于sum\n\t\t\t\t\twhile(A[first] == A[first+1]){\n\t\t\t\t\t\tcountLeft++;\n\t\t\t\t\t\tfirst++;\n\t\t\t\t\t}\n\t\t\t\t\twhile(A[end] == A[end-1]){\n\t\t\t\t\t\tcountRight++;\n\t\t\t\t\t\tend--;\n\t\t\t\t\t}\n\t\t\t\t\tcount += (countLeft*countRight);\n\t\t\t\t}\n\t\t\t\tfirst++;\n\t\t\t\tend--;\n\t\t\t\tcountLeft=1;\t\t//注意开始计数是1\n\t\t\t\tcountRight=1;\n\t\t\t}else{\n\t\t\t\tif(s>sum)\n\t\t\t\t\tend--;\n\t\t\t\telse\n\t\t\t\t\tfirst++;\n\t\t\t}\n\t\t}\n\t\treturn count;\n    }\n\n}\n\n```\n\n&nbsp;\n\n**面试题17.13：有个简单的类似结点的数据结构BiNode，包含两个指向其他结点的指针。数据结构BiNode可用来表示二叉树（其中node1为左子节点，node2为右子节点）或双向链表（其中node1为前趋结点，node2为后继结点）。编写一个方法，将二叉查找树（用BiNode实现）转换为双向链表。要求所有数值的排序不变，转换操作不得引入其他数据结构（即直接操作原先的数据结构）。**\n\n```\npackage cc150.middle;\n\npublic class Converter {\n\n\tpublic static void main(String[] args) {\t\t//二叉查找树转换成双向链表\n\t\t// TODO 自动生成的方法存根\n\t\tConverter cv = new Converter();\n\t\tTreeNode t4 = cv.new TreeNode(4);\n\t\tTreeNode t2 = cv.new TreeNode(2);\n\t\tTreeNode t5 = cv.new TreeNode(5);\n\t\tTreeNode t1 = cv.new TreeNode(1);\n\t\tTreeNode t3 = cv.new TreeNode(3);\n\t\tTreeNode t6 = cv.new TreeNode(6);\n\t\tTreeNode t0 = cv.new TreeNode(0);\n\t\tt4.left = t2;\n\t\tt4.right = t5;\n\t\tt2.left = t1;\n\t\tt2.right = t3;\n\t\tt5.right = t6;\n\t\tt1.left = t0;\n\t\tcv.treeToList(t4);\n\t\twhile(cv.head.next != null){\n\t\t\tSystem.out.println(cv.head.val);\n\t\t\tcv.head = cv.head.next;\n\t\t}\n\t\t\t\n\t}\n\t\n\tprivate ListNode head  = new ListNode(-1);\n\tprivate ListNode q  = head;\n\t\n\tpublic ListNode treeToList(TreeNode root) {\t\t//使用中序遍历\n        // write code here\n\t\tif(root != null){\n\t\t\ttreeToList(root.left);\n\t\t\tq.next = new ListNode(root.val);\n\t\t\tq = q.next;\n\t\t\ttreeToList(root.right);\n\t\t}\n\t\treturn head.next;\n\t\t\n    }\n\t\n\tpublic class ListNode {\n\t    int val;\n\t    ListNode next = null;\n\n\t    ListNode(int val) {\n\t        this.val = val;\n\t    }\n\t}\n\t\n\tpublic class TreeNode {\n\t    int val = 0;\n\t    TreeNode left = null;\n\t    TreeNode right = null;\n\t    public TreeNode(int val) {\n\t        this.val = val;\n\t    }\n\t}\n\t\n\t\n}\n\n```\n\n&nbsp;\n","tags":["刷题"]},{"title":"面试题目——《CC150》线程与锁","url":"/面试题目——《CC150》线程与锁.html","content":"<img src=\"/images/517519-20161014110333984-2132382338.png\" alt=\"\" />\n\n<img src=\"/images/517519-20161014142100468-319038504.png\" alt=\"\" />\n\n```\npackage cc150.thread_lock;\n\npublic class RunnableThreadExample implements Runnable{\n\n\tpublic int count = 0;\n\t\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tRunnableThreadExample instance = new RunnableThreadExample();\n\t\tThread thread = new Thread(instance);\n\t\tthread.start();\n\t\t//等到上面的线程数到5\n\t\twhile(instance.count != 5){\n\t\t\ttry{\n\t\t\t\tThread.sleep(250);\n\t\t\t\tSystem.out.println(\"等待\");\n\t\t\t}catch(InterruptedException exc){\n\t\t\t\texc.printStackTrace();\n\t\t\t}\n\t\t}\n\t}\n\n\t@Override\n\tpublic void run() {\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"RunnableThread开始\");\n\t\ttry{\n\t\t\twhile(count < 5){\n\t\t\t\tThread.sleep(500);\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}catch(InterruptedException exc){\n\t\t\tSystem.out.println(\"RunnableThread中断\");\n\t\t}\n\t\tSystem.out.println(\"RunnableThread终止\");\n\t}\n\n}\n\n```\n\n<img src=\"/images/517519-20161014142131312-655039173.png\" alt=\"\" />\n\n```\npackage cc150.thread_lock;\n\npublic class ThreadExample{\n\n\tpublic static void main(String[] args){\n\t\t// TODO 自动生成的方法存根\n\t\tThreadExample th = new ThreadExample();\n\t\tthreadExample instance = th.new threadExample();\n\t\tinstance.start();\n\t\t//等到上面的线程数到5\n\t\twhile(instance.count != 5){\n\t\t\ttry{\n\t\t\t\tThread.sleep(250);\n\t\t\t\tSystem.out.println(\"等待\");\n\t\t\t}catch(InterruptedException exc){\n\t\t\t\texc.printStackTrace();\n\t\t\t}\n\t\t}\n\t}\n\n\tpublic class threadExample extends Thread{\n\t\tint count = 0;\n\t\tpublic void run() {\n\t\t\t// TODO 自动生成的方法存根\n\t\t\tSystem.out.println(\"Thread开始\");\n\t\t\ttry{\n\t\t\t\twhile(count < 5){\n\t\t\t\t\tThread.sleep(500);\n\t\t\t\t\tSystem.out.println(\"在线程中，count是\"+count);\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t}catch(InterruptedException exc){\n\t\t\t\tSystem.out.println(\"RunnableThread中断\");\n\t\t\t}\n\t\t\tSystem.out.println(\"RunnableThread终止\");\n\t\t}\n\t}\n}\n\n```\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20161014144903281-1354088842.png\" alt=\"\" />\n\n&nbsp;\n\n&nbsp;\n\n<img src=\"/images/517519-20161014145001734-686783629.png\" alt=\"\" />\n\n```\npackage cc150.thread_lock;\n\nclass MyObject{\n\tpublic synchronized void foo(String name){\t\t//加上synchronized关键字，给foo提供同步\n\t\ttry{\n\t\t\tSystem.out.println(\"线程\"+name+\".foo()开始\");\n\t\t\tThread.sleep(3000);\n\t\t\tSystem.out.println(\"线程\"+name+\".foo()结束\");\n\t\t}catch(InterruptedException exc){\n\t\t\tSystem.out.println(\"线程\"+name+\"中断\");\n\t\t}\n\t}\n}\n\npublic class MyClass extends Thread{\t\t//不同的线程，来调用上面的foo\n\n\tprivate String name;\n\tprivate MyObject myObj;\n\t\n\tpublic MyClass(MyObject obj,String n){\n\t\tname = n;\n\t\tmyObj = obj;\n\t}\n\t\n\tpublic void run(){\n\t\tmyObj.foo(name);\n\t}\n\t\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n//\t\tMyObject obj1 = new MyObject();\n//\t\tMyObject obj2 = new MyObject();\n//\t\tMyClass thread1 = new MyClass(obj1,\"1\");\n//\t\tMyClass thread2 = new MyClass(obj2,\"2\");\n//\t\tthread1.start();\n//\t\tthread2.start();\n\t\t\n\t\t//相同的obj引用，只能一个线程可以调用foo，另一个线程必须等待\n\t\tMyObject obj = new MyObject();\n\t\tMyClass thread11 = new MyClass(obj,\"1\");\n\t\tMyClass thread22 = new MyClass(obj,\"2\");\n\t\tthread11.start();\n\t\tthread22.start();\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20161014154014578-893921761.png\" alt=\"\" />\n\n```\npackage cc150.thread_lock;\n\nclass MyObject{\n\tpublic static synchronized void foo(String name){\t\t//加上synchronized关键字，给foo提供同步\n\t\ttry{\n\t\t\tSystem.out.println(\"线程\"+name+\".foo()开始\");\n\t\t\tThread.sleep(3000);\n\t\t\tSystem.out.println(\"线程\"+name+\".foo()结束\");\n\t\t}catch(InterruptedException exc){\n\t\t\tSystem.out.println(\"线程\"+name+\"中断\");\n\t\t}\n\t}\n\t\n\tpublic static synchronized void bar(String name){\t\t//加上synchronized关键字，给bar提供同步\n\t\ttry{\n\t\t\tSystem.out.println(\"线程\"+name+\".bar()开始\");\n\t\t\tThread.sleep(3000);\n\t\t\tSystem.out.println(\"线程\"+name+\".bar()结束\");\n\t\t}catch(InterruptedException exc){\n\t\t\tSystem.out.println(\"线程\"+name+\"中断\");\n\t\t}\n\t}\n}\n\npublic class MyClass extends Thread{\t\t//不同的线程，来调用上面的foo\n\n\tprivate String name;\n\tprivate MyObject myObj;\n\t\n\tpublic MyClass(MyObject obj,String n){\n\t\tname = n;\n\t\tmyObj = obj;\n\t}\n\t\n\tpublic void run(){\n\t\t//myObj.foo(name);\n\t\tif(name.equals(\"1\"))\n\t\t\tMyObject.foo(name);\n\t\telse if(name.equals(\"2\"))\n\t\t\tMyObject.bar(name);\n\t}\n\t\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n//\t\tMyObject obj1 = new MyObject();\n//\t\tMyObject obj2 = new MyObject();\n//\t\tMyClass thread1 = new MyClass(obj1,\"1\");\n//\t\tMyClass thread2 = new MyClass(obj2,\"2\");\n//\t\tthread1.start();\n//\t\tthread2.start();\n\t\t\n\t\t//相同的obj引用，只能一个线程可以调用foo，另一个线程必须等待\n\t\tMyObject obj = new MyObject();\n\t\tMyClass thread11 = new MyClass(obj,\"1\");\n\t\tMyClass thread22 = new MyClass(obj,\"2\");\n\t\tthread11.start();\n\t\tthread22.start();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n&nbsp;\n\n<img src=\"/images/517519-20161014154627125-1416729200.png\" alt=\"\" />\n\n```\npackage cc150.thread_lock;\n\nclass MyObject2{\n\tpublic synchronized void foo(String name){\t\t//加上synchronized块，给foo提供同步\n\t\tsynchronized(this){\t\n\t\t\ttry{\n\t\t\t\tSystem.out.println(\"线程\"+name+\".foo()开始\");\n\t\t\t\tThread.sleep(3000);\n\t\t\t\tSystem.out.println(\"线程\"+name+\".foo()结束\");\n\t\t\t}catch(InterruptedException exc){\n\t\t\t\tSystem.out.println(\"线程\"+name+\"中断\");\n\t\t\t}\n\t\t}\n\t}\n\t\n}\n\npublic class MyClass2 extends Thread{\t\t//不同的线程，来调用上面的foo\n\n\tprivate String name;\n\tprivate MyObject2 myObj;\n\t\n\tpublic MyClass2(MyObject2 obj,String n){\n\t\tname = n;\n\t\tmyObj = obj;\n\t}\n\t\n\tpublic void run(){\n\t\tmyObj.foo(name);\n\t}\n\t\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n//\t\tMyObject obj1 = new MyObject();\n//\t\tMyObject obj2 = new MyObject();\n//\t\tMyClass thread1 = new MyClass(obj1,\"1\");\n//\t\tMyClass thread2 = new MyClass(obj2,\"2\");\n//\t\tthread1.start();\n//\t\tthread2.start();\n\t\t\n\t\t//相同的obj引用，只能一个线程可以调用foo，另一个线程必须等待\n\t\tMyObject2 obj = new MyObject2();\n\t\tMyClass2 thread11 = new MyClass2(obj,\"1\");\n\t\tMyClass2 thread22 = new MyClass2(obj,\"2\");\n\t\tthread11.start();\n\t\tthread22.start();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n&nbsp;<img src=\"/images/517519-20161014155505984-1122736709.png\" alt=\"\" />\n\n```\npackage cc150.thread_lock;\n\nimport java.util.concurrent.locks.Lock;\nimport java.util.concurrent.locks.ReentrantLock;\n\nclass MyObj {\n\n\tprivate Lock lock;\n\tprivate int balance = 100;\t//剩余\n\t\n\tpublic MyObj(){\t\t\t\t\t\t//构造函数\n\t\tlock = new ReentrantLock();\t\t//可重入锁\n\t}\n\t\n\tpublic int withdraw(int value){\t//取款\n\t\tlock.lock();\n\t\tint temp = balance;\n\t\ttry{\n\t\t\tThread.sleep(100);\n\t\t\ttemp = temp-value;\n\t\t\tThread.sleep(100);\n\t\t\tbalance = temp;\n\t\t}catch(InterruptedException e){\n\t\t\t\n\t\t}\n\t\tlock.unlock();\n\t\treturn temp;\n\t}\n\t\n\tpublic int deposit(int value){\t//存款\n\t\tlock.lock();\n\t\tint temp = balance;\n\t\ttry{\n\t\t\tThread.sleep(100);\n\t\t\ttemp = temp+value;\n\t\t\tThread.sleep(100);\n\t\t\tbalance = temp;\n\t\t}catch(InterruptedException e){\n\t\t\t\n\t\t}\n\t\tlock.unlock();\n\t\treturn temp;\n\t}\n}\n\t\npublic class LockedATM extends Thread{\n\t\n\tprivate int value;\n\tprivate MyObj myObj;\n\tprivate String str;\n\t\n\tpublic LockedATM(MyObj obj,String  s,int v){\n\t\tvalue = v;\n\t\tmyObj = obj;\n\t\tstr = s;\n\t}\n\t\n\tpublic void run(){\n\t\tif(str.equals(\"withdraw\"))\n\t\t\tSystem.out.println(myObj.withdraw(value));\n\t\telse if(str.equals(\"deposit\"))\n\t\t\tSystem.out.println(myObj.deposit(value));\n\t}\n\t\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tMyObj obj1 = new MyObj();\n\t\tMyObj obj2 = new MyObj();\n\t\tLockedATM thread1 = new LockedATM(obj1,\"withdraw\",50); \t//两个线程执行的顺序不一定\n\t\tLockedATM thread2 = new LockedATM(obj2,\"deposit\",50);\t\t//可能先存款，也可能先取款\n\t\tthread1.start();\n\t\tthread2.start();\n\t}\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20161014170111015-1112999408.png\" alt=\"\" />\n\n&nbsp;\n\n<img src=\"/images/517519-20161014170547781-657002329.png\" alt=\"\" />\n\n<img src=\"/images/517519-20161014170614609-326924667.png\" alt=\"\" width=\"652\" height=\"439\" />\n\n<img src=\"/images/517519-20161014170755968-830732139.png\" alt=\"\" />\n\n&nbsp;\n","tags":["刷题"]},{"title":"Java数据结构——图","url":"/Java数据结构——图.html","content":"**点<br />**\n\n```\n//类名：Vertex\n//属性：\n//方法：\nclass Vertex{\n\tpublic char label;\t//点的名称，如A\n\tpublic boolean wasVisited;\n\t\n\tpublic Vertex(char lab){\t//构造函数\n\t\tlabel = lab;\n\t\twasVisited = false;\n\t}\n}\n\n```\n\n**建立无权图，添加新的顶点，添加边，显示顶点，返回一个和v邻接的未访问顶点，无权图的深度搜索，广度搜索，基于深度搜索的最小生成树，删除顶点，有向图的拓扑排序**\n\n```\n//类名：Graph\n//属性：\n//方法：\nclass Graph{\n\tprivate final int MAX_VERTS = 20;\n\tprivate Vertex vertexList[];\t//顶点列表数组\n\tprivate int adjMat[][];\t\t\t//邻接矩阵\n\tprivate int nVerts;\t\t\t\t\t//当前的顶点\n\tprivate char sortedArray[];\n\t\n\tpublic Graph(){\t//构造函数\n\t\tvertexList = new Vertex[MAX_VERTS];\n\t\tadjMat = new int[MAX_VERTS][MAX_VERTS];\n\t\tnVerts = 0;\n\t\tfor(int j=0;j<MAX_VERTS;j++){\n\t\t\tfor(int k=0;k<MAX_VERTS;k++)\n\t\t\t\tadjMat[j][k] = 0;\n\t\t}\n\t\tsortedArray = new char[MAX_VERTS];\n\t}\n\t\n\tpublic void addVertex(char lab){\t//添加新的顶点，传入顶点的lab，并修改nVerts\n\t\tvertexList[nVerts++] = new Vertex(lab);\n\t}\n\t\n\tpublic void addEdge(int start,int end){\t//添加边，这里是无向图\n\t\tadjMat[start][end] = 1;\n\t\t//adjMat[end][start] = 1;\n\t}\n\t\n\tpublic void displayVertex(int v){\t//显示顶点\n\t\tSystem.out.print(vertexList[v].label);\n\t}\n\t\n\tpublic int getAdjUnvisitedVertex(int v){\t//返回一个和v邻接的未访问顶点\n\t\tfor(int j=0;j<nVerts;j++)\n\t\t\tif(adjMat[v][j] == 1 &amp;&amp; vertexList[j].wasVisited == false){\n\t\t\t\treturn j;\n\t\t\t}\n\t\t\treturn -1;\t//如果没有，返回-1\n\t}\n\t\n\tpublic void dfs(){\t//深度搜索\n\t\tStack<Integer> theStack = new Stack<Integer>();\n\t\tvertexList[0].wasVisited = true;\n\t\tdisplayVertex(0);\n\t\ttheStack.push(0);\t//把根入栈\n\t\t\n\t\twhile(!theStack.empty()){\n\t\t\tint v = getAdjUnvisitedVertex(theStack.peek());//取得一个和栈顶元素邻接的未访问元素\n\t\t\tif(v == -1)\t\t//如果没有和栈顶元素邻接的元素，就弹出这个栈顶\n\t\t\t\ttheStack.pop();\n\t\t\telse{\t\t\t\t//如果有这个元素，则输出这个元素，标记为已访问，并入栈\n\t\t\t\tvertexList[v].wasVisited = true;\n\t\t\t\tdisplayVertex(v);\n\t\t\t\ttheStack.push(v);\n\t\t\t}\n\t\t}\n\t\tfor(int j=0;j<nVerts;j++)\t\t//全部置为未访问\n\t\t\tvertexList[j].wasVisited = false;\n\t}\n\t\n\tpublic void bfs(){\t//广度搜索\n\t\tQueue<Integer> theQueue = new LinkedList<Integer>();\n\t\tvertexList[0].wasVisited = true;\n\t\tdisplayVertex(0);\n\t\ttheQueue.offer(0);\t//把根入队列\n\t\tint v2;\n\t\t\n\t\twhile(!theQueue.isEmpty()){\n\t\t\tint v1 = theQueue.remove();//v1记录第1层的元素，然后记录第2层第1个元素...\n\t\t\t\n\t\t\twhile((v2=getAdjUnvisitedVertex(v1)) != -1){//输出所有和第1层邻接的元素，输出和第2层第1个元素邻接的元素...\n\t\t\t\tvertexList[v2].wasVisited = true;\n\t\t\t\tdisplayVertex(v2);\n\t\t\t\ttheQueue.offer(v2);\n\t\t\t}\n\t\t}\n\t\t\n\t\tfor(int j=0;j<nVerts;j++)\t\t//全部置为未访问\n\t\t\tvertexList[j].wasVisited = false;\n\t}\n\t\n\tpublic void mst(){\t//基于深度搜索的最小生成树\n\t\tStack<Integer> theStack = new Stack<Integer>();\n\t\tvertexList[0].wasVisited = true;\n\t\ttheStack.push(0);\t//把根入栈\n\t\t\n\t\twhile(!theStack.empty()){\n\t\t\tint currentVertex = theStack.peek();\t//记录栈顶元素，当有为邻接元素的时候，才会输出\n\t\t\tint v = getAdjUnvisitedVertex(theStack.peek());//取得一个和栈顶元素邻接的未访问元素\n\t\t\tif(v == -1)\t\t//如果没有和栈顶元素邻接的元素，就弹出这个栈顶\n\t\t\t\ttheStack.pop();\n\t\t\telse{\t\t\t\t//如果有这个元素，则输出这个元素，标记为已访问，并入栈\n\t\t\t\tvertexList[v].wasVisited = true;\n\t\t\t\ttheStack.push(v);\n\t\t\t\t\n\t\t\t\tdisplayVertex(currentVertex);\n\t\t\t\tdisplayVertex(v);\n\t\t\t\tSystem.out.println();\n\t\t\t}\n\t\t}\n\t\tfor(int j=0;j<nVerts;j++)\t\t//全部置为未访问\n\t\t\tvertexList[j].wasVisited = false;\n\t}\n\t\n\tpublic int noSuccessors(){\t//使用邻接矩阵找到没有后继的顶点，有后继顶点返回行数，没有返回-1\n\t\tboolean isEdge;\n\t\t\n\t\tfor(int row=0;row<nVerts;row++){//从第1行开始\n\t\t\tisEdge = false;\n\t\t\tfor(int col=0;col<nVerts;col++){//如果某一行某一列为1，返回这个行的行数\n\t\t\t\tif(adjMat[row][col] > 0){\n\t\t\t\t\tisEdge = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif(!isEdge)\n\t\t\t\treturn row;\n\t\t}\n\t\treturn -1;\n\t}\n\t\n\tpublic void moveRowUp(int row,int length){\n\t\tfor(int col=0;col<length;col++)\n\t\t\tadjMat[row][col] = adjMat[row+1][col];\n\t}\n\t\n\tpublic void moveColLeft(int col,int length){\n\t\tfor(int row=0;row<length;row++)\n\t\t\tadjMat[row][col] = adjMat[row][col+1];\n\t}\n\t\n\tpublic void deleteVertex(int delVert){\n\t\tif(delVert != nVerts-1){\n\t\t\tfor(int j=delVert;j<nVerts-1;j++)//在数组中去掉这个顶点\n\t\t\t\tvertexList[j] = vertexList[j+1];\n\t\t\tfor(int row=delVert;row<nVerts-1;row++)//在邻接矩阵中把删除的这一行下的所有行上移\n\t\t\t\tmoveRowUp(row,nVerts);\n\t\t\tfor(int col=delVert;col<nVerts-1;col++)//在邻接矩阵中把删除的这一列下的所有列左移\n\t\t\t\tmoveColLeft(col,nVerts-1);\n\t\t}\n\t\tnVerts--;\n\t}\n\t\n\tpublic void topo(){\t//拓扑排序，必须在无环的有向图中进行，必须在有向图中\n\t\tint orig_nVerts = nVerts;\t//记录有多少个顶点\n\t\t\n\t\twhile(nVerts > 0){\n\t\t\tint currentVertex = noSuccessors();\n\t\t\tif(currentVertex == -1){\n\t\t\t\tSystem.out.println(\"错误：图含有环！\");\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tsortedArray[nVerts-1] = vertexList[currentVertex].label;\n\t\t\tdeleteVertex(currentVertex);\n\t\t}\n\t\tSystem.out.println(\"拓扑排序结果：\");\n\t\tfor(int j=0;j<orig_nVerts;j++)\n\t\t\tSystem.out.println(sortedArray[j]);\n\t\t\n\t}\n\t\n}\n\n```\n\n**<!--more-->\n&nbsp;有向图的连通性，Warshall算法**\n\n&nbsp;\n\n**主函数**\n\n```\npublic class graph_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tGraph theGraph = new Graph();\n\t\ttheGraph.addVertex('A');\t//数组元素0\n\t\ttheGraph.addVertex('B');\t//数组元素1\n\t\ttheGraph.addVertex('C');\t//数组元素2\n\t\ttheGraph.addVertex('D');\t//数组元素3\n\t\ttheGraph.addVertex('E');\t//数组元素4\n\t\t\n//\t\ttheGraph.addEdge(0, 1);\t//AB\n//\t\ttheGraph.addEdge(1, 2);\t//BC\n//\t\ttheGraph.addEdge(0, 3);\t//AD\n//\t\ttheGraph.addEdge(3, 4);\t//DE\n\t\t\n//\t\tSystem.out.println(\"dfs访问的顺序：\");\n//\t\ttheGraph.dfs();\n//\t\tSystem.out.println();\n//\t\t\n//\t\tSystem.out.println(\"bfs访问的顺序：\");\n//\t\ttheGraph.bfs();\n\t\t\n\t\t\n\t\t\n//\t\ttheGraph.addEdge(0, 1);\t//AB\n//\t\ttheGraph.addEdge(0, 2);\t//AC\n//\t\ttheGraph.addEdge(0, 3);\t//AD\n//\t\ttheGraph.addEdge(0, 4);\t//AE\n//\t\ttheGraph.addEdge(1, 2);\t//BC\n//\t\ttheGraph.addEdge(1, 3);\t//BD\n//\t\ttheGraph.addEdge(1, 4);\t//BE\n//\t\t//theGraph.addEdge(2, 3);\t//CD\n//\t\t//theGraph.addEdge(2, 4);\t//CE\n//\t\ttheGraph.addEdge(3, 4);\t//DE\n\t\t\n//\t\tSystem.out.println(\"最小生成树：\");\n//\t\ttheGraph.mst();\n\t\t\n\t\t\n\t\ttheGraph.addVertex('F');\t//数组元素5\n\t\ttheGraph.addVertex('G');\t//数组元素6\n\t\ttheGraph.addVertex('H');\t//数组元素6\n\t\t\n\t\ttheGraph.addEdge(0, 3);\t//AD\n\t\ttheGraph.addEdge(0, 4);\t//AE\n\t\ttheGraph.addEdge(1, 4);\t//BE\n\t\ttheGraph.addEdge(2, 5);\t//CF\n\t\ttheGraph.addEdge(3, 6);\t//DG\n\t\ttheGraph.addEdge(4, 6);\t//EG\n\t\ttheGraph.addEdge(5, 7);\t//FH\n\t\ttheGraph.addEdge(6, 7);\t//GH\n\t\t\n\t\ttheGraph.topo();\n\t}\n\t\n\t\n\n}\n\n```\n\n&nbsp;\n","tags":["数据结构"]},{"title":"Java数据结构——带权图","url":"/Java数据结构——带权图.html","content":"**<strong>带权图的最小生成树&mdash;&mdash;Prim算法和Kruskal算法**</strong>\n\n<!--more-->\n&nbsp;\n\n**带权图的最短路径算法&mdash;&mdash;Dijkstra算法**\n\n```\npackage graph;\n// path.java\n// demonstrates shortest path with weighted, directed graphs 带权图的最短路径算法\n// to run this program: C>java PathApp\n////////////////////////////////////////////////////////////////\nclass DistPar             // distance and parent\n   {                      // items stored in sPath array\n   public int distance;   // distance from start to this vertex\n   public int parentVert; // current parent of this vertex\n// -------------------------------------------------------------\n   public DistPar(int pv, int d)  // constructor\n      {\n      distance = d;\n      parentVert = pv;\n      }\n// -------------------------------------------------------------\n   }  // end class DistPar\n///////////////////////////////////////////////////////////////\nclass Vertex\n   {\n   public char label;        // label (e.g. 'A')\n   public boolean isInTree;\n// -------------------------------------------------------------\n   public Vertex(char lab)   // constructor\n      {\n      label = lab;\n      isInTree = false;\n      }\n// -------------------------------------------------------------\n   }  // end class Vertex\n////////////////////////////////////////////////////////////////\nclass Graph\n   {\n   private final int MAX_VERTS = 20;\n   private final int INFINITY = 1000000;\n   private Vertex vertexList[]; // list of vertices\n   private int adjMat[][];      // adjacency matrix\n   private int nVerts;          // current number of vertices\n   private int nTree;           // number of verts in tree\n   private DistPar sPath[];     // array for shortest-path data\n   private int currentVert;     // current vertex\n   private int startToCurrent;  // distance to currentVert\n// -------------------------------------------------------------\n   public Graph()               // constructor\n      {\n      vertexList = new Vertex[MAX_VERTS];\n                                         // adjacency matrix\n      adjMat = new int[MAX_VERTS][MAX_VERTS];\n      nVerts = 0;\n      nTree = 0;\n      for(int j=0; j<MAX_VERTS; j++)     // set adjacency\n         for(int k=0; k<MAX_VERTS; k++)  //     matrix\n            adjMat[j][k] = INFINITY;     //     to infinity\n      sPath = new DistPar[MAX_VERTS];    // shortest paths\n      }  // end constructor\n// -------------------------------------------------------------\n   public void addVertex(char lab)\n      {\n      vertexList[nVerts++] = new Vertex(lab);\n      }\n// -------------------------------------------------------------\n   public void addEdge(int start, int end, int weight)\n      {\n      adjMat[start][end] = weight;  // (directed)\n      }\n// -------------------------------------------------------------\n   public void path()                // find all shortest paths\n      {\n      int startTree = 0;             // start at vertex 0\n      vertexList[startTree].isInTree = true;\n      nTree = 1;                     // put it in tree\n\n      // transfer row of distances from adjMat to sPath\n      for(int j=0; j<nVerts; j++)\n         {\n         int tempDist = adjMat[startTree][j];\n         sPath[j] = new DistPar(startTree, tempDist);\n         }\n\n      // until all vertices are in the tree\n      while(nTree < nVerts)\n         {\n         int indexMin = getMin();    // get minimum from sPath\n         int minDist = sPath[indexMin].distance;\n\n         if(minDist == INFINITY)     // if all infinite\n            {                        // or in tree,\n            System.out.println(\"There are unreachable vertices\");\n            break;                   // sPath is complete\n            }\n         else\n            {                        // reset currentVert\n            currentVert = indexMin;  // to closest vert\n            startToCurrent = sPath[indexMin].distance;\n            // minimum distance from startTree is\n            // to currentVert, and is startToCurrent\n            }\n         // put current vertex in tree\n         vertexList[currentVert].isInTree = true;\n         nTree++;\n         adjust_sPath();             // update sPath[] array\n         }  // end while(nTree<nVerts)\n\n      displayPaths();                // display sPath[] contents\n\n      nTree = 0;                     // clear tree\n      for(int j=0; j<nVerts; j++)\n         vertexList[j].isInTree = false;\n      }  // end path()\n// -------------------------------------------------------------\n   public int getMin()               // get entry from sPath\n      {                              //    with minimum distance\n      int minDist = INFINITY;        // assume minimum\n      int indexMin = 0;\n      for(int j=1; j<nVerts; j++)    // for each vertex,\n         {                           // if it's in tree and\n         if( !vertexList[j].isInTree &amp;&amp;  // smaller than old one\n                               sPath[j].distance < minDist )\n            {\n            minDist = sPath[j].distance;\n            indexMin = j;            // update minimum\n            }\n         }  // end for\n      return indexMin;               // return index of minimum\n      }  // end getMin()\n// -------------------------------------------------------------\n   public void adjust_sPath()\n      {\n      // adjust values in shortest-path array sPath\n      int column = 1;                // skip starting vertex\n      while(column < nVerts)         // go across columns\n         {\n         // if this column's vertex already in tree, skip it\n         if( vertexList[column].isInTree )\n            {\n            column++;\n            continue;\n            }\n         // calculate distance for one sPath entry\n                       // get edge from currentVert to column\n         int currentToFringe = adjMat[currentVert][column];\n                       // add distance from start\n         int startToFringe = startToCurrent + currentToFringe;\n                       // get distance of current sPath entry\n         int sPathDist = sPath[column].distance;\n\n         // compare distance from start with sPath entry\n         if(startToFringe < sPathDist)   // if shorter,\n            {                            // update sPath\n            sPath[column].parentVert = currentVert;\n            sPath[column].distance = startToFringe;\n            }\n         column++;\n         }  // end while(column < nVerts)\n   }  // end adjust_sPath()\n// -------------------------------------------------------------\n   public void displayPaths()\n      {\n      for(int j=0; j<nVerts; j++) // display contents of sPath[]\n         {\n         System.out.print(vertexList[j].label + \"=\"); // B=\n         if(sPath[j].distance == INFINITY)\n            System.out.print(\"inf\");                  // inf\n         else\n            System.out.print(sPath[j].distance);      // 50\n         char parent = vertexList[ sPath[j].parentVert ].label;\n         System.out.print(\"(\" + parent + \") \");       // (A)\n         }\n      System.out.println(\"\");\n      }\n// -------------------------------------------------------------\n   }  // end class Graph\n////////////////////////////////////////////////////////////////\nclass path\n   {\n   public static void main(String[] args)\n      {\n      Graph theGraph = new Graph();\n      theGraph.addVertex('A');     // 0  (start)\n      theGraph.addVertex('B');     // 1\n      theGraph.addVertex('C');     // 2\n      theGraph.addVertex('D');     // 3\n      theGraph.addVertex('E');     // 4\n\n      theGraph.addEdge(0, 1, 50);  // AB 50\n      theGraph.addEdge(0, 3, 80);  // AD 80\n      theGraph.addEdge(1, 2, 60);  // BC 60\n      theGraph.addEdge(1, 3, 90);  // BD 90\n      theGraph.addEdge(2, 4, 40);  // CE 40\n      theGraph.addEdge(3, 2, 20);  // DC 20\n      theGraph.addEdge(3, 4, 70);  // DE 70\n      theGraph.addEdge(4, 1, 50);  // EB 50\n\n      System.out.println(\"Shortest paths\");\n      theGraph.path();             // shortest paths\n      System.out.println();\n      }  // end main()\n   }  // end class PathApp\n////////////////////////////////////////////////////////////////\n\n```\n\n&nbsp;\n","tags":["数据结构"]},{"title":"npm和yarn更换源","url":"/npm和yarn更换源.html","content":"## npm安装\n\n查看npm版本\n\n```\nnpm -v\n\n```\n\nnpm查看所有版本\n\n```\nnpm view npm versions\n\n```\n\nnpm更新到最新版\n\n```\nnpm install -g npm\n\n```\n\n``查看npm当前镜像源\n\n```\nnpm config get registry\n\n```\n\n设置npm镜像源为淘宝镜像\n\n```\nnpm config set registry https://registry.npm.taobao.org/\n\n```\n\n遇到如下报错，是npm版本过低\n\n```\nBlock-scoped declarations (let, const, function, class) not yet supported outside strict mode at\n\n```\n\n需要ubuntu升级npm和node\n\n```\nnode -v\nv4.2.6\nnpm -v\n3.5.2\n\ncurl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash -\nsudo apt-get install -y nodejs\n\nnode -v\nv12.22.12\nnpm -v\n6.14.16\n\n```\n\nnpm安装项目依赖\n\n```\nnpm install --dependencies\n\n```\n\nmac升级node，参考：[Mac升级node14.17.2版本](https://blog.csdn.net/qq_35463964/article/details/125404969)\n\n清除nodejs的缓存\n\n```\nsudo npm cache clean -f\n\n```\n\n## node版本管理\n\n### 1.使用n模块\n\n安装n模块\n\n```\nsudo npm install -g n\n\n```\n\n查看所有能安装的版本\n\n```\nnpm view node versions\n\n```\n\n安装特定版本\n\n```\nsudo n 14.17.2\n\n```\n\n验证版本\n\n```\nnode -v\nv14.17.2\n\n```\n\n查看安装的node版本\n\n```\nn list\nnode/14.17.2\nnode/18.15.0\n\n```\n\n切换系统node版本\n\n```\n$ sudo node -v\nv18.15.0\n$ sudo n 14.17.2\n     copying : node/14.17.2\n   installed : v14.17.2 (with npm 6.14.13)\n$ sudo node -v\nv14.17.2\n\n```\n\n临时使用某个node版本\n\n```\nn use 18.15.0\n\n```\n\n### 2.使用nvm模块\n\n<!--more-->\n&nbsp;\n\n## pnpm安装\n\n```\nbrew install pnpm\n\n```\n\n参考：[https://pnpm.io/zh-TW/next/installation](https://pnpm.io/zh-TW/next/installation)\n\n## yarn安装\n\n参考：[Yarn for mac 安装教程 ](https://juejin.cn/post/6844903953675583496)\n\n查看yarn版本\n\n```\nyarn -v\n\n```\n\nyarn查看所有版本\n\n```\nnpm view yarn versions\n\n```\n\n`yarn更新到最新版<br />`\n\n```\nnpm install yarn@latest -g\n\n```\n\nyarn 升级指定版本\n\n```\nyarn upgrade v1.21.3\n\n```\n\nyarn 降低到指定版本（先卸载，再安装)\n\n```\nnpm uninstall yarn -g\nnpm install -g yarn@1.3.2\n\n```\n\n查看yarn当前镜像源\n\n```\nyarn config get registry\n\n```\n\n设置yarn镜像源为淘宝镜像\n\n```\nyarn config set registry https://registry.npm.taobao.org/\n\n```\n\n镜像源地址\n\n```\nnpm --- https://registry.npmjs.org/\ncnpm --- https://r.cnpmjs.org/\ntaobao --- https://registry.npm.taobao.org/\nnj --- https://registry.nodejitsu.com/\nrednpm --- https://registry.mirror.cqupt.edu.cn/\nnpmMirror --- https://skimdb.npmjs.com/registry/\ndeunpm --- http://registry.enpmjs.org/\n\n```\n\n也可以在工程目录下添加 .npmrc 文件来指定该项目的源\n\n```\nregistry=https://registry.npm.taobao.org/\n\n```\n\n&nbsp;\n","tags":["前端"]},{"title":"英文写作——收集的一些词","url":"/英文写作——收集的一些词.html","content":"1.减轻/对抗...的影响/风险\n\n　　combat/decrease/deal with/handle the effect\n\n　　alleviate/relieve/mitigate the risk\n\n2.接触风险\n\n　　defuse the risk\n\n3.由什么导致\n\n　　caused by/suffer from\n\n4.升级给老板\n\n　　escalate the issue<!--more-->\n&nbsp;to the manager\n\n5.xx的负责人\n\n　　person in charge of xx\n\n　　\n","tags":["杂谈"]},{"title":"面试题目——《CC150》Java","url":"/面试题目——《CC150》Java.html","content":"<img src=\"/images/517519-20160930223012344-57576457.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160930223038313-873132167.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160930223108360-1359027289.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n<img src=\"/images/517519-20161010155247196-1701199011.png\" alt=\"\" />\n\n<img src=\"/images/517519-20161010155312180-1708734838.png\" alt=\"\" />\n\n<img src=\"/images/517519-20161010155330586-405391616.png\" alt=\"\" />\n\n<img src=\"/images/517519-20161010155409039-1060929091.png\" alt=\"\" />\n\n<img src=\"/images/517519-20161010155435071-1061292682.png\" alt=\"\" />\n\n<img src=\"/images/517519-20161010155502430-1252385453.png\" alt=\"\" />\n\n&nbsp;\n\n```\npackage cc150.java;\n\nimport java.util.Iterator;\n\npublic class CircularArray {\n\n\tpublic static void main(String[] args) {\t\t//实现一个类似数组的数据结构，可以进行高效的旋转\n\t\t// TODO 自动生成的方法存根\n\t\tCircularArray ca_out = new CircularArray();\n\t\tcircularArray<Integer> ca = ca_out.new circularArray<Integer>(5);\n\t\tca.set(0, 0);\n\t\tca.set(1, 1);\n\t\tca.set(2, 2);\n\t\tca.set(3, 3);\n\t\tca.set(4, 4);\n\t\tca.rotate(5);\t\t//数组向右移位，也就是数组的head改变，到末尾之后会返回前\n\t\tSystem.out.println(ca.get(4));\n\t\t\n\t}\n\t\n\tpublic class circularArray<T> implements Iterable<T>{\t//环形数组\n\t\tprivate T[] items;\t\t//无法创建泛型的数组，所以必须将数组转型为List<T>或者将items定义为List<T>\n\t\tprivate int head=0;\t//指向数组开头的元素\n\t\t\n\t\tpublic circularArray(int size){\t//构造函数\n\t\t\titems = (T[]) new Object[size];\n\t\t}\n\t\t\n\t\tprivate int convert(int index){\t\t//转换正确的数组下标，index加上当前head\n\t\t\tif(index < 0)\t\t\t\t\t\t\t\t\t\t\t\t//负数都会是0\n\t\t\t\tindex += items.length;\n\t\t\treturn (head+index) % items.length;\n\t\t}\n\t\t\n\t\tpublic void rotate(int shiftRight){\t\t//轮换，改变数组的head下标\n\t\t\thead = convert(shiftRight);\n\t\t}\n\t\t\n\t\tpublic T get(int i){\t\t\t//取得数组中某个下标的元素\n\t\t\tif(i<0 || i>=items.length)\n\t\t\t\tthrow new java.lang.IndexOutOfBoundsException(\"...\");\n\t\t\treturn items[convert(i)];\n\t\t}\n\t\t\n\t\tpublic void set(int i,T item){\t\t//赋值\n\t\t\titems[convert(i)] = item;\n\t\t}\n\n\t\t//实现迭代器接口\n\t\t@Override\n\t\tpublic Iterator<T> iterator() {\n\t\t\t// TODO 自动生成的方法存根\n\t\t\treturn new circularArrayIterator<T>(this);\n\t\t}\n\t\t\n\t\tprivate class circularArrayIterator<TI> implements Iterator<TI>{\n\t\t\tprivate int _current = -1;\n\t\t\tprivate TI[] _items;\n\t\t\t\n\t\t\tpublic circularArrayIterator(circularArray<TI> array){\t//传递的是circularArray<TI>本身\n\t\t\t\t_items = array.items;\t\t\t\t\t\t\t\t\t//_items和item相等\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic boolean hasNext() {\n\t\t\t\t// TODO 自动生成的方法存根\n\t\t\t\treturn _current < items.length-1;\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic TI next() {\n\t\t\t\t// TODO 自动生成的方法存根\n\t\t\t\t_current++;\n\t\t\t\tTI item = (TI) _items[convert(_current)];\n\t\t\t\treturn item;\n\t\t\t}\n\t\t\t\n\t\t\t@Override\n\t\t\tpublic void remove(){\n\t\t\t\tthrow new UnsupportedOperationException(\"...\");\n\t\t\t}\n\t\t}\n\t\t\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["刷题"]},{"title":"面试题目——《CC150》排序与查找","url":"/面试题目——《CC150》排序与查找.html","content":"<img src=\"/images/517519-20160929204151844-760874957.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160929204220422-1093311054.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160929204424578-432324135.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160929204444969-899236624.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160929204503594-852464767.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160929204528453-2057566011.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160929204553578-198099017.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n**面试题11.1：给定两个排序后的数组A和B，其中A的末端有足够的缓冲空间容纳B。编写一个方法，将B合并入A并排序。**\n\n&nbsp;\n\n```\npackage cc150.sort_search;\n\npublic class MergeTwoSortedArr {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tint[] a = {1,3,5,7,9,11,13,15,0,0,0,0,0,0,0,0};\n\t\tint[] b = {0,2,4,6,8,10,12,14};\n\t\tMergeTwoSortedArr mt = new MergeTwoSortedArr();\n\t\tmt.merge(a,b,8,8);\n\t\tfor(int i=0;i<a.length;i++)\n\t\t\tSystem.out.println(a[i]);\n\t}\n\t\n\tpublic void merge(int[] a,int[] b,int lastA,int lastB){\n\t\tint indexA = lastA-1;\n\t\tint indexB = lastB-1;\n\t\tint indexMerge = lastA+lastB-1;\n\t\t//递减循环\n\t\twhile(indexA >= 0 &amp;&amp; indexB >= 0){\n\t\t\tif(a[indexA] > b[indexB])\t{\t//如果数组A的元素大于数组B的元素，先放入A\n\t\t\t\ta[indexMerge] = a[indexA];\n\t\t\t\tindexA--;\n\t\t\t\tindexMerge--;\n\t\t\t}else{\n\t\t\t\ta[indexMerge] = b[indexB];\n\t\t\t\tindexB--;\n\t\t\t\tindexMerge--;\n\t\t\t}\n\t\t}\n\t\t//如果B还有元素没有放进去的话，继续放，A不用，因为仍然在原来的位置\n\t\twhile(indexB >= 0){\n\t\t\ta[indexMerge] = b[indexB];\n\t\t\tindexB--;\n\t\t\tindexMerge--;\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题11.2：编写一个方法，对字符串数组进行排序，将所有变位词排在相邻的位置。**\n\n```\npackage cc150.sort_search;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.Comparator;\nimport java.util.Hashtable;\nimport java.util.Iterator;\n\npublic class AnagramSort {\n\n\t//牛客网题目中要求变位词的字典排序最小的数的集合，且这个集合也要是字典排序的\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tAnagramSort as = new AnagramSort();\n\t\tString[] arr = {\"ab\",\"ba\",\"abc\",\"cba\"};\n\t\tIterator ire = as.sortStrings(arr).iterator();\n\t\twhile(ire.hasNext())\n\t\t\tSystem.out.println(ire.next());\n\t}\n\t\n\tpublic String sortChars(String str) {\t\t//把字符串排序后返回\n\t\tchar[] content = str.toCharArray();\n\t\tArrays.sort(content);\n\t\treturn new String(content);\n    }\n\t\n\tpublic ArrayList<String> sortStrings(String[] array){\n\t\tHashtable<String,ArrayList<String>> hash = new Hashtable<String,ArrayList<String>>();\n\t\tArrayList<String> anagrams = new ArrayList<String>();\t//存放每个变位词集合\n\t\tArrayList<String> result = new ArrayList<String>();\t\t\t//存放每个变位词集合排序后的第一个\n\t\t//将同为变位词的单词分到同一组\n\t\tfor(String s:array){\n\t\t\tString key = sortChars(s);\n\t\t\tif(!hash.containsKey(key)){\n\t\t\t\thash.put(key, new ArrayList<String>());\t//如果不包含变位词，建立key对应的链表\n\t\t\t}\n\t\t\tanagrams = hash.get(key);\t//取得链表，并往其中加入变位词\n\t\t\tanagrams.add(s);\t\t\t\t\t\t//加入变位词\n\t\t}\n\t\t\n\t\t//将散列表排序后转换成ArrayList\n\t\tfor(String key:hash.keySet()){\t\t\t\t\t\t\t//取得key的集合，并遍历\n\t\t\tArrayList<String> list = hash.get(key);\t\t//一个一个取得ArrayList\n\t        Collections.sort(list,new Comparator<String>(){\n\t            @Override\n\t            public int compare(String str1, String str2) {\n\t                // TODO Auto-generated method stub                             \n\t                    return str1.compareTo(str2);\n\t            }\n\t        });\n\t\t\tresult.add( list.get(0));\t\t//取得排序后的每个变位词集合的第一个元素，加入新的链表中\n\t\t\tCollections.sort(result,new Comparator<String>(){\t//把新的链表排序后返回\n\t            @Override\n\t            public int compare(String str1, String str2) {\n\t                // TODO Auto-generated method stub                             \n\t                    return str1.compareTo(str2);\n\t            }\n\t        });\n\t\t}\n\t\treturn result;\n\t}\n\t\n}\n\n```\n\n&nbsp;\n\n**面试题11.3：给定一个排序后的数组，包含n个整数，但这个数组已被旋转过很多次，次数不详。请编写代码找出数组中的某个元素。可以假定数组元素原先是按从小到大的顺序排列的。**\n\n**　　旋转的意思是：Array1:{10,15,20,0,5}中有一半是升序排序的**\n\n```\npackage cc150.sort_search;\n\npublic class Search {\n\n\t//注意可能有重复的元素\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tint[] a = {2,2,2,3,4,2};\n\t\tSearch s = new Search();\n\t\tSystem.out.println(s.search(a,0,a.length-1,3));\n\t}\n\t\n\tpublic int search(int a[],int left,int right,int x){\t\t//查找x\n\t\tint mid = (left+right)/2;\n\t\tif(x == a[mid])\t\t//找到元素\n\t\t\treturn mid;\n\t\tif(right < left)\t\t//没找到元素\n\t\t\treturn -1;\n\t\t//二分查找\n\t\tif(a[left] < a[mid]){\t\t\t\t\t\t\t\t//左半边正常排序\n\t\t\tif(a[left] <= x &amp;&amp; a[mid] >= x)\t\t//x在左半边的范围内\n\t\t\t\treturn search(a,left,mid-1,x);\n\t\t\telse\t\t\t\t\t\t\t\t\t\t\t\t\t//x不在左半边的范围内\n\t\t\t\treturn search(a,mid+1,right,x);\n\t\t}else if(a[left] > a[mid]){\t\t\t\t\t//右半边正常排序\n\t\t\tif(a[mid] <= x &amp;&amp; a[right] >= x)\t//x在右半边的范围内\n\t\t\t\treturn search(a,mid+1,right,x);\n\t\t\telse\t\t\t\t\t\t\t\t\t\t\t\t\t//x不在右半边的范围内\n\t\t\t\treturn search(a,left,mid-1,x);\n\t\t}else if(a[left] == a[mid]){\t\t\t\t//左半边都是重复元素，{2,2,2,3,4,2}的情况\n\t\t\tif(a[mid] != a[right])\t\t\t\t\t\t//如果右边不等于中间，只需要搜索左半边\n\t\t\t\treturn search(a,left,mid-1,x);\n\t\t\telse{\t\t\t\t\t\t\t\t\t\t\t\t\t//否则两边都要搜索\n\t\t\t\tint result = search(a,left,mid-1,x);\t\t//先搜索左半边\n\t\t\t\tif(result == -1)\t\t\t\t//如果没有搜索到\n\t\t\t\t\treturn  search(a,mid+1,right,x);\t\t//再搜索右半边\n\t\t\t\telse\n\t\t\t\t\treturn result;\n\t\t\t}\n\t\t}\n\t\treturn -1;\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题11.4：设想你有一个20GB的文件，每一行一个字符串。请说明将如何对这个文件进行排序。**\n\n**　　在不能全部载入内存的情况下，**使用**外部排序**，看**《数据结构与算法分析》**\n\n&nbsp;\n\n**面试题11.5：有一个排序后的字符串数组，其中散步着一些空字符串，编写一个方法，找出给定字符串的位置。**\n\n```\npackage cc150.sort_search;\n\npublic class Finder {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tFinder fd = new Finder();\n\t\tString[] arr = {\"a\",\"b\",\"\",\"c\",\"\",\"d\"};\n\t\tSystem.out.println(fd.findString(arr, 6, \"c\"));\n\t}\n\t\n\t//因为是有序数组，所以使用二分查找，但是有空字符串，所有当mid遇到空字符串的时候要转到最近的非空字符串\n\tpublic int findString(String[] str, int n, String x) {\n        // write code here\n\t\tif(str == null || str == null || str.length <= 0)\n\t\t\treturn -1;\n\t\treturn searchR(str,x,0,n-1);\n    }\n\t\n\tpublic int searchR(String[] strings,String str,int first,int last){\n\t\tif(first > last)\n\t\t\treturn -1;\n\t\tint mid = (first+last)/2;\n\t\t//如果mid是空的情况\n\t\tif(strings[mid].isEmpty()){\n\t\t\tint left = mid-1;\n\t\t\tint right = mid+1;\n\t\t\twhile(true){\t\t//循环\n\t\t\t\tif(left < first &amp;&amp;right > last)\t\t//返回错误\n\t\t\t\t\treturn -1;\n\t\t\t\telse if(right <= last &amp;&amp; !strings[right].isEmpty()){\t//如果mid的右边不超过last，且不为空的话，赋值为mid，跳出\n\t\t\t\t\tmid = right;\n\t\t\t\t\tbreak;\n\t\t\t\t}else if(left >= first &amp;&amp; !strings[left].isEmpty()){\t//如果mid的左边不超过last，且不为空的话，赋值为mid，跳出\n\t\t\t\t\tmid = left;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tright++;\n\t\t\t\tleft--;\n\t\t\t}\n\t\t}\n\t\t//对不为空的mid进行检查\n\t\tif(str.equals(strings[mid]))\n\t\t\treturn mid;\n\t\telse if(strings[mid].compareTo(str)<0)\t\t//mid小于str，对右半边继续二分查找\n\t\t\treturn searchR(strings,str,mid+1,last);\n\t\telse\n\t\t\treturn searchR(strings,str,first,mid-1);\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题11.6：给定M&times;N矩阵，每一行、每一列都按升序排列，请编写代码找出某元素。**\n\n```\npackage cc150.sort_search;\n\npublic class FindElement {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\n\t}\n\t\n\tpublic int[] findElement(int[][] mat, int n, int m, int x) {\n        // write code here\n\t\tint row = 0;\n\t\tint column = m-1;\n\t\twhile(row < n &amp;&amp; column >= 0){\t//从右上角开始寻找\n\t\t\tif(mat[row][column] == x){\n\t\t\t\tint[] result = {row,column};\n\t\t\t\treturn result;\n\t\t\t}else if(mat[row][column] > x)\n\t\t\t\tcolumn--;\n\t\t\telse\n\t\t\t\trow++;\n\t\t}\n\t\treturn null;\n    }\n\n}\n\n```\n\n&nbsp;\n\n**面试题11.7：有一个马戏团正在设计叠罗汉的表演节目，一个人要站在另一个人的肩膀上。出于实际和美观的考虑，在上面的人要比下面的人矮一点、轻一点。已知马戏团每个人的高度和重量，请编写代码计算叠罗汉最多能叠到几个人。（最长递增子序列）**\n\n```\npackage cc150.sort_search;\n\npublic class Dieluohan {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\n\t}\n\t\n\tpublic int getHeight(int[] men, int n) {\t//建立一个数组记录每一位的时候最长序列的长度\n        // write code here\n\t\tint[] dp = new int[n];\t\t//存放到达每一位的时候的最长序列的长度\n\t\tint max = 0;\n\t\tfor(int i=0;i<n;i++){\n\t\t\tdp[i] = 1;\t//先把每一个置为1，因为至少是1\n\t\t\tfor(int j=0;j<i;j++){\t//加上了第i个后，循环前i-1个，如果有值小于等于第i个值的话，加上1，注意加上i后最长的\n\t\t\t\tif(men[j] <= men[i])\n\t\t\t\t\tdp[i] = Math.max(dp[i], dp[j]+1);\n\t\t\t}\n\t\t\tmax = Math.max(dp[i], max);\n\t\t}\n\t\treturn max;\n    }\n\n}\n\n```\n\n&nbsp;\n","tags":["刷题"]},{"title":"面试题目——系统设计","url":"/面试题目——系统设计.html","content":"## **4S法则：[《系统设计面试题精选》 ](https://soulmachine.gitbooks.io/system-design/content/cn/)**\n\n```\n1.Scenario场景：功能、需求、QPS、存储容量\n    需要设计哪些功能、到哪个程度\n    询问： Features / QPS / DAU / Interfaces\n2.Service：逻辑块聚类、函数设计、访问接口设计\n    将大系统拆分为小服务\n    拆分、Application、模块化\n3.Storage：选择SQL或NoSQL、OOD细化Schema\n    数据如何存储与访问\n    Schema/Data/SQL/NoSQL/File System\n4.Scale：可能遇到的问题、优化系统\n    解决缺陷，处理可能遇到的问题\n    Sharding/ Optimize / Special Case\n\n```\n\n<!--more-->\n&nbsp;\n\n## 系统设计常用算法：\n\n1.一致性hash算法（consistent hashing）\n\n使用场景：Cassandra，redis，dynamoDB\n\n2.Geohash算法，Quadtree算法（四叉树算法）\n\n使用场景：地点类应用\n\n3.漏桶（leaky bucket）算法、令牌桶（token bucket）\n\n使用场景：限流\n\n4.Trie（字典树）\n\n使用场景：搜索，自动补全\n\n5.bloom过滤器\n\n使用场景：快速检查一个元素是否存在\n\n6.Raft/Paxos算法\n\n使用场景：一致性算法\n\n<img src=\"/images/517519-20231001104510176-224900691.png\" width=\"600\" height=\"625\" />\n\n&nbsp;\n\n## 题目：\n\n### [1.如何设计一个分布式ID生成器(Distributed ID Generator)，并保证ID按时间粗略有序？](https://soulmachine.gitbooks.io/system-design/content/cn/distributed-id-generator.html)\n\nid的要求：\n\n- 全局唯一(unique)\n- 按照时间粗略有序(sortable by time)\n- 尽可能短\n\n设计：\n\n1.类似mongo的UUID，unix时间戳+机器id+进程id+计数器，每台机器可以独立生成，但是缺点是长度太长，占用内存，且索引查询效率低，不是单调递增的\n\n2.基于数据库集群（双主）模式：如果id是给多台mysql使用的，可以给两台mysql设置不同的初始id和自增步长\n\n3.基于Redis：原理就是利用`redis`的 `incr`命令实现ID的原子性自增。参考：[一口气说出 9种 分布式ID生成方式，面试官有点懵了](https://juejin.cn/post/6844904065747402759)\n\n4.Twitter的snowflake算法，参考：[基于 Snowflake 算法搭建发号器](https://zq99299.github.io/note-architect/hc/02/04.html#%E5%9F%BA%E4%BA%8E-snowflake-%E7%AE%97%E6%B3%95%E6%90%AD%E5%BB%BA%E5%8F%91%E5%8F%B7%E5%99%A8)\n\nSnowflake 的核心思想是将 64bit 的二进制数字分成若干部分，每一部分都存储有特定含义的数据，比如说时间戳、机器 ID、序列号等等，最终生成全局唯一的有序 ID。它的标准算法是这样的：\n\n<img src=\"/images/517519-20230919134329970-324092484.png\" width=\"500\" height=\"109\" />\n\n### [2.如何设计一个短网址服务(TinyURL)？](https://soulmachine.gitbooks.io/system-design/content/cn/tinyurl.html)\n\n短链的要求：尽可能短\n\n1.确定短链的长度：假设网页数量1亿，即1*10^8；a-zA-Z0-9（26+26+10=62种可能性）；100*10^6 小于 64*10^7，所以能用7位长度的短链来表示1亿的网页\n\n2.确定一个长链是否需要对应多个短链：短链一般用于推广，多个短链就可以对推广渠道（社交媒体、邮件和印刷广告），推广方式（图文、视频、问答；免费推广、付费推广），推广人员（销售，客服）、推广用户等做区分（新客户、老客户；低客单客户、高客单客户）\n\n3.短链的生成：可以使用hash算法，不过需要额外处理hash冲突，比如MurmurHash算法，该算法因为**效率高**和**冲突概率低**而出名，Redis，Google的[guava](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fgoogle%2Fguava)，apache的[commons-codec](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fapache%2Fcommons-codec)内部都有这种算法的实现，hash出来的值是数字，可以将其转换成62进制，然后就可以转换成a-zA-Z0-9了；\n\n处理hash冲突的时候，可以去kv里面查找这个短链是否存在，如果存在且长链不同，则发生了hash冲突，这时候可以给长链加上随机数，再hash来进行解决；判断hash冲突的时候可以使用bloom过滤器\n\n4.如何存储：kv存储，短链是key，长链是value\n\n参考：[高性能短链(短网址)算法](https://www.jianshu.com/p/7189521b5cfb)\n\n### 3.设计API限流\n\n设计要求：\n\n- 低延迟，性能要好\n- 需要适用于分布式场景。\n- 用户的请求受到限制时，需要提示具体的原因。\n- 高容错，如果限速器故障，不应该影响整个系统。\n\n常用限流算法：\n\n- 令牌桶 （Token bucket）固定速度往桶里面放令牌，令牌取完拒绝请求\n- 漏桶（Leaking bucket）来一个请求就填充到桶里，桶满了就拒绝请求，可以使用先进先出队列来实现\n- 固定窗口计数器（Fixed window counter）\n- 滑动窗口日志（Sliding window log）\n- 滑动窗口计数器（Sliding window counter）可以解决窗口边缘请求超过的问题，使用redis的sorted set，[Redis-Sorted-Set底层数据结构](https://www.jianshu.com/p/14dde3031e0b)\n\n高性能计数器：限速器需要一个计数器，想要快的话，需要使用redis这种内存缓存，而且还需要分布式\n\n限速器的位置：一般来说是限速器是在客户端和服务端中间添加一个中间件，限速器拒绝请求之后返回HTTP状态码429，too many request\n\n参考：[【系统设计】设计一个限流组件](https://www.cnblogs.com/myshowtime/p/16313623.html)\n\n### [4.请实现一个定时任务调度器，有很多任务，每个任务都有一个时间戳，任务会在该时间点开始执行。](https://soulmachine.gitbooks.io/system-design/content/cn/task-scheduler.html)\n\n### [5.Feed 流系统设计总纲](https://www.infoq.cn/article/t0qlhfk7uxxzwo0uo*9s)\n\n### 6.push系统\n\n参考：[设计一个百万级的消息推送系统](https://crossoverjie.top/2018/09/25/netty/million-sms-push/)\n\n### 7.评论系统\n\n参考：[日均数十亿请求！京东评价系统海量数据存储高可用设计](http://dbaplus.cn/news-21-613-1.html?hmsr=toutiao.io&amp;utm_campaign=toutiao.io&amp;utm_medium=toutiao.io&amp;utm_source=toutiao.io)\n\n### 8.朋友圈\n\n参考：[feed流拉取，读扩散，究竟是啥？](https://cloud.tencent.com/developer/article/1168948)\n\n### [9.Designing Pastebin](https://blog.csdn.net/qq_41202486/article/details/112181491)\n\n**功能需求：**<br /> 1.用户应该能够上传或&ldquo;粘贴&rdquo;他们的数据，并获得一个唯一的URL来访问数据。<br /> 2.用户只能上传文本。<br /> 3.数据和链接将在特定的时间段后自动过期；用户还应该能够指定过期时间。<br /> 4.用户应该可以随意地为他们的粘贴选择一个自定义别名。\n\n### [10.搜索引擎联想提示功能](https://wizardforcel.gitbooks.io/the-art-of-programming-by-july/content/a.9.html)\n\n### [11.爬虫，webmagic](http://webmagic.io/docs/zh/posts/ch1-overview/architecture.html)\n\n### [12. top k](https://soulmachine.gitbooks.io/system-design/content/cn/bigdata/heavy-hitters.html)\n\n### [13.亿万级别的网页的去重任务&mdash;&mdash;simhash](https://wizardforcel.gitbooks.io/the-art-of-programming-by-july/content/06.03.html)\n\n### 14.Twitter发推和订阅设计\n\n参考：[【系统设计】新鲜事系统设计的学习笔记](https://zhuanlan.zhihu.com/p/103484396)\n\n### 15.权限系统\n\n### 16.秒杀系统\n\n### 17.设计uber or lyft\n\n[https://www.youtube.com/watch?v=R_agd5qZ26Y](https://www.youtube.com/watch?v=R_agd5qZ26Y)\n\n### 18.设计搜索引擎\n\n[https://www.youtube.com/watch?v=0LTXCcVRQi0&amp;ab_channel=InterviewPen](https://www.youtube.com/watch?v=0LTXCcVRQi0&amp;ab_channel=InterviewPen)\n\n### 19.设计延时队列\n\n[如何实现一个延时队列](https://xie.infoq.cn/article/a7eb2a84e4e04adc0c098dec1)\n\n### 20.AB系统设计\n\n<img src=\"/images/517519-20230910200722261-868197113.png\" width=\"500\" height=\"342\" />\n\n参考：[架构解析 | 从ABTest是啥开始说](https://developer.jdcloud.com/article/2297)\n\n1.试验管理模块\n\n2.分流模块\n\n3.业务接入模块\n\n4.数据收集模块\n\n5.结果分析/效果评估模块\n\n<img src=\"/images/517519-20230910201341499-41456032.png\" width=\"600\" height=\"459\" />\n\n参考：[打造工业级推荐系统（八）：AB 测试平台的工程实现](https://www.infoq.cn/article/lctiipkrh-lpl9pitkhf)\n\n### 21.如何保证mysql和redis的一致性\n\n容易产生不一致的方法：\n\n1.先写mysql，再写redis（2个请求都是更新的时候有可能不一致）；\n\n2.先写redis，再写mysql（2个请求都是更新的时候有可能不一致）；\n\n3.先删除redis，再写 MySQL（1个请求是更新，1个请求是查询的时候有可能不一致：更新请求删除redis，查询请求查到旧数据，先回填redis，然后更新请求更新mysql）\n\n较好的方法：\n\n1.延时双删（先删除 Redis，再写 MySQL，延时一会再删除 Redis）\n\n2.先写Mysql，再删除redis（可能出现短暂的不一致，不是秒杀或者库存这种的要求强一致的场景比较推荐）\n\n3.先写 MySQL，通过 Binlog，异步更新 Redis（可以达到最终一致性，但是不建议用于秒杀或者抢购场景）\n\n方案对比：\n\n1.先写 Redis，再写 MySQL<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 这种方案，我肯定不会用，万一 DB 挂了，你把数据写到缓存，DB 无数据，这个是灾难性的；<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 我之前也见同学这么用过，如果写 DB 失败，对 Redis 进行逆操作，那如果逆操作失败呢，是不是还要搞个重试？<br />2.先写 MySQL，再写 Redis<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 对于并发量、一致性要求不高的项目，很多就是这么用的，我之前也经常这么搞，但是不建议这么做；<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 当 Redis 瞬间不可用的情况，需要报警出来，然后线下处理。<br />3.先删除 Redis，再写 MySQL<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 这种方式，我还真没用过，直接忽略吧。<br />4.先删除 Redis，再写 MySQL，再删除 Redis<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 这种方式虽然可行，但是感觉好复杂，还要搞个消息队列去异步删除 Redis。<br />5.先写 MySQL，再删除 Redis<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 比较推荐这种方式，删除 Redis 如果失败，可以再多重试几次，否则报警出来；<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 这个方案，是实时性中最好的方案，在一些高并发场景中，推荐这种。<br />6.先写 MySQL，通过 Binlog，异步更新 Redis<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 对于异地容灾、数据汇总等，建议会用这种方式，比如 binlog + kafka，数据的一致性也可以达到秒级；<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 纯粹的高并发场景，不建议用这种方案，比如抢购、秒杀等。\n\n参考：[美团四面：如何保障 MySQL 和 Redis 的数据一致性？ ](https://learnku.com/articles/75370)\n\n&nbsp;\n\n## [《Designing Data Intensive Applications》](https://vonng.gitbooks.io/ddia-cn/content/ch1.html)\n\n&nbsp;\n\n## 《Grokking System Design》\n\n### **1. 负载均衡 **\n\n概念：\n\n```\n负载平衡器（LB）是任何分布式系统的另一个关键部分。它有助于根据某种度量标准（随机，循环，对内存或CPU利用率加权的随机变量）在多个资源之间分配负载。\nLB在分发请求时还跟踪所有资源的状态。如果服务器不可用于接受新请求，服务器无响应或错误率升高，则LB将停止向该服务器发送流量。\n\n```\n\n场景：\n\n```\n为了充分利用可伸缩性和冗余性，我们可以尝试平衡系统每一层的负载。我们可以在三个位置添加LB：\n●在用户和Web服务器之间\n●在Web服务器和内部平台层之间，例如应用程序服务器或缓存服务器\n●在内部平台层和数据库之间。\n\n```\n\n<img src=\"/images/517519-20210326174435433-312403976.png\" alt=\"\" />\n\n实现：\n\n1. Smart Client\n\n```\n其一是Smart Client，即将负载均衡的功能添加到数据库（以及缓存或服务）的客户端中。这是一种通过软件来实现负载均衡的方式，它的缺点是方案会比较复杂，不够健壮，也很难被重用（因为协调请求的逻辑会混杂在业务系统中）。\n\n```\n\n2. Hardware Load Balancers\n\n```\n第二种方式是采用硬件负载均衡器，例如Citrix NetScaler。不过，购买硬件的费用不菲，通常是一些大型公司才会考虑此方案\n\n```\n\n3. Software Load Balancers\n\n```\n比如HAProxy\n\n```\n\n目前haproxy支持的**负载均衡算法**有如下8种，参考：https://my.oschina.net/BambooLi/blog/506397\n\n```\n1、roundrobin\n表示简单的轮询，每个服务器根据权重轮流使用，在服务器的处理时间平均分配的情况下这是最流畅和公平的算法。该算法是动态的，对于实例启动慢的服务器权重会在运行中调整。\n\n2、leastconn\n连接数最少的服务器优先接收连接。leastconn建议用于长会话服务，例如LDAP、SQL、TSE等，而不适合短会话协议。如HTTP.该算法是动态的，对于实例启动慢的服务器权重会在运行中调整。\n\n3、static-rr\n每个服务器根据权重轮流使用，类似roundrobin，但它是静态的，意味着运行时修改权限是无效的。另外，它对服务器的数量没有限制。\n该算法一般不用；\n\n4、source\n对请求源IP地址进行哈希，用可用服务器的权重总数除以哈希值，根据结果进行分配。只要服务器正常，同一个客户端IP地址总是访问同一个服务器。如果哈希的结果随可用服务器数量而变化，那么客户端会定向到不同的服务器；\n该算法一般用于不能插入cookie的Tcp模式。它还可以用于广域网上为拒绝使用会话cookie的客户端提供最有效的粘连；\n该算法默认是静态的，所以运行时修改服务器的权重是无效的，但是算法会根据&ldquo;hash-type&rdquo;的变化做调整。\n\n5、uri\n表示根据请求的URI左端（问号之前）进行哈希，用可用服务器的权重总数除以哈希值，根据结果进行分配。只要服务器正常，同一个URI地址总是访问同一个服务器。一般用于代理缓存和反病毒代理，以最大限度的提高缓存的命中率。该算法只能用于HTTP后端；\n该算法一般用于后端是缓存服务器；\n该算法默认是静态的，所以运行时修改服务器的权重是无效的，但是算法会根据&ldquo;hash-type&rdquo;的变化做调整。\n\n6、url_param\n在HTTP GET请求的查询串中查找<param>中指定的URL参数，基本上可以锁定使用特制的URL到特定的负载均衡器节点的要求；\n该算法一般用于将同一个用户的信息发送到同一个后端服务器；\n该算法默认是静态的，所以运行时修改服务器的权重是无效的，但是算法会根据&ldquo;hash-type&rdquo;的变化做调整。\n\n7、hdr(name)\n在每个HTTP请求中查找HTTP头<name>，HTTP头<name>将被看作在每个HTTP请求，并针对特定的节点；\n如果缺少头或者头没有任何值，则用roundrobin代替；\n该算法默认是静态的，所以运行时修改服务器的权重是无效的，但是算法会根据&ldquo;hash-type&rdquo;的变化做调整。\n\n8、rdp-cookie（name）\n为每个进来的TCP请求查询并哈希RDP cookie<name>；\n该机制用于退化的持久模式，可以使同一个用户或者同一个会话ID总是发送给同一台服务器。如果没有cookie，则使用roundrobin算法代替；\n该算法默认是静态的，所以运行时修改服务器的权重是无效的，但是算法会根据&ldquo;hash-type&rdquo;的变化做调整。\n\n```\n\n&nbsp;\n\n### **2. 缓存**\n\n缓存几乎用于计算的每个层：硬件，操作系统，Web浏览器，Web应用程序等等。\n\n1.硬件：CPU一级缓存，二级缓存\n\n2.操作系统，参考：[Linux系统中的Page cache和Buffer cache](https://zhuanlan.zhihu.com/p/35277219)\n\n**Page cache（页面缓存）**\n\nPage cache 也叫页缓冲或文件缓冲，是由好几个磁盘块构成，大小通常为4k，在64位系统上为8k，构成的几个磁盘块在物理磁盘上不一定连续，文件的组织单位为一页， 也就是一个page cache大小，文件读取是由外存上不连续的几个磁盘块，到buffer cache，然后组成page cache，然后供给应用程序。\n\nPage cache在linux读写文件时，它用于缓存文件的逻辑内容，从而加快对磁盘上映像和数据的访问。具体说是加速对文件内容的访问，buffer cache缓存文件的具体内容&mdash;&mdash;物理磁盘上的磁盘块，这是加速对磁盘的访问。\n\n**Buffer cache（块缓存）**\n\nBuffer cache 也叫块缓冲，是对物理磁盘上的一个磁盘块进行的缓冲，其大小为通常为1k，磁盘块也是磁盘的组织单位。设立buffer cache的目的是为在程序多次访问同一磁盘块时，减少访问时间。系统将磁盘块首先读入buffer cache，如果cache空间不够时，会通过一定的策略将一些过时或多次未被访问的buffer cache清空。程序在下一次访问磁盘时首先查看是否在buffer cache找到所需块，命中可减少访问磁盘时间。不命中时需重新读入buffer cache。对buffer cache的写分为两种，一是直接写，这是程序在写buffer cache后也写磁盘，要读时从buffer cache上读，二是后台写，程序在写完buffer cache后并不立即写磁盘，因为有可能程序在很短时间内又需要写文件，如果直接写，就需多次写磁盘了。这样效率很低，而是过一段时间后由后台写，减少了多次访磁盘的时间。\n\nBuffer cache是由物理内存分配，Linux系统为提高内存使用率，会将空闲内存全分给buffer cache ，当其他程序需要更多内存时，系统会减少cache大小。\n\n3.Web浏览器，参考：[HTTP缓存机制 &amp; cookie/localStorage/sessionStorage](https://www.jianshu.com/p/cb72dfb0f7b3)\n\n4.应用服务器缓存（Application server cache），参考：[浅谈分布式缓存](https://xta0.me/2016/06/22/System-Design-Cache.html)\n\n缓存的设计方案有很多种，最简单的是单机缓存，即当请求到来时，服务器查自己的缓存，并返回数据给Client。这种方式适用于业务场景简单，没有大规模请求的应用。但是请求量上来，单台服务器会横向扩展到多台，这种缓存方式就会出现问题，虽然被扩展出来的多台服务器仍可以自己维护自己的缓存，但是同一个用户的请求可能被负载均衡投递到不同的服务器上，会大大增加缓存Miss的概率。解决这个问题，可以使用两种方式，一种是分布式缓存，一种是全局缓存。\n\n5.分布式缓存（Distributed cache），参考：[一致性哈希结构or分布式缓存扩容](https://zhuanlan.zhihu.com/p/76721465)\n\n在分布式缓存中，其每个节点都拥有部分缓存数据。通常，使用一致的哈希函数对高速缓存进行划分，这样，如果请求节点正在查找特定数据，则它可以快速知道在分布式高速缓存中查找的位置，以确定该数据是否可用。在这种情况下，每个节点都有一小部分缓存，然后将在发送到原始节点之前向另一个节点发送数据请求。因此，分布式缓存的优点之一是我们可以轻松地增加缓存空间，只需将节点添加到请求池即可实现。\n\n分布式缓存的一个缺点是解决丢失的节点。一些分布式缓存通过在不同节点上存储数据的多个副本来解决此问题。但是，您可以想象这种逻辑如何迅速变得复杂，尤其是当您从请求层添加或删除节点时。尽管即使节点消失并且部分缓存丢失，请求也只会从源中拉出，因此不一定是灾难性的。\n\n6.全局缓存（Global Cache）\n\n全局缓存同样是使用多台机器构成一个集群，不同的是，每台机器上所存放的缓存数据均相同，全局缓存通常由两种设计方式，如下图所示\n\n<img src=\"/images/517519-20210327002340563-118961767.png\" width=\"400\" height=\"413\" />\n\n大部分应用会采用第一种方式，这种方式缓存会自动代理对DB的查询操作，省去了业务的额外调用。但并不是所有场景都适合使用第一种方式，如果缓存中的数据都是大文件，当读缓存miss的时候，缓存会自动从数据库中读取该文件，由于数据较大，读取的耗时会很长，这会导致缓存中pending的查询越来越多。这时如果使用第二种方式，由业务查询DB则会大大降低缓存压力。\n\n全局缓存的优点是无状态，缓存节点的增删对集群没有太大的影响，缺点是单个节点能承受的数据容量有限，而且节点之间维护数据一致性也很麻烦。\n\n缓存失效（一致性）：Write-through cache；Write-around cache；Write-back cache\n\n缓存淘汰机制：\n\n&nbsp;&nbsp;&nbsp; 1.先进先出（FIFO): 高速缓存逐出最先访问的第一个数据块，而无需考虑之前被访问的频率或次数。\n\n&nbsp;&nbsp;&nbsp; 2.后进先出（LIFO):高速缓存逐出最近访问的数据块，而无需考虑它之前被访问的频率或次数。&nbsp;&nbsp;\n\n&nbsp;&nbsp;&nbsp; 3.最近最少使用（LRU)：首先丢弃最近最少使用的数据块。&nbsp;&nbsp;\n\n&nbsp;&nbsp;&nbsp; 4.最近使用（MRU): 与LRU相比，首先丢弃最近使用的数据。\n\n&nbsp;&nbsp;&nbsp; 5.最少使用次数（LFU): 计算各个数据的使用频率。最不常用的那些将首先被丢弃。\n\n&nbsp;&nbsp;&nbsp; 6.随机替换（RR): 随机选择一个候选项目，并在必要时将其丢弃以腾出空间。\n\n&nbsp;\n\n### **3. CDN<br />**\n\n提高性能，降低Web服务器负载的另一种常见做法是将静态媒体放入CDN（Content Distribution Network）中。如下图所示\n\n**<img src=\"/images/517519-20210327003556938-1702827935.png\" width=\"400\" height=\"178\" />**\n\n&nbsp;\n\n### **4. Sharding or Data Partitioning**\n\n参考：[数据库分片（Database Sharding)详解](https://zhuanlan.zhihu.com/p/57185574)\n\n1.水平分片：在此方案中，我们将不同的行放入不同的表中。例如，如果我们在一个表中存储不同的位置，我们可以决定将邮政编码少于10000的位置存储在一个表中，而将邮政编码大于10000的位置存储在单独的表中。这也称为基于范围的分片，因为我们将不同范围的数据存储在单独的表中。\n \n\n这种方法的关键问题是，如果未仔细选择其范围用于分片的值，则分区方案将导致服务器不平衡。在前面的示例中，基于邮政编码的位置划分位置假定地点将平均分布在不同的邮政编码中。这种假设是无效的，因为与郊区城市相比，像曼哈顿这样的人口稠密地区将有很多地方。\n\n2.垂直分片：在此方案中，我们将数据划分为将与特定功能相关的表存储到其自己的服务器。例如，如果我们正在构建类似Instagram的应用程序，我们需要在其中存储与用户，上传的所有照片以及他们所关注的人有关的数据，我们可以决定将用户配置文件信息放置在一个数据库服务器上，将好友列表放置在另一个数据库服务器上，并放置照片在第三台服务器上。\n \n\n垂直分区易于实现，对应用程序的影响很小。这种方法的主要问题是，如果我们的应用程序实现了额外的增长，那么可能有必要在各个服务器之间进一步划分特定于功能的数据库（例如，单个服务器不可能处理100亿个元数据的所有查询） 1.4亿用户的照片）\n\n3.基于目录的分片：要实现基于目录的分片，必须创建并维护一个查找表，该查找表使用分片键来跟踪哪个分片包含哪些数据。简而言之，查找表是一个表，其中包含有关可以找到特定数据的静态信息集\n\n&nbsp;\n\n### 5.推荐系统\n\n参考：[Recommendation System](https://bigfantheory.medium.com/recommendation-system-660bda6ad820)\n\n[推荐系统的架构-冷启动-召回-粗排-精排-重排](https://zhuanlan.zhihu.com/p/572998087)\n\n推荐系统在架构上可以分成online training和offline training两个部分\n\n#### 1.offline training\n\noffline training推荐系统是最常见的推荐系统之一。离线字面上的意思是用一周或几周前生成的数据进行训练。模型的更新通常发生在一个或几个小时内。它持续跟踪中长期客户利益。\n\n一个典型的离线推荐系统具有数据采集、离线训练、在线存储、在线预测和A/B测试模块。数据收集和离线训练是监督学习部分。在线预测和A/B测试是预测部分。除了模型之外，还有一个在线存储空间，用于存储模型和在线预测所需的特征信息。模块分为一个训练数据流和一个预测数据流。用于训练模型的数据收集也将存储在在线存储中。在预测方面，web发送请求进行A/B测试，并从在线预测模块中检索预测结果。\n\n<img src=\"/images/517519-20230807132835384-828012939.png\" width=\"600\" height=\"238\" />\n\n&nbsp;\n\n(1)data collection：从web上收集日志;通常有采集、验证、清洗和改造步骤。该模块收集数据并将其转换为可直接用于训练的格式，并将其保存在离线存储中。 \n\n(2)offline training：可以进一步分为线下存储和线下计算。实际应用中，离线训练数据量巨大，需要分布式文件系统进行存储。离线计算包括:数据采样、特征工程、模型训练、相似度计算。 \n\n(3)online storage：对响应速度要求更严格。如果用户打开应用程序，应用程序需要很长时间来响应，这将以一种非常糟糕的方式影响用户体验。一般来说，推荐系统应该在用户发送请求后几毫秒内响应。为了满足对速度的高要求，设计了在线存储技术来存储在线模型和特征数据。 \n\n(4)oneline recommendation：根据用户刚刚发布的新请求进行预测。该模块主要做了以下工作：a.获取用户特征；B.检索模型；C.排序结果。\n\n实际上，产品的数量是巨大的；如果我们对每个产品进行预测，返回用户请求将花费太长时间。一种常见的方法是将推荐列表分为过滤和排序两步。过滤就是从海量产品池(几百万)中挑选出少数(几百)个产品。分拣是对过滤后的少数产品进行评分。为了提供更多样化的推荐，在排序步骤之后，还需要第三步重新排序和过滤。\n\n(5) A/B测试：是任何基于web的应用程序的必要模块。它可以帮助开发人员评估算法如何影响用户行为。除了离线获得的指标外，在新算法上线之前，通常还会进行A/B测试来测试其有效性。\n\n#### 2.online training\n\n出于业务的考虑，我们希望利用用户对上一个产品的反应(喜欢或不喜欢，点击或不点击)的反馈来推荐下一个产品。这需要在线培训。 \n\n在线培训推荐系统可用于广告、电子商务等领域，这些领域的数据通常是高维的，需要即时响应。与offline training系统相比，online training系统不再需要将训练(离线)和预测(在线)分开。当用户每次提供反馈时，系统都会进行学习，并及时调整策略。一方面，在线系统要求样本、特征和模型及时更新，以便更快地提供用户感兴趣的推荐；另一方面，在线系统不需要系统保存所有用于训练的用户数据，降低了巨大的存储成本。这使得系统更加灵活，可以接收更大的模型和/或更大的数据量。\n\n(1)数据收集：离线时，将数据收集到分布式文件系统中，等待离线计算任务。在线情况下，需要立即进行数据采集，包括过滤、去重、采样等。 \n\n(2)即时特征：该模块主要进行特征工程和组织，为流式计算模块做准备 \n\n(3)流式计算：该模块根据即时特征输入更新模型。它支持模型的稀疏存储。模型训练可以初始化为离线训练的模型。 \n\n(4)模型上传：模型通常先保存在服务器中，然后再上传到模块中在线存储\n","tags":["刷题"]},{"title":"面试题目——《CC150》数学与概率","url":"/面试题目——《CC150》数学与概率.html","content":"**面试题7.2：三角形的三个顶点上各有一只蚂蚁。如果蚂蚁开始沿着三角形的边爬行，两只或三只蚂蚁撞到一起的概率有多大？假定每只蚂蚁会随机选一个方向，每个方向被选到的几率相等，而且三只蚂蚁的爬行速度相同。**\n\n```\npackage cc150.intelligence;\n\npublic class Ants {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tAnts at = new Ants();\n\t\tSystem.out.println(at.antsCollision(1));\n\t}\n\t\n\tpublic double antsCollision(int n) {\n        // write code here\n\t\tif(n < 3 || n > 10000)\n\t\t\treturn 1;\n\t\treturn 1-Math.pow(0.5, (n-1));\n    }\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n\n**面试题7.3：给定直角坐标系上的两条线，确定这两条线会不会相交。**\n\n```\npackage cc150.intelligence;\n\npublic class CrossLine {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tCrossLine cl = new CrossLine();\n\t\tSystem.out.println(cl.checkCrossLine(0.48900,0.48900,0.32700,0.32700));\n\t}\n\t\n\tpublic boolean checkCrossLine(double s1, double s2, double y1, double y2) {\n        // write code here\n\t\tdouble abs=1e-6;\n\t\tif(Math.abs(s1-s2)<abs &amp;&amp; Math.abs(y1-y2)>abs)\t//斜率相同，截距不同\n\t\t\treturn false;\n\t\treturn true;\n    }\n\n}\n\n```\n\n&nbsp;\n\n**面试题7.4：编写方法，实现整数的乘法、减法和除法运算。只允许使用加号。**\n\n```\npackage cc150.intelligence;\n\npublic class AddSubstitution {\n\n\tpublic static void main(String[] args) {\t\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t}\n\t\n\tpublic int calc(int a, int b, int type) {\t//1是乘法，0是除法，-1是减法\n        // write code here\n\t\tif(type == 1)\n\t\t\treturn multiply(a,b);\n\t\telse if(type == 0)\n\t\t\treturn divide(a,b);\n\t\telse if(type == -1)\n\t\t\treturn minus(a,b);\n\t\telse\n\t\t\treturn 0;\n    }\n\t\n\tpublic int negate(int a){\t//乘以-1，负号变正，正号变负\n\t\tint neg = 0;\n\t\tint sym = a > 0 ? -1: 1;\t\t//判断正负\n\t\twhile(a != 0){\n\t\t\ta += sym;\t\t//a为正，sym为负；a为负，sym为正，相加直到0\n\t\t\tneg += sym;\n\t\t}\n\t\treturn neg;\n\t}\n\t\n\tpublic int minus(int a,int b){\t//只使用加法的减法，即连续加b次正1或者负1\n\t\treturn a + negate(b);\n\t}\n\t\n\tpublic int multiply(int a,int b){\t//只使用加法的乘法，b个a相加\n\t\tif(a < b)\n\t\t\treturn multiply(b,a);\n\t\tint sum = 0;\n\t\tfor(int i=Math.abs(b);i>0;i--)\n\t\t\tsum += a;\n\t\tif(b < 0)\n\t\t\tsum = negate(sum);\n\t\treturn sum;\n\t}\n\t\n\tpublic int divide(int a,int b) throws java.lang.ArithmeticException{\t//只使用加法的除法\n\t\tif(b == 0){\n\t\t\tthrow new java.lang.ArithmeticException(\"ERROR\");\n\t\t}\n\t\tint absa = Math.abs(a);\t\t//先不考虑正负的问题\n\t\tint absb = Math.abs(b);\n\t\tint result = 0;\n\t\tint count = 0;\n\t\twhile(result + absb <= absa){\t//循环加absb，直到超过absa，计数加了几次\n\t\t\tresult += absb;\n\t\t\tcount++;\n\t\t}\n\t\tif((a < 0 &amp;&amp; b < 0) || (a > 0 &amp;&amp; b > 0))\n\t\t\treturn count;\n\t\telse\n\t\t\treturn negate(count);\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题7.5：在二维平面上，有两个正方形，请找出一条直线，能够将这两个正方形对半分。假定正方形的上下两条边与x轴平行。**\n\n```\npackage cc150.intelligence;\n\npublic class Bipartition {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tBipartition bp = new Bipartition();\n\t\tPoint[] A = {new Point(136,6278),new Point(3958,6278),new Point(3958,2456),new Point(136,2456)};\n\t\tPoint[] B = {new Point(-3898,11132),new Point(7238,11132),new Point(7238,-4),new Point(-3898,-4)};\n\t\t\n\t\tSystem.out.println(\"\"+bp.getBipartition(A,B)[0]+\",\"+bp.getBipartition(A,B)[1]);\n\t}\n\t\n\tpublic double[] getBipartition(Point[] A, Point[] B) {\n        // write code here\n\t\tdouble a_x = getCenter(A)[0];\t//正方形Ａ的中点坐标\n\t\tdouble a_y = getCenter(A)[1];\n\t\tdouble b_x = getCenter(B)[0];\t//正方形B的中点坐标\n\t\tdouble b_y = getCenter(B)[1];\n\t\t\n\t\tdouble[] result = new double[2];\n\t\tresult[0] = (a_y-b_y)/(a_x-b_x);\n\t\tdouble min = 10e-6;\n\t\tif(Math.abs(result[0]) < min)\n\t\t\tresult[0] = 0;\n\t\tresult[1] =a_y - result[0] * a_x;\n\t\treturn result;\n    }\n\t\n\tpublic double[] getCenter(Point[] p){\t//返回一个正方形的中点坐标\n\t\tdouble[] re= {(p[2].x+p[3].x)/2.0,(p[0].y+p[3].y)/2.0};\t\t//注意是double类型的，要除以2.0\n\t\treturn re;\n\t}\n\t\n\t\n\t\n\tpublic static class Point {\n\t    int x;\n\t    int y;\n\t    public Point(int x, int y) {\n\t        this.x = x;\n\t        this.y = y;\n\t    }\n\t    public Point() {\n\t        this.x = 0;\n\t        this.y = 0;\n\t    }\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题7.6：在二维平面上，有一些点，请找出经过点数最多的那条线。**\n\n&nbsp;\n\n**面试题7.7：有些数的素因子只有3、5、7，请设计一个算法，找出其中第k个数。**\n\n```\npackage cc150.intelligence;\n\nimport java.util.LinkedList;\nimport java.util.Queue;\n\npublic class KthNumber {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tKthNumber kn = new KthNumber();\n\t\tSystem.out.println(kn.findKth(0));\n\t}\n\t\n\tpublic int findKth(int k) {\n        // write code here\n\t\tif(k < 0)\n\t\t\treturn 0;\n\t\tint val = 0;\t\t//存放3,5,7的倍数中的最小值\n\t\tQueue<Integer> queue3 = new LinkedList<Integer>();\t//存放3的倍数的队列\n\t\tQueue<Integer> queue5 = new LinkedList<Integer>();\n\t\tQueue<Integer> queue7 = new LinkedList<Integer>();\n\t\tqueue3.add(1);\t\t\t//一定要先放进一个1，否则v3为Integer.MAX_VALUE\n\t\t\n\t\tfor(int i=0;i<=k;i++){\n\t\t\tint v3 = queue3.size() > 0 ? queue3.peek():Integer.MAX_VALUE;\t//如果队列不为空的话，取得队列的头，即最小值\n\t\t\tint v5 = queue5.size() > 0 ? queue5.peek():Integer.MAX_VALUE;\n\t\t\tint v7 = queue7.size() > 0 ? queue7.peek():Integer.MAX_VALUE;\n\t\t\tval = Math.min(v3, Math.min(v5, v7));\n\t\t\t\n\t\t\tif(val == v3){\t//求出了最小值，原本队列的队首要移除\n\t\t\t\tqueue3.remove();\n\t\t\t\tqueue3.add(val * 3);\n\t\t\t\tqueue5.add(val * 5);\n\t\t\t}else if(val == v5){\n\t\t\t\tqueue5.remove();\n\t\t\t\tqueue5.add(val * 5);\n\t\t\t}else if(val == v7){\n\t\t\t\tqueue7.remove();\n\t\t\t}\n\t\t\tqueue7.add(val * 7);\n\t\t}\n\t\treturn val;\n    }\n\n}\n\n```\n\n&nbsp;\n","tags":["刷题"]},{"title":"面试题目——《CC150》递归与动态规划","url":"/面试题目——《CC150》递归与动态规划.html","content":"**<img src=\"/images/517519-20160929203400672-2116596367.png\" alt=\"\" />**\n\n<!--more-->\n&nbsp;\n\n<img src=\"/images/517519-20160929203644281-1456433191.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160929203709797-643533614.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160929203727875-1763126860.png\" alt=\"\" />\n\n&nbsp;\n\n**面试题9.1：有个小孩正在上楼梯，楼梯有n个台阶，小孩一次可以上1阶、2阶或者3阶。实现一个方法，计算小孩有多少种上楼梯的方式。**\n\n**　　思路：第4个数是前三个数之和**\n\n**　　注意：能不能使用递归，能不能建立一个很大的数组来存储传递的参数（因为可能有空间的限制），要%1000000007防止超出范围**\n\n```\npackage cc150.recursion_dp;\n\npublic class GoUpstairs {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tGoUpstairs gu = new GoUpstairs(); \n\t\tSystem.out.println(gu.countWays(4));\n\n\t}\n\t\n\tpublic int countWays(int n) {\n        int index1 = 1;\n        int index2 = 2;\n        int index3 = 4;\n        int sum = 0;\n        if(n == 1)\n            return index1;\n        else if(n == 2)\n            return index2;\n        else if(n == 3)\n            return index3;\n        else{\n            while(n-- >= 4){\t\t\t//规律是第4个数是前三个数之和\n                sum = ((index1 + index2)%1000000007 + index3)%1000000007;\t\n                index1 = index2;\n                index2 = index3;\n                index3 = sum;\n            }\n            return sum;\n        }\n    }\n\t\n//\tpublic int countWays(int n) {\n//        // write code here\n//        if(n < 0)\n//        \treturn 0;\n//        else if(n == 0)\n//        \treturn 1;\n//        else\n//        \treturn countWays(n-3) + countWays(n-2) + countWays(n-1);\n//    }\n\t\n//\tpublic int countWays(int n,int[] map) {\t//使用动态规划\n//        // write code here\n//        if(n < 0)\n//        \treturn 0;\n//        else if(n == 0)\n//        \treturn 1;\n//        else if(map[n] > 0)\n//        \treturn map[n] % 1000000007;\n//        else{\n//        \tmap[n] = countWays(n-1,map) + countWays(n-2,map) + countWays(n-3,map);\n//        \treturn map[n] % 1000000007;\n//        }\n//    }\n\t\n//\tpublic int[] map = new int[100000];//使用动态规划，有空间限制32768K，不能到100000\n//    public int countWays(int n) {\n//        // write code here\n//        if(n < 0)\n//        \treturn 0;\n//        else if(n == 0)\n//        \treturn 1;\n//        else if(map[n] > 0)\n//        \treturn map[n] % 1000000007;\n//        else{\n//        \tmap[n] = countWays(n-1) + countWays(n-2) + countWays(n-3);\n//        \treturn map[n] % 1000000007;\n//        }\n//    }\n\n}\n\n```\n\n&nbsp;\n\n**面试题9.2：设想有个机器人坐在X&times;Y网格的左上角，只能向右、向下移动。机器人从（0,0）到（X，Y）有多少种走法？**\n\n```\npackage cc150.recursion_dp;\n\npublic class Robot1 {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tRobot1 rb = new Robot1();\n\t\tSystem.out.println(rb.countWays(3, 3));\n\t}\n\t\n\tpublic int countWays(int x, int y)\n    {\n        if(x==0||y==0)return 0;\n        if(x==1||y==1)return 1;\n        return countWays(x-1,y)+countWays(x,y-1);\t//递归，把最后一步分解成两步\n    }\n\t\n//\tpublic int countWays(int x, int y) {\n//        // write code here\n//\t\tif(x == 1 || y ==1)\n//\t\t\treturn 1;\n//\t\tif(x > 1&amp;&amp; y > 1){\n//\t\t\tint sum = x + y -2;\n//\t\t\tint sum_jiecheng = sum;\n//\t\t\twhile(--sum >= 1){\n//\t\t\t\tsum_jiecheng *= sum;\n//\t\t\t}\n//\t\t\tx--;\n//\t\t\tint x_jiecheng = x;\n//\t\t\twhile(--x >= 1){\n//\t\t\t\tx_jiecheng *= x;\n//\t\t\t}\n//\t\t\ty--;\n//\t\t\tint y_jiecheng = y;\n//\t\t\twhile(--y >= 1){\n//\t\t\t\ty_jiecheng *= y;\n//\t\t\t}\n//\t\t\treturn (sum_jiecheng/x_jiecheng)/y_jiecheng;\n//\t\t}\n//\t\treturn 0;\n//    }\n\n}\n\n```\n\n&nbsp;\n\n**有障碍的机器人寻路**\n\n```\npackage cc150.recursion_dp;\n\npublic class Robot2 {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tRobot2 rb = new Robot2();\n\t\tint[][] a = {{0,1}};\n\t\tSystem.out.println(rb.countWays(a,2,2));\n\t}\n\t\n\tpublic int countWays(int[][] map, int x, int y) {\n        // write code here\n\t\tint[][] f = new int[x][y];\t\t//f记录的是到达这个f[x][y]的路径数量\n        for(int i=0;i<x;i++){\n            for(int j=0;j<y;j++){\n              if(map[i][j] != 1)\n            \t  f[i][j] = 0; \t\t\t\t// 不能走，就是方法数==0\n              else if(i==0 &amp;&amp; j==0)\n            \t  f[i][j] = 1; // 起点，1种走法\n              else if(i==0 &amp;&amp; j!=0)\n            \t  f[i][j] = f[i][j-1]; // 上边沿：只能从左边来\n              else if(i!=0 &amp;&amp; j==0)\n            \t  f[i][j] = f[i-1][j]; // 左边沿：只能从上边来\n              else \n            \t  f[i][j] = (f[i-1][j]+f[i][j-1]) % 1000000007; // 其他点：左边+上边\n            }\n        }\n        return f[x-1][y-1];\n    }\n\n}\n\n```\n\n&nbsp;\n\n**面试题9.3：在数组A[0...n-1]中，有所谓的魔术索引，满足条件A[i]=i。给定一个有序整数数组，元素值各不相同，编写一个方法，在数组A中找出一个魔术索引，若存在的话。**\n\n```\npackage cc150.recursion_dp;\n\npublic class MagicIndex {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\n\t}\n\t\n\t//二分查询法\n\tpublic boolean findMagicIndex(int[] A, int n) {\t//n为数组的大小\n        // write code here\n\t\tif(findMagic(A,0,n-1) == -1)\t\t//是n-1\n\t\t\treturn false;\n\t\telse\n\t\t\treturn true;\n    }\n\t\n\tpublic int findMagic(int[] A, int start,int end) {\t//n为数组的大小\n        // write code here\n\t\tif(start < 0 || end < start || end >= A.length)\n\t\t\treturn -1;\n\t\tint mid = (start+end) >> 1;\n\t\tif(A[mid] == mid)\n\t\t\treturn mid;\n\t\telse if(A[mid] > mid)\t//大于说明在只能左边\n\t\t\treturn findMagic(A,start,mid-1);\n\t\telse\n\t\t\treturn findMagic(A,mid+1,end);\n    }\n\t\n\t//暴力查询法\n\tpublic boolean findMagicIndex(int[] A, int n) {\t//n为数组的大小\n        // write code here\n\t\tfor(int i=0;i<n;i++){\n\t\t\tif(A[i] == i)\n\t\t\t\treturn true;\n\t\t}\n\t\treturn false;\n    }\n\n}\n\n```\n\n&nbsp;\n\n**如果数组中有重复的元素的情况**\n\n```\npackage cc150.recursion_dp;\n\npublic class MagicIndex {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\n\t}\n\t\n\t//如果数组中有重复的元素的情况\n\t//二分查询法\n\tpublic boolean findMagicIndex(int[] A, int n) {\t//n为数组的大小\n        // write code here\n\t\tif(findMagic(A,0,n-1) == -1)\t\t//是n-1\n\t\t\treturn false;\n\t\telse\n\t\t\treturn true;\n    }\n\t\n\t//数组中有重复元素的情况\n\tpublic int findMagic(int[] A, int start,int end) {\t//n为数组的大小\n        // write code here\n\t\tif(start < 0 || end < start || end >= A.length)\n\t\t\treturn -1;\n\t\tint midIndex = (start+end) >> 1;\n\t\tint midValue = A[midIndex];\n\t\tif(midValue == midIndex)\n\t\t\treturn midIndex;\n\t\t//有可能在左边，也有可能在右边\n\t\t//搜索左半部分\n\t\tint leftIndex = Math.min(midIndex-1,midValue);\t//比较下标减1和值的大小，较小的作为end，因为值和下标要相等\n\t\tint left = findMagic(A,start,leftIndex);\n\t\tif(left >= 0)\n\t\t\treturn left;\n\t\t//搜索右半部分\n\t\tint rightIndex = Math.max(midIndex+1,midValue);\t//较大的作为start，因为值和下标要相等\n\t\tint right = findMagic(A,rightIndex,end);\n\t\treturn right;\n    }\n\n}\n\n```\n\n&nbsp;\n\n**面试题9.4：编写一个方法，返回某集合的所有子集。**\n\n&nbsp;\n\n**面试题9.5：编写一个方法，确定某字符串的所有排列组合。**\n\n```\npackage cc150.recursion_dp;\n\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.Comparator;\nimport java.util.Iterator;\n\npublic class Subset {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tSubset ss = new Subset();\n\t\tint[] a = {1,2,3};\n\t\tArrayList<ArrayList<Integer>> arr = ss.getSubsets(a, 0);\n\t\tarr.remove(0);\n\t\tIterator ire = arr.iterator();\n\t\twhile(ire.hasNext())\n\t\t\tSystem.out.println(ire.next());\n\t}\n\t\n\t//输出的结果非字典逆序\n\tpublic ArrayList<ArrayList<Integer>> getSubsets(int[] A, int index) {\t//若n表示集合的大小，有2^n个子集\n        // write code here\n\t\tArrayList<ArrayList<Integer>> allsubsets;\n\t\tif(A.length == index){\t//如果index是达到length，就加上空集\n\t\t\tallsubsets = new ArrayList<ArrayList<Integer>>();\t//空集子集\n\t\t\tallsubsets.add(new ArrayList<Integer>());\n\t\t}else{\n\t\t\tallsubsets = getSubsets(A,index + 1);\t\t//直到等于length，加上空集后继续执行\n\t\t\tint item = A[index];\n\t\t\tArrayList<ArrayList<Integer>> moresubsets = new ArrayList<ArrayList<Integer>>();\n\t\t\tfor(ArrayList<Integer> subset : allsubsets){\t\t//遍历原来的子集，一个一个加上item后放入新的子集\n\t\t\t\tArrayList<Integer> newsubset = new ArrayList<Integer>();\n\t\t\t\tnewsubset.addAll(subset);\t\t\t//新的子集先放入原来的子集\n\t\t\t\tnewsubset.add(item);\t\t\t\t\t\t//新的子集放入新加入的A[index]，只有一个\n\t\t\t\tmoresubsets.add(newsubset);\t//把新的子集放入moresubsets中,比如空集加上3后是3，空集和3加上2后是2和3,2\n\t\t\t}\n\t\t\tallsubsets.addAll(moresubsets);\t\t//moresubsets用于在循环中存放加上新元素的子集，allsubsets用与存放每一次的moresubsets\n\t\t}\n\t\treturn allsubsets;\n    }\n\n}\n\n```\n\n&nbsp;\n\n&nbsp;\n\n**面试题9.6：实现一种算法，打印n对括号的全部有效组合（即左右括号正确匹配）。（牛客网里面是判断是否正确匹配）**\n\n```\nimport java.util.*;\n \npublic class Parenthesis {\n    public boolean chkParenthesis(String s, int n) {\n        // write code here\n        int stackSize = s.length();\n        int count = 0;\n        Stack<Character> theStack = new Stack<Character>();\n        for(int i=0;i<s.length();i++){\n            char ch = s.charAt(i);          //遍历每一个字符\n            switch(ch){\n                case '{':             \n                case '[':             \n                case '(':             \n                    theStack.push(ch);            //遇到'{[('就入栈\n                    count++;\n                    break;\n                      \n                case '}':\n                case ']':\n                case ')':\n                    count++;\n                    if( !theStack.isEmpty()){\n                         char chx = theStack.pop();       //遇到'}])'弹出堆栈\n                         if(    (chx=='{' &amp;&amp; ch!='}')   ||  (chx=='[' &amp;&amp; ch!=']')   ||  (chx=='(' &amp;&amp; ch!=')')){\n                             return false;\n                         }\n                    }  \n                    else{\n                         return false;\n                    }\n                    break;\n                    default:break;\n            }\n        }\n        if(count != stackSize)\n            return false;\n        if( !theStack.isEmpty()){             //如果栈不为空的话，证明缺少右括号\n            return false;\n        }\n        return true;\n    }\n}\n\n```\n\n&nbsp;\n\n**面试题9.7：编写函数，实现许多图片编辑软件都支持的&ldquo;填充颜色&rdquo;功能。给定一个屏幕（以二维数组表示，元素为颜色值）、一个点和一个新的颜色值，将新颜色值填入这个点的周围区域，直到原来的颜色值全都改变。**\n\n```\npackage cc150.recursion_dp;\n\npublic class PaintFill {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tPaintFill pf = new PaintFill();\n\t\tColor[][] cl = {{Color.Black,Color.Black,Color.Black},{Color.Black,Color.White,Color.Black},{Color.Black,Color.Black,Color.Black}};\n\t\tpf.paintFill(cl,1,1,Color.White,Color.Green);\n\t\tfor(int i=0;i<cl.length;i++){\n\t\t\tfor(int j=0;j<cl[0].length;j++){\n\t\t\t\tSystem.out.print(cl[i][j]);\n\t\t\t}\n\t\t\tSystem.out.println();\n\t\t}\n\t}\n\t\n\t//枚举\n\tenum Color{\n\t\tBlack,White,Red,Yellow,Green;\n\t}\n\t\n\t//x,y表示填充的坐标，ocolor表示原来的颜色，ncolor表示现在的颜色\n\tpublic boolean paintFill(Color[][] screen,int x,int y,Color ocolor,Color ncolor){//x是横坐标，screen[0].length\n\t\tif(x < 0 || x >= screen[0].length || y < 0 || y > screen.length)\n\t\t\treturn false;\n\t\tif(screen[y][x] == ocolor){\t//只有颜色等于原来的颜色的点才填充\n\t\t\tscreen[y][x] = ncolor;\n\t\t\tpaintFill(screen,x-1,y,ocolor,ncolor);\n\t\t\tpaintFill(screen,x+1,y,ocolor,ncolor);\n\t\t\tpaintFill(screen,x,y-1,ocolor,ncolor);\n\t\t\tpaintFill(screen,x,y+1,ocolor,ncolor);\n\t\t}\n\t\treturn true;\n\t}\n\t\n\tpublic boolean paintFill(Color[][] screen,int x,int y,Color ncolor){\n\t\tif(screen[y][x] == ncolor)\n\t\t\treturn false;\n\t\treturn paintFill(screen,x,y,screen[y][x],ncolor);\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题9.8：给定数量不限的硬币，币值为25分，10分，5分和1分，编写代码计算n分有几种表示法。**\n\n```\npackage cc150.recursion_dp;\n\npublic class MakeChange {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tMakeChange mc = new MakeChange();\n\t\tSystem.out.println(mc.countWays(100000));\n\t}\n\t\n\t//二维dp\n//\tpublic int countWays(int n) {\n//\t    int A[] = {1, 5, 10, 25}, dp[][] = new int[A.length][n + 1];\n//\t    for (int j = 0; j <= n; j++) {\n//\t        dp[0][j] = 1;\n//\t    }\n//\t    for (int i = 1; i < A.length; i++) {\n//\t        for (int j = 0; j <= n; j++) {\n//\t            int t = j - A[i];\n//\t            if (t >= 0) {\n//\t                dp[i][j] = (dp[i - 1][j] + dp[i][t]) % 1000000007;\n//\t            } else {\n//\t                dp[i][j] = dp[i - 1][j];\n//\t            }\n//\t        }\n//\t    }\n//\t    return dp[A.length - 1][n];\n//\t}\n\t\n\t//一维dp，递归求每次减去1,5,10,25后剩下的次数\n\tpublic int countWays(int n) {\n\t    int dp[] = new int[n + 1], i, A[] = {1, 5, 10, 25};\t//dp数组中存储的是组合的总数\n\t    for (i = 0, dp[0] = 1; i < A.length; i++) {\t\t//在{1,5,10,25}中遍历；当j=A[1]=5的时候会计算两次，1增加到5的时候一次，5到5的时候一次\n\t    \tfor (int j = A[i]; j <= n; j++) {\t\t\t\t\t\t//在j小于n的条件下，求1,5,10,25到n中每一个的可能性\n\t            dp[j] = (dp[j] + dp[j - A[i]]) % 1000000007;\t\t//j-A[i]是在已经选择了A[i]的情况下，求剩下的可能性\n\t        }\n\t    }\n\t    return dp[n];\n\t}\n\t\n\t//很慢，会超时\n\tpublic int makeChange(int n,int denom){\n\t\tint next_denom = 0;\n\t\tswitch(denom){\n\t\t\tcase 25:\n\t\t\t\tnext_denom = 10;\n\t\t\t\tbreak;\n\t\t\tcase 10:\n\t\t\t\tnext_denom = 5;\n\t\t\t\tbreak;\n\t\t\tcase 5:\n\t\t\t\tnext_denom = 1;\n\t\t\t\tbreak;\n\t\t\tcase 1:\t\t\t\t//如果到最后返回1，表示有1种方法\n\t\t\t\treturn 1;\n\t\t}\n\t\tint ways = 0;\n\t\tfor(int i=0;i*denom <= n;i++){\n\t\t\tways += makeChange(n-i*denom,next_denom)%1000000007;\t\t//返回的方法的总数的和\n\t\t}\n\t\treturn ways%1000000007;\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题9.9：设计一种算法，打印八皇后在8&times;8棋盘上的各种摆法，其中每个皇后都不同行、不同列，也不在对角线上。这里的&ldquo;对角线&rdquo;指的是所有的对角线，不只是平分整个棋盘的那两条对角线。**\n\n```\npackage cc150.recursion_dp;\n\nimport java.util.ArrayList;\n\npublic class Queens {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tQueens q = new Queens();\n\t\tSystem.out.println(q.nQueens(8));\n\t\t}\n\n\tpublic int nQueens(int row){\n\t\tint[] arr = new int[row];\n\t\tArrayList<int[]> list = new ArrayList<int[]>();\n\t\treturn placeQueens(0,arr,list,row);\n\t}\n\t\n\tint count=0;\t\t//计数\n\t\n\tpublic int placeQueens(int row,int[] columns,ArrayList<int[]> results,int size){//理解的时候画一个3&times;3的矩阵理解\n\t\tif(row == size){\n\t\t\tcount++;\n\t\t\t//results.add(columns.clone());\n\t\t}\n\t\telse{\n\t\t\tfor(int col=0;col<size;col++){\t\t\t\t//递归后行数递增，这里for循环检查每一列，然后再返回上一层递归中行数继续增加\n\t\t\t\tif(checkValid(columns,row,col)){\t\t//从左向右检查每一列，检查同一列，对角线有没有其他皇后\n\t\t\t\t\tcolumns[row] = col;\t\t\t\t\t\t\t//columns的下标表示行，值表示列\n\t\t\t\t\tplaceQueens(row+1,columns,results,size);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn count;\n\t}\n\t\n\tpublic boolean checkValid(int[] columns,int row1,int column1){\t//columns表示一列，检查有无其他皇后在同一列，columns有可能包含column1\n\t\tfor(int row2 = 0;row2<row1;row2++){\n\t\t\tint column2 = columns[row2];\t//row2,column2的元素\n\t\t\t//检查row2,column2是否会让row1,column1变成无效\n\t\t\tif(column1 == column2)\n\t\t\t\treturn false;\n\t\t\t//检查对角线\n\t\t\tint columnDistance = Math.abs(column2-column1);\t\n\t\t\tint rowDistance = row1-row2;\t\t\t\t\t//row1只可能大于row2\n\t\t\tif(columnDistance == rowDistance)\n\t\t\t\treturn false;\n\t\t}\n\t\treturn true;\n\t}\n\t\n}\n\n```\n\n&nbsp;\n","tags":["刷题"]},{"title":"面试题目——《CC150》智力题","url":"/面试题目——《CC150》智力题.html","content":"**面试题6.1：有20瓶药丸，其中19瓶装有1克/粒的药丸，余下一瓶装有1.1克/粒的药丸。给你一台称重精准的天平，怎么找出比较重的那瓶药丸？天平只能用一次。**\n\n　　思路：第1瓶取1颗，第2瓶取2颗。。。。最后是（总重-210克）/0.1=第几瓶\n\n<!--more-->\n&nbsp;\n\n**面试题6.2：有8&times;8棋盘，其中对角的角落上，两个方格被切掉了。给定31块多米诺骨牌，一块骨牌恰好可以覆盖两个方格。用这31块骨牌能否盖住整个棋盘？**\n\n　　思路：假设棋盘是黑白相间的，则两个对角的颜色是一样的，则有30个黑色和32个白色，但是31块骨牌必定是盖住31块白色和31块黑色，所以不可能。\n\n&nbsp;\n\n**面试题6.3：有两个水壶，容量分别为5夸脱和3夸脱，若水的供应不限量（但没有量杯），怎么用这两个水壶得到刚好4夸脱的水？注意，这两个水壶呈不规则形状，无法精准地装满&ldquo;半壶&rdquo;水。**\n\n　　思路：1.先装满5夸脱的水壶，倒入3夸脱的水壶中，得到2夸脱的水\n\n　　　　　2.倒掉3夸脱的水，并把2夸脱的水倒入3夸脱的水壶中\n\n　　　　　3.再次装满5夸脱的水壶，倒入装有2夸脱水的3夸脱水壶到满，得到4夸脱的水。\n\n&nbsp;\n\n<img src=\"/images/517519-20160922160137496-752308569.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160922160249543-170727470.png\" alt=\"\" />\n\n&nbsp;\n\n**面试题6.5：有栋建筑物高100层。若从第N层或更高的楼层扔下来，鸡蛋就会破掉。若从第N层以下的楼层扔下来则不会破掉。给你2个鸡蛋，请找出N，并要求最差情况下扔鸡蛋的次数为最少。**\n\n**　　思路；考虑不管哪种扔法，最后的最差结果要是稳定的。**\n\n**　　　　　设第一次从x层扔下来，最差要扔1+x-2次，比如第一个在3层扔，破了，第2个从最底下开始扔，只需扔1次，结果为2**\n\n**　　　　　设第二次从y层扔下来，最差要扔1+1+y-2=x-1,得y=x-1，所以x+(x-1)+(x-2)+...+1=100，得x=14**\n\n**<img src=\"/images/517519-20160922164044199-303995872.png\" alt=\"\" />**\n\n&nbsp;\n\n&nbsp;\n\n6.6思路：每个数分解成两个数的乘机之后，会产生两个数（2*1=2）或者1个数（3*3=9）,加上数到本身的时候会翻转一次，所以总共翻转的次数要是偶数才能是开的状态，比如1,4,9,16....\n\n<img src=\"/images/517519-20160922164839762-765305402.png\" alt=\"\" />\n\n&nbsp;\n","tags":["刷题"]},{"title":"面试题目——《CC150》位操作","url":"/面试题目——《CC150》位操作.html","content":"<img src=\"/images/517519-20160920153108637-324670319.png\" alt=\"\" width=\"625\" height=\"371\" />\n\n<img src=\"/images/517519-20160920153216902-1044936400.png\" alt=\"\" width=\"636\" height=\"423\" />\n\n<img src=\"/images/517519-20160920153324215-1626417888.png\" alt=\"\" width=\"636\" height=\"455\" />\n\n<!--more-->\n&nbsp;\n\n**面试题5.1：给定两个32位的整数N与M，以及表示比特位置的i与j。编写一个方法，将M插入N，使得M从N的第j位开始，到第i位结束。假定从j位到i位足以容纳M，也即若M=10011，那么j与i之间至少可容纳5个位。例如，不可能出现j=3和i=2的情况，因为第3位和第2位之间放不下M。**\n\n**　　输入：N=10000000000，M=10011，i=2,j=6，最后是第0位**\n\n**　　输出：N=10001001100**\n\n```\npackage cc150.bit;\n\npublic class UpdateBits {\n\n\t//面试题5.1：给定两个32位的整数N与M，以及表示比特位置的i与j。\n\t//编写一个方法，将M插入N，使得M从N的第j位开始，到第i位结束\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tUpdateBits ub = new UpdateBits();\n\t\tSystem.out.println(ub.updateBits(1,1,2,4));\n\t}\n\t\n\tpublic int updateBits(int n,int m,int i,int j){\t//M插入N,i为右边第i位，j为右边第j位\n\t\tint allOnes = ~0;\t\t//一串1\t,~0连符号位都是１\n\t\t int left = allOnes << (j+1);\n\t\t int right = (1 << i) -1;\n\t\t int mask = left | right;\t\t//置零了从第i位到第j位\n\t\t int n_clear = n &amp; mask;\t//把n置零\n\t\t int m_shifted = m << i;\t//把ｍ左移动i位对其\n\t\t //return Integer.toBinaryString(mask);\n\t\t return n_clear | m_shifted;\t\t\t//返回合并后的值\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题5.2: 给定一个介于0和1之间的实数（如0.72），类型为double，打印它的二进制表示。如果该数字无法精确地用32位以内的二进制表示，则打印&ldquo;ERROR&rdquo;。**\n\n```\npackage cc150.bit;\n\npublic class BinDecimal {\n\n\t//面试题5.2: 给定一个介于0和1之间的实数（如0.72），类型为double，打印它的二进制表示。\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tBinDecimal bd = new BinDecimal();\n\t\tSystem.out.println(bd.printBin(0.625));\t\t//0.5+0.125\n\t}\n\n\tpublic String printBin(double num) {\n        // write code here\n\t\tif(num >= 1 || num <= 0)\n\t\t\treturn \"ERROR\";\n\t\tStringBuffer bf = new StringBuffer();\n\t\twhile(num > 0){\n\t\t\tif(bf.length() >= 32)\n\t\t\t\treturn \"ERROR\";\n\t\t\tdouble result = num * 2;\t//在循环中不断&times;2，如果大于1就减去1\n\t\t\tif(result >=1){\n\t\t\t\tbf.append(1);\n\t\t\t\tnum = result - 1;\n\t\t\t}else{\t\t\t\t\t\t//如果没有大于1，就把result赋值给num，继续&times;2\n\t\t\t\tbf.append(0);\n\t\t\t\tnum = result;\n\t\t\t}\n\t\t}\n\t\treturn \"0.\"+bf.toString();\n    }\n}\n\n```\n\n&nbsp;\n\n**面试题5.3：给定一个正整数，找出与其二进制表示中1的个数相同、且大小最接近的那两个数（一个略大，一个略小）。**\n\n```\npackage cc150.bit;\n\npublic class CloseNumber {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tCloseNumber cn = new CloseNumber();\n\t\tSystem.out.println(Integer.toBinaryString(cn.getPre(13948)));\n\t\tSystem.out.println(Integer.toBinaryString(13948));\n\t}\n\t\n\tpublic int[] getCloseNumber(int x) {\n        // write code here\n\t\tint[] result = null;\n\t\tint a = getPre(x);\n\t\tint b = getNext(x);\n\t\tif(a != -1 &amp;&amp; b != -1){\n\t\t\tresult = new int[2];\n\t\t\tresult[0] = a;\n\t\t\tresult[1] = b;\n\t\t\treturn result;\n\t\t}\n\t\telse if(a != -1 &amp;&amp; b == -1){\n\t\t\tresult = new int[1];\n\t\t\tresult[0] = a;\n\t\t\treturn result;\n\t\t}\n\t\telse if(a == -1 &amp;&amp; b != -1){\n\t\t\tresult = new int[1];\n\t\t\tresult[0] = b;\n\t\t\treturn result;\n\t\t}\n\t\treturn result;\n    }\n\t\n\tpublic int getNext(int n){\t\t\n\t\tint c = n;\n\t\tint c0 = 0;\t\t\t//计算最右边的1的位置，都是从0开始计数\n\t\tint c1 = 0;\t\t\t//计算非尾部的最右边的0的位置，是去掉尾部的0之后的位置\n\t\twhile(((c &amp; 1) == 0) &amp;&amp; (c != 0)){\t//当最后一位是1或者c等于0的时候停止\n\t\t\tc0++;\n\t\t\tc >>= 1;\t\t\t\t//右移并用1填充\n\t\t}\n\t\t\n\t\twhile((c &amp; 1) == 1){\t//因为上一步已经把最右边所有的零去掉，所以这里计算最右边的0的位置\n\t\t\tc1++;\n\t\t\tc >>= 1;\n\t\t}\n\n\t\tif(c0 + c1 == 31 || c0 + c1 == 0)\n\t\t\treturn -1;\n\t\t\n\t\tint p = c0+c1;\t\t//计算非尾部的最右边的0的位置\n\t\t\n\t\tn |= (1 << p);\t\t//把非尾部的最右边的0翻转成1，计算大于的那个值\n\t\tn &amp;= ~((1 << p) - 1);\t\t//将p右边的所有位清0\n\t\tn |= (1 << (c1-1)) - 1;\t\t//在右边插入(c1-1)个1\n\t\t\n\t\treturn n;\n\t}\n\t\n\tpublic int getPre(int n){\n\t\tint temp = n;\n\t\tint c0 = 0;\t\t//计算最右边1的位置，是去掉尾部的1之后的位置\n\t\tint c1 = 0;\t\t//计算最右边0的位置\n\t\tSystem.out.println(\"n=\"+Integer.toBinaryString(n));\n\t\t\n\t\twhile((temp &amp; 1) == 1){\t//计算最右边0的位置\n\t\t\tc1++;\n\t\t\ttemp >>= 1;\n\t\t}\n\t\tSystem.out.println(\"c1=\"+Integer.toBinaryString(c1));\n\t\t\n\t\tif(temp == 0)\n\t\t\treturn -1;\n\t\t\n\t\twhile(((temp &amp; 1) == 0) &amp;&amp; (temp != 0)){//当最后一位是1或者c等于0的时候停止\n\t\t\tc0++;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//计算最右边1的位置，去掉了尾部的1之后的位置\n\t\t\ttemp >>= 1;\n\t\t}\n\t\tSystem.out.println(\"c0=\"+Integer.toBinaryString(c0));\n\t\t\n\t\tint p = c0+c1;\t\t\t\t\t\t//计算最右边1的位置\n\t\tn &amp;= ((~0) << (p+1));\t\t\t//将p右边的所有位清0（包括p），p是非尾部的最右边的1\n\t\tSystem.out.println(\"清零后：\"+Integer.toBinaryString(n));\n\t\t\n\t\tint mask = (1 << (c1 + 1)) - 1;\t\t//c1+1个1，清零之后，需要补充c1+1个1\n\t\tSystem.out.println(\"补充1的个数：\"+Integer.toBinaryString(mask));\n\t\t\n\t\tn |= (mask << (c0 - 1));\t\t\t\t\t\t//需要补充的1多大能是多少，然后和清零后的合并\n\t\tSystem.out.println(\"需要补充的1多大能是多少：\"+Integer.toBinaryString(mask << (c0 - 1)));\n\t\t\n\t\treturn (n);\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题5.4：解释代码&nbsp; ((n&amp;(n-1)) == 0) 的具体含义。**\n\n**<img src=\"/images/517519-20160921133908371-2022976814.png\" alt=\"\" width=\"610\" height=\"425\" />**\n\n<img src=\"/images/517519-20160921133935340-2077186008.png\" alt=\"\" width=\"609\" height=\"111\" />\n\n&nbsp;\n\n**面试题5.5：编写一个函数，确定需要改变几个位，才能将整数A转成整数B。**\n\n```\npackage cc150.bit;\n\npublic class Transform {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tTransform tf = new Transform();\n\t\tSystem.out.println(tf.calcCost(2,1));\n\t}\n\t\n\tpublic int calcCost(int A, int B) {\n        // write code here\n        int count = 0;\n        int c = A ^ B;\n        for(int i=c;i != 0;i=i >> 1)\n            count += i &amp; 1;\n        return count;\n    }\n\n}\n\n```\n\n&nbsp;\n\n**面试题5.6：编写程序，交换某个整数的奇数位和偶数位，使用指令越少越好（也就是说，位0与位1交换，位2与位3交换，依次类推）。**\n\n```\npackage cc150.bit;\n\npublic class Exchange {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tExchange ec = new Exchange();\n\t\tSystem.out.println(ec.exchangeOddEven(2));\n\t}\n\t\n\tpublic int exchangeOddEven(int x) {\n\t\t// write code here\n\t\treturn ((x &amp; 0xaaaaaaaa) >> 1) | ((x &amp; 0x55555555) << 1);\t//0xaaaaaaaa是提取1010，0x55555555是提取0101\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//1010右移，0101左移\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题5.7：数组A包含0到n的所有整数，但其中缺了一个。在这个问题中，只用一次操作无法取得数组A里某个整数的完整内容。此外，数组A的元素皆以二进制表示，唯一可用的访问操作是&ldquo;从A[i]取出第j位数据&rdquo;，该操作的时间复杂度为常数。请编写代码找出那个缺失的整数。你有办法在O(n)时间内完成吗？**\n\n```\npackage cc150.bit;\n\npublic class Finder {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tint[][] a = {{0},{1,0},{0,1},{0,0,1},{1,0,1}};\t//少了1,1\n\t\tFinder fd = new Finder();\n\t\tSystem.out.println(fd.findMissing(a,5));\n\t}\n\t\n\tpublic int findMissing(int[][] numbers, int n) {\n        // write code here\n\t\t  for(int i = 0; i < n; i ++)\t\t\t\t\t//注意数组的长度只有n-1\n\t            if(i % 2 != numbers[i][0]){\t\t//按顺序必定是最后一位奇偶交替\n\t            \treturn i;\n\t            }\n\t        return n;\n    }\n\n}\n\n```\n\n&nbsp;\n\n**面试题5.8：有个单色屏幕存储在一个一维字节数组中，使得8个连续像素可以存放在一个字节里。屏幕宽度为w，且w可被8整除（即一个字节不会分布在两行上），屏幕高度可由数组长度及屏幕宽度推算出。请实现一个函数drawHorizontalLine(byte[] screen,int width,int x1,int x2,int y)，绘制从点（x1,y）到点（x2,y）的水平线。**\n\n```\npackage cc150.bit;\n\npublic class Render {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tint[] a = {0,0,0,0,0,0};\n\t\tRender rd = new Render();\n\t\tfor(int i=0;i<a.length;i++)\n\t\t\tSystem.out.println(rd.renderPixel(a, 0, 47)[i]);\n\t}\n\t\n\tpublic int[] renderPixel(int[] screen, int x, int y) {\n        // write code here\n\t\tfor(int i = x; i < y + 1; ++i){\t\t\t\n            screen[i / 8] |= 1 << (i % 8);\t\t//screen中一个元素代表8位，i%8求出每8位中哪一位是1，并左移到相应位置准备合并\n        }\n        return screen;\n    }\n\n}\n\n```\n\n&nbsp;\n","tags":["刷题"]},{"title":"面试题目——《CC150》树与图","url":"/面试题目——《CC150》树与图.html","content":"<img src=\"/images/517519-20160916212844211-52365746.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160916212928555-798703200.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160916212957852-829490415.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n**面试题4.1：实现一个函数，检查二叉树是否平衡。在这个问题中，平衡树的定义如下：任意一个结点，其两颗子树的高度差不超过1。**\n\n**　　思路：两个方法，第一种速度较快**\n\n```\npackage cc150;\n\npublic class Balance {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\n\t}\n\t\n\tpublic class TreeNode {\n\t    int val = 0;\n\t    TreeNode left = null;\n\t    TreeNode right = null;\n\t    public TreeNode(int val) {\n\t        this.val = val;\n\t    }\n\t}\n\t\n\tpublic static boolean isBalance(TreeNode root){\n\t\tif(checkHeight(root) == -1)\n\t\t\treturn false;\n\t\telse\n\t\t\treturn true;\n\t}\n\t\n\tpublic static int checkHeight(TreeNode root){\n\t\tif(root == null)\n\t\t\treturn 0;\n\t\t\n\t\t//检查左子树是否平衡\n\t\tint leftHeight = checkHeight(root.left);\n\t\tif(leftHeight == -1)\n\t\t\treturn -1;\n\t\t\n\t\t//检查右子树是否平衡\n\t\tint rightHeight = checkHeight(root.right);\n\t\tif(rightHeight == -1)\n\t\t\treturn -1;\n\t\t\n\t\t//检查当前结点是否平衡\n\t\tint heightDiff = leftHeight - rightHeight;\n\t\tif(Math.abs(heightDiff) > 1)\n\t\t\treturn -1;\n\t\telse\n\t\t\treturn Math.max(leftHeight, rightHeight) + 1;//返回当前结点的高度\n\t}\n\t\n//\tpublic boolean isBalance(TreeNode root) {\n//        // write code here\n//        int countLeft = 0;\n//        TreeNode temp=root;\n//        if(root == null){\n//            return true;\n//        }\n//        if(root.left==null &amp;&amp; root.right==null){ \n//            return true; \n//        } \n//        if(Math.abs(treeDepth(root.left)-treeDepth(root.right)) > 1){ \n//            return false; \n//        }\n//        return isBalance(root.left) &amp;&amp; isBalance(root.right);\n//         \n//    }\n//     \n//    public int treeDepth(TreeNode root){\n//        if(root == null)  //如果pRoot为NULL，则深度为0，这也是递归的返回条件\n//            return 0;\n//        //如果pRoot不为NULL，那么深度至少为1，所以left和right=1\n//        int left=1;\n//        int right=1;\n//        left += treeDepth(root.left);   //求出左子树的深度\n//        right += treeDepth(root.right); //求出右子树深度\n//         \n//        return (left>right?left:right); //返回深度较大的那一个\n//    }\n\n}\n\n```\n\n&nbsp;\n\n**面试题4.2： 给定有向图，设计一个算法，找出两个结点之间是否存在一条路径。**\n\n**　　思路：解法中使用了递归以及深度遍历，并没有通过栈来优化空间的占用（参考牛客网更好的解法），注意图中可能存在环和反向的问题**\n\n```\npackage cc150;\n\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic class Path {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tUndirectedGraphNode a = new UndirectedGraphNode(1);\n\t\tUndirectedGraphNode a1 = new UndirectedGraphNode(2);\n\t\tUndirectedGraphNode a2 = new UndirectedGraphNode(3);\n\t\tUndirectedGraphNode a3 = new UndirectedGraphNode(4);\n\t\tUndirectedGraphNode a11 = new UndirectedGraphNode(5);\n\t\tUndirectedGraphNode a12 = new UndirectedGraphNode(6);\n\t\tUndirectedGraphNode a13 = new UndirectedGraphNode(7);\n\t\tUndirectedGraphNode b = new UndirectedGraphNode(8);\n\t\ta1.neighbors.add(a);\n\t\ta.neighbors.add(a2);\n\t\ta.neighbors.add(a3);\n\t\ta.neighbors.add(a1);\n\t\ta11.neighbors.add(a1);\n\t\ta1.neighbors.add(a12);\n\t\ta1.neighbors.add(a13);\n\t\tb.neighbors.add(a11);\n\t\tSystem.out.println(checkPath(a,b));\n\t}\n\t\n\tpublic static boolean checkPath(UndirectedGraphNode a, UndirectedGraphNode b) {\n\t\tmap.clear();\n\t\tboolean bool1 = checkPathCore(a,b);\n\t\tmap.clear();\n\t\tboolean bool2 = checkPathCore(b,a);\n\t\treturn bool1 || bool2;\n\t}\n\t\n\tpublic static Map<UndirectedGraphNode,Boolean> map = new HashMap<UndirectedGraphNode,Boolean>();\n\t\n\tpublic static boolean checkPathCore(UndirectedGraphNode a, UndirectedGraphNode b) {\n        // write code here\n\t\tif(a == null || b == null)\n\t\t\treturn false;\n\t\tif(a == b)\n\t\t\treturn true;\n\t\tmap.put(a, true);\n\t\tint num = a.neighbors.size();\n\t\tfor(int i=0;i<num;i++){\n\t\t\tif(map.containsKey(a.neighbors.get(i)) == false &amp;&amp; checkPathCore(a.neighbors.get(i),b))//深度遍历\n\t\t\t\treturn true;\n\t\t}\n\t\treturn false;\n    }\n\t\n\tpublic static class UndirectedGraphNode {\n\t    int label = 0;\n\t    UndirectedGraphNode left = null;\n\t    UndirectedGraphNode right = null;\n\t    ArrayList<UndirectedGraphNode> neighbors = new ArrayList<UndirectedGraphNode>();\n\n\t    public UndirectedGraphNode(int label) {\n\t        this.label = label;\n\t    }\n\t}\n\n}\n\n```\n\n&nbsp;\n\n&nbsp;\n\n```\npackage graph;\n\npublic class Graph {\t\t\t//建立图和添加顶点\n\tprivate Node vertices[];\n\tpublic int count;\n\tpublic Graph() {\n\t\tvertices = new Node[6];\n\t\tcount = 0;\n    }\n\t\n    public void addNode(Node x) {\n\t\tif (count < 30) {\n\t\t\tvertices[count] = x;\n\t\t\tcount++;\n\t\t} else {\n\t\t\tSystem.out.print(\"Graph full\");\n\t\t}\n    }\n    \n    public Node[] getNodes() {\n        return vertices;\n    }\n}\n\n```\n\n```\npackage graph;\n\nclass Node {\n    private Node adjacent[];\t\t\t//邻接的所有顶点，存放在一个数组中\n    public int adjacentCount;\t\n    private String vertex;\t\t\t\t\t//当前顶点的值\n    public Question.State state;\n    public Node(String vertex, int adjacentLength) {\t//adjacentLength是和该顶点邻接的顶点的数量\n        this.vertex = vertex;                \n        adjacentCount = 0;        //每实例化一个顶点，邻接顶点的数量开始是0\n        adjacent = new Node[adjacentLength];\n    }\n    \n    public void addAdjacent(Node x) {\t\t//添加邻接的顶点\n        if (adjacentCount < 30) {\t\t\t\t\t\t\t\t//邻接的顶点的数量不要操作30\t\t\t\n            this.adjacent[adjacentCount] = x;\n            adjacentCount++;\t\t//邻接的顶点的数量加1\n        } else {\n            System.out.print(\"No more adjacent can be added\");\n        }\n    }\n    public Node[] getAdjacent() {\t\t//返回所有的邻接顶点的数组\n        return adjacent;\n    }\n    public String getVertex() {\t\t//取得当前顶点的值\n        return vertex;\n    }\n}\n\n```\n\n&nbsp;\n\n```\npackage graph;\n\nimport java.util.LinkedList;\n\npublic class Question {\n\tpublic enum State {\n\t\tUnvisited, Visited, Visiting;\n\t} \n\n\tpublic static void main(String a[]){\t\t//cc150面试题4.2，给定有向图，找出两个结点之间是否存在一条路径\n\t\tGraph g = createNewGraph();\n\t\tNode[] n = g.getNodes();\n\t\tNode start = n[3];\n\t\tNode end = n[5];\n\t\tSystem.out.println(search(g, start, end));\n\t}\n\t\n\tpublic static Graph createNewGraph()\n\t{\n\t\tGraph g = new Graph();        \n\t\tNode[] temp = new Node[6];\n\n\t\ttemp[0] = new Node(\"a\", 3);\n\t\ttemp[1] = new Node(\"b\", 0);\n\t\ttemp[2] = new Node(\"c\", 0);\n\t\ttemp[3] = new Node(\"d\", 1);\n\t\ttemp[4] = new Node(\"e\", 1);\n\t\ttemp[5] = new Node(\"f\", 0);\n\n\t\ttemp[0].addAdjacent(temp[1]);\n\t\ttemp[0].addAdjacent(temp[2]);\n\t\ttemp[0].addAdjacent(temp[3]);\n\t\ttemp[3].addAdjacent(temp[4]);\n\t\ttemp[4].addAdjacent(temp[5]);\n\t\tfor (int i = 0; i < 6; i++) {\n\t\t\tg.addNode(temp[i]);\n\t\t}\n\t\treturn g;\n\t}\n\n    public static boolean search(Graph g,Node start,Node end) {  \n        LinkedList<Node> q = new LinkedList<Node>();\n        for (Node u : g.getNodes()) {\n            u.state = State.Unvisited;\n        }\n        start.state = State.Visiting;\n        q.add(start);\n        Node u;\n        while(!q.isEmpty()) {\n            u = q.removeFirst();\n            if (u != null) {\n\t            for (Node v : u.getAdjacent()) {\n\t                if (v.state == State.Unvisited) {\n\t                    if (v == end) {\n\t                        return true;\n\t                    } else {\n\t                        v.state = State.Visiting;\n\t                        q.add(v);\n\t                    }\n\t                }\n\t            }\n\t            u.state = State.Visited;\n            }\n        }\n        return false;\n    }\n}\n\n```\n\n&nbsp;\n\n**面试题4.3：给定一个有序整数数组，元素各不相同且按升序排列，编写一个算法，创建一颗高度最小的二叉查找书。**\n\n**　　思路：要创建一颗高度最小的树，必须让左右子树的结点数量越接近。即要用数组中间的值作为根节点，然后递归。**\n\n**　　注意：题目要求的是这颗二叉树的高度 还是 创建这颗二叉树**\n\n```\npackage cc150;\n\npublic class MinimalBST {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tint[] Arr = {0,1,2,3,4,5,6,7,8,9,10}; \n\t\tSystem.out.println(buildMinimalBST(Arr));\n\t}\n\t\n\tpublic static int buildMinimalBST(int[] vals) {\n        // write code here\n\t\tif(vals == null || vals.length <= 0)\n\t\t\treturn 0;\n\t\treturn creatMinimalBST(vals,0,vals.length-1);\n    }\n\t\n//\tpublic static TreeNode creatMinimalBST(int[] vals,int start,int end) {\n//\t\tif(end < start)\n//\t\t\treturn null;\n//\t\tint mid = (start+end)/2;\n//\t\t\n//\t\tTreeNode n = new TreeNode(vals[mid]);\n//\t\tcount++;\n//\t\tn.left = creatMinimalBST(vals,start,mid-1);\n//\t\tn.right = creatMinimalBST(vals,mid+1,end);\n//\t\treturn n;\n//    }\n\t\n\tpublic static int creatMinimalBST(int[] vals,int start,int end) {\n\t\tif(end <= start)\n\t\t\treturn 1;\n\t\tint mid = (start+end) >> 1;\n\t\t\n\t\tint left = 1 + creatMinimalBST(vals,start,mid-1);\n\t\tint right = 1 + creatMinimalBST(vals,mid+1,end);\n\t\treturn Math.max(left, right);\n    }\n\t\n\tpublic static class TreeNode {\n\t    int val = 0;\n\t    TreeNode left = null;\n\t    TreeNode right = null;\n\t    public TreeNode(int val) {\n\t        this.val = val;\n\t    }\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题4.4：给定一颗二叉树，设计一个算法，创建含有某一深度上所有结点的链表（比如，若一棵树的深度为D，则会创建出D个链表）。**\n\n**　　牛客网上求的是输出单层结点**\n\n**　　思路：方法1 层次遍历<br />**\n\n**　　　　　方法2 递归遍历**\n\n使用递归遍历，去掉程序中的static用例就可以跑过，不然有用例不通过\n\n```\npackage cc150;\n\npublic class TreeLevel {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tTreeNode a1 = new TreeNode(1);\n\t\tTreeNode a2 = new TreeNode(2);\n\t\tTreeNode a3 = new TreeNode(3);\n\t\tTreeNode a4 = new TreeNode(4);\n\t\tTreeNode a5 = new TreeNode(5);\n\t\tTreeNode a6 = new TreeNode(6);\n\t\tTreeNode a7 = new TreeNode(7);\n\t\t\n\t\ta1.left = a2;\n\t\ta1.right = a3;\n\t\ta2.left = a4;\n\t\ta2.right = a5;\n\t\ta3.left = a6;\n\t\ta3.right = a7;\n\t\t\n\t\tTreeLevel tl = new TreeLevel();\n\t\tListNode  Arr = tl.getTreeLevel(a1,2);\n\t\twhile(Arr != null){\n\t\t\tSystem.out.println(Arr.val);\n\t\t\tArr = Arr.next;\n\t\t}\n\t}\n\t\n\tpublic ListNode start = new ListNode(-1);\t//链表头\n    public ListNode start_temp = start;\t\t\t\t//链表头的拷贝\n\t\n\tpublic ListNode getTreeLevel(TreeNode root, int dep) {\n        // write code here\n        if (root == null || dep <= 0)\t//当root为null或者dep小于等于0的时候结束\n            return null;\n        if (dep == 1) {\t\t\t\t\t//只有当dep等于1的时候加入链表\n        \tstart_temp.next = new ListNode(root.val);\n        \tstart_temp = start_temp.next;\n        } else {\n            getTreeLevel(root.left, dep - 1);\n            getTreeLevel(root.right, dep - 1);\n        }\n        return start.next;\n    }\n\t\n\tpublic static class ListNode {\n\t    int val;\n\t    ListNode next = null;\n\n\t    ListNode(int val) {\n\t        this.val = val;\n\t    }\n\t}\n\t\n\tpublic static class TreeNode {\n\t    int val = 0;\n\t    TreeNode left = null;\n\t    TreeNode right = null;\n\t \n\t    public TreeNode(int val) {\n\t        this.val = val;\n\t    }\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题4.5：实现一个函数，检查一颗二叉树是否为二叉查找树。**\n\n**　　思路：方法1 中序遍历法 中序遍历的顺序是：左 中 右，把中序遍历的结果存放在一个数组之中，之后判断数组是否是升序的即可。<br />**\n\n**　　　　　方法2 最小/最大法 使用递归，深度遍历并判断每一个左结点的范围是否在MIN_VALUE和父节点之间，右结点范围是否在父节点和MAX_VALUE之间**\n\n```\npackage cc150;\n\npublic class CheckBST {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tTreeNode a1 = new TreeNode(4);\n\t\tTreeNode a2 = new TreeNode(2);\n\t\tTreeNode a3 = new TreeNode(6);\n\t\tTreeNode a4 = new TreeNode(1);\n\t\tTreeNode a5 = new TreeNode(3);\n\t\tTreeNode a6 = new TreeNode(5);\n\t\tTreeNode a7 = new TreeNode(7);\n\t\t\n\t\ta1.left = a2;\n\t\ta1.right = a3;\n\t\ta2.left = a4;\n\t\ta2.right = a5;\n\t\ta3.left = a6;\n\t\ta3.right = a7;\n\t\t\n\t\tCheckBST ch = new CheckBST();\n\t\tSystem.out.println(ch.checkBST(a1));\n\n\t}\n\t\n\tpublic boolean checkBST(TreeNode root) {\n        // write code here\n\t\treturn checkBST(root,Integer.MIN_VALUE,Integer.MAX_VALUE);\n    }\n\t\n\tpublic boolean checkBST(TreeNode root,int min,int max) {//不断的缩小范围\n        // write code here\n\t\tif(root == null)\n\t\t\treturn true;\n\t\tif(root.val < min || root.val >= max)\t\n\t\t\treturn false;\n\t\t//每一个左结点的范围在MIN_VALUE和父节点之间，右结点范围在父节点和MAX_VALUE之间\n\t\tif(!checkBST(root.left,min,root.val) || !checkBST(root.right,root.val,max))\n\t\t\treturn false;\n\t\treturn true;\n    }\n\t\n\tpublic static class TreeNode {\n\t    int val = 0;\n\t    TreeNode left = null;\n\t    TreeNode right = null;\n\t    public TreeNode(int val) {\n\t        this.val = val;\n\t    }\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题4.6：设计一个算法，找出二叉查找树中指定结点的&ldquo;下一个&rdquo;结点（也即中序后继）。可以假定每个结点都含有指向父节点的连接。**\n\n**<img src=\"/images/517519-20160918093008064-1652890902.jpg\" alt=\"\" />**\n\n**　　思路：情况1：结点2的情况，有右子树，下一个结点为右子树中最小的结点<br />**\n\n**　　　　　情况2：结点1的情况，无右子树且为左孩子，返回父结点**\n\n**　　　　　情况3：结点3的情况，无右子树且为右孩子，返回父节点的父节点**\n\n**有parent指针的解法：**\n\n```\npackage cc150;\n\npublic class Successor {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tTreeNode a1 = new TreeNode(4);\n\t\tTreeNode a2 = new TreeNode(2);\n\t\tTreeNode a3 = new TreeNode(6);\n\t\tTreeNode a4 = new TreeNode(1);\n\t\tTreeNode a5 = new TreeNode(3);\n\t\tTreeNode a6 = new TreeNode(5);\n\t\tTreeNode a7 = new TreeNode(7);\n\t\t\n\t\ta1.left = a2;\n\t\ta1.right = a3;\n\t\ta2.parent = a1;\n\t\ta3.parent = a1;\n\t\t\n\t\ta2.left = a4;\n\t\ta2.right = a5;\n\t\ta4.parent = a2;\n\t\ta5.parent = a2;\n\t\t\n\t\ta3.left = a6;\n\t\ta3.right = a7;\n\t\ta6.parent = a3;\n\t\ta7.parent = a3;\n\t\t\n\t\tSuccessor su = new Successor();\n\t\tSystem.out.println(su.findSucc(a2,2).val);\n\t\tSystem.out.println(su.findSucc(a4,1).val);\n\t\tSystem.out.println(su.findSucc(a5,3).val);\n\t}\n\t\n\tpublic TreeNode findSucc(TreeNode root, int p) {\n        // write code here\n\t\tif(root == null)\n\t\t\treturn null;\n\t\t\n\t\tif(root.right != null)　　　　　　//当右子树不为空的情况，找到右子树的最小值\n\t\t\treturn rightMinChild(root.right);\n\t\telse{\n\t\t\tTreeNode root_copy = root;\n\t\t\tTreeNode par = root_copy.parent;\n\t\t\twhile(par != null &amp;&amp; par.left  != root_copy){//向上直到位于左边，3在2的右边继续向上，直到2在4的左边\n\t\t\t\troot_copy = par;\n\t\t\t\tpar = par.parent;\n\t\t\t}\n\t\t\treturn par;\n\t\t}\n    }\n\t\n\tpublic TreeNode rightMinChild(TreeNode root){\n\t\tif(root == null)\n\t\t\treturn null;\n\t\twhile(root.left != null)\n\t\t\troot = root.left;\n\t\treturn root;\n\t}\n\t\n\tpublic static class TreeNode {\n\t    int val = 0;\n\t    TreeNode left = null;\n\t    TreeNode right = null;\n\t    TreeNode parent = null;\n\t    public TreeNode(int val) {\n\t        this.val = val;\n\t    }\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**无parent指针的解法：//使用中序遍历，按遍历的顺序放进ArrayList，然后通过p的值查找下一个结点**\n\n```\npackage cc150;\n\nimport java.util.ArrayList;\n\npublic class Successor {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tTreeNode a1 = new TreeNode(4);\n\t\tTreeNode a2 = new TreeNode(2);\n\t\tTreeNode a3 = new TreeNode(6);\n\t\tTreeNode a4 = new TreeNode(1);\n\t\tTreeNode a5 = new TreeNode(3);\n\t\tTreeNode a6 = new TreeNode(5);\n\t\tTreeNode a7 = new TreeNode(7);\n\t\t\n\t\ta1.left = a2;\n\t\ta1.right = a3;\n\t\ta2.parent = a1;\n\t\ta3.parent = a1;\n\t\t\n\t\ta2.left = a4;\n\t\ta2.right = a5;\n\t\ta4.parent = a2;\n\t\ta5.parent = a2;\n\t\t\n\t\ta3.left = a6;\n\t\ta3.right = a7;\n\t\ta6.parent = a3;\n\t\ta7.parent = a3;\n\t\t\n\t\tSuccessor su = new Successor();\n\t\tSystem.out.println(su.findSucc(a1,2));\n\t\tSystem.out.println(su.findSucc(a1,1));\n\t\tSystem.out.println(su.findSucc(a1,3));\n\t}\n\t\n\tpublic ArrayList<TreeNode> theList = new ArrayList<TreeNode>();\n\t\n\tpublic int findSucc(TreeNode root, int p) {\t//使用中序遍历，按遍历的顺序放进ArrayList，然后通过p的值查找下一个结点\n\t\tfind(root);\t\t\t\t\t//其中最开始root要是树的根节点\n\t\tint size = theList.size();\n\t\tfor(int i=0;i<size-1;i++){\t\t//是size-1\n\t\t\tif(theList.get(i).val == p)\n\t\t\t\treturn theList.get(i+1).val;\n\t\t}\n\t\treturn -1;\n\t}\n\t\n\tpublic void find(TreeNode root){\t//按中序遍历的顺序把结点放进ArrayList\n\t\tif(root != null){\n\t\t\tfind(root.left);\n\t\t\ttheList.add(root);\n\t\t\tfind(root.right);\n\t\t}\n\t}\n\t\n\tpublic TreeNode rightMinChild(TreeNode root){\n\t\tif(root == null)\n\t\t\treturn null;\n\t\twhile(root.left != null)\n\t\t\troot = root.left;\n\t\treturn root;\n\t}\n\t\n\tpublic static class TreeNode {\n\t    int val = 0;\n\t    TreeNode left = null;\n\t    TreeNode right = null;\n\t    TreeNode parent = null;\n\t    public TreeNode(int val) {\n\t        this.val = val;\n\t    }\n\t}\n\n}\n\n```\n\n&nbsp;\n\n&nbsp;\n\n**面试题4.7：设计并实现一个算法，找出二叉树中某两个结点的第一个公共祖先。不得将额外的结点存储在另外的数据结构中。注意：这不一定是二叉查找树。**\n\n《剑指Offer》 最后的一两题\n\n&nbsp;\n\n**面试题4.8：你有两棵非常大的二叉树：T1，有几百万个结点；T2，有几百个结点。设计一个算法，判断T2是否为T1的子树。**\n\n**　　如果T1有这么一个结点n，其子树与T2一模一样，则T2为T1的子树。也就是说，从结点n处把树砍掉，得到的树与T2完全相同。<br />**\n\n```\npackage cc150;\n\nimport cc150.PathSum.TreeNode;\n\npublic class ContainsTree {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tTreeNode a1 = new TreeNode(4);\n\t\tTreeNode a2 = new TreeNode(2);\n\t\tTreeNode a3 = new TreeNode(6);\n\t\tTreeNode a4 = new TreeNode(5);\n\t\tTreeNode a5 = new TreeNode(7);\n\t\tTreeNode a6 = new TreeNode(1);\n\t\tTreeNode a7 = new TreeNode(3);\n\t\t\n\t\ta1.left = a2;\n\t\ta1.right = a3;\n\t\t\n\t\ta2.left = a4;\n\t\ta2.right = a5;\n\t\t\n\t\ta3.left = a6;\n\t\ta3.right = a7;\n\t\t\n\t\tTreeNode a66 = new TreeNode(6);\n\t\tTreeNode a11 = new TreeNode(1);\n\t\tTreeNode a33 = new TreeNode(3);\n\t\t\n\t\ta66.left = a11;\n\t\ta66.right = a33;\n\t\t\n\t\tContainsTree ct = new ContainsTree();\n\t\tSystem.out.println(ct.subTree(a1,a66));\n\t}\n\t\n\tpublic boolean subTree(TreeNode r1,TreeNode r2){\t\t//r1是大树，r2是小树，在大树中寻找和小树的根节点相等的结点\n\t\tif(r1 == null)\n\t\t\treturn false;\n\t\tif(r1.val == r2.val){\t\t//找到和小树根节点相等的结点\n\t\t\tif(matchTree(r1,r2))\t\t//判断是否所有的结点都相等，相等返回true\n\t\t\t\treturn true;\n\t\t}\n\t\treturn (subTree(r1.left,r2) || subTree(r1.right,r2));\t\t//递归所有左孩子和右孩子\n\t}\n\t\n\tpublic boolean matchTree(TreeNode r1,TreeNode r2){\n\t\tif(r1 == null &amp;&amp; r2 == null)\t//子树中已经没有结点，子树相同\n\t\t\treturn true;\n\t\tif(r1 == null || r2 == null)\t\t//不同时为空说明两棵树不一样\n\t\t\treturn false;\n\t\tif(r1.val != r2.val)\t\t\t\t//两个结点的值不想等，说明两棵树不一样\n\t\t\treturn false;\n\t\treturn  (matchTree(r1.left,r2.left)) &amp;&amp;  (matchTree(r1.right,r2.right));\t//递归左结点和右结点\n\t}\n\t\n\tpublic static class TreeNode {\n\t    int val = 0;\n\t    TreeNode left = null;\n\t    TreeNode right = null;\n\t    TreeNode parent = null;\n\t    public TreeNode(int val) {\n\t        this.val = val;\n\t    }\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题4.9：给定一棵二叉树，其中每个结点都含有一个数值。设计一个算法，打印结点数值总和等于某个给定值的所有路径。注意，路径不一定非得从二叉树的根节点或叶节点开始或结束。&mdash;&mdash;《Leetcode》112 [Path Sum](https://leetcode.com/problems/path-sum/)**\n\n```\npackage cc150;\n\nimport java.util.ArrayList;\nimport java.util.Iterator;\n\npublic class PathSum {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tTreeNode a1 = new TreeNode(4);\n\t\tTreeNode a2 = new TreeNode(2);\n\t\tTreeNode a3 = new TreeNode(6);\n\t\tTreeNode a4 = new TreeNode(5);\n\t\tTreeNode a5 = new TreeNode(7);\n\t\tTreeNode a6 = new TreeNode(1);\n\t\tTreeNode a7 = new TreeNode(3);\n\t\t\n\t\ta1.left = a2;\n\t\ta1.right = a3;\n\t\t\n\t\ta2.left = a4;\n\t\ta2.right = a5;\n\t\t\n\t\ta3.left = a6;\n\t\ta3.right = a7;\n\t\t\n\t\tPathSum sum = new PathSum();\n\t\t ArrayList<ArrayList<Integer>> theArr =sum.PathSum(a1,13);\n\t\tIterator ire = theArr.iterator();\n\t\twhile(ire.hasNext())\n\t\t\tSystem.out.println(ire.next());\n\t}\n\t\n\tpublic ArrayList<ArrayList<Integer>> PathSum(TreeNode root, int sum) {\n\t\tArrayList<ArrayList<Integer>> arr = new ArrayList<ArrayList<Integer>>();\n\t\tif(root == null)\n\t\t\treturn arr;\n\t\tArrayList<Integer> a = new ArrayList<Integer>();\n        depthTraversal(root,sum,0,arr,a);\n        return arr;\n\t}\n\t\n\tpublic void depthTraversal(TreeNode root, int sum,int currentSum,ArrayList<ArrayList<Integer>> arr,ArrayList<Integer> a){\n        if(root == null)\n        \treturn;\n        if(root != null){\n            currentSum += root.val;\n            if(root.left == null &amp;&amp; root.right == null){   //如果无子节点\n                if (currentSum == sum){     //如果相等\n                \ta.add(root.val);\t\t\t\t\t//把结点加进ArrayList\n                \tarr.add(new ArrayList<Integer>(a));             \t\n                \t//System.out.println(a);\n                \ta.remove(a.size()-1);\t\t//如果相等，把ArrayList存起来，并去掉最后一个\n                }\n                return;\n            }\n            a.add(root.val);\t\t\t\t//如果还有子结点的话，加上这个结点之后，重复左右结点\n            depthTraversal(root.left,sum,currentSum,arr,a);\n            depthTraversal(root.right,sum,currentSum,arr,a);\n            a.remove(a.size()-1);\t//遍历完左右子节点，去掉本身结点\n        }\n    }\n\t\n\tpublic static class TreeNode {\n\t    int val = 0;\n\t    TreeNode left = null;\n\t    TreeNode right = null;\n\t    TreeNode parent = null;\n\t    public TreeNode(int val) {\n\t        this.val = val;\n\t    }\n\t}\n}\n\n```\n\n&nbsp;\n\n&nbsp;\n\n**&nbsp;**\n\n&nbsp;\n","tags":["刷题"]},{"title":"面试题目——《CC150》栈与队列","url":"/面试题目——《CC150》栈与队列.html","content":"<img src=\"/images/517519-20160914164048195-1808628311.png\" alt=\"\" width=\"696\" height=\"342\" />\n\n<img src=\"/images/517519-20160914164128711-1285800842.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n**面试题3.1：描述如何只用一个数组来实现三个栈。**\n\n**方法1：固定分割**\n\n**方法2：弹性分割（较难）**\n\n```\npackage cc150;\n\npublic class ArrayStack {\n\n\tpublic static void main(String[] args) throws Exception {\n\t\t// TODO 自动生成的方法存根\n\t\tArrayStack theStack = new ArrayStack();\n\t\ttheStack.push(0, 1);\n\t\ttheStack.push(0, 2);\n\t\ttheStack.push(1, 10);\n\t\ttheStack.push(1, 11);\n\t\ttheStack.push(2, 20);\n\t\ttheStack.push(2, 21);\n\t\t\n\t\tSystem.out.println(theStack.peek(1));\n\t}\n\t\n\tint stackSize = 100;\n\tint[] buffer = new int[stackSize * 3];\n\tint[] stackPointer = {-1,-1,-1};\t//用于记录每个栈的栈顶在数组的位置，开始是0,100,200\n\t\n\t//返回不是的栈的栈顶在数组中的当前位置\n\tint absTopOfStack(int stackNum){\n\t\treturn stackNum * stackSize + stackPointer[stackNum];\n\t}\n\t\n\t//入栈\n\tvoid push(int stackNum,int value) throws Exception{\n\t\tif(stackPointer[stackNum] + 1 >= stackSize){\n\t\t\tthrow new Exception(\"超出范围\");\n\t\t} \n\t\t//栈顶指针自增，并把值放进对应的栈\n\t\tstackPointer[stackNum]++;\n\t\tbuffer[absTopOfStack(stackNum)] = value;\n\t}\n\t\n\t//出栈\n\tint pop(int stackNum)throws Exception{\n\t\tif(stackPointer[stackNum] == -1){\n\t\t\tthrow new Exception(\"当前栈为空\");\n\t\t} \n\t\tint value = buffer[absTopOfStack(stackNum)];\n\t\tbuffer[absTopOfStack(stackNum)] = 0;\t//清零指定的数组\n\t\tstackPointer[stackNum]--;\t//指针自减\n\t\treturn value;\n\t}\n\t\n\t//返回栈顶元素\n\tint peek(int stackNum){\n\t\tint index = absTopOfStack(stackNum);\n\t\treturn buffer[index];\n\t}\n\t\n\t//判断栈是否为空\n\tboolean isEmpty(int staqckNum){\n\t\treturn (stackPointer[staqckNum] == -1);\n\t}\n\t\n}\n\n```\n\n&nbsp;\n\n**面试题3.2：请设计一个栈，除pop与push方法，还支持min方法，可返回栈元素中的最小值。push、pop和min三个方法的时间复杂度必须为O（1）。&mdash;&mdash;《Leetcode》155. Min Stack**\n\n&nbsp;　　思路：利用额外的栈来记录每一个状态的最小值，注意当所有小于栈底的元素都弹出后，最小的就是栈底元素\n\n```\npublic class MinStack {\n    Stack<Integer> theStack;\n    Stack<Integer> theStack_temp;    //最小的元素\n    /** initialize your data structure here. */\n    public MinStack() {\n        theStack = new Stack<Integer>();\n        theStack_temp = new Stack<Integer>();    //最小的元素\n    }\n    \n    public void push(int x) {\n        if(theStack.empty() || x<=theStack_temp.peek())\n            theStack_temp.push(x);\n        theStack.push(x);//这句放在前会空栈\n    }\n    \n    public void pop() {\n        int temp = theStack.pop();\n        if(temp == theStack_temp.peek())\n            theStack_temp.pop();\n    }\n    \n    public int top() {\n        return theStack.peek();\n    }\n    \n    public int getMin() {\n        return theStack_temp.peek();\n    }\n}\n\n/**\n * Your MinStack object will be instantiated and called as such:\n * MinStack obj = new MinStack();\n * obj.push(x);\n * obj.pop();\n * int param_3 = obj.top();\n * int param_4 = obj.getMin();\n */\n\n```\n\n&nbsp;\n\n**面试题3.3：设想有一堆盘子，堆太高可能会倒下来。因此，在现实生活中，盘子堆到一定高度时，我们就会另外堆一堆盘子。请实现数据结构SetOfStacks，模拟这种行为。SetOfStacks应该由多个栈组成，并且在前一个栈填满时新建一个栈。此外，SetOfStacks.push()和SetOfStacks.pop()应该与普通栈的操作方法相同（也就是说，pop()返回的值，应该跟只有一个栈时的情况一样）。**\n\n```\npackage cc150;\n\nimport java.util.ArrayList;\n\npublic class SetOfStacks {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t}\n\t\n\tArrayList<ArrayList<Integer>> stacks = null;\n\tArrayList<Integer> curStack = null;\n\t\n\tpublic ArrayList<ArrayList<Integer>> setOfStacks(int[][] ope, int size) {//第一个表示push或者pop，第二个为待处理的元素\n        // write code here\n\t\tstacks = new ArrayList<ArrayList<Integer>>(size);\n\t\tcurStack = new ArrayList<Integer>(size);\n\t\tstacks.add(curStack);\n\t\t\n\t\tfor(int i=0;i<ope.length;i++){\n\t\t\tif(ope[i][0] == 1)\n\t\t\t\tpush(ope[i][1],size);\n\t\t\telse if(ope[i][0] == 2)\n\t\t\t\tpop(size);\n\t\t}\n\t\treturn stacks;\n    }\n\t\n\tpublic void push(int v,int size){\n\t\tif(curStack.size() != size){//数组未满\n\t\t\tcurStack.add(v);\n\t\t}else{\n\t\t\tcurStack = new ArrayList<Integer>(size);\n\t\t\tcurStack.add(v);\n\t\t\tstacks.add(curStack);\n\t\t}\n\t}\n\t\n\tpublic void pop(int size){\n\t\tint temp=0;\n\t\tif(curStack.size() != 0){//数组未满\n\t\t\ttemp = curStack.get(curStack.size()-1);\n\t\t\tcurStack.remove(curStack.size()-1);\n\t\t}else{\n\t\t\tstacks.remove(stacks.size()-1);\n\t\t\tcurStack = stacks.get(stacks.size()-1);\n\t\t\ttemp = curStack.get(size-1);\n\t\t\tcurStack.remove(curStack.size()-1);\n\t\t}\n\t\t//return temp;\n\t}\n\t\n\n\n}\n\n```\n\n&nbsp;\n\n**面试题3.4：汉诺塔问题&mdash;&mdash;部分参考 [Java递归算法&mdash;&mdash;汉诺塔问题](http://www.cnblogs.com/tonglin0325/p/5362236.html)**\n\n&nbsp;\n\n**面试题3.5：实现一个MyQueue类，该类用两个栈实现一个队列。&mdash;&mdash;《Leetcode》232 [Implement Queue using Stacks](https://leetcode.com/problems/implement-queue-using-stacks/)**\n\n&nbsp;\n\n**面试题3.6：编写程序，按升序对栈进行排序（即最大元素位于栈顶）。最多只能使用一个额外的栈存放临时数据，但不得讲数据复制到别的数据结构中（如数组）。该栈支持如下操作：push、pop、peek和isEmpty。**\n\n&nbsp;\n\n**面试题3.7：猫狗收容所**\n","tags":["刷题"]},{"title":"面试题目——《CC150》链表","url":"/面试题目——《CC150》链表.html","content":"<img src=\"/images/517519-20160912113533852-1074067476.png\" alt=\"\" width=\"700\" height=\"498\" />\n\n<img src=\"/images/517519-20160912113603570-1842466528.png\" alt=\"\" width=\"711\" height=\"148\" />\n\n<img src=\"/images/517519-20160912113631555-2085646067.png\" alt=\"\" width=\"710\" height=\"612\" />\n\n<!--more-->\n&nbsp;\n\n**面试题2.1：编写代码，移除未排序链表中的重复结点　　进阶：如果不得使用临时缓冲区，该怎么解决？**\n\n```\npackage cc150;\n\nimport java.util.HashMap;\nimport java.util.Map;\n\n\npublic class DeleteDups {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tLinkedListNode Node0 = new LinkedListNode(1);\n\t\tLinkedListNode Node1 = new LinkedListNode(1);\n\t\tLinkedListNode Node2 = new LinkedListNode(2);\n\t\tLinkedListNode Node3 = new LinkedListNode(3);\n\t\tNode0.next = Node1;\n\t\tNode1.next = Node2;\n\t\tNode2.next = Node3;\n\t\tdeleteDups(Node0);\n\t\tLinkedListNode temp = Node0;\n\t\twhile(temp != null){\n\t\t\tSystem.out.println(temp.iData);\n\t\t\ttemp = temp.next;\n\t\t}\n\t}\n\t\n//\tpublic static void deleteDups(LinkedListNode n){\n//\t\tMap<Integer,Boolean> map = new HashMap<Integer,Boolean>();\n//\t\tLinkedListNode pre = null;\t//记录上一次不重复的结点\n//\t\twhile(n != null){\n//\t\t\tif(map.containsKey(n.iData)){\n//\t\t\t\tpre.next = n.next;\n//\t\t\t}else{\n//\t\t\t\tmap.put(n.iData, true);\n//\t\t\t\tpre = n;\n//\t\t\t}\n//\t\t\tn = n.next;\n//\t\t}\n//\t\t\n//\t}\n\t\n\tpublic static void deleteDups(LinkedListNode head){\n\t\tif(head == null)\n\t\t\treturn;\n\t\tLinkedListNode current = head;\t\n\t\twhile(current != null){\n\t\t\tLinkedListNode runner = current;\n\t\t\twhile(runner.next != null){\t//是runner.next\n\t\t\t\tif(runner.next.iData == current.iData)\t//判断内层所有结点是否等于外层结点\n\t\t\t\t\trunner.next = runner.next.next;\n\t\t\t\telse\n\t\t\t\t\trunner = runner.next;\n\t\t\t}\n\t\t\tcurrent = current.next;\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题2.2：实现一个算法，找出单向链表中倒数第k个结点。&mdash;&mdash;《剑指Offer》面试题15 (找出) &amp;《Leetcode》removeNthNode （移除）<br />**\n\n&nbsp;\n\n```\npackage cc150;\n\npublic class NthToLast {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tListNode Node1 = new ListNode(1);\n\t\tListNode Node2 = new ListNode(2);\n\t\tListNode Node3 = new ListNode(3);\n\t\tNode1.next = Node2;\n\t\tNode2.next = Node3;\n\t\tSystem.out.println(nthToLast(Node1,1).val);\n\t}\n\t\n\tpublic static ListNode nthToLast(ListNode head,int k){\t//返回链表的倒数第k个结点\n\t\tif(k <= 0)\n\t\t\treturn null;\n\t\tListNode p1 = head;\n\t\tListNode p2 = head;\n\t\t\n\t\tfor(int i=0;i<k-1;i++){\n\t\t\tif(p2 == null)\n\t\t\t\treturn null;\n\t\t\tp2 = p2.next;\n\t\t}\n\t\tif(p2 == null)\n\t\t\treturn null;\n\t\t\n\t\twhile(p2.next != null){\n\t\t\tp1 = p1.next;\n\t\t\tp2 = p2.next;\n\t\t}\n\t\treturn p1;\n\t}\n\t\n\tpublic static class ListNode {\n\t\tint val;\n\t\tListNode next;\n\t\tListNode(int x) { val = x; }\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题2.3：实现一个算法，删除单向链表中间的某个结点，假定你只能访问该结点。**\n\n**　　思路：因为是单向链表，所以不知道一个结点的前一个结点，所以当这个结点是最后一个的时候，无解**\n\n**　　　　　只要把要删除的结点的下一个结点的数据复制到这个结点即可。**\n\n```\nimport java.util.*;\n \n/*\npublic class ListNode {\n    int val;\n    ListNode next = null;\n \n    ListNode(int val) {\n        this.val = val;\n    }\n}*/\npublic class Remove {\n    public boolean removeNode(ListNode pNode) {\n        // write code here\n        if(pNode == null || pNode.next == null)\n            return false;\n        ListNode nextNode = pNode.next;//取得要删除结点的下一个结点\n        pNode.val = nextNode.val;\n        pNode.next = nextNode.next;\n        return true;\n    }\n}\n\n```\n\n&nbsp;\n\n**&nbsp;面试题2.4：编写代码，以给定值x为基准讲链表分割成两部分，所有小于x的结点排在大于或者等于x的结点之前。**\n\n```\npackage cc150;\n\npublic class Partition {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tListNode Node1 = new ListNode(5);\n\t\tListNode Node2 = new ListNode(4);\n\t\tListNode Node3 = new ListNode(3);\n\t\tListNode Node4 = new ListNode(2);\n\t\tListNode Node5 = new ListNode(1);\n\t\tNode1.next = Node2;\n\t\tNode2.next = Node3;\n\t\tNode3.next = Node4;\n\t\tNode4.next = Node5;\n\t\tListNode temp = partition(Node1,3);\n\t\t\n\t\twhile(temp != null){\n\t\t\tSystem.out.println(temp.val);\n\t\t\ttemp = temp.next;\n\t\t}\n\t}\n\t\n\t public static ListNode partition(ListNode pHead, int x) {\n\t\t // write code here\n\t\t ListNode beforeStart = null;\t//记录链表的头结点\n\t\t ListNode afterStart = null;\t\t//记录链表的头结点\n\t\t ListNode beforeEnd = null;\n\t\t ListNode afterEnd = null;\n\t\t \n\t\t ListNode pNext = pHead;\n\t\t while(pNext != null){\n\t\t\t\n\t\t\t if(pNext.val < x){\n\t\t\t\t if(beforeStart == null){\n\t\t\t\t\t beforeStart = pNext;\n\t\t\t\t\t beforeEnd = pNext;\n\t\t\t\t }else{\n\t\t\t\t\t beforeEnd.next = pNext;\n\t\t\t\t\t beforeEnd = beforeEnd.next;\n\t\t\t\t }\n\t\t\t }else{\n\t\t\t\t if(afterStart == null){\n\t\t\t\t\t afterStart = pNext;\n\t\t\t\t\t afterEnd = pNext;\n\t\t\t\t }else{\n\t\t\t\t\t afterEnd.next = pNext;\n\t\t\t\t\t afterEnd = afterEnd.next;\n\t\t\t\t }\n\t\t\t }\n\t\t\t pNext = pNext.next;\n\t\t }\n\t\t \n\t\t //切记断掉after最后一个元素的next，不然会形成环\n\t\t if(afterEnd != null)\n\t\t\t afterEnd.next = null;\n\t\t \n\t\t //如果beforeStart为null，返回afterStart头结点\n\t\t if(beforeStart == null)\n\t\t\t return afterStart;\n\t\t\n\t\t beforeEnd.next = afterStart;\n\t\t return beforeStart;\n\t }\n\t \n\t public static class ListNode {\n\t\t\tint val;\n\t\t\tListNode next;\n\t\t\tListNode(int x) { val = x; }\n\t\t}\n\n}\n\n```\n\n&nbsp;\n\n**&nbsp;面试题2.5：给定两个用链表表示的整数，每个结点包含一个数位。这些数位是反向存放的，也就是个位排在链表首部。编写函数对这两个整数求和，并用链表形式返回结果。**\n\n**　　进阶：假设这些数位是正向存放的，请再做一遍。**\n\n```\npackage cc150;\n\nimport cc150.Partition.ListNode;\n\npublic class Plus {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tListNode Node1 = new ListNode(7);\n\t\tListNode Node2 = new ListNode(2);\n\t\tListNode Node3 = new ListNode(3);\n\t\tNode1.next = Node2;\n\t\tNode2.next = Node3;\n\n\t\tListNode Node4 = new ListNode(4);\n\t\tListNode Node5 = new ListNode(5);\n\t\tListNode Node6 = new ListNode(6);\n\t\tListNode Node7 = new ListNode(7);\n\t\tNode4.next = Node5;\n\t\tNode5.next = Node6;\n\t\tNode6.next = Node7;\n\t\t\n\t\tListNode temp = plusAB(Node1,Node4);\n\t\tSystem.out.println(temp.val);\n\t\twhile(temp !=null){\n\t\t\tSystem.out.print(temp.val);\n\t\t\ttemp = temp.next;\n\t\t}\n\t}\n\t\n\tpublic static ListNode plusAB(ListNode a, ListNode b) {\n        // write code here\n\t\tif(a == null &amp;&amp; b == null)\n\t\t\treturn null;\n\t\tint carry = 0;\n\t\tListNode result_temp = new ListNode(0);\n\t\tListNode result = result_temp;\n\t\twhile(a != null &amp;&amp; b != null){\n\t\t\tint value = a.val + b.val + carry;\n\t\t\t\n\t\t\tif(value>=10){\n\t\t\t\tresult_temp.next = new ListNode(value-10);\n\t\t\t\tcarry = 1;\n\t\t\t}else{\n\t\t\t\tresult_temp.next = new ListNode(value);\n\t\t\t\tcarry = 0;\n\t\t\t}\n\t\t\tresult_temp = result_temp.next;\n\t\t\ta = a.next;\n\t\t\tb = b.next;\n\t\t}\n\t\tif(a != null){\n\t\t\tresult_temp.next = new ListNode(a.val+carry);\n\t\t\tresult_temp.next.next = a.next;\n\t\t}\n\t\telse if(b != null){\n\t\t\tresult_temp.next = new ListNode(b.val+carry);\n\t\t\tresult_temp.next.next = b.next;\n\t\t}\n\t\telse if(carry == 1){\n\t\t\tresult_temp.next = new ListNode(carry);\n\t\t}\n\t\t\n\t\treturn result.next;\n    }\n\t\n\t public static class ListNode {\n\t\t\tint val;\n\t\t\tListNode next;\n\t\t\tListNode(int x) { val = x; }\n\t\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题2.6：给定一个有环链表，实现一个算法返回环路的开头结点。**\n\n```\npackage cc150;\n\nimport cc150.Plus.ListNode;\n\npublic class FindBeginning {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tListNode Node1 = new ListNode(1);\n\t\tListNode Node2 = new ListNode(2);\n\t\tListNode Node3 = new ListNode(3);\n\t\tListNode Node4 = new ListNode(4);\n\t\tListNode Node5 = new ListNode(5);\n\t\tListNode Node6 = new ListNode(6);\n\t\tListNode Node7 = new ListNode(7);\n\t\tListNode Node8 = new ListNode(8);\n\t\tListNode Node9 = new ListNode(9);\n\t\t\n\t\tNode1.next = Node2;\n\t\tNode2.next = Node3;\n\t\tNode3.next = Node4;\n\t\tNode4.next = Node5;\n\t\tNode5.next = Node6;\n\t\tNode6.next = Node7;\n\t\tNode7.next = Node8;\n\t\tNode8.next = Node9;\n\t\tNode9.next = Node4;\n\t\t\n\t\tSystem.out.println(findBeginning(Node1).val);\n\t}\n\t\n\tpublic static ListNode findBeginning(ListNode head){\n\t\tListNode slow = head;\n\t\tListNode fast = head;\n\t\t//slow移动一步，fast移动两步，链表到头或者碰撞的时候停止\n\t\twhile(fast != null &amp;&amp; fast.next != null){\n\t\t\tslow = slow.next;\n\t\t\tfast = fast.next.next;\n\t\t\tif(slow == fast)\n\t\t\t\tbreak;\n\t\t}\n\t\t//如果是链表到头，没有环路\n\t\tif(fast == null || fast.next == null)\n\t\t\treturn null;\n\t\t//将slow指向链表头部，fast指向碰撞处，两者同时移动，必回在环路起点相遇\n\t\tslow = head;\n\t\twhile(slow != fast){\n\t\t\tslow = slow.next;\n\t\t\tfast = fast.next;\n\t\t}\n\t\treturn fast;\n\t}\n\t\n\t public static class ListNode {\n\t\tint val;\n\t\tListNode next;\n\t\tListNode(int x) { val = x; }\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题2.7： 编写一个函数，检查链表是否为回文。**\n\n**　　CC150第一种解法：反转并比较**\n\n```\nimport java.util.*;\n \n/*\npublic class ListNode {\n    int val;\n    ListNode next = null;\n \n    ListNode(int val) {\n        this.val = val;\n    }\n}*/\npublic class Palindrome {\n    public boolean isPalindrome(ListNode pHead) {//反转并比较，CC150第一种解法\n        // write code here\n        if(pHead == null || pHead.next == null)\n            return true;\n        ListNode slow,fast;     //快慢指针法查找链表的中心\n        slow = fast = pHead;\n        while(fast != null &amp;&amp; fast.next != null){\n            slow = slow.next;\n            fast = fast.next.next;\n        }\n        if(fast != null){    //链表个数奇数个\n            slow.next = reverseList(slow.next);\n            slow = slow.next;   //去掉中间的数\n        }else{\n            slow = reverseList(slow);\n        }\n        while(slow != null){\n            if(pHead.val != slow.val)\n                return false;\n            slow = slow.next;\n            pHead = pHead.next;\n        }\n        return true;\n    }\n     \n    public ListNode reverseList(ListNode head) {\n        if(head ==null){\n            return null;\n        }\n        Stack<ListNode> stack = new Stack<ListNode>();\n        ListNode current = head;\n        while(current != null){\n            stack.add(current);\n            current = current.next;\n        }\n        head = stack.pop();\n        current = head;\n        while(stack.empty() != true){\n            current.next = stack.pop();\n            current = current.next;\n        }\n        current.next = null;  //Memory Limit Exceeded\n        return head;\n    }\n}\n\n```\n\n&nbsp;\n","tags":["刷题"]},{"title":"网络分层模型","url":"/网络分层模型.html","content":"## **1.TCP/IP网络分层模型**\n\n**TCP/IP通常被认为是一个四层网络通信协议系统：**\n\n**1.链路层**，有时也称作**数据链路层**或**网络接口层**，通常包括操作系统中的设备驱动程序和计算机中对应的网络接口卡。它们一起处理与电缆(或其他任何传输媒介)的物理接口细节。<br />**2.网络层**，有时也称作互联网层，处理分组在网络中的活动,例如分组的选路。在TCP/IP协议族中，网络层协议包括 **IP协议(网际协议)**，**ICMP协议(Internet互联网控制报文协议)**以及**IGMP协议(Internet组管理协议)**。<br />**3.传输层**，主要为两台主机上的应用程序提供端到端的通信。在 TCP/IP协议族中，有两个互不相同的传输协议：**TCP(传输控制协议)**和 **UDP(用户数据报协议)**。<br />TCP为两台主机提供高可靠性的数据通信。它所做的工作包括把应用程序交给它的数据分成合适的小块交给下面的网络层，确认接收到的分组，设置发送最后确认分组的超时时钟等。由于运输层提供了高可靠性的端到端的通信，因此应用层可以忽略所有这些细节。<br />而另一方面，UDP则为应用层提供一种非常简单的服务。它只是把称作数据报的分组从一台主机发送到另一台主机，但并不保证该数据报能到达另一端。任何必需的可靠性必须由应用层来提供。<br />**4.应用层**，负责处理特定的应用程序细节。几乎各种不同的 TCP/IP实现都会提供下面这些通用的应用程序:\n\n&bull; Telnet 远程登录。<br />&bull; FTP 文件传输协议。<br />&bull; SMTP 简单邮件传送协议。<br />&bull; SNMP 简单网络管理协议。\n\n## **2.OSI网络分层模型**\n\n开放网络互联（OSI，Open Systems Interconnection）模型将网络通信功能分为**七层**：\n\n1<!--more-->\n&nbsp;**[物理层](https://baike.baidu.com/item/%E7%89%A9%E7%90%86%E5%B1%82/4329158)**\n\n物理层是指物理通信介质和通过该介质传输数据的技术。数据通信的核心是通过光纤电缆、铜缆和空气等各种物理通道传输数字和电子信号。物理层包括与信道密切相关的技术和指标的标准，例如蓝牙、NFC 和数据传输速度。2&nbsp;**[数据链路层](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/4329290)** (ARP)\n\n数据链路层是指用于通过物理层已经存在的网络连接两台计算机的技术。该层管理数据帧，这些数据帧是封装在数据包中的数字信号。数据的流量控制和错误控制通常是数据链路层的重点。以太网是该级别标准的一个示例。数据链路层通常分为两个子层：介质访问控制（MAC）层和逻辑链路控制（LLC）层。 <br />3 **网络层** (IP, ICMP, IGMP)\n\n网络层涉及的概念包括跨分散网络或者节点或计算机的多个互连网络进行的路由、转发和寻址。网络层也可以管理流量控制。在整个互联网上，互联网协议 v4（IPv4）和 [IPv6](https://aws.amazon.com/vpc/ipv6/) 是主要的网络层协议。<br />4&nbsp;**[传输层](https://baike.baidu.com/item/%E4%BC%A0%E8%BE%93%E5%B1%82/4329536)** (TCP, UDP) \n\n传输层的主要重点是确保数据包以正确的顺序到达，没有丢失或错误，或者在需要时可以无缝恢复。流量控制和错误控制通常是传输层的重点。在这一层，常用的协议包括传输控制协议（TCP）（一种近乎无损、基于连接的协议）和用户数据报协议（UDP）（一种有损的无连接协议）。TCP 通常用于所有数据必须完好无损的情况（例如文件共享），而 UDP 则用于没有必要保留所有数据包的情况（例如视频流式传输）。<br />5 **会话层**\n\n会话层负责会话中两个独立应用程序之间的网络协调。会话管理一对一应用程序连接的开始和结束以及同步冲突。[网络文件系统（NFS）和服务器消息块（SMB）](https://aws.amazon.com/compare/the-difference-between-nfs-smb/)是会话层的常用协议。<br />6&nbsp;**[表示层](https://baike.baidu.com/item/%E8%A1%A8%E7%A4%BA%E5%B1%82/4329716)**\n\n表示层主要关注应用程序发送和使用的数据本身的语法。例如，[超文本标记语言（HTML）](https://aws.amazon.com/compare/the-difference-between-html-and-xml/)、[JavaScript 对象标记（JSON）](https://aws.amazon.com/documentdb/what-is-json/)和逗号分隔值（CSV）都是描述表示层数据结构的建模语言。  <br />7&nbsp;**[应用层](https://baike.baidu.com/item/%E5%BA%94%E7%94%A8%E5%B1%82/4329788)** \n\n**应用层关注应用程序本身的特定类型及其标准化通信方法。例如，浏览器可以使用超文本传输安全协议（HTTPS）进行通信，而 HTTP 和电子邮件客户端可以使用 POP3（邮局协议版本 3）和 SMTP（简单邮件传输协议）进行通信。**\n\n参考： [什么是 OSI 模型？](https://aws.amazon.com/cn/what-is/osi-model/)\n\n&nbsp;\n\n**链路层**主要有**三个目的**：\n\n(1)为IP模块发送和接收IP数据报;\n\n(2)为ARP模块发送 ARP请求和接收 ARP应答;\n\n(3)为RARP发送RARP请求和接收RARP应答。\n\nTCP / IP支持多种不同的链路层协议,这取决于网络所使用的硬件,如以太网、令牌环网、 FDDI(光纤分布式数据接口)及 RS-232串行线路等。\n\n**RFC 1042（IEEE802）帧格式**\n\n**<img src=\"/images/517519-20160907232758160-1699770171.png\" alt=\"\" width=\"714\" height=\"316\" />**\n\n**<strong>RFC 894（以太网）帧格式**</strong>\n\n**<strong><img src=\"/images/517519-20160907232850769-406128683.png\" alt=\"\" width=\"720\" height=\"311\" />**</strong>\n\n&nbsp;\n","tags":["计算机网络"]},{"title":"面试题目——《CC150》数组与字符串","url":"/面试题目——《CC150》数组与字符串.html","content":"<img src=\"/images/517519-20160906091742285-1376466686.png\" alt=\"\" width=\"746\" height=\"363\" />\n\n<img src=\"/images/517519-20160906092051441-1608405020.png\" alt=\"\" width=\"736\" height=\"390\" />\n\n<img src=\"/images/517519-20160906103344957-647148121.png\" alt=\"\" width=\"765\" height=\"496\" />\n\n<img src=\"/images/517519-20160906104656504-999400421.png\" alt=\"\" width=\"752\" height=\"431\" />\n\n<img src=\"/images/517519-20160906104718488-2058198759.png\" alt=\"\" width=\"741\" height=\"500\" />\n\n<!--more-->\n&nbsp;\n\n**面试题1.1：实现一个算法，确定一个字符串的所有字符是否全都不同。假使不允许使用额外的数据结构，又该如何处理？**\n\n**　　注意：ASCII字符共有255个，其中0-127的字符有字符表<img src=\"/images/517519-20160906143156957-92226008.gif\" alt=\"\" width=\"855\" height=\"584\" />**\n\n&nbsp;\n\n&nbsp;\n\n　　第一种解法：是《CC150》里面的解法\n\n```\n\tpublic static boolean checkDifferent(String iniString) {\n\t\tif(iniString == null || iniString.length() <= 0)\n\t\t\treturn true;\n\t\t\n\t\tString newString = iniString.trim();\t//去掉左右空格\n\t\tint len = newString.length();\n\t\tif(len > 256)\n\t\t\treturn false;\n\n\t\tboolean[] char_set = new boolean[65536];\t//256的牛客网会报数组越界\n\t\tfor(int i=0;i<len;i++){\n\t\t\tint val = newString.charAt(i);\t//取得字符串每一个字符的ASCII码值\n\t\t\tif(char_set[val] == true)\t\t\t\t//有val表示出现过\n\t\t\t\treturn false;\n\t\t\tchar_set[val] = true;\t\t\t\t\t\t//表示ASCII为val的字符出现过了\n\t\t}\n\t\treturn true;\n\t}\n\n```\n\n&nbsp;\n\n　　第二种解法：先排序，然后通过异或运算判断是否有重复的字符\n\n```\n\tpublic static boolean checkDifferent(String iniString) {\n\t\tif(iniString == null || iniString.length() <= 0)\n\t\t\treturn true;\n\t\t\n\t\tString newString = iniString.trim();\t//去掉左右空格\n\t\tint len = newString.length();\n\t\tif(len > 256)\n\t\t\treturn false;\n\n\t\tchar[] sortedArr = newString.toCharArray();\n\t\tArrays.sort(sortedArr);\n\t\tfor(int i=1;i<len;i++){\n\t\t\tint a = sortedArr[i-1];\n\t\t\tint b = sortedArr[i];\n\t\t\tif((a^b) == 0)\n\t\t\t\treturn false;\n\t\t}\n\t\treturn true;\n\t}\n\n```\n\n&nbsp;\n\n**面试题1.2：实现void reverse(char* str)函数，即反转一个null结尾的字符串**\n\n**&nbsp;　　注意：不分配额外空间，直接就地反转字符串，注意nul字符**\n\n```\nimport java.util.*;\n \npublic class Reverse {\n    public String reverseString(String iniString) {\n        // write code here\n        StringBuffer Buf = new StringBuffer();\n        if(iniString == null || iniString.length() <=0)\n            return Buf.toString();\n        int length = iniString.length();\n        for(int i=length-1;i>=0;i--){\n            Buf.append(iniString.charAt(i));\n        }\n        return Buf.toString();\n    }\n}\n\n```\n\n&nbsp;\n\n**面试题1.3：给定两个字符串，请编写程序，确定其中一个字符串的字符重新排列后，能否变成另一个字符串&mdash;&mdash;《Leetcode》242.&nbsp;[Valid Anagram](https://leetcode.com/problems/valid-anagram)**\n\n&nbsp;　　思路：变位词问题（anagram），注意是否区分大小写，是否考虑空白字符，如果两个字符串的长度不一样，那么就不可能变位词\n\n　　解法1：对两个字符串的字符重新排序后，再组成字符串，然后equals两个字符串是否相等\n\n```\npackage cc150;\n\nimport java.util.Arrays;\n\npublic class Anagram {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tString str1 = \"CBA\";\n\t\tString str2 = \"ABC\";\n\t\tSystem.out.println(anagram(str1, str2));\n\t}\n\t\n\tpublic static String sort(String s){\n\t\tchar[] content = s.toCharArray();\n\t\tArrays.sort(content);\n\t\treturn new String(content);\t//使用new String，toString是StringBuffer用的\n\t}\n\n\tpublic static boolean anagram(String s1,String s2){\n\t\tif(s1.length() != s2.length())\n\t\t\treturn false;\n\n\t\treturn sort(s1).equals(sort(s2));\n\t}\n\t\n}\n\n```\n\n&nbsp;\n\n　　解法2：建一个256大小的字符数组，然后在这个数组记录每个字母出现的次数，最后比较两个数组是否相等（比第一种慢，且占用空间多）（Leetcode用的类似这种）\n\n```\npackage cc150;\n\nimport java.util.Arrays;\n\npublic class Anagram {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tString str1 = \"CBA\";\n\t\tString str2 = \"ABC\";\n\t\tSystem.out.println(anagram(str1, str2));\n\t}\n\t\n\tpublic static boolean anagram(String s1,String s2){\n\t\tif(s1.length() != s2.length())\n\t\t\treturn false;\n\t\t\n\t\tint[] Arr = new int[256];\n\t\tchar[] Arr1 = s1.toCharArray();\n\t\tfor(char c:Arr1){\n\t\t\tArr[c]++;\n\t\t}\n\t\t\n\t\tfor(int i=0;i<s2.length();i++){\n\t\t\tint c = (int) s2.charAt(i);\n\t\t\tif(--Arr[c] < 0)\n\t\t\t\treturn false;\n\t\t}\n\t\treturn true;\n\t}\n\t\n}\n\n```\n\n&nbsp;\n\n**面试题1.4：编写一个方法，讲字符串中的空格全部替换为&ldquo;%20&rdquo;。假定该字符串尾部有足够的空间存放新增字符，并且知道字符串的&ldquo;真实&rdquo;长度。（注：用Java实现的话，请使用字符数组实现，以便直接在数组上操作。）&mdash;&mdash;《剑指Offer》P61**\n\n```\npackage cc150;\n\npublic class ReplaceSpaces {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tString str = \"AB  C\";\n\t\tSystem.out.println(replaceSpace(str,5));\n\t}\n\t\n\tpublic static String replaceSpace(String iniString, int length) {\n        // write code here\n\t\tint spaceCount = 0;\n\t\tfor(int i=0;i<length;i++){\n\t\t\tif(iniString.charAt(i) == ' ')\n\t\t\t\tspaceCount++;\n\t\t}\n\t\tint newLength = length + spaceCount * 2;\n\t\tchar[] newString = new char[newLength];\n\t\t\n\t\tfor(int i=length-1;i>=0;i--){\n\t\t\tSystem.out.println(iniString.charAt(i) == ' ');\n\t\t\tif(iniString.charAt(i) == ' '){\n\t\t\t\tnewString[newLength-1] = '0';\n\t\t\t\tnewString[newLength-2] = '2';\n\t\t\t\tnewString[newLength-3] = '%';\n\t\t\t\tnewLength = newLength-3; \n\t\t\t}else{\n\t\t\t\tnewString[newLength-1] = iniString.charAt(i);\n\t\t\t\tnewLength = newLength-1; \n\t\t\t}\n\t\t}\n\t\t\n\t\tStringBuffer buf = new StringBuffer();\n\t\tnewLength = length + spaceCount * 2;\n\t\tfor(int i=0;i<newLength;i++)\n\t\t\tbuf.append(newString[i]);\n\t\treturn buf.toString();\n    }\n\n}\n\n```\n\n&nbsp;\n\n**面试题1.5：利用字符重复出现的次数，编写一个方法，实现基本的字符串压缩功能。比如，字符串aabcccccaaa会变为a2b1c5a3。若&ldquo;压缩&rdquo; 后的字符串没有变短，则返回原先的字符串。**\n\n&nbsp;\n\n```\npackage cc150;\n\npublic class Zipper {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tString str = \"aabcccccaaa\";\n\t\tSystem.out.println(zipString(str));\n\t}\n\t\n\tpublic String zipString(String iniString) {\n        // write code here\n        int len = iniString.length();\n        String str = zip(iniString);\n        int lenzip = str.length();\n        if(lenzip < len)\n        \treturn str;\n        else\n        \treturn iniString;\n    }\n    \n    public static String zip(String iniString) {\n        // write code here\n\t\tString str = null;\n\t\tif(iniString == null || iniString.length() <= 0)\n\t\t\treturn str;\n\t\tchar last = iniString.charAt(0);\t//记录第一个字符\n\t\tint length = iniString.length();\n\t\tStringBuffer buf = new StringBuffer();\n\t\tbuf.append(last);\t\t\t\t\t\t//把第一个字符放进buf\n\t\tint count = 1;\t\t\t\t\t\t\t//记录重复字符的数量\n\t\tfor(int i=1;i<length;i++){\n\t\t\tif(iniString.charAt(i) == last)\n\t\t\t\tcount++;\n\t\t\telse{\n\t\t\t\tlast = iniString.charAt(i);\t//修改字符\n\t\t\t\tbuf.append(count);\t\t\t//放上一个字符的数量\n\t\t\t\tbuf.append(last);\t\t\t\t//放下一个字符\n\t\t\t\tcount = 1;\n\t\t\t}\n\t\t}\n\t\tbuf.append(count);\t\t\t\t\t//放最后一个字符的数量\n\t\treturn buf.toString();\n    }\n\t\n\t\n\n}\n\n```\n\n&nbsp;\n\n**面试题1.6：给定一幅有N&times;N矩阵表示的图像，其中每个像素的大小为4个字节，编写一个方法，将图像旋转90度。不占用额外内存空间能否做到？**\n\n```\npackage cc150;\n\npublic class Transform {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tint[][] mat = {{1,2,3},{4,5,6},{7,8,9}};\n\t\ttransformImage(mat, 3);\n\t\tfor(int i=0;i<3;i++){\n\t\t\tfor(int j=0;j<3;j++){\n\t\t\t\tSystem.out.print(mat[i][j]);\n\t\t\t}\n\t\t\t\t\n\t\t}\n\t}\n\t\n\tpublic static int[][] transformImage(int[][] mat, int n) {\n        // write code here\n\t\tfor(int layer=0;layer<n/2;layer++){\t//从最外层开始，总共有n/2层\n\t\t\tint first = layer;\t\t\t//确定每一层开始和结束的值\n\t\t\tint last = n-1-layer;\n\t\t\tfor(int i=first;i<last;i++){\n\t\t\t\tint offset = i-first;\t//记录移动的偏移量，循环的时候第一个从左上角开始向下\n\t\t\t\tint top = mat[first][i];\n\t\t\t\tmat[first][i] = mat[last-offset][first];\n\t\t\t\tmat[last-offset][first] = mat[last][last-offset];\n\t\t\t\tmat[last][last-offset] = mat[i][last];\n\t\t\t\tmat[i][last] = top;\n\t\t\t}\n\t\t}\n\t\treturn mat;\n    }\n\n}\n\n```\n\n&nbsp;\n\n**面试题1.7：编写一个算法，若M&times;N矩阵中某个元素为0，则将其所在的行与列清零。**\n\n　　注意：一个一个清零会导致最后矩阵中所有的元素都变成零，所以要记录矩阵中所有零元素的位置再清零。并不用建立一个M&times;N的数组来标记零元素的位置，只用建立两个数组分别来记录每一个零元素的横坐标和纵坐标就行了。\n\n```\npackage cc150;\n\npublic class SetZeros {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tint[][] a = {\n\t\t\t\t{1,0,3},\n\t\t\t\t{4,5,6},\n\t\t};\n\t\t//原来的数组\n\t\tfor(int i=0;i<a.length;i++){\n\t\t\tfor(int j=0;j<a[0].length;j++){\n\t\t\t\tSystem.out.print(a[i][j]+\",\");\n\t\t\t}\n\t\t\tSystem.out.println();\n\t\t}\n\t\t//置零\n\t\tsetZeros(a);\n\t\t//现在的数组\n\t\tfor(int i=0;i<a.length;i++){\n\t\t\tfor(int j=0;j<a[0].length;j++){\n\t\t\t\tSystem.out.print(a[i][j]+\",\");\n\t\t\t}\n\t\t\tSystem.out.println();\n\t\t}\n\t}\n\t\n\tpublic static void setZeros(int[][] matrix ){\n\t\tboolean[] row = new boolean[matrix.length];\n\t\tboolean[] column = new boolean[matrix[0].length];\n\t\t//记录值为0的元素所在的行和列\n\t\tfor(int i=0;i<matrix.length;i++){//行\n\t\t\tfor(int j=0;j<matrix[0].length;j++){//列\n\t\t\t\tif(matrix[i][j] == 0){\n\t\t\t\t\trow[i] = true;\n\t\t\t\t\tcolumn[j] = true;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tfor(int i=0;i<matrix.length;i++){//行\n\t\t\tfor(int j=0;j<matrix[0].length;j++){//列\n\t\t\t\tif(row[i] || column[j]){\t//\t其中有一个为true\n\t\t\t\t\tmatrix[i][j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**面试题1.8：假定有一个方法isSubstring，可检查一个单词是否为其他字符串的子串。给定两个字符串s1和s2，请编写代码检查s2是否为s1旋转而成，要求只能调用一次isSubstring。（比如，waterbottle是erbottlewat旋转后的字符串。**）\n\n　　思路：令s1为**waterbottle**，s2为**erbottlewat，**则s2（**erbottlewat**）肯定是s1s1（wat**erbottle****wat**erbottle）的子串\n\n```\npackage cc150;\n\npublic class ReverseEqual {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tString s1 = \"waterbottlea\";\n\t\tString s2 = \"erbottlewatb\";\n\t\t\n\t\tSystem.out.println(checkReverseEqual(s1,s2));\n\t}\n\t\n\tpublic static boolean checkReverseEqual(String s1, String s2) {\n        // write code here\n\t\tint len = s1.length();\n\t\tif(len == s2.length() &amp;&amp; len > 0){\n\t\t\tString s1s1 = s1 + s1;\n\t\t\t//return s1s1.indexOf(s2)>0;\n\t\t\treturn s1s1.contains(s2);\n\t\t}\n\t\treturn false;\n    }\n\n}\n\n```\n\n&nbsp;\n","tags":["刷题"]},{"title":"机器学习——GBDT","url":"/机器学习——GBDT.html","tags":["ML"]},{"title":"Java排序算法——基数排序","url":"/Java排序算法——基数排序.html","content":"<img src=\"/images/517519-20160903110212652-979739846.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160903110245558-1484076329.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160903110310386-1644400707.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160903110338933-474623252.png\" alt=\"\" />\n","tags":["算法"]},{"title":"Java排序算法——希尔排序","url":"/Java排序算法——希尔排序.html","content":"<img src=\"/images/517519-20160903104349824-1510623333.png\" alt=\"\" width=\"734\" height=\"421\" />\n\n<img src=\"/images/517519-20160903104450199-647855135.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160903104523980-1070707708.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160903104657090-515932226.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160903104717746-267566819.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n```\npackage sort;\n\nimport java.util.Arrays;\n\nclass Arrays_Shell{\n\tprivate int[] arrays;\n\tprivate int curNum;\n\n\tpublic Arrays_Shell(int max) {\t\t\t//建立一个max长度的空数组\n\t\tsuper();\n\t\tarrays = new int[max];\n\t\tcurNum = 0;\n\t}\n\t\n\tpublic void insert(int value){\t\t\t\t\t//往空的数组里面增加元素\n\t\tarrays[curNum] = value;\n\t\tcurNum++;\n\t}\n\t\n\tpublic void display(){\t\t\t\t\t\t\t\t\t//显示数组\n\t\tSystem.out.println(Arrays.toString(arrays));\n\t}\n\t\n\tpublic void ShellSort(){\n\t\tint out,in;\n\t\tint temp;\n\t\t\n\t\tint h = 1;\n\t\twhile(h <= curNum/3)\t//求出最大的增量，5刚开始的增量为4\n\t\t\th = h*3+1;\t\t\t\t\t\t//1,4,13,40,121,....\n\t\twhile(h>0){\n\t\t\tfor(out=h;out<curNum;out++){//out从1开始递增，把out前的数两两排序\n\t\t\t\ttemp = arrays[out];\n\t\t\t\tin = out;\n\t\t\t\twhile(in>h-1 &amp;&amp; arrays[in-h] >= temp){//刚开始in是比较0和h的大小\n\t\t\t\t\tarrays[in] = arrays[in-h];\n\t\t\t\t\tin -= h;\n\t\t\t\t}\n\t\t\t\tarrays[in] = temp;\n\t\t\t\t//display();\n\t\t\t}\n\t\t\t\n\t\t\th = (h-1)/3;\n\t\t}\n\t\t\n\t}\n\t\n}\n\npublic class ShellSort {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tint maxSize = 100;\n\t\tArrays_Shell arrays_demo = new Arrays_Shell(maxSize);\n\t\tarrays_demo.insert(58);\n\t\tarrays_demo.insert(57);\n\t\tarrays_demo.insert(56);\n\t\tarrays_demo.insert(60);\n\t\tarrays_demo.insert(59);\n\t\tarrays_demo.display();\n\t\tarrays_demo.ShellSort();\n\t\tarrays_demo.display();\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["算法"]},{"title":"Java排序算法——拓扑排序","url":"/Java排序算法——拓扑排序.html","content":"<img src=\"/images/517519-20160903201200764-2103385503.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160903201226077-751540767.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160903201307171-924196792.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160903201402842-1684094542.png\" alt=\"\" />\n\n课程表\n\n<img src=\"/images/517519-20200812215758645-207830337.png\" width=\"500\" height=\"320\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n邻接矩阵\n\n&nbsp;\n\n&nbsp;\n\n```\npackage graph;\n\nimport java.util.LinkedList;\nimport java.util.Queue;\n\nimport thinkinjava.net.mindview.util.Stack;\n\n//类名：Vertex\n//属性：\n//方法：\nclass Vertex{\n\tpublic char label;\t//点的名称，如A\n\tpublic boolean wasVisited;\n\t\n\tpublic Vertex(char lab){\t//构造函数\n\t\tlabel = lab;\n\t\twasVisited = false;\n\t}\n}\n\n//类名：Graph\n//属性：\n//方法：\nclass Graph{\n\tprivate final int MAX_VERTS = 20;\n\tprivate Vertex vertexList[];\t//顶点列表数组\n\tprivate int adjMat[][];\t\t\t//邻接矩阵\n\tprivate int nVerts;\t\t\t\t\t//当前的顶点\n\tprivate char sortedArray[];\n\t\n\tpublic Graph(){\t//构造函数\n\t\tvertexList = new Vertex[MAX_VERTS];\n\t\tadjMat = new int[MAX_VERTS][MAX_VERTS];\n\t\tnVerts = 0;\n\t\tfor(int j=0;j<MAX_VERTS;j++){\n\t\t\tfor(int k=0;k<MAX_VERTS;k++)\n\t\t\t\tadjMat[j][k] = 0;\n\t\t}\n\t\tsortedArray = new char[MAX_VERTS];\n\t}\n\t\n\tpublic void addVertex(char lab){\t//添加新的顶点，传入顶点的lab，并修改nVerts\n\t\tvertexList[nVerts++] = new Vertex(lab);\n\t}\n\t\n\tpublic void addEdge(int start,int end){\t//添加边，这里是无向图\n\t\tadjMat[start][end] = 1;\n\t\t//adjMat[end][start] = 1;\n\t}\n\t\n\tpublic void displayVertex(int v){\t//显示顶点\n\t\tSystem.out.print(vertexList[v].label);\n\t}\n\t\n\tpublic int getAdjUnvisitedVertex(int v){\t//返回一个和v邻接的未访问顶点\n\t\tfor(int j=0;j<nVerts;j++)\n\t\t\tif(adjMat[v][j] == 1 &amp;&amp; vertexList[j].wasVisited == false){\n\t\t\t\treturn j;\n\t\t\t}\n\t\t\treturn -1;\t//如果没有，返回-1\n\t}\n\t\n\tpublic void dfs(){\t//深度搜索\n\t\tStack<Integer> theStack = new Stack<Integer>();\n\t\tvertexList[0].wasVisited = true;\n\t\tdisplayVertex(0);\n\t\ttheStack.push(0);\t//把根入栈\n\t\t\n\t\twhile(!theStack.empty()){\n\t\t\tint v = getAdjUnvisitedVertex(theStack.peek());//取得一个和栈顶元素邻接的未访问元素\n\t\t\tif(v == -1)\t\t//如果没有和栈顶元素邻接的元素，就弹出这个栈顶\n\t\t\t\ttheStack.pop();\n\t\t\telse{\t\t\t\t//如果有这个元素，则输出这个元素，标记为已访问，并入栈\n\t\t\t\tvertexList[v].wasVisited = true;\n\t\t\t\tdisplayVertex(v);\n\t\t\t\ttheStack.push(v);\n\t\t\t}\n\t\t}\n\t\tfor(int j=0;j<nVerts;j++)\t\t//全部置为未访问\n\t\t\tvertexList[j].wasVisited = false;\n\t}\n\t\n\tpublic void bfs(){\t//广度搜索\n\t\tQueue<Integer> theQueue = new LinkedList<Integer>();\n\t\tvertexList[0].wasVisited = true;\n\t\tdisplayVertex(0);\n\t\ttheQueue.offer(0);\t//把根入队列\n\t\tint v2;\n\t\t\n\t\twhile(!theQueue.isEmpty()){\n\t\t\tint v1 = theQueue.remove();//v1记录第1层的元素，然后记录第2层第1个元素...\n\t\t\t\n\t\t\twhile((v2=getAdjUnvisitedVertex(v1)) != -1){//输出所有和第1层邻接的元素，输出和第2层第1个元素邻接的元素...\n\t\t\t\tvertexList[v2].wasVisited = true;\n\t\t\t\tdisplayVertex(v2);\n\t\t\t\ttheQueue.offer(v2);\n\t\t\t}\n\t\t}\n\t\t\n\t\tfor(int j=0;j<nVerts;j++)\t\t//全部置为未访问\n\t\t\tvertexList[j].wasVisited = false;\n\t}\n\t\n\tpublic void mst(){\t//基于深度搜索的最小生成树\n\t\tStack<Integer> theStack = new Stack<Integer>();\n\t\tvertexList[0].wasVisited = true;\n\t\ttheStack.push(0);\t//把根入栈\n\t\t\n\t\twhile(!theStack.empty()){\n\t\t\tint currentVertex = theStack.peek();\t//记录栈顶元素，当有为邻接元素的时候，才会输出\n\t\t\tint v = getAdjUnvisitedVertex(theStack.peek());//取得一个和栈顶元素邻接的未访问元素\n\t\t\tif(v == -1)\t\t//如果没有和栈顶元素邻接的元素，就弹出这个栈顶\n\t\t\t\ttheStack.pop();\n\t\t\telse{\t\t\t\t//如果有这个元素，则输出这个元素，标记为已访问，并入栈\n\t\t\t\tvertexList[v].wasVisited = true;\n\t\t\t\ttheStack.push(v);\n\t\t\t\t\n\t\t\t\tdisplayVertex(currentVertex);\n\t\t\t\tdisplayVertex(v);\n\t\t\t\tSystem.out.println();\n\t\t\t}\n\t\t}\n\t\tfor(int j=0;j<nVerts;j++)\t\t//全部置为未访问\n\t\t\tvertexList[j].wasVisited = false;\n\t}\n\t\n\tpublic int noSuccessors(){\t//使用邻接矩阵找到没有后继的顶点，有后继顶点返回行数，没有返回-1\n\t\tboolean isEdge;\n\t\t\n\t\tfor(int row=0;row<nVerts;row++){//从第1行开始\n\t\t\tisEdge = false;\n\t\t\tfor(int col=0;col<nVerts;col++){//如果某一行某一列为1，返回这个行的行数\n\t\t\t\tif(adjMat[row][col] > 0){\n\t\t\t\t\tisEdge = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif(!isEdge)\n\t\t\t\treturn row;\n\t\t}\n\t\treturn -1;\n\t}\n\t\n\tpublic void moveRowUp(int row,int length){\n\t\tfor(int col=0;col<length;col++)\n\t\t\tadjMat[row][col] = adjMat[row+1][col];\n\t}\n\t\n\tpublic void moveColLeft(int col,int length){\n\t\tfor(int row=0;row<length;row++)\n\t\t\tadjMat[row][col] = adjMat[row][col+1];\n\t}\n\t\n\tpublic void deleteVertex(int delVert){\n\t\tif(delVert != nVerts-1){\n\t\t\tfor(int j=delVert;j<nVerts-1;j++)//在数组中去掉这个顶点\n\t\t\t\tvertexList[j] = vertexList[j+1];\n\t\t\tfor(int row=delVert;row<nVerts-1;row++)//在邻接矩阵中把删除的这一行下的所有行上移\n\t\t\t\tmoveRowUp(row,nVerts);\n\t\t\tfor(int col=delVert;col<nVerts-1;col++)//在邻接矩阵中把删除的这一列下的所有列左移\n\t\t\t\tmoveColLeft(col,nVerts-1);\n\t\t}\n\t\tnVerts--;\n\t}\n\t\n\tpublic void topo(){\t//拓扑排序，必须在无环的有向图中进行，必须在有向图中\n\t\tint orig_nVerts = nVerts;\t//记录有多少个顶点\n\t\t\n\t\twhile(nVerts > 0){\n\t\t\tint currentVertex = noSuccessors();\n\t\t\tif(currentVertex == -1){\n\t\t\t\tSystem.out.println(\"错误：图含有环！\");\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tsortedArray[nVerts-1] = vertexList[currentVertex].label;\n\t\t\tdeleteVertex(currentVertex);\n\t\t}\n\t\tSystem.out.println(\"拓扑排序结果：\");\n\t\tfor(int j=0;j<orig_nVerts;j++)\n\t\t\tSystem.out.println(sortedArray[j]);\n\t\t\n\t}\n\t\n}\n\n\npublic class graph_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tGraph theGraph = new Graph();\n\t\ttheGraph.addVertex('A');\t//数组元素0\n\t\ttheGraph.addVertex('B');\t//数组元素1\n\t\ttheGraph.addVertex('C');\t//数组元素2\n\t\ttheGraph.addVertex('D');\t//数组元素3\n\t\ttheGraph.addVertex('E');\t//数组元素4\n\t\t\n//\t\ttheGraph.addEdge(0, 1);\t//AB\n//\t\ttheGraph.addEdge(1, 2);\t//BC\n//\t\ttheGraph.addEdge(0, 3);\t//AD\n//\t\ttheGraph.addEdge(3, 4);\t//DE\n\t\t\n//\t\tSystem.out.println(\"dfs访问的顺序：\");\n//\t\ttheGraph.dfs();\n//\t\tSystem.out.println();\n//\t\t\n//\t\tSystem.out.println(\"bfs访问的顺序：\");\n//\t\ttheGraph.bfs();\n\t\t\n\t\t\n\t\t\n//\t\ttheGraph.addEdge(0, 1);\t//AB\n//\t\ttheGraph.addEdge(0, 2);\t//AC\n//\t\ttheGraph.addEdge(0, 3);\t//AD\n//\t\ttheGraph.addEdge(0, 4);\t//AE\n//\t\ttheGraph.addEdge(1, 2);\t//BC\n//\t\ttheGraph.addEdge(1, 3);\t//BD\n//\t\ttheGraph.addEdge(1, 4);\t//BE\n//\t\t//theGraph.addEdge(2, 3);\t//CD\n//\t\t//theGraph.addEdge(2, 4);\t//CE\n//\t\ttheGraph.addEdge(3, 4);\t//DE\n\t\t\n//\t\tSystem.out.println(\"最小生成树：\");\n//\t\ttheGraph.mst();\n\t\t\n\t\t\n\t\ttheGraph.addVertex('F');\t//数组元素5\n\t\ttheGraph.addVertex('G');\t//数组元素6\n\t\ttheGraph.addVertex('H');\t//数组元素6\n\t\t\n\t\ttheGraph.addEdge(0, 3);\t//AD\n\t\ttheGraph.addEdge(0, 4);\t//AE\n\t\ttheGraph.addEdge(1, 4);\t//BE\n\t\ttheGraph.addEdge(2, 5);\t//CF\n\t\ttheGraph.addEdge(3, 6);\t//DG\n\t\ttheGraph.addEdge(4, 6);\t//EG\n\t\ttheGraph.addEdge(5, 7);\t//FH\n\t\ttheGraph.addEdge(6, 7);\t//GH\n\t\t\n\t\ttheGraph.topo();\n\t}\n\t\n\t\n\n}\n\n```\n\n&nbsp;\n","tags":["算法"]},{"title":"PrestoSQL330集成ranger2.3.0","url":"/PrestoSQL330集成ranger2.3.0.html","tags":["presto"]},{"title":"arthas使用方法","url":"/arthas使用方法.html","content":"arthas是阿里开发的一个java诊断工具，官网\n\n```\nhttps://arthas.aliyun.com/zh-cn/\n\n```\n\n下载地址\n\n```\nhttps://github.com/alibaba/arthas/releases\n\n```\n\n<!--more-->\n&nbsp;\n\n## 启动arthas-boot.jar\n\n选择java程序的pid，注意启动arthas-boot.jar的用户需要和所监控的java进程的用户是同一个，否则会报下面的错误\n\n```\n[ERROR] Start arthas failed, exception stack trace:\njava.io.IOException: well-known file /tmp/.java_pid29923 is not secure: file should be owned by the current user (which is 0) but is owned by 987\n\tat sun.tools.attach.LinuxVirtualMachine.checkPermissions(Native Method)\n\tat sun.tools.attach.LinuxVirtualMachine.<init>(LinuxVirtualMachine.java:117)\n\tat sun.tools.attach.LinuxAttachProvider.attachVirtualMachine(LinuxAttachProvider.java:78)\n\tat com.sun.tools.attach.VirtualMachine.attach(VirtualMachine.java:250)\n\tat com.taobao.arthas.core.Arthas.attachAgent(Arthas.java:102)\n\tat com.taobao.arthas.core.Arthas.<init>(Arthas.java:27)\n\tat com.taobao.arthas.core.Arthas.main(Arthas.java:161)\n\n```\n\n如果切换用户的时候遇到\n\n```\nThis account is currently not available.\n\n```\n\n则可以使用下面命令来切换\n\n```\nsudo su -l xxx -s /bin/bash\n\n```\n\n启动arthas\n\n```\njava -jar ./arthas-boot.jar\n[INFO] arthas-boot version: 3.6.0\n[INFO] Found existing java process, please choose one and input the serial number of the process, eg : 1. Then hit ENTER.\n* [1]: 23235\n  [2]: 66309 math-game.jar\n  [3]: 78485\n  [4]: 85979\n  [5]: 16061 org.jd.gui.OsxApp\n  [6]: 90607 org.jetbrains.jps.cmdline.Launcher\n  [7]: 88623\n2\n[INFO] arthas home: /Users/lintong/Downloads/arthas-bin\n[INFO] Try to attach process 66309\n[INFO] Attach process 66309 success.\n[INFO] arthas-client connect 127.0.0.1 3658\n  ,---.  ,------. ,--------.,--.  ,--.  ,---.   ,---.\n /  O  \\ |  .--. ''--.  .--'|  '--'  | /  O  \\ '   .-'\n|  .-.  ||  '--'.'   |  |   |  .--.  ||  .-.  |`.  `-.\n|  | |  ||  |\\  \\    |  |   |  |  |  ||  | |  |.-'    |\n`--' `--'`--' '--'   `--'   `--'  `--'`--' `--'`-----'\n\nwiki       https://arthas.aliyun.com/doc\ntutorials  https://arthas.aliyun.com/doc/arthas-tutorials.html\nversion    3.6.0\nmain_class demo.MathGame\npid        66309\ntime       2022-04-10 17:08:50\n\n[arthas@66309]$\n\n```\n\n　　\n\n## **1.查看仪表盘**\n\n输入dashboard，退出control+C\n\n## **2.查看主线程**\n\n```\n[arthas@66309]$ thread 1\n\"main\" Id=1 TIMED_WAITING\n    at java.lang.Thread.sleep(Native Method)\n    at java.lang.Thread.sleep(Thread.java:340)\n    at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)\n    at demo.MathGame.main(MathGame.java:17)\n\n```\n\n## **3.watch命令监控方法**\n\n监控返回值\n\n```\nwatch demo.MathGame primeFactors returnObj\nPress Q or Ctrl+C to abort.\nAffect(class count: 1 , method count: 1) cost in 119 ms, listenerId: 1\nmethod=demo.MathGame.primeFactors location=AtExit\nts=2022-04-10 17:16:30; [cost=1.662911ms] result=@ArrayList[\n    @Integer[2],\n    @Integer[2],\n    @Integer[2],\n    @Integer[2],\n    @Integer[2],\n    @Integer[2],\n    @Integer[2],\n    @Integer[3],\n    @Integer[419],\n]\n\n```\n\n监控参数值\n\n```\nwatch demo.MathGame primeFactors params\nPress Q or Ctrl+C to abort.\nAffect(class count: 1 , method count: 1) cost in 23 ms, listenerId: 2\nmethod=demo.MathGame.primeFactors location=AtExceptionExit\nts=2022-04-10 17:16:44; [cost=0.266471ms] result=@Object[][\n    @Integer[-150713],\n]\n\n```\n\n同时监控多个值\n\n```\nwatch org.xx.xx Func \"{params[0],params[1],target,returnObj}\" -x 2 -b -s -n 2\n\n```\n\n查看匿名内部类\n\n```\nwatch org.xx.yy$zz Func \"{params[0],params[1],target,returnObj}\" -x 2 -b -s -n 2\n\n```\n\n## 4.监控静态变量\n\n```\ngetstatic org.xx.yy.ZZ var \n\n```\n\n## **5.反编译**\n\n```\njad demo.MathGame\n\n```\n\n　　\n\n其他参考：[https://arthas.aliyun.com/doc/advanced-use.html](https://arthas.aliyun.com/doc/advanced-use.html)\n\n　　\n","tags":["arthas"]},{"title":"英文写作——冠词的使用(Use 0f Articles)","url":"/英文写作——冠词的使用(Use 0f Articles).html","content":"**1.使用'a','an','the'和不使用冠词的基本规则：**\n\n　　<1>泛指，不可数名词不能有任何冠词\n\n　　<2>泛指，可数，复数名词前不能有冠词\n\n　　<3>泛指，可数，单数名词前加'a'或者'an'\n\n　　<4>特殊的，特定的，或者一类名词中的一个前要加上'the'\n\n<!--more-->\n&nbsp;\n\n**2.不定冠词'a','an'的使用：**\n\n　　使用于单数可数名词\n\n&nbsp;\n\n**3.定冠词the的使用：**\n\n　　<1>特指的时候，如上文提到电脑，后一句说这台电脑的时候要用the\n\n　　<2>分词短语作定语的时候，如:The computer sitting on his desk\n\n　　<3>独一无二的事物，如：the sun\n\n　　<4>数量，如：the most,the least,the first,the second\n\n　　<5>特指，如：the only,the final,the primary,the principal,the same\n\n　　<6>复数名词的部分，如：some of the results,none of the finding,all of...,few of...\n\n　　<7>理论/结果/设备/方法等的名字，如：The Hubble telescope\n\n　　<8>地理地区，如：The west...,The East...,The East Coast...\n\n　　<9>一个名词后面跟着一个短语，如：The purpose of the project...,The results of this study reveal...\n\n&nbsp;\n\n**4.不用冠词：**\n\n　　<1>没有特指的不可数名词\n\n　　<2>没有特指的可数复数名词\n\n　　<3>单词跟着一个数字，如：Page 3,Gate 4\n\n&nbsp;\n\n**5.其他规则：**\n\n　　<1>缩略词保持和全程的冠词一致，如：The GDP of Korea...,A CD-ROM is...\n\n　　　　缩略词已经成为一个词了，即不用按照单个字母发音了，不用任何冠词，如：NASA sends...\n\n　　<2>日期：在十年decade和世纪使用the，如：in the 1990s,in the early 18th Century,during the Cultural Revolution,during the Ming Dynasty\n\n　　　　　　　在特定的年，月，日不使用冠词，如：in 2003,in May,on Tuesday,on June 20,2004\n\n　　<3>序数词，如：the first/the second/the nth degree\n\n　　<4>身体部位，如：the brain/the liver/the tongue\n\n　　<5>动物/器件/仪器，如：the tiger/the optical scanner\n\n　　<6>熟悉的物品/人与常识，即很容易想像，如：the door is open/the patient left the hospital...\n\n　　<7>冠词和所属名词：\n\n　　　　1.特定的名字，如：Ahmed Ali,The wangs（一家）\n\n　　　　2.海/河/沙漠/岛名，加the，如：the Atlantic Ocean/the Black Sea/the Mohabi Desert\n\n　　　　3.复数山脉/湖/岛加the，单数不加the，如：Mount St. Helens/Lake Michigan/the Hawaiian Islands/the Rocky Mountains\n\n　　　　4.大陆/国家/州/省/城市不加the，但是如果国家名字是复数，如果包含'united'或者'union','of'，加the，如：the People's Republic of China/Europe/Asia/Mexico/the United States/the United Kingdom\n\n　　　　5.街道/公园/广场，不加the，如：Rodeo Drive/Central Park\n\n　　　　6.大学，不加the，除非包含'of'，如：National College/the University of Minnesota\n\n　　　　7.公司名不加the，除非含有'company','corporation','foundation'，如：IBM，the 3M Company/the Microsoft Corporation\n\n　　　　8.建筑物加the，除了'hall'，如：the Hilton Hotel/Lind Hall\n\n&nbsp;\n","tags":["杂谈"]},{"title":"Datagrip查询开启kerberos的impala","url":"/Datagrip查询开启kerberos的impala.html","content":"1.查看impala的版本，版本是2.12.0+cdh5.16.2+0\n\n<img src=\"/images/517519-20221020222551034-1923139849.png\" width=\"600\" height=\"90\" loading=\"lazy\" />\n\n2.去官网下载impala的jdbc驱动文件：ImpalaJDBC41.jar\n\n3.添加driver，注意要使用Custom JARs\n\n<img src=\"/images/517519-20221020223445340-1554340578.png\" width=\"800\" height=\"486\" loading=\"lazy\" />\n\n4.使用keytab进行认证\n\n```\nkinit -kt ~/Downloads/hive.keytab  hive/master@HADOOP.COM\n\n```\n\n需要在你运行datagrip的机器上配置好kinit的环境，参考：[kerberos相关](https://www.cnblogs.com/tonglin0325/p/11303488.html)\n\n5.创建一个impala data source\n\n<img src=\"/images/517519-20221022235334065-683174487.png\" width=\"800\" height=\"663\" loading=\"lazy\" />\n\n6.查询\n\n<img src=\"/images/517519-20221022235636211-1541520182.png\" width=\"600\" height=\"657\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n","tags":["impala"]},{"title":"HUE认证方式","url":"/HUE认证方式.html","content":"HUE是一个支持数据库和数据仓库的开源SQL编辑器，官网\n\n```\nhttps://gethue.com/\n\n```\n\nHUE由python+django开发，其登录界面如下\n\n<img src=\"/images/517519-20220824162023148-1494392529.png\" width=\"400\" height=\"353\" loading=\"lazy\" />\n\nHUE官方支持多个认证方式，比如<!--more-->\n&nbsp;django.contrib.auth.backends.ModelBackend，desktop.auth.backend.LdapBackend等，详见如下表格\n\n\n\n|HUE支持的认证方式（也可以同时配置多个认证方式，配置文件中用逗号分隔）|备注\n| ---- | ---- \n|django.contrib.auth.backends.ModelBackend|完整的Django后端认证\n|desktop.auth.backend.AllowAllBackend|没有认证，允许所有人\n|desktop.auth.backend.AllowFirstUserDjangoBackend|第一次登录的时候，会要求你创建用户\n|desktop.auth.backend.LdapBackend|连接LDAP服务器进行认证\n|desktop.auth.backend.PamBackend|使用PAM(Pluggable Authentication Modules)即可插拔式认证模块进行认证\n|desktop.auth.backend.SpnegoDjangoBackend|Spnego模式是一种由微软提出的使用GSS-API接口的认证模式，它扩展了Kerberos协议&nbsp;\n|desktop.auth.backend.RemoteUserDjangoBackend|Django支持使用远程用户方式进行认证\n|libsaml.backend.SAML2Backend|SAML认证方式，一般用于支持SSO单点登录\n|libopenid.backend.OpenIDBackend|OpenID认证方式，比如可以使用keycloak这个开源openid方案\n|liboauth.backend.OAuthBackend|新的认证方式，支持&nbsp;Twitter, Facebook, Google+ 和 Linkedin\n\n各HUE版本支持的认证方式可以去源码里面进行查询，HUE认证的代码如下\n\n```\nhttps://github.com/cloudera/hue/blob/master/desktop/core/src/desktop/auth/backend.py&nbsp;\n```\n\n比如CDH5.16.2版本中的HUE3.9.0，只支持如下几种backend\n\n<img src=\"/images/517519-20220825112407715-1830937390.png\" width=\"800\" height=\"350\" loading=\"lazy\" />\n\n```\nhttps://github.com/cloudera/hue/blob/cdh5.16.2-release/desktop/core/src/desktop/auth/backend.py\n\n```\n\n如果要支持SAML认证，即libsaml.backend.SAML2Backend，则需要额外安装依赖\n\n```\nhttps://docs.cloudera.com/documentation/enterprise/6/6.3/topics/hue_sec_saml_auth.html\n\n```\n\n以及\n\n```\nhttps://gist.github.com/jbenninghoff/75a02c446f630dfb16886c9a5491fc4e#file-emr-hue-saml-conf-md\n```\n\n如果是HUE4.9.0，则除了上面的backend之外，还增加了KnoxSpnegoDjangoBackend和OIDCBackend\n\n```\nhttps://github.com/cloudera/hue/blob/branch-4.9.0/desktop/core/src/desktop/auth/backend.py\n\n```\n\n如果要支持openid认证，除了需要在hue.ini配置文件中将backend改成libopenid.backend.OpenIDBackend之外，还需要配置oidc相关的配置，如下\n\n```\n  # The client ID as relay party set in OpenID provider\n  oidc_rp_client_id=xxx\n\n  # The client secret as relay party set in OpenID provider\n  oidc_rp_client_secret=xx-xx-xx-xx-xx\n\n  # The OpenID provider authoriation endpoint\n  oidc_op_authorization_endpoint=https://keycloak.xxx.com/auth/realms/master/protocol/openid-connect/auth\n\n  # The OpenID provider token endpoint\n  oidc_op_token_endpoint=https://keycloak.xxx.com/auth/realms/master/protocol/openid-connect/token\n\n  # The OpenID provider user info endpoint\n  oidc_op_user_endpoint=https://keycloak.xxx.com/auth/realms/master/protocol/openid-connect/userinfo\n\n  # The OpenID provider signing key in PEM or DER format\n  ## oidc_rp_idp_sign_key=/path/to/key_file\n\n  # The OpenID provider authoriation endpoint\n  oidc_op_jwks_endpoint=https://keycloak.xxx.com/auth/realms/master/protocol/openid-connect/certs\n\n  # Whether Hue as OpenID Connect client verify SSL cert\n  oidc_verify_ssl=false\n\n  # As relay party Hue URL path to redirect to after login\n  login_redirect_url=http://xxx:8888/oidc/callback/\n\n  # The OpenID provider URL path to redirect to after logout\n  logout_redirect_url=https://keycloak.xxx.com/auth/realms/master/protocol/openid-connect/logout\n\n  # As relay party Hue URL path to redirect to after login\n  login_redirect_url_failure=http://xxx:8888/hue/oidc_failed/\n\n  # Create a new user from OpenID Connect on login if it doesn't exist\n  create_users_on_login=true\n\n  # When creating a new user, which 'claims' attribute from the OIDC provider to be used for creating the username.\n  #      Default to 'preferred_username'. Possible values include: 'email'\n  oidc_username_attribute=preferred_username\n\n```\n\noidc_username_attribute配置除了preferred_username之外，还有email，name等其他选项，可以参考\n\n```\nhttps://openid.net/specs/openid-connect-core-1_0.html\n\n```\n\n注意oidc_username_attribute选择preferred_username，如果遇到以下报错\n\n```\nCaused by: org.apache.hadoop.security.authentication.util.KerberosName$NoMatchingRule: No rules applied to xxx@xxx.com\n\n```\n\n需要在core-site.xml中修改hadoop.security.auth_to_local配置，参考：[大数据Kerberos认证报No rules applied to](https://www.jianshu.com/p/7cdd45f22cc1)&nbsp;以及\n\n```\nhttps://docs.cloudera.com/documentation/enterprise/6/6.3/topics/cdh_sg_kerbprin_to_sn.html\n\n```\n\n如果在HUE4.9.0版本中使用了OIDC认证，同时想对HUE用户默认添加组，即使用useradmin下面的default_user_group这个配置，是无法生效的，因为在HUE4.9.0源码中缺号了添加默认组的逻辑，而在最新的master分支上是有的，如下\n\n如果想使用该配置，在python源码中添加上即可\n\n<img src=\"/images/517519-20221107151100896-1998056985.png\" width=\"400\" height=\"880\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n","tags":["CDH"]},{"title":"读《浪潮之巅》有感","url":"/读《浪潮之巅》有感.html","content":"\n#### 前阵子在忙考试和论文的事，有些事情就耽搁了，所以现在补上《浪潮之巅》读后感<br/>对于一个弄潮的年轻人来讲，最幸运的，莫过于赶上一波大潮。 ———— 《浪潮之巅》<br/>\n\n<!--more-->\n\n### <font color=#ff0000><1>关于《浪潮之巅》</font>\n\n<font face=\"微软雅黑\">\n2016.5.24，《浪潮之巅》的作者吴军博士来北邮开讲座，面对学生活动中心门口排的长长的队伍，自己也懒得凑热闹。\n其实早在本科实验室的时候，实验室的学长就向我推荐过《浪潮之巅》，可是我一直没去看，所以借着这次机会就抽空看一下吧。</br>\n</font>\n\n### <font color=#ff0000><2>关于作者吴军博士</font>\n\n<font face=\"微软雅黑\">\n作者：吴军博士————\n毕业于清华大学和美国约翰·霍普金斯大学\n（百度了一下：2015-16年《美国新闻与世界报道》排名世界第12；2016年英国《泰晤士报》排名为世界第11）\n曾就职于谷歌、腾讯</br>\n</font>\n\n### <font color=#ff0000><3>有感</font>\n\n#### **电子时代已经过去**\n\n<font face=\"微软雅黑\">\n在本科的时候，在实验室主要做的是电子方面的东西，对于计算机专业的涉及不多，在阅读了《浪潮之巅》之后，深感时代的浪潮在从机械时代——>电子时代——>互联网时代的转变过程之快。</br>\n书中说，二战可以称之为机械时代和电子时代的转节点，这也正是日本电子崛起的缘由，在二战之后，日本重建把工业的重心放在了电子工业上，这也正是日本的半导体产业在亚洲处于发达地位的原因，虽然还是比不上美国。本科的时候，由于阅读的书绝大部分是电子设计方面的，我也发现日本在电子方面确实有其厉害之处，铃木雅臣的《晶体管电路设计》等书也是我电子设计过程之中很重要的参考书，这一《图解使用电子技术丛书》总共有20多本，原著作者全部都是日本人，书中的实验通俗易懂而且严谨。这也许也是日本的一些电子企业耳熟能详的一方面表面，比如三菱、东芝、瑞萨、索尼、富士通等等。</br>\n而纵观美国的一些电子企业，或者说是半导体企业，其整体也是在走着下坡路。比如看了《浪潮之巅》我才知道，安捷伦公司居然是惠普分出去的仪器部门（本科的实验室有台500M的安捷伦示波器），飞思卡尔（智能车比赛比较出名）居然是摩托罗拉公司的半导体部门，不过现在飞思卡尔已经被欧洲的恩智浦公司收购了。曾经的半导体巨头的陨落和《浪潮之巅》中所说的三大定律也有着一定的关系：</br>\n1.摩尔定律 ———— 每过18-24个月，集成电路的性能提升一倍，电子产品的价格下降一半\n2.反摩尔定律 ———— 电子产品的价格下降一半也意味这这些半导体企业的利润的减半，这就逼着这些企业通过技术进步来创造新的增长点。如果不寻找新的增长点的话，可以参考柯达公司的没落。\n3.安迪·比尔定律 ———— 安迪指英特尔前CEO安迪·格鲁夫，比尔指微软前任CEO比尔·盖茨，这句话的意思是，硬件提高的性能，很快被软件消耗掉了。比如要安装Windows Vista的入门配置是1G内存，而98就不需要了。</br>\n虽然现在摩尔定律的速度整体上来已经放缓了，但是半导体公司的利润是越来越低了。华强北电子产业的没落也是这个原因（北有中关村，南有华强北，中有广埠屯，这几个都是出了名的假货市场...），去年去过一次北京中发电子商城，那里的老板和我闲谈的时候也摇摇头说现在远不比08年巅峰的时候赚钱了，可能要转行了。七八年的时间，半导体产业就从巅峰走向了低谷，这实在有些快。\n</br>\n\n</font>\n\n#### **互联网时代早就来临**\n\n<font face=\"微软雅黑\">\n来北邮之后，深感北邮真的是全民皆码农，所以花了些时间补了计算机的知识。学习计算机方面的知识的方便之处就是只需要一台电脑一根网线就行，不像本科的时候搞电子，各种仪器各种工具各种烧钱。</br>\n前阵子看了篇微信公众号推的文章，里面说2015年诞生的APP中至少挂掉了90.19%。如果没有找到合适的盈利方面，比如Google的搜索广告系统，雅虎的门户广告（虽然雅虎现在也不行了，谁让它要和Google在Google最擅长的搜索广告领域正面刚。。），还有eBay和亚马逊的在线市场以及腾讯的虚拟物品和服务等等，那么即使占领了用户群体也很难实现盈利。这也是国内一些创业在烧完了一轮投资之后，不但没能占领到用户群体，反而没能支撑到第二轮投资而变成“一轮死”的原因。</br>\n在IT界，书中还提到了还有另外几大定律：\n1.70-20-10定律 ———— 说的是在一个行业中一定有一个老大，和老二老三等等。老大占市场份额的绝大部分，其他人分食剩下的市场份额。\n2.诺威格定律 ———— 一家公司的市场占有率超过50%之后，就无法再使市场占有率翻番了。在Google高速的生长和扩张下，市场很容易就饱和了，这也就是Google需要不断的创新来寻找新的增长点的原因。\n3.基因决定定律 ———— 在一家在某个领域特别成功的大公司已经被优化得非常适应这个市场，包括它的文化，做事方式，商业模式，市场定位等。</br>\n说到创业和投资，这实在太遥远了，《浪潮之巅》中提到了创业的大体过程，有关怎么找风投，股票分配的比例等等，看过记得不是很清楚，反正其中是很有一番学问。在书中，吴军博士多次提到公司的成功和这个公司的基因和领导人有关（书中提到了一个女CEO，不记得名字了，成功搞垮了两家公司，虽然自己赚了不少钱）。在公司的基因之中， Google创新的基因是其成功背后的一个重要的原因。说到创新，其实创新也分为两种，一种是前沿性的创新，就是暂时不能带来利益，但是对公司的未来发展有好处；第二种是在短时间内能带来经济效益的创新，但是这种不利于公司的长远发展，而取决于这两种不同的发展方面的真是公司的领导人。比如AT&T的贝尔实验室，IBM的沃森实验室等，这些实验室都能在创新上对企业给予帮助。但是某些由于公司领导人的缘由，逐渐被拆分，比如贝尔实验室，最终AT&T也沦为了运营商等级的公司。\n</br>\n</font>\n\n#### **后记**\n\n<font face=\"微软雅黑\">\n比较深刻的体会就这些，已经入手《硅谷之谜》，不过准备先看《数学之美》，因为云计算和机器学习实在太火了</br>\n</font>\n\n","tags":["杂谈"]},{"title":"ubuntu下安装gedit插件","url":"/ubuntu下安装gedit插件.html","content":"因为gedit-plugins : 依赖: gir1.2-zeitgeist-2.0\n\n所以首先\n\n```\nsudo apt-get install gir1.2-zeitgeist-2.0\n\n```\n\n<!--more-->\n&nbsp;如果报错可以先\n\n```\nsudo apt-get update\n\n```\n\n&nbsp;然后\n\n```\nsudo apt-get install gedit-plugins\n\n```\n\n&nbsp;在gedit的插件里面选中嵌入终端、单词补全等插件就行了\n","tags":["开发工具"]},{"title":"Ubuntu16.04安装cuda和pytorch","url":"/Ubuntu16.04安装cuda和pytorch.html","content":"## 1.安装cuda\n\n参考：[Ubuntu下安装CUDA](https://www.jianshu.com/p/6ef7c0e54172)\n\npytorch可以不依赖GPU运行，但是如果需要使用NVIDIA的GPU，则需要安装cuda\n\n查看是否安装cuda\n\n```\nlintong@master:~$ nvcc -V\n程序&ldquo;nvcc&rdquo;尚未安装。 您可以使用以下命令安装：\nsudo apt install nvidia-cuda-toolkit\n\n```\n\n查看GPU型号，GPU型号是GTX1050Ti\n\n```\nlspci | grep -i nvidia\n01:00.0 VGA compatible controller: NVIDIA Corporation GP107 [GeForce GTX 1050 Ti] (rev a1)\n01:00.1 Audio device: NVIDIA Corporation GP107GL High Definition Audio Controller (rev a1)\n\n```\n\n查看是否安装NVIDIA GPU的驱动，驱动的版本是430.64，最高能支持到的cuda版本是10.1\n\n```\nnvidia-smi\nSun Oct 23 20:27:21 2022\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 430.64       Driver Version: 430.64       CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 105...  Off  | 00000000:01:00.0  On |                  N/A |\n| 40%   29C    P8    N/A / 100W |    370MiB /  4036MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0      1106      G   /usr/lib/xorg/Xorg                           259MiB |\n|    0     28186      G   compiz                                       106MiB |\n|    0     28455      G   ...AAAAAAAAAAAACAAAAAAAAAA= --shared-files     1MiB |\n+-----------------------------------------------------------------------------+\n\n```\n\n去官方下载runfile来安装cuda\n\n```\nhttps://developer.nvidia.com/cuda-toolkit-archive\n\n```\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20221023213431695-955643059.png\" width=\"800\" height=\"450\" loading=\"lazy\" />\n\n安装，选择continue\n\n<img src=\"/images/517519-20221023215920689-2115287616.png\" width=\"400\" height=\"296\" loading=\"lazy\" />\n\naccept\n\n<img src=\"/images/517519-20221023220056676-1721520528.png\" width=\"400\" height=\"298\" loading=\"lazy\" />\n\n去除driver选项，然后选择install\n\n<img src=\"/images/517519-20221023220201665-1167455232.png\" width=\"400\" height=\"301\" loading=\"lazy\" />\n\n安装完成\n\n```\nsudo sh cuda_10.1.243_418.87.00_linux.run\n===========\n= Summary =\n===========\n\nDriver:   Not Selected\nToolkit:  Installed in /usr/local/cuda-10.1/\nSamples:  Installed in /home/lintong/, but missing recommended libraries\n\nPlease make sure that\n -   PATH includes /usr/local/cuda-10.1/bin\n -   LD_LIBRARY_PATH includes /usr/local/cuda-10.1/lib64, or, add /usr/local/cuda-10.1/lib64 to /etc/ld.so.conf and run ldconfig as root\n\nTo uninstall the CUDA Toolkit, run cuda-uninstaller in /usr/local/cuda-10.1/bin\n\nPlease see CUDA_Installation_Guide_Linux.pdf in /usr/local/cuda-10.1/doc/pdf for detailed information on setting up CUDA.\n***WARNING: Incomplete installation! This installation did not install the CUDA Driver. A driver of version at least 418.00 is required for CUDA 10.1 functionality to work.\nTo install the driver using this installer, run the following command, replacing <CudaInstaller> with the name of this run file:\n    sudo <CudaInstaller>.run --silent --driver\n\nLogfile is /var/log/cuda-installer.log\n\n```\n\n&nbsp;在~/.bashrc或者/etc/profile中添加，然后source\n\n```\n# cuda\nexport PATH=/usr/local/cuda-10.1/bin${PATH:+:${PATH}}\nexport LD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n\n```\n\n验证是否安装成功\n\n```\nlintong@master:~/下载$ nvcc -V\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2019 NVIDIA Corporation\nBuilt on Sun_Jul_28_19:07:16_PDT_2019\nCuda compilation tools, release 10.1, V10.1.243\n\n```\n\n禁用 Nouveau，编译`/etc/modprobe.d/blacklist.conf`，添加\n\n```\nblacklist nouveau\noptions nouveau modeset=0\n\n```\n\n若下面命令没有任何输出，则说明禁用成功\n\n```\nlsmod | grep nouveau\n\n```\n\n更新并重启\n\n```\nsudo update-initramfs -u\nsudo reboot\n\n```\n\n## 2.安装nvidia驱动\n\n重启后发现nvidia驱动掉了，nvidia-smi命令无法正常工作，导致ubuntu的图形界面无法登入，所以要使用terminal再次安装nvidia驱动\n\n关闭图形界面\n\n```\nsudo service lightdm stop\n\n```\n\n卸载原有的驱动\n\n```\nsudo apt-get remove nvidia-*\n\n```\n\n下载最新的nvidia驱动，这里的版本是515.76\n\n```\nhttps://www.nvidia.com/Download/index.aspx\n\n```\n\n下载和安装nvidia驱动\n\n```\nwget https://us.download.nvidia.cn/XFree86/Linux-x86_64/515.76/NVIDIA-Linux-x86_64-515.76.run\nsudo chmod +x ./NVIDIA-Linux-x86_64-515.76.run\nsudo ./NVIDIA-Linux-x86_64-515.76.run -no-x-check -no-nouveau-check -no-opengl-files\n\n```\n\n安装过程如何选择\n\n```\n1. There appears to already be a driver installed on your system (version:      \n  515.76).  As part of installing this driver (version: 515.76), the existing  \n  driver will be uninstalled.  Are you sure you want to continue? \n  Continue installation      Abort installation \n（选择 Coninue，如果是重装的话）\n2. The distribution-provided pre-install script failed!  Are you sure you want\n  to continue?                                                                 \n Continue installation      Abort installation       \n（选择 Cotinue)\n3. Would you like to register the kernel module sources with DKMS? This will    \n  allow DKMS to automatically build a new module, if you install a different   \n  kernel later.\nYes                       No  \n（这里选 No）\n4. Install NVIDIA's 32-bit compatibility libraries?\n    Yes                       No  \n（这里选 No）\n5. Installation of the kernel module for the NVIDIA Accelerated Graphics Driver\n  for Linux-x86_64 (version 515.76) is now complete.                           \n  OK\n6.Would you like to run the nvidia-xconfigutility to automatically update your x configuration so that the NVIDIA x driver will be used when you restart x? Any pre-existing x confile will be backed up. \n    Yes                       No  \n（这里选 Yes）\n\n```\n\nreboot重启或者启动图形界面\n\n```\nsudo service lightdm start\n\n```\n\n安装成功，ubuntu图形界面也恢复正常\n\n```\nnvidia-smi\nTue Oct 25 23:51:22 2022\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 515.76       Driver Version: 515.76       CUDA Version: 11.7     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n| 40%   36C    P0    N/A / 100W |    371MiB /  4096MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      1106      G   /usr/lib/xorg/Xorg                369MiB |\n+-----------------------------------------------------------------------------+\n\n```\n\n&nbsp;\n\n## 3.安装cuDNN\n\n参考：[https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#installlinux](https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#installlinux)\n\ncuDNN是GPU 加速的深度神经网络基元库，官网地址，下载的时候需要注册nvidia账号\n\n```\nhttps://developer.nvidia.com/rdp/cudnn-archive\n\n```\n\n下载的文件：cudnn-linux-x86_64-8.5.0.96_cuda10-archive.tar.xz\n\n安装\n\n```\nsudo tar  -xvf cudnn-linux-x86_64-8.5.0.96_cuda10-archive.tar.xz\nsudo cp cudnn-*-archive/include/cudnn*.h /usr/local/cuda/include\nsudo cp -P cudnn-*-archive/lib/libcudnn* /usr/local/cuda/lib64\nsudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*\n\n```\n\n下载的文件：cudnn-local-repo-ubuntu1604-8.5.0.96_1.0-1_amd64.deb\n\n安装\n\n```\nsudo dpkg -i ./cudnn-local-repo-ubuntu1604-8.5.0.96_1.0-1_amd64.deb\n\n```\n\n验证cudnn是否安装成功\n\n```\npython3.6\nPython 3.6.13 (default, Feb 20 2021, 21:42:50)\n[GCC 5.4.0 20160609] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> from torch.backends import cudnn\n>>> print(cudnn.is_available())\nTrue\n\n```\n\n## 4.安装pytorch\n\npytorch官方安装文档\n\n```\nhttps://pytorch.org/get-started/locally/\n\n```\n\n使用pytorch进行验证GPU是否可用\n\n如果遇到下面报错的话，说明nvidia驱动的版本过低，则需要重新安装最新的版本，这里是由于安装了430.64的低版本，重新安装515.76的最新版本后就不会报错了\n\n```\npython3.6\nPython 3.6.13 (default, Feb 20 2021, 21:42:50)\n[GCC 5.4.0 20160609] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\n>>> print(torch.cuda.is_available())\n/home/lintong/.local/lib/python3.6/site-packages/torch/cuda/__init__.py:80: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n  return torch._C._cuda_getDeviceCount() > 0\nFalse\n\n```\n\n安装515.76版本后\n\n```\npython3.6\nPython 3.6.13 (default, Feb 20 2021, 21:42:50)\n[GCC 5.4.0 20160609] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\n>>> print(torch.cuda.is_available())\nTrue\n>>> print(torch.cuda.device_count())\n1\n>>> print(torch.cuda.get_device_name(0))\n'NVIDIA GeForce GTX 1050 Ti'\n\n```\n\n　　\n","tags":["ML Infra"]},{"title":"使用JMeter进行压测","url":"/使用JMeter进行压测.html","content":"1.下载jmeter\n\n```\nhttps://jmeter.apache.org/download_jmeter.cgi\nwget https://dlcdn.apache.org//jmeter/binaries/apache-jmeter-5.4.3.tgz\ntar -zxvf apache-jmeter-5.4.3.tgz\n\n```\n\n2.修改配置成中文\n\n```\n➜  /Users/lintong/software/apache-jmeter-5.4.3/bin $ vim jmeter.properties\nlanguage=zh_CN\n\n```\n\n3.启动\n\n```\n➜  /Users/lintong/software/apache-jmeter-5.4.3/bin $ sh jmeter\n\n```\n\n<img src=\"/images/517519-20220322101509551-1195033661.png\" width=\"800\" height=\"481\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n&nbsp;\n\n　　\n","tags":["JMeter"]},{"title":"读《数学之美》有感","url":"/读《数学之美》有感.html","content":"\n#### 趁着《浪潮之巅》的余温，看看《硅谷之谜》。<br/>加州人在强调平等的同时，还有另一个侧面，就是强调通过个人奋斗往上爬的保守主义倾向。 ———— 《硅谷之谜》<br/>\n\n<!--more-->\n\n### <font color=#ff0000><1>关于《硅谷之谜》</font>\n\n<font face=\"微软雅黑\">\n上一本《浪潮之巅》看的是PDF的版本，这次的《硅谷之谜》是买的正版书，因为新书网上找不到PDF，算是支持了一下原著作者吧。</br>\n</font>\n\n### <font color=#ff0000><2>有感</font>\n\n#### **关于仙童公司**\n\n<font face=\"微软雅黑\">\n可能大多数人没有听说过这家公司，不过由于本科研究电子设计比较多，所以对于这家公司还是有所耳闻。\n说到仙童公司，就要说到硅谷的起源。在《硅谷之谜》中，提到对硅谷的诞生起到了决定作用的是三件事和一个人（肖克利）。</br>\n三件事：\n1.IBM的小沃森把IBM圣贺西研究中心的地址选在了旧金山湾区。IBM的总部原来在东海岸，这次在西海岸建了个实验室，而这个实验室是用来研制计算机的。（成果包括存储设备，IBM几个型号的计算机系统，DB2数据库系统等，这个实验室给旧金山湾区带来了大量高层次的科技人才）\n2.斯坦福大学的特曼教授发现了斯坦福遗嘱上没有写不能限制大学对外长期出租土地，所以就有斯坦福科技园的诞生。\n3.科学家肖克利（晶体管的发明人、后来为贝尔实验室赢得第一个诺贝尔奖）到加州办起了肖克利半导体实验室，并网罗了一大批英才————诺伊斯（集成电路发明人）、摩尔（提出摩尔定律）和克莱尔（凯鹏华盈创始人）</br>\n但是后来，由于肖克利并不是一个优秀的领导者，所以肖克利半导体实验室有8个人打算“叛逃”，并创办了仙童公司。这也就是著名的“八叛徒”，这八个人包括罗伯特·诺依斯（集成电路发明人）、戈登·摩尔（提出摩尔定律）、朱利亚斯·布兰克（被诺伊斯派去香港考察，开启了硅谷制造业外移的先河）、尤金·克莱尔（后来离开仙童，和惠普的高管帕金斯创办了凯鹏华盈风险投资公司）、金·赫尔尼（发明了晶体管制造的平面工艺，使仙童公司迅速发展成了全球第二大半导体公司，后来离开仙童创办Amelco公司）、杰·拉斯特（后来离开仙童创办Amelco公司）、谢尔顿·罗伯茨（后来离开仙童创办Amelco公司）和维克多·格里尼克（斯坦福研究所的研究员）。\n仙童公司迅速发展成了全球第二大半导体公司，排名第一的是德州仪器（TI）。继承电路的发明人是TI的基尔比（发明了锗晶片的集成电路）和仙童的诺依斯（发明了硅和铝膜连线的集成电路）。\n\n仙童公司一路发展，继续网罗人才，其中就包括了安迪·格鲁夫（前Intel的CEO）。但是后来，由于大量高管的离职和菲尔柴尔德家族而他们控股的这家公司安排了大量的“职业经理人”，导致的管理层的官僚作风，所以罗伯特·诺依斯和戈登·摩尔决定重新创业，这次他们还拉上了安迪·格鲁夫。，在所以的“八叛徒”都离职之后，仙童公司逐渐衰落，但是在后来发展起来的其他半导体公司中，绝大部分都是仙童公司曾经的雇员，这就体现了仙童公司在硅谷发展过程中的地位。\n\n当罗伯特·诺依斯、戈登·摩尔和安迪·格鲁夫离开了仙童之后，他们创立的这家新公司的名称就是著名的英特尔（Intel）。</br>\n</font>\n\n#### **关于风险投资**\n\n<font face=\"微软雅黑\">\n在《硅谷之谜》中，吴军博士提到几点别人认为硅谷成功的原因：\n1.气候说（舒适的地中海气候）\n2.斯坦福说（吴军博士认为硅谷对斯坦福的促进作用更大些）\n3.风险投资说\n4.政府扶持说（国情不同，在美国企业自立根生不靠政府，政府也没能力帮助企业）\n5.知识产权保护说（对知识产权的保护并非硅谷的特质）\n6.波士顿地区并没有出现硅谷（波士顿地区的条件其实并不比硅谷差）</br>\n\n其中说到风险投资，这在《浪潮之巅》中作者吴军博士也说到过，这回更为详细。\n在诺依斯创办仙童公司的时候，找到了阿瑟·洛克(风险投资之父,毕业于哈佛大学商学院,“风险投资”一词的发明人,风险投资四大巨头之一，其他三人为唐·瓦伦丁、约翰·杜尔和维诺德·科斯拉）,而洛克说服了IBM当时最大的股东菲尔柴尔德家族出资150万美金，最终成立了仙童公司。而菲尔柴尔德拥有对公司的决策权（投票权），并且有权在8年内的任何时候以300万美金的价格收购所有的股份，而这就有了现代风险投资和初创公司在股权上的一些特点。到了后来，渐渐形成初创公司都会留10-15%的期权给普通员工，这就使得一个公司的老板和下属的关系从雇佣关系变成了契约合作关系，因为公司的发展和个人的利益有关（公司收益好员工能从股票中获益），所以能激励员工的积极性。而风险投资也逐渐发展成了直接影响硅谷发展的资本力量。比如上面提到的“仙童八叛徒”之一的克莱尔创办的凯鹏华盈（KPCB）、红衫资本（唐·瓦伦丁创办，风险投资四大巨头之一）等。\n\n这些风险投资的成功案例在下面列举一下：\n1.戴维斯-洛克（硅谷第一家成功的风投公司）（硅谷本土） ———— 投资仙童、英特尔和苹果\n2.凯鹏华盈（KPCB，坚持技术价值投资的原则）（硅谷本土） ———— 1977年投资天腾电脑公司、70年代中期买下基因泰克25%的股份、投资个人电脑公司康伯、投资亚马逊、AOL、Google、网景、太阳等公司。\n3.红衫资本（创始人唐·瓦伦丁,也曾在仙童任主管销售的副总裁,有一个不成文的规定：要去投资对象团队中，至少有一个人是第一代移民，用来保证这个公司未来的冒险精神，即狼性）（硅谷本土） ———— 投资苹果、甲骨文、Google等\n4.日本的软银（外来） ———— 投资雅虎和阿里巴巴\n4.俄罗斯的DST（外来） ———— 投资Facebook、Zynga、Twitter、Groupon以及中国的小米、京东和阿里巴巴等\n\n顺便也提一下在《浪潮之巅》中讲的金融公司：\n1.商业银行（花旗银行、美洲银行、J.P.摩根银行，又称大通曼哈顿银行） \n2.投资银行（又称投资公司，高盛、摩根斯坦利和巴菲特的伯克希尔-哈撒韦等），这类银行既不能接受存款也不能向联邦储备银行借钱，它们是替别人买卖有价证券、期货、不动产和任何有价格的商品 \n3.共同基金公司（掌控这美国所有的退休账户和全世界很多的财富） \n4.对冲基金（文艺复兴技术公司）\n\n其中投资公司中：\n1.高盛 ———— 主要业务是替别人做交易（财产管理和各种各样的基金）和承包公司的上市（福特汽车、微软、雅虎、腾讯和百度等公司的上市，还注资Facebook），一般来说上市承包商可以获得融资金额7%的佣金和一些期权\n2.摩根斯坦利（脱胎于美国最大的银行J.P.摩根） ———— 承包了IBM、AT&T和通用汽车公司债券的发行、甚至联合国债券的发行、Google和黑石公司的上市，还是苹果的上市承包商\n</font>\n\n#### **作者对于硅谷成功的理解**\n\n<font face=\"微软雅黑\">\n吴军博士在书中对硅谷成功的原因的理解可以有这么几点：\n1.叛逆和对叛逆的宽容 ———— 体现在创业者在不断创业的过程中的叛逆，大学和企业对这种创业的宽容，比如Google公司就是诞生在大学的实验室中的，以及公司允许员工进行创业，并通过收购的方式收购其中成功的公司。\n2.多元文化 ———— 体现在最好的资源留给最优秀的人才，比如硅谷地区的住房和公司的办公地点，没有人认为自己的硅谷的土著。\n3.拒绝平庸 ———— 因为在公成本的硅谷，平庸就意味着将会被淘汰。\n4.宽容失败的文化 ———— 在硅谷中，创业成功的人中，第三次创业的人占大部分。\n5.工程师文化 ———— 在硅谷中，产品经理和工程师的比例很小，而且大部分产品经理也是从工程师转过来的。\n6.不迷信权威 ———— 一个很典型的例子，一位资深教授推荐自己的学生去面试，通过了，但是自己去面试的时候却因为长期疏于动手编程而没有通过面试。这样的例子也出现在Leetcode某道题目的题引中。\n7.扁平式管理 ———— 减少官僚作风，能越级了解情况。\n8.世界的情怀 ———— 这讲的是一些具有世界情怀的理想主义者，比如Google的布林常常会提出和投资一些暂时看不到效益，但是却能够改变世界的项目，例如无人驾驶汽车，Google眼镜，Google气球。还有马斯克博士（创办在线支付x.com，并且卖给了Paypal，获得了1亿美元的现金），后来他又成立了Space X公司，以及投资和创立了特斯拉公司和太阳能发电的Solar City公司。</br>\n</font>\n\n#### **关于三论**\n\n<font face=\"微软雅黑\">\n三论指的是：\n1.诺伯特·维纳的《控制论》 ———— 和机械思维的从一开始设定的参数就可以预测到结果不同，《控制论》的思想在于很多因素是没办法考虑到的，所以要在过程中不断的进行调整。典型的例子是，发射的阿波罗飞船的精准性 \n2.香农的《信息论》———— 香农用热学中熵的概念来描述不确定性，不确定越大熵就越大。典型的例子：在搜索广告中，不确定性为14比特左右，而每一词平均能提供大约10-12比特的信息量，尤其大大减小了不确定性。\n3.《系统论》 ———— 即使每一个部分做到了优秀，整体也不会是最优，典型的例子是Iphone手机。\n</br>\n</font>\n\n#### **完**\n\n","tags":["杂谈"]},{"title":"MPP数据仓库简介","url":"/MPP数据仓库简介.html","content":"MPP（**Massively Parallel Processor/大规模并行处理**）数据仓库，其属于OLAP（Online analytical processing，联机分析处理）的范畴\n\n<img src=\"/images/517519-20230825135228288-1940908073.png\" width=\"400\" height=\"340\" loading=\"lazy\" />\n\n其中，ROLAP指的是（Relational OLAP/关系OLAP）；MOLAP指的是（Multi-dimensional OLAP/多维OLAP），参考：[主流开源OLAP引擎大比拼](https://www.modb.pro/db/562271)\n\n**<strong>ROLAP 的优点和缺点**</strong>\n\nROLAP的典型代表是：Presto，Impala，Doris，GreenPlum，Clickhouse，Elasticsearch，Hive，Spark SQL，Flink SQL\n\n数据写入时，ROLAP并未使用像MOLAP那样的预聚合技术；ROLAP收到Query请求时，会先解析Query，生成执行计划，扫描数据，执行关系型算子，在原始数据上做过滤(Where)、聚合(Sum, Avg, Count)、关联(Join)，分组（Group By)、排序（Order By）等，最后将结算结果返回给用户，整个过程都是即时计算，没有预先聚合好的数据可供优化查询速度，拼的都是资源和算力的大小。\n\nROLAP 不需要进行数据预处理 ( pre-processing )，因此查询灵活，可扩展性好。这类引擎使用 MPP 架构 ( 与Hadoop相似的大型并行处理架构，可以通过扩大并发来增加计算资源 )，可以高效处理大量数据。\n\n但是当数据量较大或 query 较为复杂时，查询性能也无法像 MOLAP 那样稳定。所有计算都是即时触发 ( 没有预处理 )，因此会耗费更多的计算资源，带来潜在的重复计算。\n\n因此，ROLAP 适用于对查询模式不固定、查询灵活性要求高的场景。如数据分析师常用的数据分析类产品，他们往往会对数据做各种预先不能确定的分析，所以需要更高的查询灵活性。\n\n**MOLAP 的优点和缺点**\n\nMOLAP的典型代表是：Druid，Kylin，MOLAP一般会根据用户定义的数据维度、度量（也可以叫指标）在数据写入时生成预聚合数据；Query查询到来时，实际上查询的是预聚合的数据而不是原始明细数据，在查询模式相对固定的场景中，这种优化提速很明显。\n\nMOLAP 的优点和缺点都来自于其数据预处理 ( pre-processing ) 环节。数据预处理，将原始数据按照指定的计算规则预先做聚合计算，这样避免了查询过程中出现大量的即使计算，提升了查询性能。\n\n但是这样的预聚合处理，需要预先定义维度，会限制后期数据查询的灵活性；如果查询工作涉及新的指标，需要重新增加预处理流程，损失了灵活度，存储成本也很高；同时，这种方式不支持明细数据的查询，仅适用于聚合型查询（如：sum，avg，count）。\n\n因此，MOLAP 适用于查询场景相对固定并且对查询性能要求非常高的场景。如广告主经常使用的广告投放报表分析。虽然这么说，但是随着Doris的发布，其被广告应用于统计以及广告主报表的场景，参考：[Doris简史](https://www.oomspot.com/post/dorisjianshi)，[Apache Doris在美团外卖数仓中的应用实践](https://tech.meituan.com/2020/04/09/doris-in-meituan-waimai.html)，[日增百亿数据，查询结果秒出， Apache Doris 在 360 商业化的统一 OLAP 应用实践](https://my.oschina.net/u/5735652/blog/8694738)，[Apache Doris产品调研报告](https://www.gbase8.cn/5113)\n\n参考：[常见开源OLAP技术架构对比](http://www.hangdaowangluo.com/archives/2914)\n","tags":["MPP"]},{"title":"ant-design-pro v5学习笔记","url":"/ant-design-pro v5学习笔记.html","content":"**ant-design-pro**基于ant design和umijs，v5是目前的最新版本\n\nant design官方文档：[https://ant.design/index-cn/](https://ant.design/index-cn/)\n\numijs官方文档：[https://v3.umijs.org/zh-CN/docs/directory-structure](https://v3.umijs.org/zh-CN/docs/directory-structure)\n\n项目demo：[https://preview.pro.ant.design/dashboard/analysis](https://preview.pro.ant.design/dashboard/analysis)\n\n## 1.初始化ant-design-pro v5项目\n\n快速创建项目，ant-design-pro v5需要node14，如果版本过低的话则需要升级\n\n```\nhttps://pro.ant.design/zh-CN/docs/getting-started/\n\n```\n\n选项我选了umi@3和simple，创建出来项目结构如下\n\n```\n├── config                   # umi 配置，包含路由，构建等配置\n├── mock                     # 本地模拟数据\n├── public\n│   └── favicon.png          # Favicon\n├── src\n│   ├── assets               # 本地静态资源\n│   ├── components           # 业务通用组件\n│   ├── e2e                  # 集成测试用例\n│   ├── layouts              # 通用布局\n│   ├── models               # 全局 dva model\n│   ├── pages                # 业务页面入口和常用模板\n│   ├── services             # 后台接口服务\n│   ├── utils                # 工具库\n│   ├── locales              # 国际化资源\n│   ├── global.less          # 全局样式\n│   └── global.ts            # 全局 JS\n├── tests                    # 测试工具\n├── README.md\n└── package.json\n\n```\n\n<img src=\"/images/517519-20221117233004453-936629982.png\" width=\"200\" height=\"592\" loading=\"lazy\" />\n\n如果需要去掉手机登录相关的，参考：[修改 Antd Pro V5 登录页面](https://www.sunzhongwei.com/modify-antd-pro-v5-login-page)\n\n如果需要去掉国家化的代码，参考：[https://pro.ant.design/zh-CN/docs/getting-started](https://pro.ant.design/zh-CN/docs/getting-started)\n\n```\nnpm run i18n-remove\n\n```\n\n## 2.ant-design-pro v5项目的路由\n\n参考：[Ant Design Pro V5 的路由设置](https://www.sunzhongwei.com/ant-design-pro-v5-routing-setup)\n\nant-design-pro<!--more-->\n&nbsp;v5使用的路由是静态路由，默认配置文件在config/routes.ts，如下\n\n```\nexport default [\n  {\n    path: '/user',\n    layout: false,\n    routes: [\n      { name: '登录', path: '/user/login', component: './user/Login' },\n      { component: './404' },\n    ],\n  },\n  { path: '/welcome', name: '欢迎', icon: 'smile', component: './Welcome' },\n  {\n    path: '/admin',\n    name: '管理页',\n    icon: 'crown',\n    access: 'canAdmin',\n    routes: [\n      { path: '/admin/sub-page', name: '二级管理页', icon: 'smile', component: './Welcome' },\n      { component: './404' },\n    ],\n  },\n  { name: '查询表格', icon: 'table', path: '/list', component: './TableList' },\n  { path: '/', redirect: '/welcome' },\n  { component: './404' },\n];\n\n```\n\n效果如下\n\n<img src=\"/images/517519-20221119223710586-1003827586.png\" width=\"200\" height=\"171\" loading=\"lazy\" />\n\n路由参数含义，参考：[https://v5-pro.ant.design/zh-CN/docs/new-page](https://v5-pro.ant.design/zh-CN/docs/new-page)\n\n```\npath: 地址栏的访问路径\nname : 配置菜单的 name，如果配置了国际化，name 为国际化的 key\nicon： 配置菜单的图标，默认使用 antd 的 icon 名，默认不适用二级菜单的 icon\ncomponent：对应的文件夹目录\nredirect：重定向后的地址\naccess: 权限配置，需要预先配置权限\nhideInMenu: 可以在菜单中不展示这个路由，包括子路由。\nroutes：对应的子路由\n\n```\n\n## 3.修改页面布局\n\n如果想要修改页面的布局，可以在antd pro的演示页面中查看效果，然后复制配置，[https://preview.pro.ant.design/dashboard/analysis/?primaryColor=daybreak](https://preview.pro.ant.design/dashboard/analysis/?primaryColor=daybreak)\n\n<img src=\"/images/517519-20221124233348768-1276402377.png\" width=\"200\" height=\"486\" loading=\"lazy\" />\n\n在config/defaultSettings.ts中进行配置，参考：[https://pro.ant.design/zh-CN/docs/layout](https://pro.ant.design/zh-CN/docs/layout)\n\n<img src=\"/images/517519-20221124233419915-485295594.png\" width=\"600\" height=\"346\" loading=\"lazy\" />\n\n## 4.新增模板页面\n\n可以添加现成的umi ui模板，参考：[https://v3.umijs.org/zh-CN/docs/use-umi-ui](https://v3.umijs.org/zh-CN/docs/use-umi-ui)\n\n```\nyarn add @umijs/preset-ui -D\nUMI_UI=1 umi dev\n\n```\n\n然后访问localhost:3000的umi ui\n\n<img src=\"/images/517519-20221122234908662-1759781133.png\" width=\"500\" height=\"89\" loading=\"lazy\" />\n\n添加项目\n\n<img src=\"/images/517519-20221122234526722-399338099.png\" width=\"300\" height=\"160\" loading=\"lazy\" />\n\n添加一个监控页到项目中\n\n<img src=\"/images/517519-20221123213004661-1312941315.png\" width=\"900\" height=\"369\" loading=\"lazy\" />\n\n配置路由\n\n<img src=\"/images/517519-20221122235437130-1854292517.png\" width=\"300\" height=\"363\" loading=\"lazy\" /> <img src=\"/images/517519-20221122235751187-267257345.png\" width=\"300\" height=\"351\" loading=\"lazy\" />\n\n可以发现项目/pages目录下多了DashboardMonitor页面\n\n<img src=\"/images/517519-20221123002816022-1422440595.png\" width=\"200\" height=\"132\" loading=\"lazy\" />\n\n同时config/routes.ts也多了dashboardmonitor相关的路由\n\n<img src=\"/images/517519-20221123211934254-326555538.png\" width=\"600\" height=\"351\" loading=\"lazy\" />\n\n添加的dashboard monitor页面效果\n\n<img src=\"/images/517519-20221123212056079-784123435.png\" width=\"1000\" height=\"509\" loading=\"lazy\" />\n\nant design支持的图表可以参考：[https://charts.ant.design/zh](https://charts.ant.design/zh)\n\n如果在添加模板页面的时候遇到\n\n```\n✖  error     AssertionError [ERR_ASSERTION]: /Users/xx/.umi3/blocks/github.com/ant-design/pro-blocks/XXX don't exists \n\n```\n\n那有可能是umi3和umi2在提供的模板上有些许不同，比如umi3移除了UserLogin模板，可以对比umi项目的umi@2分支和umi@3分支\n\n```\nhttps://github.com/ant-design/pro-blocks/\n\n```\n\n如果想要添加**动态菜单**，参考：[https://procomponents.ant.design/components/layout/#%E4%BB%8E%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%8A%A0%E8%BD%BD-menu](https://procomponents.ant.design/components/layout/#%E4%BB%8E%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%8A%A0%E8%BD%BD-menu)\n\n&nbsp;\n\n## 5.对接后端API\n\n### 1.生成service\n\n参考文档：[https://pro.ant.design/zh-CN/docs/openapi](https://pro.ant.design/zh-CN/docs/openapi)\n\n可以通过后端提供的swagger-ui文档来快速生成前端代码\n\n编辑config/config.ts配置文件，在openAPI中添加如下配置\n\n<img src=\"/images/517519-20221126203239181-1551676850.png\" width=\"500\" height=\"286\" loading=\"lazy\" />\n\n然后运行命令\n\n```\nnpm run openapi\n\n```\n\n然后可以在services目录下查看新增的文件\n\n<img src=\"/images/517519-20221126203338414-436335729.png\" width=\"300\" height=\"97\" loading=\"lazy\" />\n\n### 2.修改proxy\n\n接下来需要配置proxy，参考：[https://pro.ant.design/zh-CN/docs/proxy](https://pro.ant.design/zh-CN/docs/proxy)\n\n修改config/proxy.ts配置文件\n\n```\nexport default {\n  dev: {\n    // localhost:8000/api/v1/** -> localhost:8080/api/v1/**\n    '/api/v1/': {\n      // 要代理的地址\n      target: 'http://localhost:8080/',\n      changeOrigin: true,\n      pathRewrite: { \"^\": \"\" }\n    },\n  },\n};\n\n```\n\n<img src=\"/images/517519-20221127204711737-517308601.png\" width=\"400\" height=\"175\" loading=\"lazy\" />\n\n配置过后，对于前端localhost:8000/api/v1/**的请求就会路由到后端localhost:8080/api/v1/**的地址上\n\n### 3.接入page\n\n参考：[https://v1.pro.ant.design/docs/server-cn](https://v1.pro.ant.design/docs/server-cn) 以及 [https://pro.ant.design/zh-CN/docs/request](https://pro.ant.design/zh-CN/docs/request)\n\n调用service中的方法请求接口，获得数据\n\n```\nconst data1: API.ControllerResponseT = useRequest(listDashboardUsingGET);\n\n```\n\n<img src=\"/images/517519-20221127205214334-2087047953.png\" width=\"600\" height=\"141\" loading=\"lazy\" />\n\n调整组件的对应代码，比如传值和坐标等\n\n<img src=\"/images/517519-20221127210152793-264040211.png\" width=\"500\" height=\"92\" loading=\"lazy\" />\n\n<img src=\"/images/517519-20221127210218268-2127543354.png\" width=\"500\" height=\"169\" loading=\"lazy\" />\n\n查看效果，可以根据接口返回的值绘制出对应的图表\n\n<img src=\"/images/517519-20221127210307573-1610841252.png\" width=\"600\" height=\"276\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["React"]},{"title":"ubuntu下gedit和vim输入中文和中文显示","url":"/ubuntu下gedit和vim输入中文和中文显示.html","content":"安装和配置VIM，参考<!--more-->\n&nbsp;&nbsp; http://jingyan.baidu.com/album/046a7b3efd165bf9c27fa915.html?picindex=4\n\n&nbsp;\n\n在home/你的用户名 这个目录下打开.vimrc文件，由于这种文件是隐藏的，查看的时候要使用ls -al命令查看\n\n没有就建立一个\n\n```\nvim .vimrc\n\n```\n\n&nbsp;\n\n然后输入以下配置\n\n```\nset fenc=utf-8\nset fencs=utf-8,usc-bom,euc-jp,gb18030,gbk,gb2312,cp936,big－5                     \nset enc=utf-8\nlet &amp;termencoding=&amp;encoding\n\n```\n\n&nbsp;\n\n接下来esc，:w保存就能输出中文了\n\n&nbsp;\n\n如果是gedit，使用下列命令，即可输入中文\n\n```\ngconftool-2 --set --type=list --list-type=string /apps/gedit-2/preferences/encodings/auto_detected \"[UTF-8,CURRENT,GB18030,BIG5-HKSCS,UTF-16]\"\n\n```\n\n&nbsp;\n","tags":["Linux"]},{"title":"Python学习笔记——基本语法","url":"/Python学习笔记——基本语法.html","content":"## **1.<strong>程序输入和输出**</strong>\n\n**<strong>raw_input()内建函数**</strong>\n\n```\n>>> user = raw_input('Enter your name:')\nEnter your name:root\n>>> print 'Your name is:',user\nYour name is: root\n\n```\n\n**print语句**\n\n```\n>>> myString = 'Hello World!'\n>>> print myString\nHello World!\n>>> myString\n'Hello World!'\n\n```\n\n```\n>>> print type(u\"你\")    #都能输出汉字，但是输出的类型不一样\n<type 'unicode'>\n>>> print type(\"你\")\n<type 'str'>\n\n```\n\n<!--more-->\n&nbsp;注意：在仅用变量名的时候，输出的字符串是用单引号括起来的\n\n## **2.操作符**\n\n**<1>两种除法&mdash;&mdash;单斜杠/用于传统的除法，双斜杠//用作浮点除法（对结果进行四舍五入）**\n\n**<2>乘方操作符&mdash;&mdash;双星号****\n\n**<3>标准比较操作符&mdash;&mdash;< <= > >= == != <>**\n\n**<4>逻辑操作符&mdash;&mdash;and or not**\n\n## **3.变量和赋值**\n\n**Python是动态类型语言，不需要预先声明变量的类型**\n\n**Python不支持C语言中的自增1和自减1操作符**\n\n## **4.数字**\n\n**<1>有符号整型 长整型 布尔值**\n\n**<2>浮点值**\n\n**<3>复数**\n\n## **5.字符串**\n\n```\n>>> myString = \"Hello\"\n>>> myString[0]\n'H'\n>>> myString[2:4]\n'll'\n>>> myString[1:]\n'ello'\n>>> myString[:4]\n'Hell'\n>>> myString[-1]\n'o'\n>>> myString * 2\n'HelloHello'\n>>> myString + myString\n'HelloHello'\n\n```\n\n&nbsp;\n\n```\n>>> myString = '''python\n... is cool'''\n>>> myString\n'python\\nis cool'\n>>> print myString\npython\nis cool\n\n```\n\n## **6.条件语句**\n\n```\n>>> if x>1:\n...     x -= 1\n...     print '#%d' % (x)\n... \n#2\n\n```\n\n**条件表达式**\n\n```\n>>> x = 3\n>>> x = 1 if x<3 else 2\n>>> x\n2\n\n```\n\n&nbsp;\n\n**else语句**\n\n```\n#coding:utf-8\n#!/usr/bin/env python\n'maxFact.py -- 寻找一个数的最大约数'\n\ndef showMaxFactor(num):\n\tcount = num/2\n\twhile count > 1:\n\t\tif num % count == 0:\n\t\t\tprint '%d 的最大约数是 %d' % (num,count)\n\t\t\tbreak\n\t\tcount -= 1\n\telse:\n\t\tprint num,'没有最大公约数'\n\t\t\n\t\t\nfor eachNum in range(10,21):\n\tshowMaxFactor(eachNum)\n\n```\n\n## **7.循环语句**\n\n**while循环**\n\n```\n>>> x = 0\n>>> while x<3:\n...     print x\n...     x += 1\n... \n0\n1\n2\n>>> \n\n```\n\n&nbsp;**for循环和range()内建函数**\n\n```\n>>> for x in ['A','B','C','D']:\n...     print x\n... \nA\nB\nC\nD\n\n```\n\n&nbsp;\n\n```\n>>> for x in ['A','B','C','D']:\n...     print x,\n... \nA B C D\n\n```\n\n**使用range()内建函数**\n\n```\n>>> for x in range(3):　　#循环从0到2\n...     print x,\n... \n0 1 2\n>>> \n\n```\n\n&nbsp;\n\n```\n>>> myString = 'ABCD'　　#循环字符串中的每一个字母\n>>> for x in myString:\n...     print x,\n... \nA B C D\n>>> \n\n```\n\n**使用len()内建函数**\n\n```\n>>> myString\n'ABCD'\n>>> for i in range(len(myString)):\n...     print myString[i],i\n... \nA 0\nB 1\nC 2\nD 3\n\n```\n\n**for语句用于序列类型**\n\n<1>通过序列项迭代\n\n```\n>>> List = ['a','b','c','d']\n>>> for eachList in List:\n...     print eachList\n... \na\nb\nc\nd\n\n```\n\n<2>通过序列索引迭代\n\n```\n>>> for eachList in range(len(List)):\n...     print List[eachList]\n... \na\nb\nc\nd\n\n```\n\n<3>使用项和索引迭代\n\n```\n>>> for i,eachList in enumerate(List):\n...     print \"%d %s\" % (i,eachList)\n... \n0 a\n1 b\n2 c\n3 d\n\n```\n\n&nbsp;\n\n&nbsp;**列表解析**\n\n```\n>>> square = [x ** 2 for x in range(4)]\n>>> for i in square:\n...     print i\n... \n0\n1\n4\n9\n\n```\n\n&nbsp;\n\n```\n>>> square = [x ** 2 for x in range(4) if not x % 2]    #如果整除2\n>>> for i in square:\n...     print i\n... \n0\n4\n\n```\n\n## **8.迭代器（RandSeq和AnyIter）**\n\n### **1.RandSeq**\n\n```\n#coding:utf-8\n#!/usr/bin/env python\n'randSeq.py -- 迭代'\n\n#从random模块里仅仅导入choice方法\nfrom random import choice\t\n\nclass RandSeq(object):\n\tdef __init__(self,seq):\n\t\tself.data = seq;\n\t\t\n\tdef __iter__(self):\n\t\treturn self;\n\t\n\tdef next(self):\n\t\treturn choice(self.data)\n\t\t\nif __name__ == '__main__':\n\tfor eachItem in RandSeq(('rock','paper','scisc')):\n\t\tprint eachItem\n\n\n\n\n\t\n\n```\n\n## **9.文件和内建函数open()、file()**\n\n```\n>>> filename = raw_input('Enter file name:')\nEnter file name:/home/common/software/hexo/source/_posts/book_2016.md\n>>> fobj = open(filename,'r')\n>>> for eachLine in fobj:\n...     print eachLine,\n...\n\n\n>>> fobj.close()\n\n```\n\n### **文件写入**\n\n```\n#coding:utf-8\n#!/usr/bin/env python\n'makeTextPyhton.py -- create text file'\n\nimport os\nls = os.linesep\n\n#输入文件名\nfname = raw_input('Enter file name:')\n\n# 判断输入的文件名是否存在\nwhile True:\n\n\tif os.path.exists(fname):\n\t\tprint \"错误：'%s' 已经存在\" % fname\n\telse:\n\t\tbreak\n\n#按行输入文件的内容，以.为结束每一行，以单个.结束整个输入\nall = []\nprint \"\\n请以.结束每一行内容\\n\"\n\n#循环\nwhile True:\n\tentry = raw_input(\"输入每一行： \")\n\tif entry == \".\":\n\t\tbreak\n\telse:\n\t\tall.append(entry)\n\n#把所有的内容写入到文件中\nfobj = open(fname,'w')\nfobj.writelines([\"%s%s\" % (x,ls) for x in all])\nfobj.close()\nprint \"完成！\"\n\n```\n\n### **文件读取和显示**\n\n```\n#coding:utf-8\n#!/usr/bin/env python\n'readTextPyhton.py -- read and display text file'\n\n#输入文件名\nfname = raw_input('Enter file name:')\nprint\n\n#尝试打开和显示文件\ntry:\n\tfobj = open(fname,'r')\nexcept IOError,e:\n\tprint \"*** file open error:\",e\nelse:\n\t#显示内容\n\tfor eachLine in fobj:\n\t\tprint eachLine,\n\tfobj.close()\n\n```\n\n## **10.函数**\n\n```\n>>> def addMe2Me(x):\n...     return (x+x)\n... \n>>> addMe2Me(2.5)\n5.0\n\n```\n\n### **标准类型内建函数**\n\n**　　<1>type() &mdash;&mdash;&nbsp;**返回对象的类型**<br />**\n\n**　　<2>cmp() &mdash;&mdash;&nbsp;**比较两个对象，返回两个对象的ASCII码的差**<br />**\n\n**　　<3>str()、repr() &mdash;&mdash;&nbsp;**以字符串的方式获取对象的内容，str()适合于输出，repr()适合于使用eval()重新得到该对象，此外``不推荐使用**<br />**\n\n**　　<4>type()、isinstance() &mdash;&mdash;&nbsp;**确认一个对象的类型**<br />**\n\n### **序列类型函数**\n\n**&nbsp;　<1>len() &mdash;&mdash;&nbsp;**返回字符串的字符数　\n\n**　 <2>max()和min() &mdash;&mdash;&nbsp;**返回字符串中最大或者最小的字符（按照ASCII码值排列）\n\n**　 <3>enumerate() &mdash;&mdash;&nbsp;**用于for循环\n\n```\n>>> s = 'ABCDEF'\n>>> for i,t in enumerate(s):\n...     print i,t\n... \n0 A\n1 B\n2 C\n3 D\n4 E\n5 F\n\n```\n\n**&nbsp;　<4>zip() &mdash;&mdash;&nbsp;**返回一个列表\n\n```\n>>> s,t = 'ABC','DEF'\n>>> zip(s,t)\n[('A', 'D'), ('B', 'E'), ('C', 'F')]\n\n```\n\n### **字符串类型函数**\n\n**　　<1>raw_input() &mdash;&mdash;**&nbsp;用于输入\n\n```\n>>> userinput = raw_input(\"Enter user name:\")\nEnter user name:XiaoMing\n>>> userinput\n'XiaoMing'\n\n```\n\n**&nbsp;　　<2>chr() &mdash;&mdash;**&nbsp;输入是0-255的整数，输出的是一个对应的字符\n\n　　　　　**unichr() &mdash;&mdash;**&nbsp;输入是整数，返回的是一个对应的Unicode字符\n\n**　　　　&nbsp; ord() &mdash;&mdash;**&nbsp;输入是一个字符，返回是对应的ASCII码或者Unicode数值\n\n　　&nbsp;**<3>其他**&nbsp;&mdash;&mdash; 《Pyhton核心编程》P122\n\n## **11.类**\n\n```\n>>> class myClass(object):\n...     def _init_(self,nm='123'):\n...             self.name = nm\n...             print 'Constructor'\n...     def showname(self):\n...             print 'My name is',self.name\n... \n>>> newClass = myClass()\n>>> newClass._init_()\nConstructor\n>>> newClass.showname()\nMy name is 123\n\n```\n\n## **12.模块**\n\n```\n>>> import sys\n>>> sys.stdout.write('Hello Word!\\n')\nHello Word!\n>>> sys.platform\n'linux2'\n>>> sys.version\n'2.7.6 (default, Jun 22 2015, 18:00:18) \\n[GCC 4.8.2]'&nbsp;\n```\n\n## **13.基本规则和特殊字符**\n\n<1># &mdash;&mdash; 注释\n\n<2>\\n &mdash;&mdash; 行分隔符\n\n<3>\\ &mdash;&mdash; **继续上一行**\n\n<4>; &mdash;&mdash; 将两个语句连接到一行中\n\n<5>: &mdash;&mdash; 将代码块的头和体分开\n\n## **14.Python对象**\n\n\n\n|**标准类型**|**其他内建类型**\n| ---- | ---- \n|**数字&mdash;&mdash;Integer整型；Boolean布尔型；Long integer 长整型；Floating point number 浮点型；Complex number复数型**|**类型&mdash;&mdash;type(1)&mdash;&mdash;><type,'int'> 得到特定对象的类型信息**\n|**字符串&mdash;&mdash;String**|**Null对象（None）&mdash;&mdash;不支持任何运算，也没有任何内建方法**\n|**列表&mdash;&mdash;List []**|**文件**\n|**元组&mdash;&mdash;Tuple ()**|**集合/固定集合**\n|**字典&mdash;&mdash;Dictionary**|**函数/方法**\n|&nbsp;|**模块**\n|&nbsp;|**类**\n\n### **列表**\n\n**1.创建列表类型数据并给其赋值**\n\n```\n>>> aList = [123,'abc',4.56,['inner','list'],7-9j]\n>>> aList\n[123, 'abc', 4.56, ['inner', 'list'], (7-9j)]\n>>> list('ABC')\n['A', 'B', 'C']\n\n```\n\n**2.访问列表中的值**\n\n```\n>>> aList[0]\n123\n>>> aList[1:4]\n['abc', 4.56, ['inner', 'list']]\n>>> aList[:3]\n[123, 'abc', 4.56]\n>>> aList[3][1]\n'list'\n\n```\n\n**&nbsp;3.如何更新列表**\n\n```\n>>> aList[2]='ABC'\n>>> aList\n[123, 'abc', 'ABC', ['inner', 'list'], (7-9j)]\n>>> aList.append(\"Hello Word\")\n>>> aList\n[123, 'abc', 'ABC', ['inner', 'list'], (7-9j), 'Hello Word']\n\n```\n\n**4.删除列表中的元素或者列表本身**\n\n```\n>>> del aList[1]\n>>> aList\n[123, 'ABC', ['inner', 'list'], (7-9j), 'Hello Word']\n>>> aList.remove(123)\n>>> aList\n['ABC', ['inner', 'list'], (7-9j), 'Hello Word']<br />>>> del aList\n\n```\n\n**5.标准类型操作符**\n\n列表比较操作的时候，是两个列表的元素分别比较，直到有一方的元素胜出\n\n**6.序列类型操作符**\n\n**　　<1>切片（[]和[:]）**\n\n```\n>>> aList\n[123, 'abc', 'ABC', ['inner', 'list'], (7-9j), 'Hello Word']\n>>> aList[-2:-1]\n[(7-9j)]\n>>> aList[2:-1]\n['ABC', ['inner', 'list'], (7-9j)]\n\n```\n\n　　**<2>成员关系操作（in,not in）**\n\n　　　　输出的是True或者False\n\n**　　<3>连接操作符（+）**\n\n```\n>>> aList=[1,2,3,4]\n>>> bList=['A','B','C','D']\n>>> cList = aList+bList\n>>> cList\n[1, 2, 3, 4, 'A', 'B', 'C', 'D']\n\n```\n\n**&nbsp;　　<4>重复操作符（*）**\n\n```\n>>> aList * 2\n[1, 2, 3, 4, 1, 2, 3, 4]\n\n```\n\n**7.标准类型函数cmp()**\n\n　　比较的规则P140\n\n**8.序列类型函数**\n\n**　　<1>len()&mdash;&mdash;**对列表和元组返回的是列表或者元组的元素个数\n\n**&nbsp;　&nbsp; <2>max()和min()&mdash;&mdash;**比较大小\n\n**　　<3>sorted()和reversed()&mdash;&mdash;**　\n\n```\n>>> bList\n['A', 'B', 'C', 'D']\n>>> for t in reversed(bList):\n...     print t\n... \nD\nC\nB\nA\n>>> bList=['D','C','B','A']\n>>> sorted(bList)\n['A', 'B', 'C', 'D']\n\n```\n\n**&nbsp;　　<4>enumerate()和zip()&mdash;&mdash;**\n\n```\n['A', 'B', 'C', 'D']\n>>> for i,t in enumerate(bList):\n...     print i,t\n... \n0 A\n1 B\n2 C\n3 D\n>>> aList\n[1, 2, 3, 4]\n>>> bList\n['A', 'B', 'C', 'D']\n>>> for i,j in zip(aList,bList):\n...     print ('%s %s' % (i,j)).title()\n... \n1 A\n2 B\n3 C\n4 D\n\n```\n\n**&nbsp;　　<5>sum()**\n\n```\n>>> aList\n[1, 2, 3, 4]\n>>> sum(aList)\n10\n\n```\n\n**&nbsp;　　<6>list()和tuple()&mdash;&mdash;**用于列表和元组之间的转换\n\n**9.列表类型的内建函数&mdash;&mdash;P142**\n\n```\n>>> dir(aList)\n['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__delslice__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getslice__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__setslice__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']\n\n>>> aList\n[1, 2, 3, 4]\n>>> aList.append('A')\n>>> aList\n[1, 2, 3, 4, 'A']\n>>> aList.count('A')\n1\n>>> aList.insert(1,1.5)\n>>> aList\n[1, 1.5, 2, 3, 4, 'A']\n>>> 'A' in aList\nTrue\n>>> aList.index(1.5)\n1\n\n>>> aList.sort()\n>>> aList\n[1, 1.5, 2, 3, 4, 'A']\n>>> aList.reverse()　　#没有返回值的函数会直接改变对象的值\n>>> aList\n['A', 4, 3, 2, 1.5, 1]\n\n>>> bList.extend(aList)\n>>> bList\n['A', 'B', 'C', 'D', 'A', 4, 3, 2, 1.5, 1]\n\n```\n\n&nbsp;\n\n**10.append和extend**\n\n**<img src=\"/images/517519-20161110163304264-1576701809.png\" alt=\"\" />**\n\n&nbsp;\n\n### 元组\n\n**1.创建一个元组并给它赋值**\n\n```\n>>> aTuple = (123,'abc',4.56,['inner','tuple'],7-9j)\n>>> aTuple\n(123, 'abc', 4.56, ['inner', 'tuple'], (7-9j))\n>>> tuple('ABC')\n('A', 'B', 'C')\n\n```\n\n**&nbsp;2.访问元组中的值**\n\n```\n>>> aTuple[0]\n123\n>>> aTuple[1:4]\n('abc', 4.56, ['inner', 'tuple'])\n>>> aTuple[:3]\n(123, 'abc', 4.56)\n>>> aTuple[3][1]\n'tuple'\n\n```\n\n**&nbsp;3.更新元组**\n\n　　元组和字符串一样，是不可变类型，不能更新或者改变元组的元素\n\n**4.移除一个元组以及元组本身**\n\n　　删除一个单独的元组元素是不可能的，可以用del语句删除一个元组\n\n**5.元组操作符和内建函数**\n\n　　标准类型操作符、序列类型操作符和内建函数和列表的相似\n\n**6.元组的特殊特性**\n\n　　3个标准的**不可变类型**&mdash;&mdash;**数字、字符串和元组字符串**\n\n**　　元组类型包含的可变对象是可以改变的，比如元组中包含一个列表**\n\n### **集合**\n\n有两种不同的类型&mdash;&mdash;**可变集合**（set）和**不可变集合**（frozenset）\n\n&nbsp;\n\n**可变集合**不是可哈希的，不能用作字典的键，也不能用做其他集合中的元素\n\n**不可变集合**是有哈希值的，能被用做字典的键或者是作为集合中的一个成员\n\n&nbsp;\n\n**1.创建集合类型和给集合赋值**\n\n```\n>>> s = set('cheeseshop')\n>>> s\nset(['c', 'e', 'h', 'o', 'p', 's'])\n>>> t = frozenset('bookshop')\n>>> t\nfrozenset(['b', 'h', 'k', 'o', 'p', 's'])\n>>> type(s)\n<type 'set'>\n>>> type(t)\n<type 'frozenset'>\n>>> len(s)\n6\n>>> len(t)\n6\n>>> s == t\nFalse\n\n```\n\n**2.访问集合中的值**\n\n```\n>>> 'k' in s\nFalse\n>>> 'k' in t\nTrue\n>>> for i in s:\n...     print i\n... \nc\ne\nh\no\np\ns\n\n```\n\n**3.更新集合**\n\n```\n>>> s.add('z')\n>>> s\nset(['c', 'e', 'h', 'o', 'p', 's', 'z'])\n>>> s.update('abc')\n>>> s\nset(['a', 'c', 'b', 'e', 'h', 'o', 'p', 's', 'z'])\n>>> s.remove('z')\n>>> s\nset(['a', 'c', 'b', 'e', 'h', 'o', 'p', 's'])\n>>> s -= set('abc')\n>>> s\nset(['e', 'h', 'o', 'p', 's'])\n\n```\n\n&nbsp;　　不可变集合不能修改\n\n**4.删除集合中的成员和集合**\n\n```\n>>> del t\n\n```\n\n**5.集合类型操作符**\n\n　　<1>标准类型操作符\n\n　　　　**1.成员关系**(in,not in)\n\n　　　　**2.集合等价/不等价**(==,!=)\n\n　　　　**3.子集/超集**(<,<=,>,>=)\n\n　　<2>集合类型操作符\n\n　　　　**1.联合(|)**\n\n```\n>>> s|t\nset(['b', 'e', 'h', 'k', 'o', 'p', 's'])\n\n```\n\n　　　**　2.交集(&amp;)**\n\n```\n>>> s&amp;t\nset(['h', 's', 'o', 'p'])\n\n```\n\n　　　　**3.差补/相对补集(-)**\n\n```\n>>> s-t\nset(['e'])\n\n```\n\n**　　　&nbsp; 4.对称差分(^)，即(XOR)异或，只能是属于集合s或者集合t的成员，不能同时属于两个集合**\n\n```\n>>> s^t\nset(['b', 'e', 'k'])\n\n```\n\n**　　　&nbsp; 5.混合集合类型操作**\n\n&nbsp;　　　　　　类型不相同的时候，产生的结果类型和**左操作数的类型相同**\n\n　　<3>集合类型操作符(仅适用于可变集合)\n\n　　　　**1.update()方法或者|=**\n\n```\n>>> s|=t\n>>> s\nset(['b', 'e', 'h', 'k', 'o', 'p', 's'])\n\n```\n\n**　　&nbsp; 　2.intersection_update()方法或者&amp;=，保留两个集合重复的成员**\n\n```\n>>> s\nset(['a', 'c', 'b', 'd'])\n>>> t\nset(['e', 'd', 'f'])\n>>> s&amp;=t\n>>> s\nset(['d'])\n\n```\n\n**&nbsp;　　　3.difference_update()方法或者-=**\n\n**　　　 4.symmetric_difference_update()方法或者^=，差分操作**\n\n**6.内建函数**\n\n　　<1>标准类型函数&mdash;&mdash;len()\n\n&nbsp;　&nbsp; <2>集合类型工厂函数&mdash;&mdash;set()和frozenset()\n\n**7.集合类型内建方法&mdash;&mdash;P184**\n\n### 字典\n\n**1.创建字典和给字典赋值**，可以使用**工厂方法dict()**来创建字典，也可以使用**fromkeys()**来创建一个**元素具有相同值**的字典\n\n```\n>>> dict = {'name':'XiaoMing','age':20}\n>>> dict\n{'age': 20, 'name': 'XiaoMing'}\n\n\n>>> dict2 = dict((['x',1],['y',2]))\n>>> dict2\n{'y': 2, 'x': 1}\n\n>>> dict2 = {}.fromkeys(('x','y'),1)\n>>> dict2\n{'y': 1, 'x': 1\n\n```\n\n**&nbsp;2.访问字典中的值**\n\n```\n>>> dict = {'name':'XiaoMing','age':20}\n>>> dict\n{'age': 20, 'name': 'XiaoMing'}\n>>> for key in dict.keys():\n...     print 'key=%s,value=%s' % (key,dict[key])\n... \nkey=age,value=20\nkey=name,value=XiaoMing\n\n```\n\n```\n>>> for key in dict:\n...     print 'key=%s,value=%s' % (key,dict[key])\n... \nkey=age,value=20\nkey=name,value=XiaoMing\n\n```\n\n&nbsp;\n\n```\n>>> dict['name']\n'XiaoMing'\n\n```\n\n```\n>>> 'name' in dict\nTrue\n\n```\n\n**&nbsp;3.更新字典**\n\n```\n>>> dict['name'] = '123'\n>>> dict['name']\n'123'\n\n```\n\n```\n>>> del dict['name']\n>>> dict\n{'age': 20}\n\n```\n\n&nbsp;\n\n```\n>>> dict\n{'age': 20}\n>>> dict.pop('age')\n20\n\n```\n\n**&nbsp;4.映射类型相关的函数**\n\n　　<1>dict()&mdash;&mdash;创建字典\n\n　　<2>len()&mdash;&mdash;返回键值对的数目\n\n　　<3>hash()&mdash;&mdash;可以判断某个对象是否可以做一个字典的值\n\n**5.映射类型内建方法**\n\n　　<1>keys()&mdash;&mdash;返回一个列表，包含字典中所有的键\n\n　　<2>values()&mdash;&mdash;返回一个列表，包含字典中所有的值\n\n　　<3>items()&mdash;&mdash;返回一个包含所有（键，值）元组的列表\n\n注意：返回的元素是没有顺序的，可以通过sorted()方法进行排序\n\n　　<4>update()&mdash;&mdash;将一个字典的内容添加到另外一个字典中\n\n　　<5>clear()&mdash;&mdash;删除字典中的所有条目\n\n　　<6>copy()&mdash;&mdash;返回一个字典的副本\n\n　　<7>get()&mdash;&mdash;根据键查询值，键不存在的话返回None\n\n　　<8>setdefault()&mdash;&mdash;检查字典中是否含有某个键，如果存在就返回这个值；不存在就赋值并返回这个值\n\n## **15.操作符**\n\n### **<strong>比较类型操作符**</strong>\n\n<1>对象值的比较 < > <= >= == !=**<br />**\n\n<2>对象身份比较 is　　is not　　比较是否指向同一个对象 注意：Python中整型对象和字符串对象是不可变对象\n\n### **格式化操作符（%）**\n\n字符串模板&mdash;&mdash;字符串Template对象\n\n```\n>>> from string import Template\n>>> s = Template(\"This is ${name} ${num}\")\n>>> print s.substitute(name=\"Python\",num=3)\nThis is Python 3\n\n```\n\n## **16.其他**\n\n### **1.Non-ASCII character错误**\n\n**源代码文件第一行添加**\n\n```\n#coding:utf-8\n\n```\n\n### 2.Python终端自动补全\n\n在～目录下添加一个文件，名字为.pythonstartup.py\n\n```\n#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport readline, rlcompleter;\nreadline.parse_and_bind(\"tab: complete\"); # 启用Tab补全\n\ndef igtk():\n   globals()['gtk'] = __import__('gtk');\n   globals()['thread'] = __import__('thread');\n   gtk.gdk.threads_init();\n   thread.start_new_thread(gtk.main, ());\n   pass;\n\n```\n\n&nbsp;然后在.bashrc中添加\n\n```\nexport PYTHONSTARTUP=~/.pythonstartup.py\n\n```\n\n### 3.解析json\n\njson.dumps : dict转成str，将字典转换为字符串\n\njson.loads：str转成dict，将字符串转换为字典\n\n参考：[json.dumps和 json.loads 区别，如此简单](https://blog.csdn.net/qiqiyingse/article/details/70049591)\n\n&nbsp;\n","tags":["Python"]},{"title":"Presto学习笔记——Event Listener","url":"/Presto学习笔记——Event Listener.html","tags":["presto"]},{"title":"ubuntu下安装TexLive和Texmaker","url":"/ubuntu下安装TexLive和Texmaker.html","content":"**1.首先下载TexLive2015的ISO文件，挂载并安装**\n\n**参考：<strong>[ubuntu14.04配置中文latex完美环境（texlive+texmaker+lyx)](http://www.cnblogs.com/wuchanming/p/4018031.html)**</strong>\n\n**中文字体设置参考：<strong>[ubuntu 下安装 texlive 并设置 ctex 中文套装](http://www.cnblogs.com/lienhua34/p/3675027.html)**</strong>\n\n**<img src=\"/images/517519-20180503001501947-1332425070.png\" alt=\"\" />**\n\n首先新建一个目录，文件就挂载在这个目录下\n\n```\nsudo mkdir /media/cdimage\n\n```\n\n挂载，并使其有读写权限\n\n```\nsudo mount -o rw,loop /media/lintong/工作/Ubuntu_Software/64bit_Software/TeXLive/TeXLive2015/texlive2015.iso /media/cdimage\n\n```\n\n<!--more-->\n&nbsp;使用下面的命令会出现问题：mount: /dev/loop3 is write-protected, mounting read-only\n\n```\nsudo mount -o loop /media/XXXX/TeXLive/TeXLive2015/texlive2015.iso /media/cdimage\n\n```\n\n进入挂载目录\n\n```\ncd /media/cdimage\n\n```\n\n&nbsp;进行安装\n\n```\nsudo perl install-tl -gui\n\n```\n\n卸载镜像文件\n\n```\ncd ~\nsudo umount /media/cdimage\nsudo rmdir /media/cdimage\n\n```\n\n安装的路径在usr/local/下\n\n添加环境变量，之后source一下\n\n```\n#Texlive\nexport PATH=/usr/local/texlive/2015/bin/x86_64-linux:$PATH\nexport MANPATH=/usr/local/texlive/2015/texmf-dist/doc/man:$MANPATH\nexport INFOPATH=/usr/local/texlive/2015/texmf-dist/doc/info:$INFOPATH\n\n```\n\n测试是否安装成功\n\n```\ntex -version\nTeX 3.14159265 (TeX Live 2015)\nkpathsea version 6.2.1\nCopyright 2015 D.E. Knuth.\nThere is NO warranty.  Redistribution of this software is\ncovered by the terms of both the TeX copyright and\nthe Lesser GNU General Public License.\nFor more information about these matters, see the file\nnamed COPYING and the TeX source.\nPrimary author of TeX: D.E. Knuth.\n\n```\n\n&nbsp;\n\n&nbsp;**2.安装Texmaker**\n\n**参考：[Ubuntu12.04&nbsp;安装LaTex（TexLive+TexMaker+中文环境）](http://blog.sina.com.cn/s/blog_631d3a630102uys4.html)**\n\n**之后在Ubuntu软件中心安装就好了**\n\n**<img src=\"/images/517519-20180503005327514-1755900711.png\" alt=\"\" />**\n\n&nbsp;打开，终端输入texmaker\n\n&nbsp;\n\n开始安装\n\n1、清理系统环境\n\n如果你以前就安装有TexMaker或者其他Tex编辑器以及TexLive，建议全部清除重新安装&mdash;&mdash;一定要连配置文件一起删除&hellip;&hellip;\n\n我估计大部分人都是用apt-get 命令之间安装的TexLive以及其他的东西，在删除的时候请这样删除\n\n```\nsudo apt-get remove --purge 你要删除的包\n\n```\n\n2、下载TexLive的iso文件并安装\n\n推荐2013等最新的TexLive，他对于中文的支持十分的好。至于挂载什么的，请自行百度ubuntu安装TexLive有大量的文章讲述。\n\n但是我要强调的是：不要使用shell命令安装，因为命令安装貌似不会给你配置系统变量。（我会告诉你我真的这样干了一次么？）\n\n这个时候记住你的TexLive安装的目录，在ubuntu12.04下一般是/usr/local/texlive/2013/bin\n\n这里需要说明一下linux下的文件路径的一些&ldquo;潜规则&rdquo;：一般bin文件夹都是一些二进制文件&mdash;&mdash;就是win下面的可执行文件。\n\n另外，这里的2013（这个文件夹显然是按照版本明明的&hellip;&hellip;2013版）这个文件夹同父目录下有个文件夹是用来放自己下载的其他的tex模版什么的&hellip;&hellip;我这里这个文件夹叫做texmf-local。\n\n3、下载TexMaker并安装\n\n我必须要说明的是：为什么要卸载之前的TexMaker&mdash;&mdash;之前的TexMaker导致你不得不重新配置路径&hellip;&hellip;而且还很容易错&hellip;&hellip;\n\n去TexMaker的官方网站上下载一个deb包到本地直接鼠标点击用&ldquo;Ubuntu软件中心&rdquo;安装就可以了，为什么要单独下载而不用apt-get或者 &ldquo;Ubuntu软件中心&rdquo;搜索一下安装呢？主要是防止Ubuntu系统贱贱的非要给你安装个他那个过时的（大概是2009版本？）、对中文支持十分糟糕的 版本。\n\n安装好以后并不是直接能用了，在shell里面输入\n\n```\nsudo apt-get install --fix texmaker\n\n```\n\n这样安装的话，ubuntu就不会给你贱贱的安装texlive而且在这个过程中会补全texmaker所依赖的包。\n\n4、TexMaker配置\n\n首先把软件换成中文界面（事实证明，英文界面会大大减弱你接下来步骤的正确性）：菜单栏->options->interface languages->zh_CN\n\n当然了，另外那个zh也是中文&hellip;&hellip;解放台湾岛，活捉林志凌\n\n现在，重启TexMaker以后，在菜单栏->选项->配置TexMaker打开后的那个窗口里面有个&ldquo;快速配置&rdquo;->快速构建命令，选择XeLaTex->PDF View。\n\n为什么要选择这个呢？主要是原来的那个对中文的支持不好，现在配置好了以后，你就直接可以按F1直接生成pdf文件了~\n\n5、字体安装\n\nubuntu什么的字体一直是个大问题&hellip;&hellip;你的LaTex论文最终好看与否，最终还是看你系统的字体的&hellip;&hellip;你需要的字体有win字体+Adobe字体。\n\n对于win的字体，如果你能找到一台win电脑，找到C:\\\\windows\\fonts（大小写自己注意&hellip;&hellip;），把下面的字体文件（后缀为ttf的文件）全部复制到你的电脑~/.fonts下面，运行命令\n\n```\nfc-cache -fv\n\n```\n\nok~\n\n对于Adobe字体，上网络上下载就可以了~当然和win一样的节奏~~~\n\n现在你可以方便的使用LaTex了~\n","tags":["Linux","LaTex"]},{"title":"ubuntu16.04安装clickhouse","url":"/ubuntu16.04安装clickhouse.html","content":"1.安装步骤参考官方文档\n\n```\nhttps://clickhouse.com/docs/zh/getting-started/install\n\n```\n\n如下\n\n```\nsudo apt-get install -y apt-transport-https ca-certificates dirmngr\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 8919F6BD2B48D754\n\necho \"deb https://packages.clickhouse.com/deb stable main\" | sudo tee \\\n    /etc/apt/sources.list.d/clickhouse.list\nsudo apt-get update\n\nsudo apt-get install -y clickhouse-server clickhouse-client\n\nsudo service clickhouse-server start\nclickhouse-client # or \"clickhouse-client --password\" if you've set up a password.\n\n```\n\n填写default user的初始密码，回车可以看到如下\n\n```\nEnter password for default user:\nPassword for default user is saved in file /etc/clickhouse-server/users.d/default-password.xml.\nSetting capabilities for clickhouse binary. This is optional.\nCannot set 'net_admin' or 'ipc_lock' or 'sys_nice' or 'net_bind_service' capability for clickhouse binary. This is optional. Taskstats accounting will be disabled. To enable taskstats accounting you may add the required capability later manually.\n chown -R clickhouse:clickhouse '/etc/clickhouse-server'\n\nClickHouse has been successfully installed.\n\nStart clickhouse-server with:\n sudo clickhouse start\n\nStart clickhouse-client with:\n clickhouse-client --password\n\n```\n\n修改clickhouse数据路径配置，需要将所有/var/lib/clickhouse的配置修改成\n\n```\nsudo sed -i 's/var\\/lib\\/clickhouse/data01\\/clickhouse/g' /etc/clickhouse-server/config.xml\n\n```\n\n创建目录\n\n```\nlintong@master:/data01$ sudo mkdir clickhouse\nlintong@master:/data01$ sudo chown -R clickhouse:clickhouse ./clickhouse\n\n```\n\n启动clickhouse server\n\n```\nlintong@master:~$ sudo clickhouse start\n chown -R clickhouse: '/var/run/clickhouse-server/'\nWill run sudo --preserve-env -u 'clickhouse' /usr/bin/clickhouse-server --config-file /etc/clickhouse-server/config.xml --pid-file /var/run/clickhouse-server/clickhouse-server.pid --daemon\nWaiting for server to start\nWaiting for server to start\nServer started\n\n```\n\n连接clickhouse\n\n```\nlintong@master:~$ clickhouse-client\nClickHouse client version 23.10.3.5 (official build).\nConnecting to localhost:9000 as user default.\nPassword for user (default):\nConnecting to localhost:9000 as user default.\nConnected to ClickHouse server version 23.10.3 revision 54466.\n\n```\n\nclient使用的9000端口\n\n访问的话可以看到如下说明，如果想要使用http连接的话，需要使用8123端口\n\n```\nPort 9000 is for clickhouse-client program\nYou must use port 8123 for HTTP.\n\n```\n\n使用datagrip连接clickhouse\n\n<img src=\"/images/517519-20231116214654580-1651978963.png\" width=\"800\" height=\"476\" loading=\"lazy\" />\n\n基础语法参考官方文档\n\n```\nhttps://clickhouse.com/docs/en/guides/creating-tables\n\n```\n\n创建database，默认会有一个default库\n\n```\nCREATE DATABASE IF NOT EXISTS helloworld\n\n```\n\n创建table\n\n```\nCREATE TABLE helloworld.my_first_table\n(\n    user_id UInt32,\n    message String,\n    timestamp DateTime,\n    metric Float32\n)\nENGINE = MergeTree()\nPRIMARY KEY (user_id, timestamp)\n\n```\n\n写入数据\n\n```\nINSERT INTO helloworld.my_first_table (user_id, message, timestamp, metric) VALUES\n    (101, 'Hello, ClickHouse!',                                 now(),       -1.0    ),\n    (102, 'Insert a lot of rows per batch',                     yesterday(), 1.41421 ),\n    (102, 'Sort your data based on your commonly-used queries', today(),     2.718   ),\n    (101, 'Granules are the smallest chunks of data read',      now() + 5,   3.14159 )\n\n```\n\n查询数据\n\n```\nSELECT * FROM helloworld.my_first_table\n\n```\n\n<img src=\"/images/517519-20231116215344910-1284823187.png\" width=\"800\" height=\"106\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n","tags":["clickhouse"]},{"title":"java使用gRPC框架","url":"/java使用gRPC框架.html","content":"官方文档\n\n```\nhttps://grpc.io/docs/languages/java/quickstart/\n\n```\n\n官方example\n\n```\nhttps://github.com/grpc/grpc-java\n\n```\n\n## 1.定义proto文件\n\n```\nsyntax = \"proto3\";\n\noption java_multiple_files = true;\noption java_package = \"io.grpc.examples.helloworld\";\noption java_outer_classname = \"HelloWorldProto\";\noption objc_class_prefix = \"HLW\";\n\npackage helloworld;\n\n// The greeting service definition.\nservice Greeter {\n  // Sends a greeting\n  rpc SayHello (HelloRequest) returns (HelloReply) {}\n}\n\n// The request message containing the user's name.\nmessage HelloRequest {\n  string name = 1;\n}\n\n// The response message containing the greetings\nmessage HelloReply {\n  string message = 1;\n}\n\n```\n\nproto定义，参考官方文档\n\n```\nhttps://github.com/grpc/grpc-java/blob/master/examples/src/main/proto/helloworld.proto\n\n```\n\n## 2.编译proto文件\n\n### 1.使用protoc命令编译\n\n在项目目录下运行编译命令，里面使用了protoc-gen-grpc-java执行文件，需要参考：[编译grpc-java项目生成protoc-gen-grpc-java文件 ](https://www.cnblogs.com/tonglin0325/p/5572371.html)\n\n```\nprotoc --plugin=protoc-gen-grpc-java=./protoc-gen-grpc-java -I=./ --java_out=./src/main/java/ --grpc-java_out=./src/main/java/ ./src/main/proto/helloworld.proto\n\n```\n\n得到proto文件中定义的model和service的java代码\n\n<img src=\"/images/517519-20240106201959825-770337835.png\" width=\"300\" height=\"438\" loading=\"lazy\" />\n\n### 2.使用maven插件编译\n\n当然也可以使用maven插件来生成proto model和service的java代码，需要在pom.xml中添加插件，如下\n\n```\n    <build>\n        <extensions>\n            <extension>\n                <groupId>kr.motd.maven</groupId>\n                <artifactId>os-maven-plugin</artifactId>\n                <version>1.5.0.Final</version>\n            </extension>\n        </extensions>\n        <plugins>\n            <plugin>\n                <groupId>org.xolstice.maven.plugins</groupId>\n                <artifactId>protobuf-maven-plugin</artifactId>\n                <version>0.5.1</version>\n                <configuration>\n                    <protocArtifact>com.google.protobuf:protoc:3.5.1-1:exe:${os.detected.classifier}</protocArtifact>\n                    <pluginId>grpc-java</pluginId>\n                    <pluginArtifact>io.grpc:protoc-gen-grpc-java:1.14.0:exe:${os.detected.classifier}</pluginArtifact>\n                </configuration>\n                <executions>\n                    <execution>\n                        <goals>\n                            <goal>compile</goal>\n                            <goal>compile-custom</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <configuration>\n                    <source>8</source>\n                    <target>8</target>\n                </configuration>\n            </plugin>\n\n        </plugins>\n    </build>\n\n```\n\n然后点击protobuf:compile和protobuf:comile-custome即可，参考：[idea中.proto文件生成model类和service类](https://blog.csdn.net/weixin_43667830/article/details/125211659)\n\n<img src=\"/images/517519-20240106202418113-356450934.png\" width=\"300\" height=\"284\" loading=\"lazy\" />\n\ntarget目录下就会生成java代码\n\n<img src=\"/images/517519-20240106202629761-1779783601.png\" width=\"300\" height=\"288\" loading=\"lazy\" />\n\n## 3.编写server和client\n\n### 1.添加依赖\n\n```\n        <!-- log4j -->\n        <dependency>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n            <version>1.2.17</version>\n        </dependency>\n        <!--google-->\n        <dependency>\n            <groupId>com.google.guava</groupId>\n            <artifactId>guava</artifactId>\n            <version>31.1-jre</version>\n        </dependency>\n        <!--grpc-->\n        <dependency>\n            <groupId>io.grpc</groupId>\n            <artifactId>grpc-netty-shaded</artifactId>\n            <version>1.59.0</version>\n        </dependency>\n        <dependency>\n            <groupId>io.grpc</groupId>\n            <artifactId>grpc-protobuf</artifactId>\n            <version>1.59.0</version>\n        </dependency>\n        <dependency>\n            <groupId>io.grpc</groupId>\n            <artifactId>grpc-stub</artifactId>\n            <version>1.59.0</version>\n        </dependency>\n\n```\n\n### 2.server代码\n\n官方例子\n\n```\nhttps://github.com/grpc/grpc-java/blob/master/examples/src/main/java/io/grpc/examples/helloworld/HelloWorldServer.java\n\n```\n\n<img src=\"/images/517519-20240106202952531-1530723119.png\" width=\"300\" height=\"77\" />\n\n```\npackage com.interview.rpc;\n\nimport io.grpc.Server;\nimport io.grpc.ServerBuilder;\nimport io.grpc.examples.helloworld.GreeterGrpc;\nimport io.grpc.examples.helloworld.HelloReply;\nimport io.grpc.examples.helloworld.HelloRequest;\nimport io.grpc.stub.StreamObserver;\nimport org.apache.log4j.Logger;\n\nimport java.io.IOException;\n\npublic class HelloWorldServer {\n\n    private static final Logger logger = Logger.getLogger(HelloWorldServer.class.getName());\n\n    private int port = 50051;\n    private Server server;\n\n    public static void main(String[] args) throws IOException, InterruptedException {\n        final HelloWorldServer server = new HelloWorldServer();\n        server.start();\n        server.blockUntilShutdown();\n    }\n\n    private void start() throws IOException {\n        server = ServerBuilder.forPort(port)\n                .addService(new GreeterImpl())\n                .build()\n                .start();\n        logger.info(\"Server started, listening on \" + port);\n\n        Runtime.getRuntime().addShutdownHook(new Thread() {\n\n            @Override\n            public void run() {\n\n                System.err.println(\"*** shutting down gRPC server since JVM is shutting down\");\n                HelloWorldServer.this.stop();\n                System.err.println(\"*** server shut down\");\n            }\n        });\n    }\n\n    private void stop() {\n        if (server != null) {\n            server.shutdown();\n        }\n    }\n\n    // block 一直到退出程序\n    private void blockUntilShutdown() throws InterruptedException {\n        if (server != null) {\n            server.awaitTermination();\n        }\n    }\n\n    // 实现 定义一个实现服务接口的类\n    private class GreeterImpl extends GreeterGrpc.GreeterImplBase {\n        @Override\n        public void sayHello(HelloRequest req, StreamObserver<HelloReply> responseObserver) {\n            HelloReply reply = HelloReply.newBuilder().setMessage((\"Hello \" + req.getName())).build();\n            responseObserver.onNext(reply);\n            responseObserver.onCompleted();\n            System.out.println(\"Message from gRPC-Client:\" + req.getName());\n            System.out.println(\"Message Response:\" + reply.getMessage());\n        }\n    }\n\n}\n\n```\n\n### 3.client代码\n\n官方例子\n\n```\nhttps://github.com/grpc/grpc-java/blob/master/examples/src/main/java/io/grpc/examples/helloworld/HelloWorldClient.java\n\n```\n\n代码\n\n```\npackage com.interview.rpc;\n\nimport io.grpc.ManagedChannel;\nimport io.grpc.ManagedChannelBuilder;\nimport io.grpc.StatusRuntimeException;\nimport io.grpc.examples.helloworld.GreeterGrpc;\nimport io.grpc.examples.helloworld.HelloReply;\nimport io.grpc.examples.helloworld.HelloRequest;\n\nimport java.util.concurrent.TimeUnit;\nimport java.util.logging.Level;\nimport java.util.logging.Logger;\n\npublic class HelloWorldClient {\n\n    private final ManagedChannel channel;\n    private final GreeterGrpc.GreeterBlockingStub blockingStub;\n    private static final Logger logger = Logger.getLogger(HelloWorldClient.class.getName());\n\n    public static void main(String[] args) throws InterruptedException {\n        HelloWorldClient client = new HelloWorldClient(\"127.0.0.1\", 50051);\n        try {\n            String user = \"world\";\n            if (args.length > 0) {\n                user = args[0];\n            }\n            client.greet(user);\n        } finally {\n            client.shutdown();\n        }\n    }\n\n    public HelloWorldClient(String host, int port) {\n        channel = ManagedChannelBuilder.forAddress(host, port)\n                .usePlaintext()\n                .build();\n\n        blockingStub = GreeterGrpc.newBlockingStub(channel);\n    }\n\n\n    public void shutdown() throws InterruptedException {\n        channel.shutdown().awaitTermination(5, TimeUnit.SECONDS);\n    }\n\n    public void greet(String name) {\n        HelloRequest request = HelloRequest.newBuilder().setName(name).build();\n        HelloReply response;\n        try {\n            response = blockingStub.sayHello(request);\n        } catch (StatusRuntimeException e) {\n            logger.log(Level.WARNING, \"RPC failed: {0}\", e.getStatus());\n            return;\n        }\n        logger.info(\"Message from gRPC-Server: \" + response.getMessage());\n    }\n\n}\n\n```\n\n### 4.运行server和client\n\n<img src=\"/images/517519-20240106203354846-740644568.png\" width=\"500\" height=\"92\" loading=\"lazy\" />\n\n参考：[【RPC基础系列3】gRPC简单示例](https://zhuanlan.zhihu.com/p/390094013)\n","tags":["google"]},{"title":"编译grpc-java项目生成protoc-gen-grpc-java文件","url":"/编译grpc-java项目生成protoc-gen-grpc-java文件.html","content":"使用protoc命令生成service代码的时候，需要使用如下命令\n\n```\nprotoc --plugin=protoc-gen-grpc-java=./protoc-gen-grpc-java -I=./ --java_out=./src/main/java/ --grpc-java_out=./src/main/java/ ./src/main/proto/helloworld.proto\n\n```\n\n其中的protoc-gen-grpc-java执行文件需要自己编译生成\n\n<img src=\"/images/517519-20240106195739867-1980387671.png\" width=\"300\" height=\"343\" loading=\"lazy\" />\n\n编译的步骤如下\n\ngit clone grpc-java项目，\n\n```\ngit clone git@github.com:grpc/grpc-java.git\n\n```\n\n切换到使用的grpc版本，比如v1.59.0，编译这个版本需要jdk11的支持\n\n```\ngit checkout v1.59.0\n\n```\n\n使用gradlew命令进行编译\n\n```\n➜  /Users/lintong/coding/java/grpc-java/compiler git:(ae49d275b) $ ../gradlew java_pluginExecutable  -PskipAndroid=true\n  * Skipping the build of Android projects because skipAndroid=true\n\n> Configure project :grpc-compiler\n*** Building codegen requires Protobuf\n*** Please refer to https://github.com/grpc/grpc-java/blob/master/COMPILING.md#how-to-build-code-generation-plugin\n\nDeprecated Gradle features were used in this build, making it incompatible with Gradle 9.0.\n\nYou can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins.\n\nFor more on this, please refer to https://docs.gradle.org/8.3/userguide/command_line_interface.html#sec:command_line_warnings in the Gradle documentation.\n\nBUILD SUCCESSFUL in 2s\n2 actionable tasks: 1 executed, 1 up-to-date\n\n```\n\n在build目录下编译得到protoc-gen-grpc-java执行文件\n\n```\n➜  /Users/lintong/coding/java/grpc-java/compiler git:(ae49d275b) $ cd build/exe/java_plugin\n➜  /Users/lintong/coding/java/grpc-java/compiler/build/exe/java_plugin git:(ae49d275b) $ ls\nprotoc-gen-grpc-java\n\n```\n\n这时候就可以使用protoc命令到编译proto文件得到service代码，否则只能得到model代码\n\n<img src=\"/images/517519-20240106200636633-900459950.png\" width=\"300\" height=\"438\" loading=\"lazy\" />\n\n官方文档：\n\n```\nhttps://github.com/grpc/grpc-java/tree/master/compiler\n\n```\n\n其他文档：[java/go grpc 生成 service](https://www.cnblogs.com/ifnk/p/15939725.html)\n\n<!--more-->\n&nbsp;\n","tags":["google"]},{"title":"面试题目——《Leetcode》数组","url":"/面试题目——《Leetcode》数组.html","content":"**1.[Two Sum](https://leetcode.com/problems/two-sum/)(数组中两个元素相加的和等于给定的target，返回这两个元素的下标)**\n\n　　注意：1.有序数组的情况比较简单\n\n**　　思路：1.使用HashMap配对数组的每个值和其下标 <br />**\n\n**　　　　　2.然后遍历整个数组，求target-每个数组元素的值是否在HashMap中存在，且这个值不能是本身，即theMap.get(gap)!= null &amp;&amp; theMap.get(gap)!= i**\n\n**　　　　　3.最后用全局变量然后这两个下标**\n\n```\npackage leetcode;\n\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic class TwoSum {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n        int len;\n        int target;\n        InputData input = new InputData();\n        len = input.getInt(\"请输入数组的长度：\", \"输入的数据必须是整数，请重新输入！\");\n        int[] nums = new int[len];\n        for(int i = 0; i < len; i++) {\n        \tnums[i] = input.getInt(\"请按顺序输入数组的值：\", \"输入的数据必须是整数，请重新输入！\");\n        }\n        target = input.getInt(\"请输入target：\", \"输入的数据必须是整数，请重新输入！\");\n        \n        reTwo(nums,target);\n        System.out.println(two[0]);\n        System.out.println(two[1]);\n\t}\n\t\n\tpublic static int[] two = {0,0};\n\t\n\tpublic static int[] reTwo(int[] nums,int target){\n\t\tint len = nums.length;\n\t\t\n\t\tMap<Integer,Integer> theMap = new HashMap<Integer,Integer>();\n\t\tfor(int i = 0; i < len; i++){\n\t\t\ttheMap.put(nums[i], i);//有可能出现nums[i]相等的情况\n\t\t\tSystem.out.println(\"key\"+nums[i]+\"->\"+\"value\"+i);\n\t\t}\n\t\t\n\t\tfor(int i = 0; i < nums.length; i++){\n\t\t\tint gap = target - nums[i];//直接求target减去每一个元素的差，是否存在\n\t\t\tif(theMap.get(gap)!= null &amp;&amp; theMap.get(gap)!= i){//差要在Map中存在，且不能等于本身\n\t\t\t\ttwo[0] = i;\n\t\t\t\ttwo[1] = theMap.get(gap);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\treturn two;\n\t}\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n\n**11.[Container With Most Water](https://leetcode.com/problems/container-with-most-water)&nbsp; (水桶问题)**\n\n**　　**用一个数组存放若干个坐标，比如（1,1），（2,5），（3,1），在每个点上画竖直的直线，和x的相交，则这个点以及直线和x轴的交点组成的线段，就和x轴组成若干个水桶，求水桶最大的面积。\n\n**　　思路：在数组中，在while(start<end)的情况下循环，然后不断的左移和右移数组的下标，求出最大的面积<br />**\n\n**　　注意：可能求出的面积会超过Integer.MAX_VALUE**\n\n```\npackage leetcode;\n\npublic class ContainerWithMostWater {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tint[] height = {1,5,1};\n\t\tSystem.out.println(maxArea(height));\n\t}\n\t\n\tpublic static int maxArea(int[] height){\n\t\tif(height == null || height.length < 2)\n\t\t\treturn 0;\n\t\tint maxVolume = 0;\n\t\tint start = 0;\n\t\tint end = height.length - 1;\n\t\twhile(start < end){\n\t\t\tint area = Math.min(height[start], height[end]) * (end-start);\t//从左右两边开始，计算面积\n\t\t\tmaxVolume = Math.max(maxVolume, area);\n\t\t\tif(maxVolume>Integer.MAX_VALUE)\n\t\t\t\treturn Integer.MAX_VALUE;\n\t\t\tif(height[start] > height[end])\t//如果start高于end，就移动end，寻找更高的end\n\t\t\t\tend--;\n\t\t\telse\n\t\t\t\tstart++;\n\t\t}\n\t\treturn maxVolume;\n\t}\n\n}\n\n```\n\n&nbsp;\n\n&nbsp;\n\n**26.[Remove Duplicates from Sorted Array](https://leetcode.com/problems/remove-duplicates-from-sorted-array/)(删除有序数组中的重复元素)**\n\n　　注意：1.有序数组的情况比较简单\n\n**　　思路：1.检查数组是否为空**\n\n**　　　　　2.检查数组元素是否只有一个**\n\n**　　　　　3.循环，检查i-1和i是否相等，i和count增加的速度不一样**\n\n```\npackage leetcode;\n\nimport leetcode.SwapNodesInPairs.ListNode;\n\npublic class RemoveDuplicatesFromSortedArray {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t//设置数组\n        int len;\n        InputData input = new InputData();\n        len = input.getInt(\"请输入数组的长度：\", \"输入的数据必须是整数，请重新输入！\");\n        int[] nums = new int[len];\n        for(int i = 0; i < len; i++) {\n        \tnums[i] = input.getInt(\"请按顺序输入数组的值：\", \"输入的数据必须是整数，请重新输入！\");\n        }\n        \n        int count = removeDuplicatesNode(nums);\n        for(int i=0;i<count;i++){\n        \tSystem.out.println(nums[i]);\n        }\n\t}\n\t\n\t\n\tpublic static int removeDuplicatesNode(int[] nums){\n\t\tif(nums == null)\n\t\t\treturn 0;\n\t\tif(nums.length == 1)\n\t\t\treturn 1;\n\t\tint len = nums.length;\n\t\tint count = 1;\n\t\tfor(int i=1;i<len;i++){\n\t\t\tif(nums[i-1] == nums[i])\n\t\t\t\tcontinue;\n\t\t\telse{\n\t\t\t\tnums[count++] = nums[i];\n\t\t\t}\n\t\t}\n\t\treturn count;\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**27.[Remove Element](https://leetcode.com/problems/remove-element/)(删除数组中制定值的元素，返回剩下的数组的长度)**\n\n**　　思路：1.检查数组是否为空**\n\n```\npackage leetcode;\n\npublic class RemoveElement {\n\t\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tint[] theArray = {1,2,3,4,5,6};\n\t\tint reLen = removeEle(theArray,2);\n\t\tSystem.out.println(reLen);\n\t\tfor(int i=0;i<reLen;i++){\n\t\t\tSystem.out.println(theArray[i]);\n\t\t}\n\t}\n\t\n\tpublic static int removeEle(int[] nums, int val) {\n        if(nums == null || nums.length <= 0)\n            return 0;\n        int count = 0;\n        int len = nums.length;\n        for(int i=0;i<len;i++){\n        \tif(nums[i] != val){\n        \t\tnums[count++] = nums[i];\n        \t}else{\n        \t\tnums[count] = nums[i];\n        \t}\n        }\n        return count;\n    }\n\n}\n\n```\n\n&nbsp;\n\n**35.[Search Insert Position](https://leetcode.com/problems/search-insert-position/)(在一个数组中查找target应该插入的位置)**\n\n**　　思路：1.使用二分查找降低复杂度到nlogn**\n\n```\npackage leetcode;\n\npublic class SearchInsertPosition {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tint[] theArr = {0,1,2,3,4,5,6,7,8,9};\n\t\tSystem.out.println(InsertPosition(theArr,0));\n//\t\tfor(int i=0;i<theArr.length;i++){\n//\t\t\tSystem.out.println(theArr[i]);\n//\t\t}\n\t}\n\t\n\tpublic static int InsertPosition(int[] nums,int target){\n\t\tif(nums == null){\n\t\t\t\treturn 0;\n\t\t}\n\t\tint len = nums.length;\n\t\tif(target > nums[len-1])\n\t\t    return len;\n\t\tif(target <= nums[0])\n\t\t    return 0;\n\t\treturn find(nums,target,0,len-1);\n\t}\n\t\n\tpublic static int find(int[] Arr,int target,int lowerBound,int upperBound){//通过二分查找把复杂度降到nlogn\n\t\tint curNum = (lowerBound+upperBound)/2;\n\t\tif(lowerBound == upperBound){\n\t\t\tif(target <= Arr[curNum])\n\t\t\t\treturn curNum;\n\t\t\telse\n\t\t\t\treturn curNum+1;\n\t\t}\n\t\telse{\n\t\t\tif(Arr[curNum] < target)\n\t\t\t\treturn find(Arr,target,curNum+1,upperBound);\n\t\t\telse\n\t\t\t\treturn find(Arr,target,lowerBound,curNum);//注意此处不能是curNum-1\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**66.[Plus One](https://leetcode.com/problems/plus-one/)（把一个数表示成数组，并给这个数加上1），比如10表示成[1,0]，加上1后是[1,1]**\n\n```\npublic class Solution {\n    public int[] plusOne(int[] digits) {\n        int len = digits.length;\n \n\tfor (int i = len - 1; i >= 0; --i) {\n\t\tif (digits[i] == 9) {\n\t\t\tdigits[i] = 0;\n\t\t} else {\n\t\t\t++digits[i];\n\t\t\treturn digits;\n\t\t}\n    }\n    //we have to add a digit at the head\n    int[] y = new int[len + 1];\n\ty[0] = 1;\n\tfor (int j = 1; j <= len; ++j) {\n\t    y[j] = digits[j - 1];\n    }\n\treturn y;\n    }\n}\n\n```\n\n&nbsp;\n\n&nbsp;\n\n**169.[Majority Element](https://leetcode.com/problems/majority-element/)(找出数组中出现次数超过n/2的数)《剑指Offer》P180 给出了两种解法，这里用的是第二种<br />**\n\n　　注意：\n\n**　　思路：1.每找出两个不同的element，则成对删除。最终剩下的一定就是所求的**\n\n```\npackage leetcode;\n\nimport java.util.Scanner;\n\npublic class MajorityElement {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tScanner scan = new Scanner(System.in);  //从键盘接收数据\n        \n        int len = 0;\n        \n        System.out.println(\"输入数组的长度：\");\n        if(scan.hasNextInt()){\n            len = scan.nextInt();\n            System.out.println(\"输入的数组长度是\"+len);\n        }else{\n            System.out.println(\"输入的不是整数\");\n        }\n        \n        int[] strArr = new int[len];\n        System.out.println(\"输入数组的每一个成员，以空格为分界：\");\n        for(int i = 0; i < len; i++) {\n        \tstrArr[i] = scan.nextInt();\n        \tSystem.out.println(\"输入的第\"+i+\"个数组成员是\"+strArr[i]);\n        }\n        \n        //算法\n        System.out.println(\"出现次数超过n/2的数是\"+majorityElement(strArr));\n        \n\t}\n\t\n\tpublic static int majorityElement(int[] nums){//每找出两个不同的element，则成对删除。最终剩下的一定就是所求的\n\t\tint len = nums.length;\n\t\tint count = 0;//如果相同，count++\n\t\tint temp = 0;//用来存放数组的每个元素\n\t\tfor(int i=0;i<len;i++){\n\t\t\tif(count == 0){\n\t\t\t\ttemp = nums[i];\n\t\t\t\tcount++;\n\t\t\t}else{\n\t\t\t\tif(nums[i] == temp){\n\t\t\t\t\tcount++;\n\t\t\t\t}else{\n\t\t\t\t\tcount--;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t\treturn temp;\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["刷题"]},{"title":"go使用gRPC框架","url":"/go使用gRPC框架.html","content":"官方文档\n\n```\nhttps://grpc.io/docs/languages/go/quickstart/\n\n```\n\n官方example\n\n```\nhttps://github.com/grpc/grpc-go\n\n```\n\n## 1.定义proto文件\n\n```\nsyntax = \"proto3\";\n\noption go_package = \"google.golang.org/grpc/examples/helloworld/helloworld\";\noption java_multiple_files = true;\noption java_package = \"io.grpc.examples.helloworld\";\noption java_outer_classname = \"HelloWorldProto\";\n\npackage helloworld;\n\n// The greeting service definition.\nservice Greeter {\n  // Sends a greeting\n  rpc SayHello (HelloRequest) returns (HelloReply) {}\n}\n\n// The request message containing the user's name.\nmessage HelloRequest {\n  string name = 1;\n}\n\n// The response message containing the greetings\nmessage HelloReply {\n  string message = 1;\n}\n\n```\n\nproto定义，参考官方文档\n\n```\nhttps://github.com/grpc/grpc-go/blob/master/examples/helloworld/helloworld/helloworld.proto\n\n```\n\n## 2.编译proto文件\n\n使用protoc命令编译\n\n需要先安装compile插件，参考：[https://grpc.io/docs/languages/go/quickstart/](https://grpc.io/docs/languages/go/quickstart/)\n\n```\ngo install google.golang.org/protobuf/cmd/protoc-gen-go@v1.28\ngo install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.2\n\n```\n\n编译命令\n\n```\nprotoc --go_out=./ --go_opt=paths=source_relative --go-grpc_out=./cmd/proto/helloworld --go-grpc_opt=paths=source_relative ./cmd/proto//helloworld/helloworld.proto\n\n```\n\n编译得到go代码\n\n<img src=\"/images/517519-20240107140831138-1217407154.png\" width=\"300\" height=\"163\" loading=\"lazy\" />\n\n## 3.编写server和client代码\n\n### 1.server代码\n\n官方例子\n\n```\nhttps://github.com/grpc/grpc-go/blob/master/examples/helloworld/greeter_server/main.go\n\n```\n\n代码\n\n```\npackage main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"fmt\"\n\t\"log\"\n\t\"net\"\n\n\tpb \"awesome-project/cmd/proto/helloworld\"\n\t\"google.golang.org/grpc\"\n)\n\nvar (\n\tport = flag.Int(\"port\", 50051, \"The server port\")\n)\n\n// server is used to implement helloworld.GreeterServer.\ntype server struct {\n\tpb.UnimplementedGreeterServer\n}\n\n// SayHello implements helloworld.GreeterServer\nfunc (s *server) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloReply, error) {\n\tlog.Printf(\"Received: %v\", in.GetName())\n\treturn &amp;pb.HelloReply{Message: \"Hello \" + in.GetName()}, nil\n}\n\nfunc main() {\n\tflag.Parse()\n\tlis, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", *port))\n\tif err != nil {\n\t\tlog.Fatalf(\"failed to listen: %v\", err)\n\t}\n\ts := grpc.NewServer()\n\tpb.RegisterGreeterServer(s, &amp;server{})\n\tlog.Printf(\"server listening at %v\", lis.Addr())\n\tif err := s.Serve(lis); err != nil {\n\t\tlog.Fatalf(\"failed to serve: %v\", err)\n\t}\n}\n\n```\n\n### 2.client代码\n\n官方例子\n\n```\nhttps://github.com/grpc/grpc-go/blob/master/examples/helloworld/greeter_client/main.go\n\n```\n\n代码\n\n```\npackage main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"time\"\n\n\tpb \"awesome-project/cmd/proto/helloworld\"\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/grpc/credentials/insecure\"\n)\n\nconst (\n\tdefaultName = \"world\"\n)\n\nvar (\n\taddr = flag.String(\"addr\", \"localhost:50051\", \"the address to connect to\")\n\tname = flag.String(\"name\", defaultName, \"Name to greet\")\n)\n\nfunc main() {\n\tflag.Parse()\n\t// Set up a connection to the server.\n\tconn, err := grpc.Dial(*addr, grpc.WithTransportCredentials(insecure.NewCredentials()))\n\tif err != nil {\n\t\tlog.Fatalf(\"did not connect: %v\", err)\n\t}\n\tdefer conn.Close()\n\tc := pb.NewGreeterClient(conn)\n\n\t// Contact the server and print out its response.\n\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\tdefer cancel()\n\tr, err := c.SayHello(ctx, &amp;pb.HelloRequest{Name: *name})\n\tif err != nil {\n\t\tlog.Fatalf(\"could not greet: %v\", err)\n\t}\n\tlog.Printf(\"Greeting: %s\", r.GetMessage())\n}\n\n```\n\n### 3.运行server和client\n\n成功看到hello world\n\n<img src=\"/images/517519-20240107142724809-2146176167.png\" width=\"500\" height=\"98\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n","tags":["google"]},{"title":"go学习笔记——text template","url":"/go学习笔记——text template.html","content":"golang可以使用text/template来实现模板生成文本，官方文档：[https://pkg.go.dev/text/template](https://pkg.go.dev/text/template)\n\n## 1.变量\n\n可以在模板中定义变量，然后将这些变量赋值到模板的变量中\n\n```\nimport \"text/template\"\n\n// 定义结构体\ntype Inventory struct {\n\tMaterial string\n\tCount    uint\n}\n// 赋值\nsweaters := Inventory{\"wool\", 17}\n// 定义模板\ntmpl, err := template.New(\"test\").Parse(`\n\t{{.Count}} items are made of {{.Material}}\n`)\nif err != nil {\n\tpanic(err)\n}\n// 填充模板\nerr = tmpl.Execute(os.Stdout, sweaters)\nif err != nil {\n\tpanic(err)\n}\n\n```\n\n输出\n\n```\n17 items are made of wool\n\n```\n\n## 2.if else\n\n### 1.判断字符串是否相等\n\n```\n{{ if eq .Material \"wool\" }}\n  Material is \"wool\"\n{{ else if eq .Material \"not wool\" }}\n  Material is not \"wool\"\n{{ end }}\n{{ if eq .Count 17 }}\n  Count is 17\n{{ else if eq .Count 10 }}\n  Count is not 10\n{{ end }}\n\n```\n\n输出\n\n```\nMaterial is \"wool\"\n\nCount is 17\n\n```\n\n### 2.多条件，与或非\n\n```\n{{ if or (eq .Material \"not wool\") (eq .Count 17) }}\n  Count is 17\n{{ end }}\n\n```\n\n输出\n\n```\nCount is 17\n\n```\n\n## 3.for循环\n\n### 1.遍历数组\n\n在for循环中使用其他变量的时候，要用{{$.xxx}}\n\n取得index和value，使用{{$index}}和{{$value}}\n\n```\n{{range $index, $value := .Ips}}\n   index: {{$index}}, material: {{$.Material}}, value: {{$value}}\n{{end}}\n\n```\n\n输出\n\n```\nindex: 0, material: wool, value: 192.168.0.1\n\t\t\nindex: 1, material: wool, value: 192.168.0.2\n\n```\n\n### 2.遍历map\n\n同样也可以使用for循环遍历map，取得key和value\n\n```\ntype Inventory struct {\n\tMaterial string\n\tCount    uint\n\tIps      []string\n\tMaps     map[string]string\n}\nsweaters := Inventory{\n\t\"wool\",\n\t17,\n\t[]string{\"192.168.0.1\", \"192.168.0.2\"},\n\tmap[string]string{\n\t\t\"key\":   \"hello\",\n\t\t\"value\": \"world\",\n\t},\n}\ntmpl, err := template.New(\"test\").Parse(`\n\t{{range $key, $value := .Maps}}\n\t   key: {{$key}}, material: {{$.Material}}, value: {{$value}}\n\t{{end}}\n`)\nif err != nil {\n\tpanic(err)\n}\nerr = tmpl.Execute(os.Stdout, sweaters)\nif err != nil {\n\tpanic(err)\n}\n\n```\n\n## 4.函数\n\n### 1.len函数\n\n判断数组的长度是否等于1，其中Ips可以是切片或者map\n\n```\n{{if eq (len .Ips) 1}}\nlen=1\n{{else if eq (len .Ips) 2}}\nlen=2\n{{else}}\nother\n{{end}}\n\n```\n\n### 2.index函数\n\n可以使用index函数获得数组特定下标的值\n\n```\n{{index .Ips 0}}\n\n```\n\n如果和eq函数一起使用\n\n```\n{{if or (eq (index .Ips 0) \"192.168.0.1\") (eq (index .Ips 1) \"192.168.0.2\")}}\n\n{{end}}\n\n```\n\n### 3.for循环下标从1开始\n\n参考：[golang template(数组循环、在循环内使用外部变量、索引从1开始)](https://blog.csdn.net/u010918487/article/details/113555891)\n\n### 4.自定义函数\n\n可以自定义函数来自己实现函数\n\n```\npackage main\n\nimport (\n\t\"bytes\"\n\t\"strings\"\n\t\"text/template\"\n)\n\nfunc main() {\n\ttype Inventory struct {\n\t\tMaterial string\n\t\tCount    uint\n\t\tIps      []string\n\t\tMaps     map[string]string\n\t}\n\tsweaters := Inventory{\n\t\t\"test1,test2\",\n\t\t17,\n\t\t[]string{\"192.168.0.1\", \"192.168.0.2\"},\n\t\tmap[string]string{\n\t\t\t\"key\":   \"hello\",\n\t\t\t\"value\": \"world\",\n\t\t},\n\t}\n\ttmpl := template.New(\"test\")\n\tfuncs := template.FuncMap{\n\t\t\"hasSuffix\":   strings.HasSuffix,\n\t\t\"split\":       strings.Split,\n\t\t\"containItem\": containItem,\n\t\t\"renderTemplate\": func(name string, data interface{}) (string, error) {\n\t\t\tvar buf bytes.Buffer\n\t\t\terr := tmpl.ExecuteTemplate(&amp;buf, name, data)\n\t\t\treturn buf.String() + \" test\", err\n\t\t},\n\t}\n\ttemp, err := tmpl.Funcs(funcs).Parse(\n\t\t`\n\t\t{{if hasSuffix .Material \"test2\"}}\n\t\t\thasSuffix\n\t\t{{end}}\n\t`)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tvar output bytes.Buffer\n\terr = temp.Execute(&amp;output, sweaters)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tstr := output.String()\n\tprintln(str)\n}\n\nfunc containItem(slice []string, item string) bool {\n\tfor _, v := range slice {\n\t\tif v == item {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n```\n\n判断字符串是否包含特定后缀\n\n```\n{{if hasSuffix .Material \"test2\"}}\n\thasSuffix\n{{end}}\n\n```\n\n将字符串以特定字符切分成数组，然后遍历\n\n```\n{{- $items := split .Material \",\" -}}\n{{- range $index, $item := $items -}}\n\t{{$item}}\n{{end}}\n\n```\n\n输出\n\n```\ntest1\n                test2\n\n```\n\n判断切片是否包含特定字符串 \n\n```\n{{if containItem .Ips \"192.168.0.1\"}}\n\tcontainItem\n{{end}}\n\n```\n\n## 5.其他\n\n### 1.注释\n\n```\n{{/* 注释 */}}\n\n```\n\n### 2.子模板\n\n可以在template中使用define关键字定义一个子模板，然后使用template关键字添加这个子模板\n\n```\nstart define a template\n{{- define \"T1\" -}}\n   define a template\n{{- end -}}\n{{template \"T1\" .}}\nend define a template\n\n```\n\n输出\n\n```\n                start define a template\n                \n                \n                        define a template\n                \n                end define a template\n\n```\n\n### 3.去除空格\n\n可以看到上面渲染出来的字符串中有很多的空格，可以使用 - 来去掉多余的空格\n\n比如去掉前面的空格\n\n```\n{{- \"start define a template\"}}\n\n```\n\n去掉后面的空格\n\n```\n{{\"start define a template\"}}\n\n```\n\n去掉模板前后，和模板中的空格\n\n```\n{{- \"start define a template\" -}}\n{{define \"T1\" -}}\ndefine a template\n{{- end}}\n{{- template \"T1\" . -}}\n{{- \"end define a template\" -}}\n\n```\n\n输出\n\n```\nstart define a templatedefine a templateend define a template\n\n```\n\n### 4.将子模板定义成一个变量，并使用函数进行处理后输出\n\n```\npackage main\n\nimport (\n\t\"bytes\"\n\t\"text/template\"\n)\n\nfunc main() {\n\ttype Inventory struct {\n\t\tMaterial string\n\t\tCount    uint\n\t\tIps      []string\n\t\tMaps     map[string]string\n\t}\n\tsweaters := Inventory{\n\t\t\"wool\",\n\t\t17,\n\t\t[]string{\"192.168.0.1\", \"192.168.0.2\"},\n\t\tmap[string]string{\n\t\t\t\"key\":   \"hello\",\n\t\t\t\"value\": \"world\",\n\t\t},\n\t}\n\ttmpl := template.New(\"test\")\n\tfuncs := template.FuncMap{\n\t\t\"renderTemplate\": func(name string, data interface{}) (string, error) {\n\t\t\tvar buf bytes.Buffer\n\t\t\terr := tmpl.ExecuteTemplate(&amp;buf, name, data)\n\t\t\treturn buf.String() + \" test\", err\n\t\t},\n\t}\n\ttemp, err := tmpl.Funcs(funcs).Parse(\n\t\t`\n\t\t{{define \"T1\" -}}\n\t\tdefine a template\n\t\t{{- end}}\n\n\t\t{{define \"T111\"}}\n\t\t\t{{- $message := renderTemplate \"T1\" . -}}\n\t\t\t{{$message}}\n\t\t{{end}}\n\n\t\t{{template \"T111\" .}}\n\t`)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tvar output bytes.Buffer\n\terr = temp.Execute(&amp;output, sweaters)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tstr := output.String()\n\tprintln(str)\n}\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["golang"]},{"title":"面试题目——《Leetcode》字符串","url":"/面试题目——《Leetcode》字符串.html","content":"**分析整理自己做过的《Leetcode》题目**\n\n<!--more-->\n&nbsp;\n\n**6. [ZigZag Conversion](https://leetcode.com/problems/zigzag-conversion/)(Z字形转换)**\n\n**　　思路：1.通过循环和公式就能完成**\n\n```\npackage leetcode;\n\npublic class ZigZagConversion {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n        String str;\n        InputData input = new InputData();\n        str = input.getString(\"请输入一个字符串：\");\n        System.out.println(ZigZag(str,5));\n\t}\n\t\n\tpublic static String ZigZag(String str,int numRows){\n\t\tif(str == null || str.length()<=0)\n\t\t\treturn null;\n\t\tif(numRows<2)\n\t\t\treturn str;\n\t\t//String reStr = \"\";\n\t\tStringBuffer reStr = new StringBuffer();\n\t\tint len = str.length();\n\t\tint index = 2*numRows-2;//循环周期\n\t\tfor(int i=0;i<numRows;i++){\n\t\t\tfor(int j=i;j<len;j+=index){\n\t\t\t\t//reStr += str.charAt(j);\n\t\t\t\treStr.append(str.charAt(j));\n\t\t\t\t//第二行到倒数第二行之间还有再加上\n\t\t\t\tif (i > 0 &amp;&amp; i < numRows-1) {  \n                    int t = j + index - 2*i;  \n                    if (t < len) {  \n                    \t//reStr += str.charAt(t);\n                    \treStr.append(str.charAt(t));\n                    }  \n                } \n\t\t\t}\n\t\t}\n\t\t//return reStr;\n\t\treturn reStr.toString();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n\n**14. Longest Common Prefix(判断一个字符串数组中，字符串重复的最长前缀)，测试用例格式：[\"abc\",\"abc\"]**\n\n**　　注意：遍历循环的时候外层是字符串的每一个字母，内层是数组的所有元素，是限定第一个字母，检查所有的数组元素**\n\n**　　思路：1.先设数组的第一个元素是minStr，并把minLen设为它的长度**\n\n**　　　　　2.然后遍历所有的元素，找到最短的元素，设为minStr和minLen**\n\n**　　　　　3.然后以这个minStr再遍历一次数组，以minLen再遍历每一个元素的每一个字母，遇到不等就取得substring**\n\n```\npackage leetcode;\n\nimport huawei.InputData;\n\npublic class LongestCommonPrefix {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n        int len;\n        InputData input = new InputData();\n        len = input.getInt(\"请输入数组的长度：\", \"输入的数据必须是整数，请重新输入！\");\n        String[] strArr = new String[len];\n        for(int i = 0; i < len; i++) {\n        \tstrArr[i] = input.getString(\"请输入数组中的每个字符串：\");\n        }\n        System.out.println(longestCommonPrefix(strArr));\n \n\t}\n\t\n\tpublic static String longestCommonPrefix(String[] strs) {\n        if(strs.length == 0)\n            return \"\";  \n        int commonLen = strs[0].length();\n        String minStr = strs[0];\n        for(int i=0;i<strs.length;i++){\n            if(commonLen>strs[i].length()){\n                commonLen = strs[i].length();\n                minStr = strs[i];\n            }\n        }\n        for(int i=0;i<commonLen;i++){   //以最短字符串的长度循环，每个字符串的第一个\n            char c = minStr.charAt(i);\n            for(int j=0;j<strs.length;j++){ //然后循环整个数组\n                if(strs[j].charAt(i) != c){\n                    return minStr.substring(0,i);\n                }\n            }\n        }\n        return minStr;\n    }\n\n}\n\n```\n\n&nbsp;\n\n**28. [Implement strStr()](https://leetcode.com/problems/implement-strstr/)(返回needle关键字在字符串haystack中出现的第一个位置，不存在返回-1)**\n\n**　　注意：当判断一个字符串不符合条件之后，要把i返回这个字符串最初的地方重新判断**\n\n**　　思路：**\n\n```\npackage leetcode;\n\npublic class ImplementStrStr {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(strStr(\"BCCBCCBDDB\",\"BCCBD\"));\n\t}\n\t\n\tpublic static int strStr(String haystack, String needle) {\n\t\tif(haystack == null || needle == null)\n\t\t\treturn -1;\n\t\tif(needle.length() <= 0)\n\t\t\treturn 0;\n\t\tint lenSub = needle.length();\n\t\tint lenAll = haystack.length();\n\t\tint index = 0;\n\t\tfor(int i=0;i<lenAll;i++){\n\t\t\tif(haystack.charAt(i) == needle.charAt(index)){\n\t\t\t\tif(index == (lenSub-1)){\n\t\t\t\t\treturn i-index;\n\t\t\t\t}\n\t\t\t\tindex++;\n\t\t\t}else{\n\t\t\t\tif(index != 0)\n\t\t\t\t\ti -= index;//返回最初判断的位置\n\t\t\t\tindex = 0;\n\t\t\t}\n\t\t}\n\t\treturn -1;\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**38. [Count and Say](https://leetcode.com/problems/count-and-say/)(`1, 11, 21, 1211, 111221, 312211 ...`　　不断计数，输入n，返回第n个)**\n\n**　　注意：**\n\n**　　思路：1.建立一个足够大的StringBuffer，比如长度为128，用来放置生成的String**\n\n**　　　　　2.使用递归的思想，当n等于1的时候直接返回1,当n>=2的时候，通过for循环把result的结果传给countLast()**\n\n**　　　　　3.<strong>countLast**的时候从i到len，判断i-1是否等于i，相等就count++，否则就把count和i-1放进StringBuffer中，并把count=1</strong>\n\n**　　　　　4.循环结束后还要再赋一次StringBuffer，因为最后一个没有放进去**\n\n```\npackage leetcode;\n\npublic class CountAndSay {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tint num;\n        InputData input = new InputData();\n        num = input.getInt(\"请输入num：\", \"输入的数据必须是整数，请重新输入！\");\n        System.out.println(countAndSay(num));\n\n\t}\n\t\n\tpublic static String countAndSay(int n){\n\t\tString result = \"1\";\n\t\tif(n == 1)\n\t\t\treturn \"1\";\n\t\telse{\n\t\t\tfor(int i=1;i<n;i++){\n\t\t\t\tresult = countLast(result);\n\t\t\t}\n\t\t\treturn result;\n\t\t}\n\t}\n\t\n\tpublic  static String countLast(String str){\n\t\tStringBuffer theStr = new StringBuffer(128);\n\t\tint count = 1;\n\t\tint len = str.length();\n\t\tfor(int i=1;i<len;i++){\n\t\t\tif(str.charAt(i-1) == str.charAt(i)){\n\t\t\t\tcount++;\n\t\t\t}else{\n\t\t\t\ttheStr.append(count);\n\t\t\t\ttheStr.append(str.charAt(i-1));\n\t\t\t\tcount=1;\n\t\t\t}\n\t\t}\n\t\ttheStr.append(count);\n\t\ttheStr.append(str.charAt(len-1));\n\t\treturn theStr.toString();\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["刷题"]},{"title":"面试题目——《Leetcode》类型转换问题","url":"/面试题目——《Leetcode》类型转换问题.html","content":"**分析整理自己做过的《Leetcode》题目**\n\n<!--more-->\n&nbsp;\n\n**8.String to Integer (atoi)(字符串转换成整型)**\n\n　　注意：1.输入的字符串可能为空\n\n　　　　　2.正数负数\n\n　　　　　3.输入的字符串可能有空格或者不是数字的字符\n\n　　　　　4.输入的字符串可能是一个小数\n\n　　　　　5.遇到第一个不是数字的字符的时候，为数字的结尾，例如1 1.1,输出1\n\n　　　　　6.循环取每一位的时候，要把新的变量放在一个double类型中，因为输出的字符串可能超过了9223372036854775807,long的max\n\n**　　思路：1.检查str是否为空**\n\n**　　　　　2.去掉空格**\n\n**　　　　　3.取得符号位，默认符号位为正**\n\n**　　　　　4.while(len>i &amp;&amp; str.charAt[i]>='0' &amp;&amp; <strong>str.charAt[i]<='9'**)</strong>\n\n**　　　　　　　　放入一个double类型中**\n\n**　　　　　5.符号位，是否取负数**\n\n**　　　　　6.检查是否越界**\n\n```\npackage leetcode;\n\nimport leetcode.InputData;\n\npublic class StringtoInteger {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n        String str;\n        InputData input = new InputData();\n        str = input.getString(\"请输入一个字符串：\");\n        System.out.println(StrToInt(str));\n        \n\t}\n\t\n\tpublic static int StrToInt(String str) {\n        if (str == null || str.length() < 1)\n\t\treturn 0;\n \n    \t//去掉首尾的空格\n    \tstr = str.trim();\n     \n    \tchar flag = '+';\n     \n    \t//检查符号位\n    \tint i = 0;\n    \tif (str.charAt(0) == '-') {\n    \t\tflag = '-';\n    \t\ti++;\n    \t} else if (str.charAt(0) == '+') {\n    \t\ti++;\n    \t}\n    \t//用double型来存储去掉空格，因为可能超过Long.MAX_VALUE，9223372036854775807\n    \tdouble result = 0;\n     \n    \t//循环字符串的每一位，从非符号位的第一位开始，到第一个非数字位结束\n    \twhile (str.length() > i &amp;&amp; str.charAt(i) >= '0' &amp;&amp; str.charAt(i) <= '9') {\n    \t\tresult = result * 10 + (str.charAt(i) - '0');\n    \t\ti++;\n    \t}\n     \n    \tif (flag == '-')\n    \t\tresult = -result;\n     \n    \t//检查有没有越界\n    \tif (result > Integer.MAX_VALUE)\n    \t\treturn Integer.MAX_VALUE;\n    \tif (result < Integer.MIN_VALUE)\n    \t\treturn Integer.MIN_VALUE;\n    \treturn (int) result;\n    }\n\n}\n\n```\n\n&nbsp;\n\n**13.Roman to Integer(罗马字转整数)**\n\n　　Example1: \"DCXXI\", return 621\n\n　　注意：1.一个罗马数字重复几次，就表示这个数的几倍\n\n　　　　　2.右加左减：在一个较大的罗马数字的右边记上一个较小的罗马数字，表示大数字加小数字。在一个较大的数字的左边记上一个较小的罗马数字，表示大数字减小数字。\n\n　　　　　3.('I',1); ('V',5); ('X',10); ('L',50); ('C',100); ('D',500); ('M',1000);\n\n**　　思路：1.检查str是否为空,是否实例化**\n\n**　　　　　2.把所以的罗马字和对应的整数放入一个switch(ch)函数中，返回的中对应的整数值**\n\n**　　　　　3.把最后一个罗马字对应的整数先放入result中**\n\n**　　　　　4.然后循环比较倒数第二个和倒数第一个，倒数第三个和倒数第二个。。。前比后大等就加上，前比后小就减去**\n\n**　　　　　5.返回result**\n\n```\npackage leetcode;\n\npublic class RomanToInteger {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n        String str;\n        InputData input = new InputData();\n        str = input.getString(\"请输入一个罗马字符串：\");\n        System.out.println(romanToInt(str));\n\t}\n\t\n\tpublic static int romanToInt(String s) {   //一个罗马数字重复几次，就表示这个数的几倍\n\t    //右加左减：在一个较大的罗马数字的右边记上一个较小的罗马数字，表示大数字加小数字。在一个较大的数字的左边记上一个较小的罗马数字，表示大数字减小数字。\n        if(s==null || s.length()<1){\n            return 0;\n        }\n        int length = s.length();\n        int result=0;\n        result = toNumber(s.charAt(length-1));        //如果只有一个就返回第0个\n        for(int i=length-2;i>=0;i--){\n            if(toNumber(s.charAt(i))>=toNumber(s.charAt(i+1))){ //要大于等于\n                result += toNumber(s.charAt(i));    \n            }else{\n                result -= toNumber(s.charAt(i));    \n            }\n        }\n        return result;\n\t}\n\t\n\tpublic static int toNumber(char ch) {  \n        switch (ch) {  \n            case 'I': return 1;  \n            case 'V': return 5;  \n            case 'X': return 10;  \n            case 'L': return 50;  \n            case 'C': return 100;  \n            case 'D': return 500;  \n            case 'M': return 1000;  \n        }  \n        return 0;  \n    }  \n\n}\n\n```\n\n&nbsp;\n\n**12.<strong>Integer** to Roman(整数**转****罗马字**)</strong>\n\n　　Example1: 621, return \"DCXXI\"\n\n　　注意：1.罗马字表示的时候有9 5 4 1这4种可能性\n\n　　　　　M　　CM　　D　　CD　　C　　XC　　L　　XL　　X　　IX　　V　　IV　　I\n\n　　　　1000　900　500　400　　100　90　　50　　40　10　　9　　　5　　4　　1\n\n　　　　　2.使用贪心法，先判断是否大于M，要是大于就在String中减去M\n\n　　　　　3.('I',1); ('V',5); ('X',10); ('L',50); ('C',100); ('D',500); ('M',1000);\n\n**　　思路：1.建立两个对应的数组theRoman和theInt，表示罗马字和整数的对应**\n\n**　　　　　2.使用贪心法，先判断是否大于M，要是大于就在String中减去M，同时在输入的整数中减去M对应的整数**\n\n```\npackage leetcode;\n\npublic class IntegerToRoman {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n        int i;\n        InputData input = new InputData();\n        i = input.getInt(\"请输入第一个整数：\", \"输入的数据必须是整数，请重新输入！\");\n        System.out.println(intToRoman(i));\n\t}\n\t\n    public static String intToRoman(int num) {\n        String str = \"\";\n        String[] theRoman = {\"M\",\"CM\",\"D\",\"CD\",\"C\",\"XC\",\"L\",\"XL\",\"X\",\"IX\",\"V\",\"IV\",\"I\"};\n        int[] theInt = {1000,900,500,400,100,90,50,40,10,9,5,4,1};\n        while(num > 0){\n            for(int i=0;i<theInt.length;i++)\n                if(num >= theInt[i]){\n                    str += theRoman[i];\n                    num -= theInt[i];\n                    break;\n                }\n        }\n        return str;\n    }\n}\n\n```\n\n&nbsp;\n","tags":["刷题"]},{"title":"面试题目——《Leetcode》链表","url":"/面试题目——《Leetcode》链表.html","content":"**分析整理自己做过的《Leetcode》题目**\n\n<!--more-->\n&nbsp;\n\n**19.Remove Nth Node From End of List(删除链表末尾的第N个元素)**\n\n　　注意：1.输入的链表可能是空\n\n**　　思路：1.检查链表的表头是否为空**\n\n**　　　　　2.当移除的是最后一个的元素的情况，其中又分成整个链表只有一个元素的情况和不知一个元素的情况**\n\n**　　　　　3.设一个theLast，先移动n次，然后同时移动head_temp和<strong>theLast（此时还有N大于等于链表的长度的情况）**，当theLast移动到链表的末尾的时候，**head_temp**的下一个就是倒数第N个，这时候head_temp.next = head_temp.next.next就行了</strong>\n\n**　　　　　4.最后返回head**\n\n```\npackage leetcode;\n\npublic class RemoveNthNodeFromEndOfList {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t}\n\t\n\tpublic static ListNode removeNthNode(ListNode head,int n){\n\t\tif(head == null)\n\t\t\treturn null;\n\t\tListNode theLast= head;\n\t\tListNode head_temp= head;\n\t\t\n\t\t//移除最后一个的情况\n\t\tif(n == 1){\n\t\t    if(head.next == null)\n\t\t        return null;\n\t\t    else{\n\t\t        while(theLast.next.next != null){//去掉链表的最后一个元素\n    \t\t    \ttheLast = theLast.next;\n    \t\t    }\n    \t\ttheLast.next = null;\n    \t\treturn head;\n\t\t    }\n\t\t}\n\t\t\n\t\t//移除的不是最后一个的情况\n\t\tfor(int i=0;i<n;i++){//先把theLast移动n位，如果n大于等于链表的长度的话，去掉head\n\t\t\tif(theLast.next != null){\n\t\t\t    theLast = theLast.next;\n\t\t\t}\n\t\t\telse\n\t\t\t\treturn head.next;//移除的是head的情况\n\t\t}\n\t\twhile(theLast.next != null){//同时移动两个链表的头\n    \t    head_temp = head_temp.next;\n    \t\ttheLast = theLast.next;\n        }\n\t\thead_temp.next = head_temp.next.next;\n\t\treturn head;\n\t}\n\t\n\tpublic static class ListNode {\n\t\tint val;\n\t\tListNode next;\n\t\tListNode(int x) { val = x; }\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**20.[Merge Two Sorted Lists](https://leetcode.com/problems/merge-two-sorted-lists/)(合并两个排序的链表)**\n\n　　注意：1.输入的两个链表可能是空\n\n**　　思路：1.新建一个值为0的表头，并拷贝这个表头**\n\n**　　　　　2.当l1和l2链表有一个不为空的时候循环，当l1、l2两个链表都不为空的时候，比较两个值并连接到新建的表头后面**\n\n**　　　　　3.当l1和l2有一个为空的时候，把另一个链表连到新的链表的后面，并break**\n\n```\npackage leetcode;\n\nimport huawei.InputData;\n\npublic class MergeTwoSortedLists {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\t//设置链表1\n        int len;\n        InputData input = new InputData();\n        len = input.getInt(\"请输入链表节点的个数：\", \"输入的数据必须是整数，请重新输入！\");\n        ListNode head = new ListNode(0);\n        ListNode temp = head;\n        for(int i = 0; i < len; i++) {\n        \ttemp.next = new ListNode(input.getInt(\"请输入链表节点的值：\", \"输入的数据必须是整数，请重新输入！\"));\n        \ttemp =  temp.next;\n        }\n        head = head.next;//链表的头结点是head\n        \n        showList(head);\n       \n\t\t//设置链表2\n       int len1;\n//       InputData input = new InputData();\n       len1 = input.getInt(\"请输入链表节点的个数：\", \"输入的数据必须是整数，请重新输入！\");\n       ListNode head1 = new ListNode(0);\n       ListNode temp1 = head1;\n       for(int i = 0; i < len1; i++) {\n       \ttemp1.next = new ListNode(input.getInt(\"请输入链表节点的值：\", \"输入的数据必须是整数，请重新输入！\"));\n       \ttemp1 =  temp1.next;\n       }\n       head1 = head1.next;//链表的头结点是head\n       \n       showList(head1);\n       \n       showList(MergeTwo(head,head1));\n       \n\t}\n\t\n\tpublic static void showList(ListNode head){\n\t\twhile(head != null){//输出链表的所有元素\n\t\t\tSystem.out.print(head.val+\"->\");\n\t\t   \thead = head.next;\n\t\t  }\n\t\tSystem.out.println();\n\t}\n\t\n\tpublic static class ListNode {\n\t\tint val;\n\t\tListNode next;\n\t\tListNode(int x) { val = x; }\n\t}\n\t\n\tpublic static ListNode MergeTwo(ListNode l1,ListNode l2){\n\t\tListNode head = new ListNode(0);\n\t\tListNode temp = head;//操作此节点，保留head\n\t\t\n\t\twhile(l1 != null || l2 != null){\n\t\t\tif(l1 != null &amp;&amp; l2 != null){\n\t\t\t\tif(l1.val < l2.val){\n\t\t\t\t\ttemp.next = l1;\n\t\t\t\t\ttemp = temp.next;\n\t\t\t\t\tl1 = l1.next;\n\t\t\t\t}else{\n\t\t\t\t\ttemp.next = l2;\n\t\t\t\t\ttemp = temp.next;\n\t\t\t\t\tl2 = l2.next;\n\t\t\t\t}\n\t\t\t\tif(l1 == null){\n\t\t\t\t\ttemp.next = l2;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(l2 == null){\n\t\t\t\t\ttemp.next = l1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn head.next;\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**23.[Merge k Sorted Lists](https://leetcode.com/problems/merge-k-sorted-lists/)(合并k个排序的链表)**\n\n　　注意：1.输入的参数可能是Lists[]，也可能是List<ListNode>\n\n**　　思路：1.在合并2个排序的链表的基础上，使用归并的方法，使得NK的复杂度下降到NlogK**\n\n```\npackage leetcode;\n\nimport java.util.Arrays;\n\npublic class MergeNSortedLists {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\t//设置链表1\n        int len;\n        InputData input = new InputData();\n        len = input.getInt(\"请输入链表节点的个数：\", \"输入的数据必须是整数，请重新输入！\");\n        ListNode head = new ListNode(0);\n        ListNode temp = head;\n        for(int i = 0; i < len; i++) {\n        \ttemp.next = new ListNode(input.getInt(\"请输入链表节点的值：\", \"输入的数据必须是整数，请重新输入！\"));\n        \ttemp =  temp.next;\n        }\n        head = head.next;//链表的头结点是head\n        \n        showList(head);\n       \n\t\t//设置链表2\n       int len1;\n//       InputData input = new InputData();\n       len1 = input.getInt(\"请输入链表节点的个数：\", \"输入的数据必须是整数，请重新输入！\");\n       ListNode head1 = new ListNode(0);\n       ListNode temp1 = head1;\n       for(int i = 0; i < len1; i++) {\n       \ttemp1.next = new ListNode(input.getInt(\"请输入链表节点的值：\", \"输入的数据必须是整数，请重新输入！\"));\n       \ttemp1 =  temp1.next;\n       }\n       head1 = head1.next;//链表的头结点是head\n       \n       showList(head1);\n       \n     //合并\n       ListNode[] NodeArray = {head,head1};//由链表头结点组成的数组\n       \n       showList(mergeKLists(NodeArray));//\n       \n\t}\n\t\n\tpublic static void showList(ListNode head){\n\t\twhile(head != null){//输出链表的所有元素\n\t\t\tSystem.out.print(head.val+\"->\");\n\t\t   \thead = head.next;\n\t\t  }\n\t\tSystem.out.println();\n\t}\n\t\n\tpublic static class ListNode {\n\t\tint val;\n\t\tListNode next;\n\t\tListNode(int x) { val = x; }\n\t}\n\t\n\tpublic static ListNode mergeKLists(ListNode[] lists) {//用归并的思想\n        if(lists==null||lists.length==0) {  \n            return null;  \n        }  \n        if(lists.length==1) {  \n            return lists[0];\n        }  \n        int length = lists.length;  \n        int mid = (length - 1)/2 ;  \n        ListNode l1 = mergeKLists(Arrays.copyOfRange(lists, 0, mid+1)) ;  \n        ListNode l2 = mergeKLists(Arrays.copyOfRange(lists, mid+1, length)) ;  \n  \n        return MergeTwoLists(l1,l2) ; \n        \n    }\n\t\n\tpublic static ListNode MergeTwoLists(ListNode l1,ListNode l2){\n\t\tListNode head = new ListNode(0);\n\t\tListNode temp = head;//操作此节点，保留head\n\t\t\n\t\twhile(l1 != null || l2 != null){\n\t\t\tif(l1 != null &amp;&amp; l2 != null){\n\t\t\t\tif(l1.val < l2.val){\n\t\t\t\t\ttemp.next = l1;\n\t\t\t\t\ttemp = temp.next;\n\t\t\t\t\tl1 = l1.next;\n\t\t\t\t}else{\n\t\t\t\t\ttemp.next = l2;\n\t\t\t\t\ttemp = temp.next;\n\t\t\t\t\tl2 = l2.next;\n\t\t\t\t}\n\t\t\t\tif(l1 == null){\n\t\t\t\t\ttemp.next = l2;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(l2 == null){\n\t\t\t\t\ttemp.next = l1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn head.next;\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**24.[Swap Nodes in Pairs](https://leetcode.com/problems/swap-nodes-in-pairs/)(两两交换链表的节点)**\n\n　　注意：1.添加一个表头形成循环的条件\n\n**　　思路：1.当链表的头结点为空或者只有一个节点的时候，直接返回头结点**\n\n**　　　　　2.设添加的这个结点为temp1，原来的头结点为temp2，然后当temp2和temp2.next不为空的时候循环**\n\n```\npackage leetcode;\n\nimport leetcode.MergeNSortedLists.ListNode;\n\npublic class SwapNodesInPairs {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\t//设置链表1\n        int len;\n        InputData input = new InputData();\n        len = input.getInt(\"请输入链表节点的个数：\", \"输入的数据必须是整数，请重新输入！\");\n        ListNode head = new ListNode(0);\n        ListNode temp = head;\n        for(int i = 0; i < len; i++) {\n        \ttemp.next = new ListNode(input.getInt(\"请输入链表节点的值：\", \"输入的数据必须是整数，请重新输入！\"));\n        \ttemp =  temp.next;\n        }\n        head = head.next;//链表的头结点是head\n        \n        showList(head);\n        \n        showList(swapPairs(head));\n\t}\n\t\n\tpublic static void showList(ListNode head){\n\t\twhile(head != null){//输出链表的所有元素\n\t\t\tSystem.out.print(head.val+\"->\");\n\t\t   \thead = head.next;\n\t\t  }\n\t\tSystem.out.println();\n\t}\n\t\n\tpublic static class ListNode {\n\t\tint val;\n\t\tListNode next;\n\t\tListNode(int x) { val = x; }\n\t}\n\t\n\tpublic static ListNode swapPairs(ListNode head){\n\t\tif(head == null || head.next == null)\n\t\t\t\treturn head;\n\t\t\n\t\t//以下是链表至少有两个元素的情况\n\t\tListNode fakeHead= new ListNode(0);//在原来链表的表头再添加一个节点\n\t\tfakeHead.next = head;\t\n\t\tListNode temp1 = fakeHead;\n\t\tListNode temp2 = head;\n\t\twhile(temp2 != null &amp;&amp; temp2.next != null){\n\t\t\tListNode newStart = temp2.next.next;\n\t\t\ttemp2.next.next = temp2;\n\t\t\ttemp1.next = temp2.next; \n\t\t\ttemp2.next =  newStart;\n\t\t\ttemp1 = temp2;\n\t\t\ttemp2 = temp2.next;\n\t\t}\n\t\treturn fakeHead.next;\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["刷题"]},{"title":"go学习笔记——casbin权限管理","url":"/go学习笔记——casbin权限管理.html","content":"在golang web中可以使用casbin实现RBAC权限管理，类似java spring security\n\n可以参考kratos的example\n\n```\nhttps://github.com/go-kratos/examples/tree/main/casbin\n\n```\n\n以及 [golang微服务框架Kratos实现鉴权 - Casbin](https://juejin.cn/post/7183310583876288571)\n\n<!--more-->\n&nbsp;\n","tags":["golang"]},{"title":"特征平台简介","url":"/特征平台简介.html","content":"特征平台（feature store）的定义：一个用于机器学习的数据管理层，允许共享和发现特征并创建更有效的机器学习管道。\n\n## 1.特征平台业界实现\n\n### 1.uber\n\n其最早由uber于2017年提出，uber的feature store名为Michaelangelo，参考：[Meet Michelangelo: Uber&rsquo;s Machine Learning Platform](https://www.uber.com/blog/michelangelo-machine-learning-platform/)\n\n<img src=\"/images/517519-20230921205657081-1677260283.png\" width=\"800\" height=\"381\" loading=\"lazy\" />\n\nMichaelangelo主要提供以下6种特性，如下图所示：\n\n1. Manage data\n1. Train models\n1. Evaluate models\n1. Deploy models\n1. Make predictions\n1. Monitor predictions\n\n<img src=\"/images/517519-20230921210709760-217846670.png\" width=\"800\" height=\"448\" loading=\"lazy\" />\n\n### 2.美团\n\n特征平台所能解决的一些问题：\n\n- **特征迭代成本高**：框架缺乏配置化管理，新特征上线需要同时改动离线侧和在线侧代码，迭代周期较长。\n- **特征复用困难**：外卖不同业务线间存在相似场景，使特征的复用成为可能，但框架缺乏对复用能力的很好支撑，导致资源浪费、特征价值无法充分发挥。\n- **平台化能力缺失**：框架提供了特征读写的底层开发能力，但缺乏对特征迭代完整周期的平台化追踪和管理能力。\n\n参考：[美团外卖特征平台的建设与实践](https://tech.meituan.com/2021/03/04/featureplatform-in-mtwaimai.html)\n\n其他美团的文章：[美团配送实时特征平台建设实践](https://mp.weixin.qq.com/s/gafduk7jhD9QnB1Lmmb1qw)\n\n### 3.字节跳动\n\n特征存储所解决的一些问题：\n\n1. 存储原始特征：由于在线特征抽取在特征调研上的低效率，我们期望能够存储原始特征；\n\n1. 离线调研能力：在原始特征的基础上，可以进行离线调研，从而提升特征调研效率；\n\n1. 支持特征回填：支持特征回填，在调研完成后，可以将历史数据全部刷上调研好的特征；\n\n1. 降低存储成本：充分利用数据分布的特殊性，降低存储成本，腾出资源来存储原始特征；\n\n1. 降低训练成本：训练时只读需要的特征，而非全量特征，降低训练成本；\n\n1. 提升训练速度：训练时尽量降低数据的拷贝和序列化反序列化开销。\n\n参考：[字节跳动基于 Iceberg 的海量特征存储实践](https://juejin.cn/post/7109391565855916045)\n\n## 2.特征拼接问题\n\n### 1.离线拼接（离线特征）\n\n以天级的batch任务来实现特征和曝光/点击的join，比如\n\n<!--more-->\n&nbsp;\n\n曝光流，假设用户A在10点1分30秒，用户B在10点2分30秒刷到了文章1和文章2\n\n\n\n|用户id|文章id|时间戳\n| ---- | ---- | ---- \n|A|1|2022-01-01 10:01:30\n|A|2|2022-01-01 10:01:30\n|B|1|2022-01-01 10:02:30\n|B|2|2022-01-01 10:02:00\n\n&nbsp;\n\n点击流，假设用户A在10点2分的时候点击了文章1，用户B在10点3分的时候点击了文章2\n\n\n\n|用户id|文章id|时间戳\n| ---- | ---- | ---- \n|A|1|2022-01-01 10:02:00\n|B|2|2022-01-01 10:03:00\n\n&nbsp;\n\njoin曝光流和点击流，得到可用于计算CTR的曝光点击表\n\n\n\n|用户id|文章id|曝光时间戳|点击时间戳|label（0没点击/1点击）\n| ---- | ---- | ---- | ---- | ---- \n|A|1|2022-01-01 10:01:30|2022-01-01 10:02:00|1\n|A|2|2022-01-01 10:01:30|&nbsp;|0\n|B|1|2022-01-01 10:02:30|&nbsp;|0\n|B|2|2022-01-01 10:02:30|2022-01-01 10:03:00|1\n\n&nbsp;\n\n用户离线特征表，包含2个特征：年龄和性别\n\n&nbsp;\n|用户id|用户年龄|用户性别\n|A|30|男\n|B|20|女\n\n文章离线特征表，包含2个特征：标签和来源\n\n&nbsp;\n|文章id|文章标签|文章来源\n|1|体育|NBA\n|2|政治|BBC\n\n拼接特征得到训练样本\n\n\n\n|用户id|用户年龄|用户性别|文章id|文章标签|文章来源|曝光时间戳|点击时间戳|label（0没点击/1点击）\n| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- \n|A|30|男|1|体育|NBA|2022-01-01 10:01:30|2022-01-01 10:02:00|1\n|A|30|男|2|政治|BBC|2022-01-01 10:01:30|&nbsp;|0\n|B|20|女|1|体育|NBA|2022-01-01 10:02:30|&nbsp;|0\n|B|20|女|2|政治|BBC|2022-01-01 10:02:30|2022-01-01 10:03:00|1\n\n这时候就获得了一张可以用于训练的训练集，但是该方案的问题也很明显，训练的周期是天级别的，无法支持在线学习（即实时特征），join的计算量大\n\n### 2.实时拼接（离线特征+实时特征）\n\n如果想添加一些实时特征的话，比如\n\n&nbsp;\n\n用户实时特征流，包含1个特征：用户最近10分钟看的文章的数量\n\n&nbsp;\n|用户id|用户最近10分钟看的文章的数量|时间戳\n|A|10|2022-01-01 10:00:00\n|B|20|2022-01-01 10:00:00\n\n&nbsp;\n\n文章实时特征流，包含1个特征：最近10分钟文章的点赞数量\n\n&nbsp;\n|文章id|最近10分钟文章的点赞数量|时间戳\n|1|100|2022-01-01 10:00:00\n|2|200|2022-01-01 10:00:00\n\n&nbsp;\n\n可以使用flink进行双流join，在窗口10分钟内，把点击流，曝光流，实时用户特征流，实时文章特征流进行join，得到\n\n\n\n|特征时间戳|用户id|用户最近10分钟看的文章的数量|文章id|最近10分钟文章的点赞数量|曝光时间戳|点击时间戳|label（0没点击/1点击）\n| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- \n|2022-01-01 10:00:00|A|10|1|100|2022-01-01 10:01:30|2022-01-01 10:02:00|1\n|2022-01-01 10:00:00|A|10|2|200|2022-01-01 10:01:30|&nbsp;|0\n|2022-01-01 10:00:00|B|20|1|100|2022-01-01 10:02:30|&nbsp;|0\n|2022-01-01 10:00:00|B|20|2|200|2022-01-01 10:02:30|2022-01-01 10:03:00|1\n\n注意这里使用的实时特征的窗口是离曝光时间戳最近的特征时间戳，否则会产生特征穿越问题\n\n然后使用flink的lookup table join（[详解flink中Look up维表的使用](https://cloud.tencent.com/developer/article/1697903)）补齐离线特征（会消耗比较大的KV缓存资源），或者在实时用户流和实时文章流中join上离线特征（会消耗比较大的kafka资源），得到训练样本\n\n&nbsp;\n\n\n\n|特征时间戳|用户id|用户最近10分钟看的文章的数量|用户年龄|用户性别|文章id|最近10分钟文章的点赞数量|文章标签|文章来源|曝光时间戳|点击时间戳|label（0没点击/1点击）\n| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- \n|2022-01-01 10:00:00|A|10|30|男|1|100|体育|NBA|2022-01-01 10:01:30|2022-01-01 10:02:00|1\n|2022-01-01 10:00:00|A|10|30|男|2|200|政治|BBC|2022-01-01 10:01:30|&nbsp;|0\n|2022-01-01 10:00:00|B|20|20|女|1|100|体育|NBA|2022-01-01 10:02:30|&nbsp;|0\n|2022-01-01 10:00:00|B|20|20|女|2|200|政治|BBC|2022-01-01 10:02:30|2022-01-01 10:03:00|1\n\n### 3.使用支持partial update的存储\n\n比如HBase和Hudi，把两表Join过程转化为两表写入数据库的过程\n\n美团最终的流式样本生成方案如下图：\n\n1. 特征快照中只包含流式、即时特征\n1. 特征快照写入Kafka\n1. 曝光/点击流以Insert形式写入HBase\n1. 特征快照在Kafka中延迟N分钟后，以Update形式写入HBase，并在写入过程中，二次访问Feature Store完成批式特征补录\n1. 从HBase中抽取样本供在线/离线训练使用\n\n<img src=\"/images/517519-20231020105617049-807537534.png\" width=\"800\" height=\"357\" loading=\"lazy\" />\n\n参考：[模型样本构建方案演进之路](https://zhuanlan.zhihu.com/p/546493272) 和 [外卖广告大规模深度学习模型工程实践 | 美团外卖广告工程实践专题连载](https://tech.meituan.com/2022/07/06/largescaledeeplearningmodel-engineeringpractice-in-mtwaimaiad.html)\n\n## 3.特征的生成方式\n\n1.特征回填（Backfilling Data）\n\n2.特征快照（Snapshot）\n\n3.特征请求快照（Request Snapshot）\n\n参考：[AI算法模型中的特征穿越问题：原理篇](https://zhuanlan.zhihu.com/p/402812843)\n\n## 4.Embedding特征\n\n特征数据除了可以分成离线特征和实时特征之外，还可以按特征的稀疏程度进行分类，比如文本类，用户标签之类的特征就属于稀疏特征（Sparse Feature），而各种隐因子模型产出的特征，就属于稠密特征（Dense Feature），即Embedding特征。\n\n对于稠密特征向量，例如各种隐因子向量，Embedding 向量，可以考虑文件存储，采用内存映射的方式，会更加高效地读取和使用。\n\n## 5.正排和倒排\n\n正排特征就使用的是用户ID或者文章ID来作为查询的主键，正排用于将用户特征+文章特征+曝光点击数据，拼凑成训练样本用于离线模型训练；同时在推荐系统或者广告系统中也会用于模型的online serving或者过滤（比如说基于规则、黑白名单、广告主预算 pacing 过滤等）。由于查询层需要支持按一些特定的字段进行筛选过滤，所以在选择反序列化方案的时候，就需要考虑性能问题，参考：[推荐系统倒排索引](https://zhuanlan.zhihu.com/p/539966796) 和 [广告召回系统的演进](https://zhuanlan.zhihu.com/p/110112102)\n\n倒排特征则是使用特征作为查询的主键，比如用户的标签，倒排用于召回候选集的时候，比如已知用户的个人标签，要用个人标签召回新闻，那么就需要提前准备好标签对应的新闻文章的倒排索引，最后再通过itemId去正排里拉取详情。\n\n这两种形态的特征数据，需要用不同的数据库存储。正排特征如果是用于离线模型训练，一般是存在Hive或者Hudi表中；正排特征如果是用于模型的online serving或者filter的话，则需要用列式数据库存储，比如HBase 和 Cassandra；倒排索引需要用 KV 数据库存储比如 Redis 或 Memcached。\n\n参考：[30 | 推荐系统服务化、存储选型及API设计](https://time.geekbang.org/column/article/6803) 和 [推荐系统从0到1](https://www.jianshu.com/p/ed14ad295619)\n\n## 6.特征穿越问题\n\n### **1.什么是特征穿越**\n\n**特征穿越**指的是特征的时间戳和label的时间戳不匹配，特征穿越一般发生在**实时特征**上面\n\n错误的join\n\n<img src=\"/images/517519-20230921220658735-1302956805.png\" width=\"500\" height=\"353\" loading=\"lazy\" />\n\n正确的join\n\n<img src=\"/images/517519-20230921220821306-21172452.png\" width=\"500\" height=\"352\" loading=\"lazy\" />\n\n为了让大家能够理解什么叫**特征穿越**，上图给出了一个简单例子，来展现这个问题。\n\n**图左上表**是用户的一个行为特征，表达了在不同时间节点，对于一个给定 ID 的用户，在最近两分钟内的点击数。这个点击数可能帮助我们推理用户是否会点击某个广告。为了用这些特征去做训练，通常需要将特征拼接到用户带有 Label 的一些数据集上。\n\n**图左下表**展现的是一个用户实际有没有点击广告的一些正样本和负样本的数据集，标注了在不同的时间点，用户所产生的正样本或负样本。为了将这两个数据集中的特征拼接起来，形成训练用的数据集，通常需要根据用户 ID 作为 key 进行特征拼接。如果只是简单地进行 Table Join，不考虑时间戳，就可能产生特征穿越问题。&nbsp;例如在 6:03 分时，用户最近 2 分钟点击数应该是 10，但拼接得到的特征值可能是来自 7:00 分时的 6。这种特征穿越会带来实际推理效果的下降。参考：[流批一体的实时特征工程平台建设实践](https://mp.weixin.qq.com/s/43Gh-rl7oiCKEmePhNuHHA)\n\n### **2.point-in-time correct join**\n\nSQL的思路就是先将曝光点击表和特征表进行inner join，曝光点击表的时间戳取曝光时间戳event_timestamp，特征表的时间戳为feature_timestamp\n\n曝光点击表\n\n&nbsp;\n|用户id|文章id|曝光时间戳（event_timestamp）|点击时间戳|label（0没点击/1点击）\n|A|1|2022-01-01 10:01:30|2022-01-01 10:02:00|1\n|A|2|2022-01-01 10:01:30|&nbsp;|0\n|B|1|2022-01-01 10:02:30|&nbsp;|0\n|B|2|2022-01-01 10:02:30|2022-01-01 10:03:00|1\n\n<img src=\"/images/517519-20231025172509647-1243117856.png\" width=\"800\" height=\"108\" loading=\"lazy\" />\n\n特征表\n\n&nbsp;\n|特征时间戳（每10分钟一个窗口、feature_timestamp）|用户id|用户最近10分钟看的文章的数量|文章id|最近10分钟文章的点赞数量\n|2022-01-01 09:40:00|B|5|1|5\n|2022-01-01 09:50:00|A|20|1|10\n|2022-01-01 09:50:00|B|10|2|50\n|2022-01-01 10:00:00|A|10|1|100\n|2022-01-01 10:00:00|A|10|2|200\n|2022-01-01 10:00:00|B|20|1|100\n|2022-01-01 10:00:00|B|20|2|200\n\n<img src=\"/images/517519-20231025172544423-1398210801.png\" width=\"800\" height=\"145\" loading=\"lazy\" />\n\n使用用户id和文章id进行inner join，得到如下表\n\n```\nCREATE VIEW feature_data AS\n    select feature_timestamp,\n           t1.user_id,\n           view_num_10_mins,\n           t1.doc_id,\n           doc_like_num_10_mins,\n           event_timestamp,\n           click_timestamp,\n           label\n    from (select * from click_view) t1\n             inner join\n             (select * from feature_table) t2\n             on t1.user_id = t2.user_id and t1.doc_id = t2.doc_id\n\n```\n\n可以看到由于特征有多个版本（例子里面有3个版本），所以join出来的结果会比较多，但是我们需要取得离event_timestamp最近的特征（红色标记），所以需要进行DeDuplicate操作\n\n&nbsp;\n|特征时间戳（每10分钟一个窗口、feature_timestamp）|用户id|用户最近10分钟看的文章的数量|文章id|最近10分钟文章的点赞数量|曝光时间戳（event_timestamp）|点击时间戳|label（0没点击/1点击）\n|2022-01-01 09:40:00|B|5|1|5|2022-01-01 10:02:30|&nbsp;|0\n|2022-01-01 09:50:00|A|20|1|10|2022-01-01 10:01:30|2022-01-01 10:02:00|1\n|2022-01-01 09:50:00|B|10|2|50|2022-01-01 10:02:30|2022-01-01 10:03:00|1\n|2022-01-01 10:00:00|A|10|1|100|2022-01-01 10:01:30|2022-01-01 10:02:00|1\n|2022-01-01 10:00:00|A|10|2|200|2022-01-01 10:01:30|&nbsp;|0\n|2022-01-01 10:00:00|B|20|1|100|2022-01-01 10:02:30|&nbsp;|0\n|2022-01-01 10:00:00|B|20|2|200|2022-01-01 10:02:30|2022-01-01 10:03:00|1\n\n<img src=\"/images/517519-20231025160859685-1913876165.png\" alt=\"\" loading=\"lazy\" />\n\nDeDuplicate操作\n\n将event_timestamp，user_id和doc_id进行group by，找到最大的feature_timestamp时候的user_id和doc_id，\n\n然后inner join上feature_data表，找到离event_timestamp最近的feature_timestamp的特征，从而实现point-in-time correct join\n\n```\nselect\n       t1.feature_timestamp,\n       t1.user_id,\n       view_num_10_mins,\n       t1.doc_id,\n       doc_like_num_10_mins,\n       event_timestamp,\n       click_timestamp,\n       label\nfrom (select * from feature_data) t1\n        inner join\n        (select max(feature_timestamp) as feature_timestamp,user_id,doc_id from feature_data group by event_timestamp, user_id, doc_id) t2\n        on t1.feature_timestamp = t2.feature_timestamp and t1.user_id = t2.user_id and t1.doc_id = t2.doc_id\n\n```\n\n<img src=\"/images/517519-20231025164817092-1179721913.png\" alt=\"\" loading=\"lazy\" />\n\n参考：[Point-in-Time Correct Join：Feast Spark版实现](https://zhuanlan.zhihu.com/p/480122449) 和 [Feast on AWS 解决方案](https://aws.amazon.com/cn/blogs/china/feast-on-aws-solutions/)\n\n## 7.离在线一致性问题\n\n参考：[特征平台](https://qiankunli.github.io/2022/06/27/feature_platform.html)\n\n[推荐系统中模型训练及使用流程的标准化](https://www.infoq.cn/article/km8utqyaatc4uedgzpyr)\n","tags":["ML Infra"]},{"title":"kafka-ui部署","url":"/kafka-ui部署.html","content":"## 1.介绍\n\nkafka-ui是一个开源的kafka ui工具，支持kafka，schema registry（avro和protobuf都支持），kafka connect，KSQL DB等组件\n\ngithub项目\n\n```\nhttps://github.com/provectus/kafka-ui\n\n```\n\ndocker镜像地址\n\n```\nhttps://hub.docker.com/r/provectuslabs/kafka-ui\n\n```\n\n注意：该kafka-ui只支持2.x.x版本的kafka，对于低版本的kafka不支持，参考issue：[https://github.com/provectus/kafka-ui/issues/2097](https://github.com/provectus/kafka-ui/issues/2097)\n\n界面如下\n\n<img src=\"/images/517519-20230511144621127-431557646.png\" alt=\"\" loading=\"lazy\" />\n\n其中schema registry的schema类型支持AVRO，JSON和PROTOBUF\n\n<img src=\"/images/517519-20230511145247779-1535338122.png\" width=\"1000\" height=\"457\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n## 2.安装部署\n\n参考：[kafka 可视化 Web UI for Apache Kafka安装部署文档](https://segmentfault.com/a/1190000042442100)\n\n可以使用helm或者docker来部署容器\n\nhelm命令\n\n```\nhelm install kafka-ui kafka-ui/kafka-ui --set envs.config.KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=\"xxx:9092\"\n\n```\n\ndocker命令\n\n```\ndocker run -p 8080:8080 \\\n\t　　-e KAFKA_CLUSTERS_0_NAME=\"xxx\"\\\n        -e KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=\"xxx:9092\" \\\n        -d provectuslabs/kafka-ui:latest\n\n```\n\n如果想添加kafka connect等组件，配置参考\n\n```\nhttps://docs.kafka-ui.provectus.io/configuration/complex-configuration-examples/kraft-mode-+-multiple-brokers\n```\n\n## 3.集成openid\n\nkafka-ui支持丰富的认证方式，比如basic auth（账号密码），OAuth2（google/github等），AWS IAM，LDAP，SSO（openid），SASL_SCRAM\n\n```\nhttps://docs.kafka-ui.provectus.io/configuration/authentication\n\n```\n\n目前我司采用的任务方式主要有Microsoft Azure以及keycloak，Microsoft Azure测试下来需要kafka-ui启用HTTPS，因为在Microsoft Azure上填写回调地址的时候会提示只支持HTTPS，所以下面使用keycloak的认证方式\n\nkafka-ui使用docker命令启动的时候开启keyloack认证，官方文档上并没有写清楚如何集成keycloak，对应的issue也没有回答清楚\n\n```\nhttps://github.com/provectus/kafka-ui/issues/3802\n\n```\n\n所以这里给出正确的配置，其中的配置需要在keycloak上先注册好应用\n\n```\ndocker run -p 8080:8080 \\\n\t\t-e KAFKA_CLUSTERS_0_NAME=\"your_kafka_name\"\\\n        -e KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=\"ip:9092\" \\\n        -e AUTH_TYPE=OAUTH2 \\\n        -e AUTH_OAUTH2_CLIENT_KEYCLOAK_CLIENTID=kafka-ui \\\n        -e AUTH_OAUTH2_CLIENT_KEYCLOAK_CLIENTSECRET=xxxxxx-xxxx-xxxx-xxxx-xxxxxx \\\n        -e AUTH_OAUTH2_CLIENT_KEYCLOAK_SCOPE=openid \\\n        -e AUTH_OAUTH2_CLIENT_KEYCLOAK_PROVIDER=keycloak \\\n        -e AUTH_OAUTH2_CLIENT_KEYCLOAK_CLIENT-NAME=keycloak \\\n        -e AUTH_OAUTH2_CLIENT_KEYCLOAK_USER_NAME_ATTRIBUTE=preferred_username \\\n        -e AUTH_OAUTH2_CLIENT_KEYCLOAK_ISSUER_URI=\"https://keycloak.xxxx.com/auth/realms/master\" \\\n        -e AUTH_OAUTH2_CLIENT_KEYCLOAK_CUSTOM_PARAMS_TYPR=\"https://keycloak.xxxx.com/auth/realms/master\" \\\n        -d provectuslabs/kafka-ui:latest\n\n```\n\n登录后如下，点击keycloak即可实现openid登录\n\n<img src=\"/images/517519-20230606160334832-1126603990.png\" alt=\"\" loading=\"lazy\" />\n\n如果kafka-ui开启了HTTPS的话，可以使用如下方式开启Microsoft Azure认证\n\n```\ndocker run -p 8080:8080 \\\n\t\t-e KAFKA_CLUSTERS_0_NAME=\"your_kafka_name\"\\\n        -e KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=\"ip:9092\" \\\n        -e AUTH_TYPE=OAUTH2 \\\n        -e AUTH_OAUTH2_CLIENT_AZURE_CLIENTID=xxxxxx-xxxx-xxxx-xxxx-xxxxxx \\\n        -e AUTH_OAUTH2_CLIENT_AZURE_CLIENTSECRET=xxxxx \\\n        -e AUTH_OAUTH2_CLIENT_AZURE_SCOPE=\"https://graph.microsoft.com/User.Read\" \\\n        -e AUTH_OAUTH2_CLIENT_AZURE_PROVIDER=azure \\\n        -e AUTH_OAUTH2_CLIENT_AZURE_ISSUER_URI=\"https://login.microsoftonline.com/xxxxxx-xxxx-xxxx-xxxx-xxxxxx/v2.0\" \\\n        -d provectuslabs/kafka-ui:latest\n\n```\n\n　　\n\n&nbsp;\n","tags":["kafka"]},{"title":"面试题目——《剑指Offer》","url":"/面试题目——《剑指Offer》.html","content":"1.**把一个字符串转换成整数（看面试题50）**&mdash;&mdash;《剑指Offer》P29\n\n2.**求链表中的倒数第k个结点（看面试题15）**&mdash;&mdash;《剑指Offer》P30\n\n3.实现Singleton模式&mdash;&mdash;《剑指Offer》P48\n\n　　参考：[http://www.cnblogs.com/tonglin0325/p/5196818.html](http://www.cnblogs.com/tonglin0325/p/5196818.html)\n\n　　　　　以及 http://wiki.jikexueyuan.com/project/for-offer/question-two.html\n\n4.数组的内存是连续的，所以数组的时间效率很高，可以用来实现简单的哈希表&mdash;&mdash;《剑指Offer》35题&ldquo;第一个只出现一次的字母&rdquo;\n\n5.面试题3:**二维数组中的查找**&mdash;&mdash;《剑指Offer》P55 Leecode 74题　　相关题目：《leetcode》 21题 Merge Two Sorted Lists，提示：《剑指Offer》P66\n\n　　提示；矩阵从右上角开始搜索，row和column\n\n6.面试题4:**替换空格**&mdash;&mdash;《剑指Offer》P61　　\n\n7.面试题5:从尾到头打印链表&mdash;&mdash;《剑指Offer》P68　　《Leetcode》 206题<!--more-->\n&nbsp;Reverse Linked List（**leetcode写的算法速度太慢，待优化**）\n\n　　其他链表的题目：\n\n　　<img src=\"/images/517519-20160527113625381-339406825.png\" alt=\"\" width=\"448\" height=\"230\" />\n\n8.面试题6:重建二叉树，由二叉树的前序遍历和中序遍历重建二叉树&mdash;&mdash;《剑指Offer》P72　　《Leetcode》 105和106题\n\n　　思路：1.找到根节点 2.结束条件，返回 3.找到根节点在另一个遍历中的位置 4.计算左右子树长度 5.左右子树分别递归\n\n　　其他二叉树的题目：\n\n　　<img src=\"/images/517519-20160527120108538-218640073.png\" alt=\"\" width=\"496\" height=\"396\" />\n\n　　<img src=\"/images/517519-20160527120140272-1966468718.png\" alt=\"\" width=\"495\" height=\"161\" />\n\n9.面试题7:**用两个栈实现队列**&mdash;&mdash;《剑指Offer》P76　　**《Leetcode》232题&nbsp;Implement Queue using Stacks　　225题&nbsp;Implement Stacks using Queue**\n\n　　其他栈和队列的题目：\n\n　　<img src=\"/images/517519-20160527193728772-390864610.png\" alt=\"\" width=\"477\" height=\"329\" />\n\n10.面试题8:**旋转数组的最小数字**&mdash;&mdash;《剑指Offer》P83　　《leetcode》153题 Find Minimum in Rotated Sorted Array\n\n　　思路：剑指Offer中是数组中没有重复数字的情况，利用二分查找结题，\n\n　　相关题目：《左》第九章 getMin，以及**在有序旋转数组中查找某个数**\n\n11.面试题9:**斐波那契数列**&mdash;&mdash;《剑指Offer》P90　　相关题目：青蛙跳台阶、变态跳台阶（注意Math.pow()返回的值是double，要转换成int）\n\n　　相关题目：\n\n　　<img src=\"/images/517519-20160528095725272-959276643.png\" alt=\"\" width=\"490\" height=\"70\" />\n\n　　<img src=\"/images/517519-20160528095748850-1333931649.png\" alt=\"\" width=\"477\" height=\"270\" />\n\n12.面试题10:**二进制中1的个数**&mdash;&mdash;《剑指Offet》P95　　《leetcode》191题&nbsp;Number of 1 Bits 两种解法，一种要移动32次，一种有多少1移动多少次\n\n　　相关知识点：\n\n　　<img src=\"/images/517519-20160528101444100-1021592751.png\" alt=\"\" width=\"462\" height=\"346\" />\n\n　　<img src=\"/images/517519-20160528104228272-549636572.png\" alt=\"\" width=\"487\" height=\"392\" />\n\n13.面试题11:**数值的整数次方**&mdash;&mdash;《剑指Offer》P107　　注意：异常输入的处理　　参考《leetcde》50题的解法和收藏博文的解法说明，记nlogn的解法\n\n14.面试题12:打印1到最大的n位数&mdash;&mdash;《剑指Offer》P111　　两种解法\n\n```\nimport java.util.Arrays;\n\npublic class printN {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\t//System.out.println(JumpFloor(4));\n\n\t\tprint1ToMaxOfN(2);\n\t}\n\t\n    public static void print1ToMaxOfN(int n){\n    \tif(n<0)\n    \t\treturn;\n    \tchar[] number = new char[n];\n    \tArrays.fill(number, '0');\n    \twhile(!Increment(number))\n    \t\tPrintNumber(number);\n    }\n\n\t public static boolean Increment(char[] number) {\n\t\tboolean isOverflow = false;\n\t\tint nTakeOver = 0;\n\t\tint nLen = number.length;\n\n\t\tfor(int i=nLen-1;i>=0;i--){\t//当没有进位的时候会break，循环只执行一遍，有进位会执行第二遍\n\t\t\tint nSum = number[i]-'0'+nTakeOver;\t//加上进位\n\t\t\tif(i==nLen-1)\n\t\t\t\tnSum++;\n\t\t\tif(nSum>=10){\n\t\t\t\tif(i==0)\n\t\t\t\t\tisOverflow=true;\n\t\t\t\telse{\n\t\t\t\t\tnSum -= 10;\n\t\t\t\t\tnTakeOver = 1;\n\t\t\t\t\tnumber[i]=(char)(nSum+48);\n\t\t\t\t\t//System.out.println(number);\n\t\t\t\t}\n\t\t\t}else{\n\t\t\t\tnumber[i]=(char)(nSum+48);\n\t\t\t\t//System.out.println(number);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\t//PrintNumber(number);\n\t\t return isOverflow;\n\t}\n\t \n    public static void PrintNumber(char[] number){\n    \tboolean isBeginning0 = true;\n    \tint nLen = number.length;\n    \tfor(int i=0;i<nLen;i++){\n    \t\tif(isBeginning0 &amp;&amp; number[i] != '0')\n    \t\t\tisBeginning0=false;\n    \t\tif(!isBeginning0)\n    \t\t\tSystem.out.print(number[i]);\n    \t}\n    \tSystem.out.println();\n    }\n   \n\n}\n\n```\n\n&nbsp;15.面试题13:**在O(1)时间删除链表结点**&mdash;&mdash;《剑指Offer》P116　　分情况：1.结点为空 2.删除的是头结点 至少有两个结点 3.删除的是最后一个结点 4.删除的是中间的结点\n\n&nbsp;　　参考：http://wiki.jikexueyuan.com/project/for-offer/question-thirteen.html《极客学院》\n\n16.面试题14:**调整数组顺序使奇数位于偶数前面**&mdash;&mdash;《剑指Offer》P119　　《Leetcode》328题 Odd Even Linked List，这里是奇数和偶数结点\n\n17.面试题15:**链表中倒数第k个结点**&mdash;&mdash;《剑指Offer》P124　　《Leetcode》19题 Remove Nth Node From End of List(leetcode测试用例有错，[1,2,3]移除倒数4的时候是[2,3])　　两种，一种遍历两遍，另一种遍历一遍\n\n　　相关题目：\n\n　　<img src=\"/images/517519-20160529232034991-1285105609.png\" alt=\"\" width=\"475\" height=\"264\" />\n\n18.面试题16:反转链表&mdash;&mdash;《剑指Offer》P129　　《Leetcode》92和206题&nbsp;Reverse Linked List\n\n19.面试题17:**合并两个排序的链表**&mdash;&mdash;《剑指Offer》P131　　《Leetcode》21题 Merge Two Sorted Lists　　剑指offer里面用的是递归，leetcode用的分情况\n\n20.面试题18:**树的子结构**&mdash;&mdash;《剑指Offer》P134　　《牛客OJ》使用两个递归方法\n\n21.面试题19:**二叉树的镜像**&mdash;&mdash;《剑指Offer》P142　　《牛客OJ》使用递归\n\n22.面试题20:**顺时针打印矩阵**&mdash;&mdash;《剑指Offer》P144　　《Leetcode》54和59题 Spiral Matrix\n\n23.面试题21:**包含min函数的栈**&mdash;&mdash;《剑指Offer》P149　　《Leetcode》 155题 Min Stack\n\n24.面试题22:**栈的压入、弹出序列**&mdash;&mdash;《剑指Offer》P154　　《牛客OJ》栈的压入、弹出序列　　注意.length<=0可以，而==null不行的问题\n\n25.面试题23:**从上往下打印二叉树**&mdash;&mdash;《剑指Offer》P157　　注意返回的结果是ArrayList<Integer>还是ArrayList<ArrayList<Integer>>\n\n　　<img src=\"/images/517519-20160531145344274-87907206.png\" alt=\"\" width=\"472\" height=\"262\" />\n\n　　<img src=\"/images/517519-20160531145420227-701905285.png\" alt=\"\" width=\"485\" height=\"153\" />\n\n26.面试题24:二叉搜索树的后序遍历序列&mdash;&mdash;《剑指Offer》P157　　《牛客OJ》注意Arrays.copyOfRange(sequence, 0, i)可以用来取数组的部分元素，前后的范围为[],root = sequence[len-1],所以取i而不是i-1\n\n27.面试题25:**二叉树中和为某一值的路径**&mdash;&mdash;《剑指Offer》P160　　《Leetcode》112和113题&nbsp;Path Sum 注意是输出是否，还是所有路径，DFS递归（难）\n\n28.面试题26:**复杂链表的复制**&mdash;&mdash;《剑指Offer》P164　　《牛客OJ》里面讨论区中有**3种方法**\n\n29.面试题27:**二叉搜索树与双向链表**&mdash;&mdash;《剑指Offer》P168　　《牛客OJ》递归的时候要把把pLastNodeInList设为全局变量，画图理解\n\n30.面试题28:**字符串的排列**&mdash;&mdash;《剑指Offer》P171　　《牛客OJ》字符串的全排列，其中还要求字典排序，不单单只是列出所有的可能性（难）\n\n　　相关题目：\n\n　　<img src=\"/images/517519-20160602160518883-1856603977.png\" alt=\"\" width=\"480\" height=\"459\" />\n\n　　<img src=\"/images/517519-20160602160602086-2124147746.png\" alt=\"\" width=\"485\" height=\"529\" />\n\n31.面试题29:**数组中出现次数超过一半的数字（在leetcode包中的MajorityElement）**&mdash;&mdash;《剑指Offer》P180　　《Leetcode》169题 [Majority Element](https://leetcode.com/problems/majority-element)\n\n　　思路：声明一个count和temp，count代表temp出现的次数，当count等于0的时候temp = arr[i]\n\n　　类似：[Majority Element II](https://leetcode.com/problems/majority-element-ii) ，**数组中出现次数超过n/k的数字**\n\n　　思路：维护一个大小为k的map，当map的大小等于k-1的时候，map中的全部value减去1\n\n32.面试题30:**最小的k个数（<strong>GetLeastNumbers**）</strong>&mdash;&mdash;《剑指Offer》P184 　　两种：1.Partition算法　　2.利用数组的特点，见Leetcode数组篇　　\n\n　　思路：1.最简单的思路是把输入的n个数进行排序，然后取前k个数，算法的复杂度为O(nlogn)\n\n　　　　　2.利用堆排序的思想O(nlogk)\n\n　　　　　2.运用快排Partition的思想，就是先任意选取一个数，把小于这个值的数放在这个数左边，大于这个值的数放在右边，然后返回的值是这个值在数组中的位置，然后根据这个修改start或者end的值O(n)\n\n33.面试题31:**连续子数组的最大和**&mdash;&mdash;《剑指Offer》P188\n\n　　思路：1.如果前n个相加小于0，那么最大的和只能从n+1个开始算\n\n　　　　　2.设curGreatestSum=Integer.VALUE_MIN，然后比较curSum和curGreatestSum\n\n34.面试题32:**从1到n整数中1出现的次数（NumberOf1Between1AndN）**&mdash;&mdash;《剑指Offer》P191\n\n　　相关题目：**只统计一个数的二进制表示中1的位数（Leetcode CountBits）**\n\n　　思路：1.一个数一个数的计算1的位数，然后相加，时间复杂度为O(nlogn)\n\n　　　　　2.寻找规律：534 = （个位1出现次数）+（十位1出现次数）+（百位1出现次数）=（53*1+1）+（5*10+10）+（0*100+100）= 214\n\n　　注意：int n = Integer.parseInt<img id=\"selectsearch-icon\" src=\"http://img.baidu.com/img/iknow/qb/select-search.png\" alt=\"搜索\" />(\"123\");　　//将字符串转化成整形<br />　　　　　String s = String.valueOf(int a );　　　　　　//将整形转化为字符串\n\n　　　　　　int[]&nbsp;is&nbsp;=&nbsp;{1,2,3,4};\n\n　　　　　String&nbsp;str&nbsp;=&nbsp;Arrays.toString(is);　　　　　　//字符数组转化成字符串\n\n35.面试题33:**把数组排成最小的数**&mdash;&mdash;《剑指Offer》P194　　《Leetcode》179题　Largest Number\n\n　　思路：使用sort排序，重点是重新建立排序规则，重写用于排序的Comparator\n\n36.面试题34:**求从小到大的第N个丑数**（我们把**只包含**因子2、3和5的数成为丑数，习惯上把1当作第一个丑数）\n\n　　思路：**第1种解法**.是遍历所有的数，%2,%3,%5是都==0\n\n　　　　　**第2种解法**\n\n　　　　　　　　1.首先第一个丑数是1\n\n　　　　　　　　2.第2个丑数是1乘以2,3,5中结果大于1的最小的一个\n\n　　　　　　　　3.第3个丑数是1乘以3,5，和2乘以2的结果中最小的一个\n\n37.面试题35:**第一个只出现一次的字符**&mdash;&mdash;《剑指Offer》P203\n\n　　思路：使用HashMap，注意输入的时候使用Scan循环检测\n\n38.面试题36：**数组中的逆序对**&mdash;&mdash;《剑指Offer》P206\n\n　　思路：利用递归的方法，通过增加空间O（N），把空间复杂度O（N^2）降到O（NlogN）\n\n39.面试题37：**两个链表中的第一个公共节点**&mdash;&mdash;《剑指Offer》P210　　《Leetcode》160题 Intersection of Two Linked Lists\n\n　　思路：先把长的链表的表头向后移动，直到两个链表的长度相等，然后同时移动找到公共节点\n\n40.面试题38：**数字在排序数组中出现的次数**&mdash;&mdash;《剑指Offer》P221\n\n　　思路：使用递归和二分查找的思想，分别求得**第一个**数字出现的位置和**最后一个**数字出现的位置\n\n41.面试题39：**二叉树的深度**&mdash;&mdash;《剑指Offer》P224\n\n　　思路：1.使用递归的思想&mdash;&mdash;参考《Leetcode》110题 Balanced Binary Tree，查找左右子树的最大深度\n\n　　　　　2.在《Leetcode》的110题中还有一种每个结点只遍历一次的解法//不会\n\n42.面试题40：**数组中只出现一次的数字（有两个）**&mdash;&mdash;《剑指Offer》P228\n\n　　思路：1.在一个数组中出现一次的数字有2个，所以首先要把这个数组分成两个数组，把数组中的每个数用二进制表示并做异或运算，因为相同的数异或之后为0,所以最后的结果是两个只出现1次的数异或之后的结果\n\n　　　　　2.然后根据这个异或的结果，比如0010，根据最右边一位，也就是倒数第2位是否为1，把数组分成两个\n\n　　　　　3.对两个数组分别做异或，异或的结果就是要求的数的值\n\n43.面试题41：**和为s的两个数字**&mdash;&mdash;《剑指Offer》P231\n\n　　思路：因为数组是已经排好序的，所以设start和end，如果sum小于所要求的和就start++，大于end--\n\n44.面试题42：**和为s的连续正数序列**，比如10的是1,2,3,4&mdash;&mdash;《剑指Offer》P231\n\n　　思路：和上一题目的方法类似，设small为1，big为2，然后序列和大于要求的目标small++，小于就big++，注意small<(sum+1)/2)的情况不用计算\n\n45.面试题43：**翻转单词顺序**，比如\"I am a student.\"->\"student. a am I\"&mdash;&mdash;《剑指Offer》P235\n\n　　思路：1.首先翻转所有字符的顺序，\"I am a student.\"->\".tneduts a ma I\"\n\n　　　　　2.然后在翻转每个单词中字符的顺序，\".tneduts a ma I\"->\"student. a am I\"\n\n46.面试题44：**左旋转字符串**，比如输入字符串\"abcdefg\"和数字2，函数返回左旋转2位得到的结果\"cdefgab\"&mdash;&mdash;《剑指Offer》P235\n\n　　思路：1.首先按照给定的数组2，把字符串分成\"ab\"和\"cdefg\"两个部分，分别旋转这两个部分，\"abcdefg\"->\"bagfedc\"\n\n　　　　　2.在翻转整个字符串，得到\"bagfedc\"->\"cdefgab\"\n\n　　　　　或者直接用n%length，之后用java的subString方法\n\n47.面试题45：n个骰子的点数，n个骰子，所有骰子朝上一面的点数之和为s，打印出s的所有可能的值出现的概率&mdash;&mdash;《剑指Offer》P240\n\n　　思路：比较难\n\n48.面试题46：扑克牌的顺子，从扑克牌中随机抽5张牌，判断是不是一个顺子，即这5张牌是不是连续的，A为1,J为11，Q为12，K为13，大小王为任意数字&mdash;&mdash;《剑指Offer》P243\n\n　　思路：1.首先对数组进行排序\n\n　　　　　2.然后统计数组中numberOfZero和numberOfGap，如果numberOfZero>=numberOfGap就返回True\n\n　　　　　　注意数组中有对子的话，不是顺子\n\n49.面试题47：**圆圈中最后剩下的数字**，n个数字围成一个圈，每次删除第m个数字，求最后剩下的数字（**约瑟夫环问题**）&mdash;&mdash;《剑指Offer》P245\n\n　　思路：1.采用环形链表来模拟圆圈\n\n　　　　　2.利用公式（代码量很少，且时间空间复杂度优于第一种方法，用递归或者循环实现）\n\n50.面试题48：求1+2+&hellip;&hellip;+n，不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）&mdash;&mdash;《剑指Offer》P250\n\n　　思路：1.使用递归的思想\n\n51.面试题49：不用加减乘除做加法，求两个整数只和，不能使用加减乘除&mdash;&mdash;《剑指Offer》P254\n\n　　思路：1.求两个数的异或，异或结果为不包含进位的和\n\n　　　　　2.求两个数与运算后的结果，并向左移1位，这个为进位的结果\n\n　　　　　3.把异或结果和进位结果相加，但是不能使用加法，就是把这个方法循环\n\n52.面试题50：**把字符串变成整数（atoi函数）**&mdash;&mdash;《剑指Offer》P261　　《Leetcode》第8题 String to Integer (atoi)\n\n　　思路：1.去掉首尾的空格\n\n　　　　　2.检查符号位\n\n　　　　　3.用double型来存储（double result），因为可能超过Long.MAX_VALUE，9223372036854775807\n\n　　　　　4.循环字符串的每一位，从非符号位的第一位开始，到第一个非数字位结束\n\n　　　　　5.如果flag是'-'，result取反\n\n　　　　　6.检查有没有越界&mdash;&mdash;大于Integer.MAX_VALUE则返回Integer.MAX_VALUE，小于Integer.MIN_VALUE，则返回Integer.MIN_VALUE，否则返回(int) result\n\n&nbsp;53.面试题51：树中两个结点的最低公共祖先&mdash;&mdash;《剑指Offer》P269　　《Leetcode》235题&nbsp;[Lowest Common Ancestor of a Binary Search Tree](https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-search-tree/) \n\n　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　 &nbsp;236题[Lowest Common Ancestor of a Binary Tree](https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-tree/)\n\n　　思路：1.如果树是二叉搜索树，即一个结点的左子树都比这个结点小，右子树都比这个结点大。\n\n　　　　　　输入两个结点，那么从根节点开始，如果这两个结点都比根节点小，那么这两个结点都在左子树中，一直直到两个结点一个比某个结点小，一个比某个结点大，那么这个结点都是最低的公共祖先。\n\n　　　　　2.如果树不是二叉搜索树，但是每个子结点都有指向它的父结点，这个问题就变成了求两个链表的第一个交点。\n\n　　　　　3.如果树既不是二叉搜索树，子结点也没有指向父结点。用两个链表记录根节点到两个结点的路径，然后把问题转换成求两个链表的最后一个公共结点\n\n　　　　　　或者利用递归的思想，进行后序遍历\n\n```\npublic class Solution {\n    public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) {\n        if (root == null || p == null || q == null) {  \n            return null;  \n        }  \n          \n        // If any of the node is the root, just return the root.  \n        if (root == p || root == q) {  \n            return root;  \n        }  \n          \n        // if no node is in the node, just recursively find it in LEFT and RIGHT tree.  \n        TreeNode left = lowestCommonAncestor(root.left, p, q);  \n        TreeNode right = lowestCommonAncestor(root.right, p, q);  \n          \n        if (left == null) {  // If didn't found in the left tree, then just return it from right.  \n            return right;  \n        } else if (right == null) { // Or if didn't found in the right tree, then just return it from the left side.  \n            return left;  \n        }   \n          \n        // if both right and right found a node, just return the root as the Common Ancestor.  \n        return root; \n    }\n}\n\n```\n\n&nbsp;\n","tags":["刷题"]},{"title":"Spring MVC学习笔记——Controller接口","url":"/Spring MVC学习笔记——Controller接口.html","content":"<img src=\"/images/517519-20160524111559100-1690942221.png\" alt=\"\" width=\"693\" height=\"721\" />\n\n<img src=\"/images/517519-20160524111708881-460008731.png\" alt=\"\" width=\"729\" height=\"527\" />\n\n<img src=\"/images/517519-20160524111834959-1615073189.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160524192758288-405444691.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160524192904866-896404721.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160524192926006-1358703218.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160524193005803-863421126.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160524193036944-1364990501.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160524193144084-1202926974.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160524194210038-1305299628.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n<img src=\"/images/517519-20160524194429272-523898595.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160524194456788-1618498536.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160524194656522-1464557226.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160524194728006-384866944.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160524194845147-616469281.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160524195040897-770866222.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160524195309288-796248229.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160524195455350-1503112725.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160524195934334-155334449.png\" alt=\"\" />\n\n&nbsp;\n","tags":["SpringMVC"]},{"title":"mac安装go1.20","url":"/mac安装go1.20.html","content":"官方下载地址\n\n```\nhttps://go.dev/dl/\n\n```\n\n1.下载pkg版本的安装包，直接双击安装，比如\n\n```\nhttps://go.dev/dl/go1.20.12.darwin-amd64.pkg\n\n```\n\n这时默认的GOPATH路径（go依赖的下载路径）在~/go\n\n2.也可以下载tar的压缩包进行安装\n\n下载mac对应的安装版，intel版本的mac下载x86版本\n\n```\nhttps://go.dev/dl/go1.20.12.darwin-amd64.tar.gz\n\n```\n\n解压到/usr/local目录\n\n```\nsudo tar -xzvf go1.20.12.darwin-amd64.tar.gz -C /usr/local\n\n```\n\n<img src=\"/images/517519-20221208142147082-1800122387.png\" alt=\"\" loading=\"lazy\" />\n\n配置环境变量\n\n```\n# go\nexport GOROOT=/usr/local/go\nexport GOPATH=$HOME/go\nexport GO111MODULE=on\nexport GOPROXY=http://goproxy.cn,direct\nexport PATH=$GOROOT/bin:$GOPATH/bin:$PATH\n\n```\n\n其中\n\n```\nGOPATH： go工作区， 下载的依赖会存放在此目录中，默认会在~/go目录下\nGOROOT： go的安装目录\n\n```\n\n在`GOPATH`工作区目录下，有3个目录\n\n```\nbin: 存储可执行bin文件\npkg: 编译完成的文件\nsrc: 源代码文件\n\n```\n\nsource配置\n\n```\nsource ~/.bash_profile\n\n```\n\n验证安装版本\n\n```\ngo version\ngo version go1.20.12 darwin/amd64\n\n```\n\n参考：[mac下安装go开发环境](https://zhuxiongxian.cc/2021/03/18/install-go-on-mac/)\n\n如果在Goland中遇到报错：Unresolved dependency<br /><img src=\"/images/517519-20221208152656149-20447188.png\" alt=\"\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n可以在勾选Enable Go modules integration来解决\n\n<img src=\"/images/517519-20221208152726304-1012079896.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["golang"]},{"title":"Redis学习笔记——应用场景","url":"/Redis学习笔记——应用场景.html","content":"redis共有5种基本数据类型：\n\n**String（可以是字符串，整数或者浮点数）<strong>、**</strong>\n\n**Hash（哈希，**Redis hash 是一个 string 类型的 field（字段） 和 value（值） 的映射表，hash 特别适合用于存储对象。Redis 中每个 hash 可以存储 2<sup>32</sup> - 1 键值对（40多亿）**）<strong>、**</strong>\n\nRedis Hash 类型底层有两种编码格式：ziplist、hashtable，当哈希对象同时满足以下两种条件时，对象使用 `ziplist` 编码；不能满足则使用 `hashtable` 编码。\n\n- 哈希对象保存的所有键值对的键和值的字符串长度都小于 **64** 字节；\n- 哈希对象保存的键值对数量小于 **512** 个。\n\n这里是可以由使用者自定义进行控制的，redis提供了这么几个参数：\n\nhash-max-ziplist-value 64 // ziplist中最大能存放的值长度\n\nhash-max-ziplist-entries 512 // ziplist中最多能存放的\n\n**List（列表）<strong>、**</strong>\n\n**Set（集合）、**\n\n**Zset（有序集合）**\n\n随着 Redis 版本的更新，后面又支持了四种数据类型： **BitMap（2.2 版新增）、HyperLogLog（2.8 版新增）、GEO（3.2 版新增）、Stream（5.0 版新增）**。\n\n参考：[16个 Redis 常见使用场景](https://z.itpub.net/article/detail/57CC0434AC20414A5F15466616575A62)\n\n## 1.string（字符串）\n\n### 1.session\n\n在多个应用之间共享数据\n\n### 2.cache\n\n热点数据缓存\n\n### 3.分布式锁\n\n只有redis的key不存在的时候，才能获得这个锁\n\n## 2.string（数值类型）\n\n### 1.counter\n\nint类型，incr方法，例如：文章的阅读量、微博点赞数、允许一定的延迟，先写入Redis再定时同步到数据库\n\n### 2.限流器\n\nint类型，incr方法，以访问者的ip和其他信息作为key，访问一次增加一次计数，超过次数则返回false\n\n### 3.全局ID生成器\n\n场景：用于保证分库分表之后主键的全局唯一性，参考：[10丨发号器：如何保证分库分表后 ID 的全局唯一性？](https://zq99299.github.io/note-architect/hc/02/04.html#_10%E4%B8%A8%E5%8F%91%E5%8F%B7%E5%99%A8-%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%90%8E-id-%E7%9A%84%E5%85%A8%E5%B1%80%E5%94%AF%E4%B8%80%E6%80%A7)\n\n## 3.hash\n\n### 1.购物车\n\n选择 hash 的话，每个用户的购物车对应一个 hash 对象，field 存的是`skuID`，value 存的是购物车`单种sku总量`。hash 的底层编码是 `ziplist` 和 `hashtable`。\n\n除了hash，其它的数据类型，都需要 **get 出来计算后 set 回去**，有的还需保存`加入购物车时间` 来排序。用来做购物车还是可以的，就是有点小麻烦，也没有充分利用 `Redis` 的数据类型（充分利用的话性能有很大的优势）；而 hash 就不一样了，我们满足它以 `ziplist`编码的条件，它就是**`有序`**的了 ，不需要额外计算，可以直接使用 `Redis` 命令来完成对购物车的维护，性能上无疑达到了**最优**。\n\n参考：[SpringBoot2 | 第二十九篇：Redis 实现购物车](https://ynfatal.github.io/2019/08/17/SpringBoot2/SpringBoot2%E7%AC%AC%E4%BA%8C%E5%8D%81%E4%B9%9D%E7%AF%87Redis%E5%AE%9E%E7%8E%B0%E8%B4%AD%E7%89%A9%E8%BD%A6/)\n\n## 4.bitmap\n\n### **1.在线用户统计，留存用户统计**\n\n因为bit非常节省空间（1 MB=8388608 bit），可以用来做大数据量的统计。\n\n`Redis`的位图就是一个由二进制位组成的数组， 通过将数组中的每个二进制位与用户 ID 进行一一对应， 我们可以使用位图去记录每个用户是否在线。\n\n参考：[使用redis中的位图（bitmap）统计在线人数](https://zhuanlan.zhihu.com/p/409386645)\n\n## 5.list\n\n### 1.消息队列\n\n使用LPUSH指令来进行入列操作，使用RPOP指令来进行出列操作\n\n参考：[Redis系列14：使用List实现消息队列](https://www.cnblogs.com/wzh2010/p/17205390.html)\n\n## 6.zset\n\n### 1.排行榜\n\n参考：[redis zset实现排行榜](https://duyanghao.github.io/redis-rank/)\n\n### **2.最近浏览功能**\n\n使用redis的list和zset都可以实现\n\nlist实现参考：[使用 Redis 缓存来实现用户最近浏览的商品列表 ](https://www.cnblogs.com/keepsilence233/p/13668090.html)\n\nzset实现参考：[如何实现类似知乎的功能查看最近1000条记录呢?](https://zhuanlan.zhihu.com/p/258317820)\n\n参考：[Redis经典案例场景](https://zhuanlan.zhihu.com/p/146082021)\n","tags":["Redis"]},{"title":"Spring MVC学习笔记——JSR303介绍及最佳实践","url":"/Spring MVC学习笔记——JSR303介绍及最佳实践.html","content":"JSR 303 &ndash; Bean Validation 是一个数据验证的规范，2009 年 11 月确定最终方案。2009 年 12 月 Java EE 6 发布，Bean Validation 作为一个重要特性被包含其中。本文将对 Bean Validation 的主要功能进行介绍，并通过一些示例来演示如何在 Java 开发过程正确的使用 Bean Validation。\n\n<!--more-->\n&nbsp;\n\n<a>关于 Bean Validation</a>\n\n在任何时候，当你要处理一个应用程序的业务逻辑，数据校验是你必须要考虑和面对的事情。应用程序必须通过某种手段来确保输入进来的数据从语 义上来讲是正确的。在通常的情况下，应用程序是分层的，不同的层由不同的开发人员来完成。很多时候同样的数据验证逻辑会出现在不同的层，这样就会导致代码 冗余和一些管理的问题，比如说语义的一致性等。为了避免这样的情况发生，最好是将验证逻辑与相应的域模型进行绑定。\n\nBean Validation 为 JavaBean 验证定义了相应的元数据模型和 API。缺省的元数据是 Java Annotations，通过使用 XML 可以对原有的元数据信息进行覆盖和扩展。在应用程序中，通过使用 Bean Validation 或是你自己定义的 constraint，例如 `@NotNull`, `@Max`, `@ZipCode`， 就可以确保数据模型（JavaBean）的正确性。constraint 可以附加到字段，getter 方法，类或者接口上面。对于一些特定的需求，用户可以很容易的开发定制化的 constraint。Bean Validation 是一个运行时的数据验证框架，在验证之后验证的错误信息会被马上返回。\n\n下载 JSR 303 &ndash; Bean Validation 规范 [http://jcp.org/en/jsr/detail?id=303](http://jcp.org/en/jsr/detail?id=303)\n\nHibernate Validator 是 Bean Validation 的参考实现 . Hibernate Validator 提供了 JSR 303 规范中所有内置 constraint 的实现，除此之外还有一些附加的 constraint。如果想了解更多有关 Hibernate Validator 的信息，请查看 [http://www.hibernate.org/subprojects/validator.html](http://www.hibernate.org/subprojects/validator.html)\n\n<a>Bean Validation 中的 constraint</a>\n\n<br /><a>**表 1. Bean Validation 中内置的 constraint**</a>\n\n&nbsp;\n| **Constraint** | **详细信息**    \n|`@Null`|被注释的元素必须为 `null`   \n|`@NotNull`|被注释的元素必须不为 `null`   \n|`@AssertTrue`|被注释的元素必须为 `true`   \n|`@AssertFalse`|被注释的元素必须为 `false`   \n|`@Min(value)`|被注释的元素必须是一个数字，其值必须大于等于指定的最小值   \n|`@Max(value)`|被注释的元素必须是一个数字，其值必须小于等于指定的最大值   \n|`@DecimalMin(value)`|被注释的元素必须是一个数字，其值必须大于等于指定的最小值   \n|`@DecimalMax(value)`|被注释的元素必须是一个数字，其值必须小于等于指定的最大值   \n|`@Size(max, min)`|被注释的元素的大小必须在指定的范围内   \n|`@Digits (integer, fraction)`|被注释的元素必须是一个数字，其值必须在可接受的范围内   \n|`@Past`|被注释的元素必须是一个过去的日期   \n|`@Future`|被注释的元素必须是一个将来的日期   \n|`@Pattern(value)`|被注释的元素必须符合指定的正则表达式   \n\n<br /> <br /><a>**表 2. Hibernate Validator 附加的 constraint**</a>\n| **Constraint** | **详细信息**    \n|`@Email`|被注释的元素必须是电子邮箱地址   \n|`@Length`|被注释的字符串的大小必须在指定的范围内   \n|`@NotEmpty`|被注释的字符串的必须非空   \n|`@Range`|被注释的元素必须在合适的范围内   \n\n&nbsp;\n\n一个 constraint 通常由 annotation 和相应的 constraint validator \n组成，它们是一对多的关系。也就是说可以有多个 constraint validator 对应一个 annotation。在运行时，Bean \nValidation 框架本身会根据被注释元素的类型来选择合适的 constraint validator 对数据进行验证。\n\n有些时候，在用户的应用中需要一些更复杂的 constraint。Bean Validation 提供扩展 constraint \n的机制。可以通过两种方法去实现，一种是组合现有的 constraint 来生成一个更复杂的 constraint，另外一种是开发一个全新的 \nconstraint。\n\n<a>创建一个包含验证逻辑的简单应用（基于 JSP）</a>\n\n在本文中，通过创建一个虚构的订单管理系统（基于 JSP 的 web 应用）来演示如何在 Java 开发过程中应用 Bean Validation。该简化的系统可以让用户创建和检索订单。\n\n<a>系统设计和运用的技术</a>\n\n<br />[<img src=\"/images/image003.jpg\" alt=\"图 1. 系统架构\" width=\"554\" height=\"241\" style=\"cursor: pointer;\" />](/images/image003.jpg)\n\n图 1 是报表管理系统的结构图，是典型的 MVC（Model-View-Controller）应用。Controller \n负责接收和处理请求，Servlet 扮演 Controller 的角色去处理请求、业务逻辑并转向合适的 JSP 页面。在 Servlet \n中对数据进行验证。JSP 扮演 View 的角色以图型化界面的方式呈现 Model 中的数据方便用户交互。Model \n就是此系统进行操作的数据模型，我们对这部分加以简化不对数据进行持久化。\n\n<a>数据模型</a>\n\n<br />[<img src=\"/images/image005.jpg\" alt=\"图 2. 数据模型\" width=\"335\" height=\"240\" style=\"cursor: pointer;\" />](/images/image005.jpg)\n\n图 2 展示的是订单管理系统的数据模型。\n\n<a>声明了 contraint 的 JavaBean</a>\n\n<br /><a>**清单 1. Order.java**</a>\n\n```\n public class Order { \n // 必须不为 null, 大小是 10 \n @NotNull \n @Size(min = 10, max = 10) \n private String orderId; \n // 必须不为空\n @NotEmpty \n private String customer; \n // 必须是一个电子信箱地址\n @Email \n private String email; \n // 必须不为空\n @NotEmpty \n private String address; \n // 必须不为 null, 必须是下面四个字符串'created', 'paid', 'shipped', 'closed'其中之一\n // @Status 是一个定制化的 contraint \n @NotNull \n @Status \n private String status; \n // 必须不为 null \n @NotNull \n private Date createDate; \n // 嵌套验证\n @Valid \n private Product product; \n\n&hellip;\n getter 和 setter \n } \n```\n\n<a>**清单 2. Product.java**</a>\n\n```\n public class Product { \n // 必须非空\n @NotEmpty \n private String productName; \n // 必须在 8000 至 10000 的范围内\n // @Price 是一个定制化的 constraint \n @Price \n private float price; \n&hellip;\n Getter 和 setter \n }\n```\n\n&nbsp;\n\n<a>**清单 3. OrderQuery.java**</a>\n\n```\n // 'to'所表示的日期必须在'from'所表示的日期之后\n // @QueryConstraint 是一个定制化的 constraint \n @QueryConstraint \n public class OrderQuery { \n private Date from; \n private Date to; \n&hellip; omitted &hellip;\n Getter and setter \n } \n\n```\n\n<a>定制化的 constraint</a>\n\n`@Price`是一个定制化的 constraint，由两个内置的 constraint 组合而成。\n\n<br /><a>**清单 4. @Price 的 annotation 部分**</a>\n\n```\n    // @Max 和 @Min 都是内置的 constraint \n    @Max(10000)\n    @Min(8000)\n    @Constraint(validatedBy = {})\n    @Documented\n    @Target( { ElementType.ANNOTATION_TYPE, ElementType.METHOD, ElementType.FIELD })\n    @Retention(RetentionPolicy.RUNTIME)\n    public @interface Price {\n        String message() default \"错误的价格\";\n        Class<?>[] groups() default {};\n        Class<? extends Payload>[] payload() default {};\n    }\n\n```\n\n`@Status`是一个新开发的 constraint.\n\n<br /><a>**清单 5. @Status 的 annotation 部分**</a>\n\n```\n    @Constraint(validatedBy = {StatusValidator.class})\n    @Documented\n    @Target( { ElementType.ANNOTATION_TYPE, ElementType.METHOD, ElementType.FIELD })\n    @Retention(RetentionPolicy.RUNTIME)\n    public @interface Status {\n        String message() default \"不正确的状态 , 应该是 'created', 'paid', shipped', closed'其中之一\";\n        Class<?>[] groups() default {};\n        Class<? extends Payload>[] payload() default {};\n    }\n\n```\n\n&nbsp;\n\n <br /><a>**清单 6. @Status 的 constraint validator 部分**</a>\n\n```\npublic class StatusValidator implements ConstraintValidator<Status, String>{\n    private final String[] ALL_STATUS = {\"created\", \"paid\", \"shipped\", \"closed\"};\n    public void initialize(Status status) {\n    }\n    public boolean isValid(String value, ConstraintValidatorContext context) {\n        if(Arrays.asList(ALL_STATUS).contains(value))\n            return true;\n        return false;\n    }\n}\n\n```\n\n&nbsp;\n\n<a>Bean Validation API 使用示例</a>\n\n<a>创建订单</a>\n\n用户在创建一条订单记录时，需要填写以下信息：订单编号，客户，电子信箱，地址，状态，产品名称，产品价格\n\n<br />[<img src=\"/images/image007.jpg\" alt=\"图 3. 创建订单\" width=\"485\" height=\"366\" style=\"cursor: pointer;\" />](/images/image007.jpg)\n\n对这些信息的校验，使用 Bean Validation API\n\n<br /><a>**清单 7. 代码片段**</a>\n\n```\n    protected void doPost(HttpServletRequest req, HttpServletResponse resp)\n            throws ServletException, IOException {\n        HttpSession session = req.getSession();\n        // 从 request 中获取输入信息\n        String orderId = (String) req.getParameter(\"orderId\");\n        String customer = (String) req.getParameter(\"customer\");\n        String email = (String) req.getParameter(\"email\");\n        String address = (String) req.getParameter(\"address\");\n        String status = (String) req.getParameter(\"status\");\n        String productName = (String) req.getParameter(\"productName\");\n        String productPrice = (String) req.getParameter(\"productPrice\");\n        // 将 Bean 放入 session 中\n        Order order = new Order();\n        order.setOrderId(orderId);\n        order.setCustomer(customer);\n        order.setEmail(email);\n        order.setAddress(address);\n        order.setStatus(status);\n        order.setCreateDate(new Date());\n        Product product = new Product();\n        product.setName(productName);\n        if(productPrice != null &amp;&amp; productPrice.length() > 0)\n            product.setPrice(Float.valueOf(productPrice));\n        order.setProduct(product);\n        session.setAttribute(\"order\", order);\n        ValidatorFactory factory = Validation.buildDefaultValidatorFactory();\n        Validator validator = factory.getValidator();\n        Set<ConstraintViolation<Order>> violations = validator.validate(order);\n        if(violations.size() == 0) {\n            session.setAttribute(\"order\", null);\n            session.setAttribute(\"errorMsg\", null);\n            resp.sendRedirect(\"creatSuccessful.jsp\");\n        } else {\n            StringBuffer buf = new StringBuffer();\n            ResourceBundle bundle = ResourceBundle.getBundle(\"messages\");\n            for(ConstraintViolation<Order> violation: violations) {\n                buf.append(\"-\" + bundle.getString(violation.getPropertyPath().toString()));\n                buf.append(violation.getMessage() + \"<BR>\\n\");\n            }\n            session.setAttribute(\"errorMsg\", buf.toString());\n            resp.sendRedirect(\"createOrder.jsp\");\n        }\n    }\n\n```\n\n&nbsp;\n\n&nbsp;\n\n如果用户不填写任何信息提交订单，相应的错误信息将会显示在页面上\n\n<br />[<img src=\"/images/image009.jpg\" alt=\"图 4. 验证后返回错误信息\" width=\"524\" height=\"570\" style=\"cursor: pointer;\" />](/images/image009.jpg)\n\n其实在整个程序的任何地方都可以调用 JSR 303 API 去对数据进行校验，然后将校验后的结果返回。\n\n<br /><a>**清单 8. 调用 JSR 303 API 进行校验**</a>\n\n```\n Order order = new Order(); \n&hellip;\n ValidatorFactory factory = Validation.buildDefaultValidatorFactory(); \n Validator validator = factory.getValidator(); \n Set<ConstraintViolation<Order>> violations = validator.validate(order); \n\n```\n\n&nbsp;\n\n<a>结束语</a>\n\nJSR 303 的发布使得在数据自动绑定和验证变得简单，使开发人员在定义数据模型时不必考虑实现框架的限制。当然 Bean Validation 还只是提供了一些最基本的 constraint，在实际的开发过程中，用户可以根据自己的需要组合或开发出更加复杂的 constraint\n","tags":["SpringMVC"]},{"title":"Spring MVC学习笔记——用户增删该查和服务器端验证","url":"/Spring MVC学习笔记——用户增删该查和服务器端验证.html","content":"**建立一个动态web项目，起名为SpringMVC_crud**\n\n**导包，其中包括jstl的一些包等**\n\n<!--more-->\n&nbsp;\n\n**1.先写一个User.java，是用户类**\n\n**　　<img src=\"/images/517519-20170101191209101-38674362.png\" alt=\"\" />**\n\n&nbsp;\n\n**　　文件User.java文件**\n\n```\npackage org.common.model;\n\nimport javax.validation.constraints.Size;\n\nimport org.hibernate.validator.constraints.Email;\nimport org.hibernate.validator.constraints.NotEmpty;\n\npublic class User {\n\tprivate String username;\n\tprivate String password;\n\tprivate String email;\n\tprivate String nickname;\n\t\n\tpublic User() {\n\t\tsuper();\n\t}\n\t\n\tpublic User(String username, String password, String email, String nickname) {\n\t\tsuper();\n\t\tthis.username = username;\n\t\tthis.password = password;\n\t\tthis.email = email;\n\t\tthis.nickname = nickname;\n\t}\n\t\n\t@NotEmpty(message=\"用户名不能为空\")\n\tpublic String getUsername() {\n\t\treturn username;\n\t}\n\n\tpublic void setUsername(String username) {\n\t\tthis.username = username;\n\t}\n\n\t@Size(min=1,max=10,message=\"密码长度应该在1和10之间\")\n\tpublic String getPassword() {\n\t\treturn password;\n\t}\n\n\tpublic void setPassword(String password) {\n\t\tthis.password = password;\n\t}\n\n\t@Email(message=\"邮箱的格式不正确\")\n\tpublic String getEmail() {\n\t\treturn email;\n\t}\n\n\tpublic void setEmail(String email) {\n\t\tthis.email = email;\n\t}\n\n\tpublic String getNickname() {\n\t\treturn nickname;\n\t}\n\n\tpublic void setNickname(String nickname) {\n\t\tthis.nickname = nickname;\n\t}\n\t\n\t\n}\n\n```\n\n&nbsp;\n\n**2.再创建Controller控制器**\n\n**　　<img src=\"/images/517519-20170101191431507-1981721314.png\" alt=\"\" />**\n\n**　　文件UserController.java**\n\n**add的时候有两种，第一种**\n\n```\n\t//链接到add页面时候是GET请求，会访问这段代码\n\t@RequestMapping(value=\"/add\",method=RequestMethod.GET)\n\t//把一个对象放到了@ModelAttribute中，Model的Key就是user\n\tpublic String add(Model model){\n\t\t//开启modeDrive\n\t\tmodel.addAttribute(new User());\n\t\treturn \"user/add\";\n\t}\n\n```\n\n**&nbsp;第二种**\n\n```\n\t//链接到add页面时候是GET请求，会访问这段代码\n\t@RequestMapping(value=\"/add\",method=RequestMethod.GET)\n\t//把一个对象放到了@ModelAttribute中，Model的Key就是user\n\tpublic String add(@ModelAttribute(\"user\") User user){\n\t\t//开启modeDrive\n\t\t//model.addAttribute(new User());\n\t\treturn \"user/add\";\n\t}\n\n```\n\n&nbsp;\n\n```\npackage org.common.controller;\n\nimport java.util.HashMap;\nimport java.util.Map;\n\nimport org.common.model.User;\nimport org.springframework.stereotype.Controller;\nimport org.springframework.ui.Model;\nimport org.springframework.validation.BindingResult;\nimport org.springframework.validation.annotation.Validated;\nimport org.springframework.web.bind.annotation.ModelAttribute;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RequestMethod;\n\n@Controller\n@RequestMapping(\"/user\")\npublic class UserController {\n\tprivate Map<String,User> users = new HashMap<String,User>();　　//创建一个users对象\n\t\n\tpublic UserController() {\n\t\tusers.put(\"ldh\",new User(\"ldh\",\"刘德华\",\"123\",\"123123\"));\n\t\tusers.put(\"zxy\",new User(\"zxy\",\"张学友\",\"123\",\"123123\"));\n\t\tusers.put(\"gfc\",new User(\"gfc\",\"郭富城\",\"123\",\"123123\"));\n\t\tusers.put(\"lm\",new User(\"lm\",\"黎明\",\"123\",\"123123\"));\n\t}\n\t\n\t@RequestMapping(value=\"/users\",method=RequestMethod.GET)\n\tpublic String list(Model model){\n\t\tmodel.addAttribute(\"users\", users);\n\t\treturn \"user/list\";\n\t}\n\t\n\t//链接到add页面时候是GET请求，会访问这段代码\n\t@RequestMapping(value=\"/add\",method=RequestMethod.GET)\n\t//把一个对象放到了@ModelAttribute中，Model的Key就是user\n\tpublic String add(@ModelAttribute(\"user\") User user){\n\t\t//开启modeDrive\n\t\t//model.addAttribute(new User());\n\t\treturn \"user/add\";\n\t}\n\t \n\t//在具体添加用户的时候，是POST请求，就访问以下代码\n\t@RequestMapping(value=\"/add\",method=RequestMethod.POST)\n\tpublic String add(@Validated User user,BindingResult br){//一定要紧跟@Validated之后写验证结果类\n\t\tif(br.hasErrors()){\n\t\t\t//如果有错误，直接跳转到add视图\n\t\t\treturn \"user/add\";\n\t\t}\n\t\tusers.put(user.getUsername(),user);\n\t\treturn \"redirect:/user/users\";\n\t}\n\t\n//\t//链接到add页面时候是GET请求，会访问这段代码\n//\t@RequestMapping(value=\"/add\",method=RequestMethod.GET)\n//\tpublic String add(Model model){\n//\t\t//开启modeDrive\n//\t\tmodel.addAttribute(new User());\n//\t\treturn \"user/add\";\n//\t}\n//\t \n//\t//在具体添加用户的时候，是POST请求，就访问以下代码\n//\t@RequestMapping(value=\"/add\",method=RequestMethod.POST)\n//\tpublic String add(User user){\n//\t\tusers.put(user.getUsername(),user);\n//\t\treturn \"redirect:/user/users\";\n//\t}\n\t\n}\n\n```\n\n&nbsp;\n\n**&nbsp;jsp/user/add.jsp**\n\n```\nhttp://localhost:8080/springmvc_crud/user/add    \n\n```\n\n```\n<%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\"\n    pageEncoding=\"UTF-8\"%>\n<%@ taglib prefix=\"sf\" uri=\"http://www.springframework.org/tags/form\" %>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<html>\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n<title>Insert title here</title>\n</head>\n<body>\n\t<sf:form method=\"post\" modelAttribute=\"user\">\n\t\t<table width=\"700\" align=\"center\" border=\"1\">\n\t\t\t<tr>\n\t\t\t\t|用户名:|<sf:input path=\"username\"/><sf:errors path=\"username\"/>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t|用户密码:|<sf:password path=\"password\"/><sf:errors path=\"password\"/>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t|用户昵称:|<sf:input path=\"nickname\"/>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t|用户邮箱:|<sf:input path=\"email\"/><sf:errors path=\"email\"/>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td colspan=\"2\">\n\t\t\t\t<input type=\"submit\" value=\"用户添加\"/>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t</table>\n\t</sf:form>\n\t\n</body>\n</html>\n\n```\n\n&nbsp;<img src=\"/images/517519-20170101202417992-723111391.png\" alt=\"\" width=\"530\" height=\"215\" />\n\n&nbsp;\n\n**&nbsp;jsp/user/list.jsp**\n\n```\nhttp://localhost:8080/springmvc_crud/user/users\n\n```\n\n```\n<%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\"\n    pageEncoding=\"UTF-8\"%>\n<%@taglib prefix=\"c\" uri=\"http://java.sun.com/jsp/jstl/core\" %>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<html>\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n<title>用户列表</title>\n</head>\n<body>\n\t<c:forEach items=\"${users}\" var=\"um\">\n\t\t${um.value.username}\n\t\t----${um.value.nickname}\n\t\t----${um.value.password}\n\t\t----${um.value.email}<br/>\n\t</c:forEach>\n</body>\n</html>\n\n```\n\n**&nbsp;<img src=\"/images/517519-20170101202506976-1273548244.png\" alt=\"\" width=\"427\" height=\"225\" />**\n\n&nbsp;\n\n**user-servlet.xml文件**\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n    xmlns:context=\"http://www.springframework.org/schema/context\"\n    xmlns:mvc=\"http://www.springframework.org/schema/mvc\"\n    xsi:schemaLocation=\"http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-3.1.xsd\n        http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.1.xsd\">\n    \n    <!-- 使用defaultAnnotationHandleMapping -->\n    <context:component-scan base-package=\"org.common.controller\"></context:component-scan>\n\t<mvc:annotation-driven></mvc:annotation-driven>\n    <!-- InternalResourceViewResolver视图的映射关系，还有其他很多视图 -->\n    <bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\">\n        <property name=\"prefix\" value=\"/WEB-INF/jsp/\"></property>\n        <property name=\"suffix\" value=\".jsp\"></property>\n    </bean>\n    \n    \n</beans>\n\n```\n\n&nbsp;\n\n**&nbsp;web.xml文件**\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<web-app xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.jcp.org/xml/ns/javaee\" xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd\" id=\"WebApp_ID\" version=\"3.1\">\n\t<!-- 配置DispatchServlet，截获特定的URL请求 -->\n    <!-- 默认自动加载/WEB-INF/simpleSpringMVC-servlet.xml -->\n    <!-- （即<servlet-name>-servlet.xml）的Spring配置文件，启动web层的Spring容器 -->\n  <servlet>\n        <servlet-name>user</servlet-name>\n        <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>\n        <load-on-startup>1</load-on-startup>\n    </servlet>\n    <servlet-mapping>\n        <servlet-name>user</servlet-name>\n        <url-pattern>/</url-pattern>\n    </servlet-mapping>\n    \n    <!-- 设置字符编码 -->\n    <filter>\n\t\t<filter-name>CharacterFilter</filter-name>\n\t\t<filter-class>org.springframework.web.filter.CharacterEncodingFilter</filter-class>\n\t\t<init-param>\n\t\t\t<param-name>encoding</param-name>\n\t\t\t<param-value>UTF-8</param-value>\n\t\t</init-param>\n\t</filter>\n\t<filter-mapping>\n\t\t<filter-name>CharacterFilter</filter-name>\n\t\t<url-pattern>/*</url-pattern>\n\t</filter-mapping>\n    \n</web-app>\n\n```\n\n&nbsp;\n","tags":["SpringMVC"]},{"title":"Flink学习笔记——Flink Mongo CDC","url":"/Flink学习笔记——Flink Mongo CDC.html","content":"## 1.Flink CDC介绍\n\nFlink CDC提供了一系列connector，用于从其他数据源获取变更数据（change data capture）\n\n官方文档\n\n```\nhttps://ververica.github.io/flink-cdc-connectors/release-2.3/content/about.html\n\n```\n\n官方github\n\n```\nhttps://github.com/ververica/flink-cdc-connectors\n\n```\n\n各种数据源使用案例，参考：\n\n[基于 AWS S3、EMR Flink、Presto 和 Hudi 的实时数据湖仓 &ndash; 使用 EMR 迁移 CDH](https://aws.amazon.com/cn/blogs/china/use-emr-to-migrate-cdh-based-on-aws-s3-emr-flink-presto-and-hudi-real-time-data-lake/)\n\n[Flink CDC关于source和sink全调研及实践](http://junyao.tech/posts/748d18ad.html)\n\n## 2.Flink Mongo CDC\n\n官方文档和原理，参考：[Flink CDC MongoDB Connector 的实现原理和使用实践](https://www.51cto.com/article/712109.html)\n\n```\nhttps://ververica.github.io/flink-cdc-connectors/release-2.3/content/connectors/mongodb-cdc.html#\n\n```\n\n从Flink CDC 2.1.0开始，支持了Mongo CDC的connector，当前最新的版本是2.3.0\n\n相比2.2.0，2.3.0版本主要新增了Incremental Snapshot的特性（在snapshot阶段支持断点续传和并发同步），且要求至少mongo 4.0版本，这样就可以解决大表在第一次snapshot同步的时候，一旦失败就得重头再开始同步的问题，以及之前只能单并发同步很慢的问题\n\n具体新增的特性可以查看release\n\n```\nhttps://github.com/ververica/flink-cdc-connectors/releases\n\n```\n\n下面例子中的前flink mongo cdc的sink为hudi sink\n\n### 1.snapshot阶段支持checkpoint（断点续传）\n\n这个具体值的是如果在snapshot阶段，某次checkpoint failed，或者停掉flink job，然后从最新的checkpoint重新flink job，都是能实现断点续传的，使用的话只需要添加如下配置\n\n```\nscan.incremental.snapshot.enabled = true\n```\n\n<img src=\"/images/517519-20230510104736582-758175905.png\" width=\"1200\" height=\"500\" loading=\"lazy\" />\n\n### 2.snapshot阶段支持并发同步\n\n```\n/usr/lib/flink/bin/flink run -t yarn-session -p 10 ...　　\n```\n\n<img src=\"/images/517519-20230510105152426-2035315116.png\" width=\"1400\" height=\"559\" loading=\"lazy\" />\n\nbucket_write阶段的并发度由<!--more-->\n&nbsp;write.tasks 参数决定\n\ncompact_task阶段的并发度由&nbsp;compaction.tasks 参数决定\n\n其他参数参考：[flink-quick-start-guide](https://hudi.apache.org/cn/docs/0.9.0/flink-quick-start-guide#%E5%B9%B6%E8%A1%8C%E5%BA%A6-1)&nbsp;和&nbsp;[数据湖架构Hudi（五）Hudi集成Flink案例详解](https://blog.csdn.net/qq_44665283/article/details/129371508)\n\n```\nDeployment took more than 60 seconds. Please check if the requested resources are available in the YARN cluster\n```\n\n### 3.mongo snapshot的split策略\n\n作者介绍其使用了3种split策略，分别是&nbsp;SampleBucketSplitStrategy，SplitVector split和&nbsp;MongoDBChunkSplitter，参考：[Flink CDC MongoDB Connector 的实现原理和使用实践](https://juejin.cn/post/7111523733734424612)\n\n为了在snapshot阶段支持并发，所以在snapshot阶段同步的时候，会使用 SnapshotSplitAssigner 对Mongo的collection进行切分\n\n```\nhttps://github.com/ververica/flink-cdc-connectors/blob/release-2.3.0/flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/assigner/SnapshotSplitAssigner.java\n```\n\n<img src=\"/images/517519-20230612104048523-766630352.png\" width=\"500\" height=\"489\" loading=\"lazy\" />\n\ndialect实现的类为MongoDBDialect\n\n```\nhttps://github.com/ververica/flink-cdc-connectors/blob/release-2.3.0/flink-connector-mongodb-cdc/src/main/java/com/ververica/cdc/connectors/mongodb/source/dialect/MongoDBDialect.java\n```\n\n<img src=\"/images/517519-20230612104350331-818018650.png\" width=\"500\" height=\"92\" loading=\"lazy\" />\n\n在&nbsp;MongoDBChunkSplitter 类中，对于sharded collection，会使用&nbsp;ShardedSplitStrategy 策略来对mongo进行切分，否则会使用&nbsp;SplitVectorSplitStrategy 策略进行切分\n\n```\nhttps://github.com/ververica/flink-cdc-connectors/blob/release-2.3.0/flink-connector-mongodb-cdc/src/main/java/com/ververica/cdc/connectors/mongodb/source/assigners/splitters/MongoDBChunkSplitter.java\n```\n\n<img src=\"/images/517519-20230612104651336-1315496035.png\" width=\"600\" height=\"416\" loading=\"lazy\" />\n\n如果是sharded collection，当验证sharded collection失败或者没有config.collection或者config.chunks权限的时候，则会使用&nbsp;SampleBucketSplitStrategy 策略进行切分\n\n<img src=\"/images/517519-20230612105149345-1488955550.png\" width=\"800\" height=\"684\" loading=\"lazy\" />\n\n如果不是sharded collection，当没有 splitVector 权限或者无法切分collection的话，则会使用&nbsp;SampleBucketSplitStrategy 策略进行切分\n\n<img src=\"/images/517519-20230612105234270-221963043.png\" width=\"700\" height=\"705\" loading=\"lazy\" />&nbsp;\n\n### 4.Flink Mongo CDC遇到的报错处理方法\n\n#### 1.Caused by: com.mongodb.MongoQueryException: Query failed with error code 280 and error message 'cannot resume stream; the resume token was not java.lang.RuntimeException: One or more fetchers have encountered exception\n\n```\n\tat org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)\n\tat org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)\n\tat org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)\n\tat org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:385)\n\tat org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)\n\tat org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)\n\tat org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)\n\tat org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)\n\tat org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)\n\tat org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)\n\tat org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)\n\tat org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)\n\tat org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)\n\tat org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.RuntimeException: SplitFetcher thread 6 received unexpected exception while polling the records\n\tat org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150)\n\tat org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.flink.util.FlinkRuntimeException: Read split SnapshotSplit{tableId=xxx.xxx, splitId='xxxx.xx:661', splitKeyType=[`_id` INT], splitStart=[{\"_id\": 1}, {\"_id\": \"0MowwELT\"}], splitEnd=[{\"_id\": 1}, {\"_id\": \"0MoxTSHT\"}], highWatermark=null} error due to Query failed with error code 280 and error message 'cannot resume stream; the resume token was not found. {_data: \"82646B6EBA00000C5D2B022C0100296E5A1004B807779924DA402AB13486B3F67B6102463C5F6964003C306D483170686A75000004\"}' on server xxxx:27017.\n\tat com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceScanFetcher.checkReadException(IncrementalSourceScanFetcher.java:181)\n\tat com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceScanFetcher.pollSplitRecords(IncrementalSourceScanFetcher.java:128)\n\tat com.ververica.cdc.connectors.base.source.reader.IncrementalSourceSplitReader.fetch(IncrementalSourceSplitReader.java:73)\n\tat org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)\n\tat org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)\n\t... 6 more\nCaused by: com.mongodb.MongoQueryException: Query failed with error code 280 and error message 'cannot resume stream; the resume token was not found. {_data: \"82646B6EBA00000C5D2B022C0100296E5A1004B807779924DA402AB13486B3F67B6102463C5F6964003C306D483170686A75000004\"}' on server cxxxx:27017\n\tat com.mongodb.internal.operation.QueryHelper.translateCommandException(QueryHelper.java:29)\n\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:282)\n\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:512)\n\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:270)\n\tat com.mongodb.internal.operation.QueryBatchCursor.tryHasNext(QueryBatchCursor.java:223)\n\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$tryNext$0(QueryBatchCursor.java:206)\n\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:397)\n\tat com.mongodb.internal.operation.QueryBatchCursor.tryNext(QueryBatchCursor.java:205)\n\tat com.mongodb.internal.operation.ChangeStreamBatchCursor$3.apply(ChangeStreamBatchCursor.java:102)\n\tat com.mongodb.internal.operation.ChangeStreamBatchCursor$3.apply(ChangeStreamBatchCursor.java:98)\n\tat com.mongodb.internal.operation.ChangeStreamBatchCursor.resumeableOperation(ChangeStreamBatchCursor.java:195)\n\tat com.mongodb.internal.operation.ChangeStreamBatchCursor.tryNext(ChangeStreamBatchCursor.java:98)\n\tat com.mongodb.client.internal.MongoChangeStreamCursorImpl.tryNext(MongoChangeStreamCursorImpl.java:78)\n\tat com.ververica.cdc.connectors.mongodb.source.reader.fetch.MongoDBStreamFetchTask.execute(MongoDBStreamFetchTask.java:116)\n\tat com.ververica.cdc.connectors.mongodb.source.reader.fetch.MongoDBScanFetchTask.execute(MongoDBScanFetchTask.java:183)\n\tat com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceScanFetcher.lambda$submitTask$0(IncrementalSourceScanFetcher.java:94)\n\t... 5 more\n\n```\n\n这是由于flink mongo cdc 2.3.0 及其以下版本的bug导致，可以考虑自行打包2.4 snapshot分支来进行fix，参考作者官方的fix：https://github.com/ververica/flink-cdc-connectors/pull/1938\n\nresume token&nbsp;用来描述一个订阅点，本质上是 oplog 信息的一个封装，包含 clusterTime、uuid、documentKey等信息，当订阅 API 带上 resume token 时，MongoDB Server 会将 token 转换为对应的信息，并定位到 oplog 起点继续订阅操作。\n\n参考：[MongoDB 4.2 内核解析 - Change Stream](https://developer.aliyun.com/article/741514)\n\n#### 2.java.lang.NoSuchMethodError: org.apache.commons.cli.Option.builder(Ljava/lang/String;)Lorg/apache/commons/cli/Option$Builder\n\n```\njava.lang.NoSuchMethodError: org.apache.commons.cli.Option.builder(Ljava/lang/String;)Lorg/apache/commons/cli/Option$Builder;\n\tat org.apache.flink.yarn.cli.FlinkYarnSessionCli.<init>(FlinkYarnSessionCli.java:197)\n\tat org.apache.flink.yarn.cli.FlinkYarnSessionCli.<init>(FlinkYarnSessionCli.java:173)\n\tat org.apache.flink.yarn.cli.FlinkYarnSessionCli.main(FlinkYarnSessionCli.java:836)\n\n```\n\n这是由于commons-cli包版本过低导致的，从1.2.升级到1.5.0可以解决这个问题\n\n#### 3.Caused by: com.mongodb.MongoCursorNotFoundException: Query failed with error code -5 and error message 'Cursor 7652758736186712320 not found on server xxxx:27017' on server xxxx:27017\n\n```\njava.lang.RuntimeException: One or more fetchers have encountered exception\n\tat org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)\n\tat org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)\n\tat org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)\n\tat org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:385)\n\tat org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)\n\tat org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)\n\tat org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)\n\tat org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)\n\tat org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)\n\tat org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)\n\tat org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)\n\tat org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)\n\tat org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)\n\tat org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records\n\tat org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150)\n\tat org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.flink.util.FlinkRuntimeException: Read split SnapshotSplit{tableId=xxxx.xxxx, splitId='xxxx.xxxx:134146', splitKeyType=[`_id` INT], splitStart=[{\"_id\": 1}, {\"_id\": \"0mWXDXGK\"}], splitEnd=[{\"_id\": 1}, {\"_id\": \"0mWXkf7v\"}], highWatermark=null} error due to Query failed with error code -5 and error message 'Cursor 7652758736186712320 not found on server xxxx:27017' on server xxxx:27017.\n\tat com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceScanFetcher.checkReadException(IncrementalSourceScanFetcher.java:181)\n\tat com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceScanFetcher.pollSplitRecords(IncrementalSourceScanFetcher.java:128)\n\tat com.ververica.cdc.connectors.base.source.reader.IncrementalSourceSplitReader.fetch(IncrementalSourceSplitReader.java:73)\n\tat org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)\n\tat org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)\n\t... 6 more\nCaused by: com.mongodb.MongoCursorNotFoundException: Query failed with error code -5 and error message 'Cursor 7652758736186712320 not found on server xxxx:27017' on server xxxx:27017\n\tat com.mongodb.internal.operation.QueryHelper.translateCommandException(QueryHelper.java:27)\n\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:282)\n\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:512)\n\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:270)\n\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:156)\n\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:397)\n\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:143)\n\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n\tat com.ververica.cdc.connectors.mongodb.source.reader.fetch.MongoDBScanFetchTask.execute(MongoDBScanFetchTask.java:120)\n\tat com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceScanFetcher.lambda$submitTask$0(IncrementalSourceScanFetcher.java:94)\n\t... 5 more\n\n```\n\nmongo的cursor可以理解成为一个指针，使用这个指针可以访问mongo的文档，上面的报错的意思就是这个找不到这个指针\n\n游标找不到的原因主要有2个：\n\n- 客户端游标超时，被服务端回收，再用游标向服务器请求数据时就会出现游标找不到的情况。\n- 在 mongo 集群环境下，可能会出现游标找不到的情况。游标由 mongo 服务器生成，在集群环境下，当使用 find() 相关函数时返回一个游标，假设此时该游标由 A 服务器生成，迭代完数据继续请求数据时，访问到了 B 服务器，但是该游标不是 B 生成的，此时就会出现游标找不到的情况。正常情况下，在 mongo 集群时，会将 mongo 地址以 ip1:port1,ip2:port2,ip3:port3 形式传给 mongo 驱动，然后驱动能够自动完成负载均衡和保持会话转发到同一台服务器，此时不会出现游标找不到的情况。但当我们自己搭建了负载均衡层，且用load balancer地址来连接时，就会出现游标找不到的情况。\n\n游标找不到（仅考虑超时情况）的解决方案：\n\n- 在服务端增大 mongo 服务器的游标超时时间。参数是 cursorTimeoutMillis，其默认是 10 min。修改后需重启 mongo 服务器。\n- 在客户端一次性获取到全部符合条件的数据。也即将batch_size设置为很大的数，但次数若真的有很多数据的话，则对系统内存要求较高，同时如果数据量过大或处理过程过慢依旧会出现游标超时的情况。所以batch_size的评估是一个技术活。\n<li>客户端设置游标永不超时:\n<ul>\n- 这种方式的缺点是如果程序意外停止或异常，该游标永远不会被释放，除非重启 mongo，否则会一直占用系统资源，属于危险操作。经过咨询DBA，一般很少对游标的数量进行监控，一般是由其引起的连锁反应如CPU/内存过高才能引起关注，一般的处理方式也就是重启mongo服务器，这样影响就比较大了。\n- 经过查询，**在mongo 3.6版本后**，客户端就算把游标设置为永不超时。服务端仍然会在闲置30分钟后将其kill掉，所以大量查询若超过30分钟的话需要手动执行下refreshsession来防止超时。但在3.6以下版本则会一直存在。[mongo文档](https://links.jianshu.com/go?to=https%3A%2F%2Fdocs.mongodb.com%2Fmanual%2Freference%2Fmethod%2Fcursor.noCursorTimeout%2F%23mongodb-method-cursor.noCursorTimeout)。\n\n参考：[springboot mongo查询游标(cursor)不存在错误&nbsp;](https://www.jianshu.com/p/c34539fb2ffa)\n\n即在mongo-cdc connection的配置中将cursor batch size的参数batch.size调小，默认值为1024，可以尝试调成100\n\n参考：[https://ververica.github.io/flink-cdc-connectors/master/content/connectors/mongodb-cdc.html#connector-options](https://ververica.github.io/flink-cdc-connectors/master/content/connectors/mongodb-cdc.html#connector-options)\n\n[mongodb游标超时报错：com.mongodb.MongoCursorNotFoundException: Query failed with error code -5的四种处理方式](https://blog.csdn.net/qq_41018861/article/details/113104156)\n\n#### 4.java.lang.ArrayIndexOutOfBoundsException: 1&nbsp;at org.apache.hudi.index.bucket.BucketIdentifier.lambda$ getHashKeysUsingIndexFields$2(BucketIdentifier.java:74)\n\n```\njava.lang.ArrayIndexOutOfBoundsException: 1\n\tat org.apache.hudi.index.bucket.BucketIdentifier.lambda$getHashKeysUsingIndexFields$2(BucketIdentifier.java:74)\n\tat java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321)\n\tat java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)\n\tat java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)\n\tat java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)\n\tat java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)\n\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)\n\tat java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)\n\tat java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\n\tat java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)\n\tat org.apache.hudi.index.bucket.BucketIdentifier.getHashKeysUsingIndexFields(BucketIdentifier.java:74)\n\tat org.apache.hudi.index.bucket.BucketIdentifier.getHashKeys(BucketIdentifier.java:63)\n\tat org.apache.hudi.index.bucket.BucketIdentifier.getHashKeys(BucketIdentifier.java:58)\n\tat org.apache.hudi.index.bucket.BucketIdentifier.getBucketId(BucketIdentifier.java:42)\n\tat org.apache.hudi.sink.partitioner.BucketIndexPartitioner.partition(BucketIndexPartitioner.java:44)\n\tat org.apache.hudi.sink.partitioner.BucketIndexPartitioner.partition(BucketIndexPartitioner.java:32)\n\tat org.apache.flink.streaming.runtime.partitioner.CustomPartitionerWrapper.selectChannel(CustomPartitionerWrapper.java:57)\n\tat org.apache.flink.streaming.runtime.partitioner.CustomPartitionerWrapper.selectChannel(CustomPartitionerWrapper.java:36)\n\tat org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:54)\n\tat org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:104)\n\tat org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:90)\n\tat org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:44)\n\tat org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)\n\tat org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)\n\tat org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:38)\n\tat org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)\n\tat org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)\n\tat org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)\n\tat org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)\n\tat org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)\n\tat org.apache.flink.table.runtime.util.StreamRecordCollector.collect(StreamRecordCollector.java:44)\n\tat org.apache.flink.table.runtime.operators.sink.ConstraintEnforcer.processElement(ConstraintEnforcer.java:247)\n\tat org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)\n\tat org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)\n\tat org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)\n\tat org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)\n\tat org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)\n\tat org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51)\n\tat org.apache.flink.table.runtime.operators.deduplicate.DeduplicateFunctionHelper.processLastRowOnChangelog(DeduplicateFunctionHelper.java:112)\n\tat org.apache.flink.table.runtime.operators.deduplicate.ProcTimeDeduplicateKeepLastRowFunction.processElement(ProcTimeDeduplicateKeepLastRowFunction.java:80)\n\tat org.apache.flink.table.runtime.operators.deduplicate.ProcTimeDeduplicateKeepLastRowFunction.processElement(ProcTimeDeduplicateKeepLastRowFunction.java:32)\n\tat org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83)\n\tat org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)\n\tat org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)\n\tat org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)\n\tat org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)\n\tat org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)\n\tat org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)\n\tat org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)\n\tat org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)\n\tat org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)\n\tat org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)\n\tat org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)\n\tat org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)\n\tat java.lang.Thread.run(Thread.java:750)\n\n```\n\n这是由于指定的hudi recorder key中有一些异常的数据导致的，比如这里指定的recorder key是_id，这个字段是个string类型的，但是会有少部分数据是json string，其中value包含的逗号会导致split后出现array越界的情况\n\n#### 5.Caused by: java.lang.NoSuchMethodError: org.apache.hudi.org.apache.avro.specific.SpecificRecordBuilderBase.<init>(Lorg/apache/hudi/org/apache/avro/Schema;Lorg/apache/hudi/org/apache/avro/specific/SpecificData;)V\n\n```\norg.apache.flink.util.FlinkException: Global failure triggered by OperatorCoordinator for 'bucket_write: hudi_test' (operator b1cd777bdf9179b3493b6420d82b014a).\n\tat org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder$LazyInitializedCoordinatorContext.failJob(OperatorCoordinatorHolder.java:556)\n\tat org.apache.hudi.sink.StreamWriteOperatorCoordinator.lambda$start$0(StreamWriteOperatorCoordinator.java:187)\n\tat org.apache.hudi.sink.utils.NonThrownExecutor.handleException(NonThrownExecutor.java:146)\n\tat org.apache.hudi.sink.utils.NonThrownExecutor.lambda$wrapAction$0(NonThrownExecutor.java:133)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.hudi.exception.HoodieException: Executor executes action [commits the instant 20230525063249024] error\n\t... 6 more\nCaused by: java.lang.NoSuchMethodError: org.apache.hudi.org.apache.avro.specific.SpecificRecordBuilderBase.<init>(Lorg/apache/hudi/org/apache/avro/Schema;Lorg/apache/hudi/org/apache/avro/specific/SpecificData;)V\n\tat org.apache.hudi.avro.model.HoodieCompactionPlan$Builder.<init>(HoodieCompactionPlan.java:226)\n\tat org.apache.hudi.avro.model.HoodieCompactionPlan$Builder.<init>(HoodieCompactionPlan.java:217)\n\tat org.apache.hudi.avro.model.HoodieCompactionPlan.newBuilder(HoodieCompactionPlan.java:184)\n\tat org.apache.hudi.table.action.compact.strategy.CompactionStrategy.generateCompactionPlan(CompactionStrategy.java:76)\n\tat org.apache.hudi.table.action.compact.HoodieCompactor.generateCompactionPlan(HoodieCompactor.java:317)\n\tat org.apache.hudi.table.action.compact.ScheduleCompactionActionExecutor.scheduleCompaction(ScheduleCompactionActionExecutor.java:123)\n\tat org.apache.hudi.table.action.compact.ScheduleCompactionActionExecutor.execute(ScheduleCompactionActionExecutor.java:93)\n\tat org.apache.hudi.table.HoodieFlinkMergeOnReadTable.scheduleCompaction(HoodieFlinkMergeOnReadTable.java:112)\n\tat org.apache.hudi.client.BaseHoodieWriteClient.scheduleTableServiceInternal(BaseHoodieWriteClient.java:1349)\n\tat org.apache.hudi.client.BaseHoodieWriteClient.scheduleTableService(BaseHoodieWriteClient.java:1326)\n\tat org.apache.hudi.client.BaseHoodieWriteClient.scheduleCompactionAtInstant(BaseHoodieWriteClient.java:1005)\n\tat org.apache.hudi.client.BaseHoodieWriteClient.scheduleCompaction(BaseHoodieWriteClient.java:996)\n\tat org.apache.hudi.util.CompactionUtil.scheduleCompaction(CompactionUtil.java:65)\n\tat org.apache.hudi.sink.StreamWriteOperatorCoordinator.lambda$notifyCheckpointComplete$2(StreamWriteOperatorCoordinator.java:246)\n\tat org.apache.hudi.sink.utils.NonThrownExecutor.lambda$wrapAction$0(NonThrownExecutor.java:130)\n\t... 3 more\n\n```\n\n这个问题网上有人讨论过，原因是flink 1.15.x版本使用的avro版本是1.8.2，而hudi-flink and hudi-flink-bundle使用的avro版本是1.10.0，上面报错中的找不到的方法需要在avro 1.10.0及以上版本中出现\n\n参考：[https://github.com/apache/hudi/issues/7259](https://github.com/apache/hudi/issues/7259)\n\n这个报错将会导致flink任务失败，而且hive表无法同步\n\n#### 6.java.lang.OutOfMemoryError: Java heap space\n\n这是问题发生的原因是flink的**heap内存不足**，可能的原因是：\n\n**1.没有使用rocksdb状态后端**\n\n默认的backend是MemoryStateBackend。默认情况下，flink的状态会保存在taskmanager的内存中，而checkpoint会保存在jobManager的内存中。\n\n参考：[Flink状态后端配置（设置State Backend）](https://blog.csdn.net/sinat_38079265/article/details/121736045)\n\n不过在使用flink streaming job中，使用rocksdb backend可能会使用一点managed memory（属于堆外内存）\n\n参考：[https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/mem_setup_tm/#managed-memory](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/mem_setup_tm/#managed-memory)\n\n官方作者也推荐使用rocksdb作为状态后端\n\n<img src=\"/images/517519-20230612110124477-344548227.png\" alt=\"\" loading=\"lazy\" />\n\n参考：[Flink CDC MongoDB Connector 的实现原理和使用实践](https://juejin.cn/post/7111523733734424612)\n\n**2.使用了默认的FLINK STATE index type**\n\n由于FLINK STATE index type是in-memory的，都有可能导致flink任务的堆内存爆掉，解决方法是使用BUCKET index type\n\n特别是当表很大的时候，第一次snapshot同步会消耗比较多的资源以及时间，这时候如果使用的是hudi sink的话建议将hudi的index.type设置成BUCKET，因为flink默认的index type是FLINK_STATE，FLINK_STATE默认是in-memory的，所以会消耗非常多的heap内存，无论是在运行过程中，还是在从checkpoint重新flink job的时候，对task manager的内存压力会很大，所以建议使用BUCKET index type替换默认的FLINK_STATE index type\n\n参考：[HUDI-0.11.0 BUCKET index on Flink 新特性试用](https://www.cnblogs.com/magic-x/articles/16114261.html)\n\n在使用BUCKET index type的时候，需要确定&nbsp;hoodie.bucket.index.num.buckets 参数，即bucket的数量，这个官方建议是3GB左右\n\n参考：[Hudi Bucket Index 在字节跳动的设计与实践](https://zhuanlan.zhihu.com/p/476967738)\n\n#### 7.Caused by: org.apache.flink.util.SerializedThrowable: java.lang.OutOfMemoryError: Direct buffer memory\n\n```\norg.apache.flink.util.SerializedThrowable: Asynchronous task checkpoint failed.\n\tat org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.handleExecutionException(AsyncCheckpointRunnable.java:320) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:155) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_372]\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_372]\n\tat java.lang.Thread.run(Thread.java:750) [?:1.8.0_372]\nCaused by: org.apache.flink.util.SerializedThrowable: Could not materialize checkpoint 114 for operator Source: mongo_cdc_test[1] -> Calc[2] (1/10)#0.\n\tat org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.handleExecutionException(AsyncCheckpointRunnable.java:298) ~[flink-dist-1.15.2.jar:1.15.2]\n\t... 4 more\nCaused by: org.apache.flink.util.SerializedThrowable: java.lang.OutOfMemoryError: Direct buffer memory\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_372]\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[?:1.8.0_372]\n\tat org.apache.flink.util.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:645) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.<init>(OperatorSnapshotFinalizer.java:60) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.finalizeNonFinishedSnapshots(AsyncCheckpointRunnable.java:191) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:124) ~[flink-dist-1.15.2.jar:1.15.2]\n\t... 3 more\nCaused by: org.apache.flink.util.SerializedThrowable: Direct buffer memory\n\tat java.nio.Bits.reserveMemory(Bits.java:695) ~[?:1.8.0_372]\n\tat java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:123) ~[?:1.8.0_372]\n\tat java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311) ~[?:1.8.0_372]\n\tat sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:247) ~[?:1.8.0_372]\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:58) ~[?:1.8.0_372]\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211) ~[?:1.8.0_372]\n\tat java.nio.channels.Channels.writeFullyImpl(Channels.java:78) ~[?:1.8.0_372]\n\tat java.nio.channels.Channels.writeFully(Channels.java:101) ~[?:1.8.0_372]\n\tat java.nio.channels.Channels.access$000(Channels.java:61) ~[?:1.8.0_372]\n\tat java.nio.channels.Channels$1.write(Channels.java:174) ~[?:1.8.0_372]\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122) ~[?:1.8.0_372]\n\tat java.security.DigestOutputStream.write(DigestOutputStream.java:145) ~[?:1.8.0_372]\n\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.write(MultipartUploadOutputStream.java:172) ~[emrfs-hadoop-assembly-2.54.0.jar:?]\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:63) ~[hadoop-common-3.3.3-amzn-1.jar:?]\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107) ~[?:1.8.0_372]\n\tat org.apache.flink.runtime.fs.hdfs.HadoopDataOutputStream.write(HadoopDataOutputStream.java:47) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.core.fs.FSDataOutputStreamWrapper.write(FSDataOutputStreamWrapper.java:65) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.write(FsCheckpointStreamFactory.java:296) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107) ~[?:1.8.0_372]\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97) ~[?:1.8.0_372]\n\tat org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.serialize(BytePrimitiveArraySerializer.java:75) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.serialize(BytePrimitiveArraySerializer.java:31) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.state.PartitionableListState.write(PartitionableListState.java:117) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.state.DefaultOperatorStateBackendSnapshotStrategy.lambda$asyncSnapshot$1(DefaultOperatorStateBackendSnapshotStrategy.java:165) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.state.SnapshotStrategyRunner$1.callInternal(SnapshotStrategyRunner.java:91) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.state.SnapshotStrategyRunner$1.callInternal(SnapshotStrategyRunner.java:88) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.state.AsyncSnapshotCallable.call(AsyncSnapshotCallable.java:78) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_372]\n\tat org.apache.flink.util.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:642) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.<init>(OperatorSnapshotFinalizer.java:60) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.finalizeNonFinishedSnapshots(AsyncCheckpointRunnable.java:191) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:124) ~[flink-dist-1.15.2.jar:1.15.2]\n\t... 3 more\n2023-05-28 17:48:00,588 WARN  org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger or complete checkpoint 114 for job 9635c4bd284c99b94a979bd50b5fd3bb. (0 consecutive failed attempts so far)\norg.apache.flink.runtime.checkpoint.CheckpointException: Asynchronous task checkpoint failed.\n\tat org.apache.flink.runtime.messages.checkpoint.SerializedCheckpointException.unwrap(SerializedCheckpointException.java:51) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage(CheckpointCoordinator.java:1013) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$declineCheckpoint$2(ExecutionGraphHandler.java:103) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_372]\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_372]\n\tat java.lang.Thread.run(Thread.java:750) [?:1.8.0_372]\nCaused by: org.apache.flink.util.SerializedThrowable: Asynchronous task checkpoint failed.\n\tat org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.handleExecutionException(AsyncCheckpointRunnable.java:320) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:155) ~[flink-dist-1.15.2.jar:1.15.2]\n\t... 3 more\nCaused by: org.apache.flink.util.SerializedThrowable: Could not materialize checkpoint 114 for operator Source: mongo_cdc_test[1] -> Calc[2] (1/10)#0.\n\tat org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.handleExecutionException(AsyncCheckpointRunnable.java:298) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:155) ~[flink-dist-1.15.2.jar:1.15.2]\n\t... 3 more\nCaused by: org.apache.flink.util.SerializedThrowable: java.lang.OutOfMemoryError: Direct buffer memory\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_372]\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[?:1.8.0_372]\n\tat org.apache.flink.util.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:645) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.<init>(OperatorSnapshotFinalizer.java:60) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.finalizeNonFinishedSnapshots(AsyncCheckpointRunnable.java:191) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:124) ~[flink-dist-1.15.2.jar:1.15.2]\n\t... 3 more\nCaused by: org.apache.flink.util.SerializedThrowable: Direct buffer memory\n\tat java.nio.Bits.reserveMemory(Bits.java:695) ~[?:1.8.0_372]\n\tat java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:123) ~[?:1.8.0_372]\n\tat java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311) ~[?:1.8.0_372]\n\tat sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:247) ~[?:1.8.0_372]\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:58) ~[?:1.8.0_372]\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211) ~[?:1.8.0_372]\n\tat java.nio.channels.Channels.writeFullyImpl(Channels.java:78) ~[?:1.8.0_372]\n\tat java.nio.channels.Channels.writeFully(Channels.java:101) ~[?:1.8.0_372]\n\tat java.nio.channels.Channels.access$000(Channels.java:61) ~[?:1.8.0_372]\n\tat java.nio.channels.Channels$1.write(Channels.java:174) ~[?:1.8.0_372]\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122) ~[?:1.8.0_372]\n\tat java.security.DigestOutputStream.write(DigestOutputStream.java:145) ~[?:1.8.0_372]\n\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.write(MultipartUploadOutputStream.java:172) ~[emrfs-hadoop-assembly-2.54.0.jar:?]\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:63) ~[hadoop-common-3.3.3-amzn-1.jar:?]\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107) ~[?:1.8.0_372]\n\tat org.apache.flink.runtime.fs.hdfs.HadoopDataOutputStream.write(HadoopDataOutputStream.java:47) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.core.fs.FSDataOutputStreamWrapper.write(FSDataOutputStreamWrapper.java:65) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.write(FsCheckpointStreamFactory.java:296) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107) ~[?:1.8.0_372]\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97) ~[?:1.8.0_372]\n\tat org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.serialize(BytePrimitiveArraySerializer.java:75) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.serialize(BytePrimitiveArraySerializer.java:31) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.state.PartitionableListState.write(PartitionableListState.java:117) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.state.DefaultOperatorStateBackendSnapshotStrategy.lambda$asyncSnapshot$1(DefaultOperatorStateBackendSnapshotStrategy.java:165) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.state.SnapshotStrategyRunner$1.callInternal(SnapshotStrategyRunner.java:91) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.state.SnapshotStrategyRunner$1.callInternal(SnapshotStrategyRunner.java:88) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.state.AsyncSnapshotCallable.call(AsyncSnapshotCallable.java:78) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_372]\n\tat org.apache.flink.util.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:642) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.<init>(OperatorSnapshotFinalizer.java:60) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.finalizeNonFinishedSnapshots(AsyncCheckpointRunnable.java:191) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:124) ~[flink-dist-1.15.2.jar:1.15.2]\n\t... 3 more\n2023-05-28 17:48:00,591 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Trying to recover from a global failure.\norg.apache.flink.util.FlinkRuntimeException: Exceeded checkpoint tolerable failure threshold.\n\tat org.apache.flink.runtime.checkpoint.CheckpointFailureManager.checkFailureAgainstCounter(CheckpointFailureManager.java:206) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleTaskLevelCheckpointException(CheckpointFailureManager.java:191) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleCheckpointException(CheckpointFailureManager.java:124) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2082) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage(CheckpointCoordinator.java:1039) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$declineCheckpoint$2(ExecutionGraphHandler.java:103) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_372]\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_372]\n\tat java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_372]\n\n```\n\n这是flink job manager或者task manager的**堆外内存不足**，其中的可能性是：\n\n**1.flink job manager的堆外内存过小**，默认是128 MB，参考\n\n```\nhttps://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#jobmanager-memory-off-heap-size\n```\n\n<img src=\"/images/517519-20230529103820326-1293246401.png\" width=\"800\" height=\"328\" loading=\"lazy\" />\n\n可以使用 -D&nbsp;jobmanager.memory.off-heap.size=1024m 手动指定大小，比如\n\n```\n/usr/lib/flink/bin/yarn-session.sh -s 1 -jm 51200 -tm 51200 -qu data -D jobmanager.memory.off-heap.size=1024m --detached\n\n```\n\n参考&nbsp;\n\n```\nhttps://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/mem_setup_jobmanager/\n```\n\n**2.flink task manager的堆外内存过小**，在使用task executor的时候其默认是0，即不使用堆外内存，参考\n\n```\nhttps://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#taskmanager-memory-task-off-heap-size\n```\n\n<img src=\"/images/517519-20230529144635584-993756234.png\" alt=\"\" loading=\"lazy\" />\n\n可以使用 -D&nbsp;taskmanager.memory.task.off-heap.size=4g手动指定大小，比如\n\n```\n/usr/lib/flink/bin/yarn-session.sh -s 1 -jm 51200 -tm 51200 -qu data -D taskmanager.memory.task.off-heap.size=4G --detached\n```\n\n参考\n\n```\nhttps://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/mem_setup_tm/#configure-off-heap-memory-direct-or-native\n```\n\n<img src=\"/images/517519-20230529111626345-2107398364.png\" alt=\"\" loading=\"lazy\" />\n\n如果是 compaction 阶段发生内存溢出，还会导致 compaction 一直卡住 INFLIGHT 状态，这时需要查看日志排查是否是内存爆掉了\n\n<img src=\"/images/517519-20230530113021882-1800563154.png\" alt=\"\" loading=\"lazy\" />\n\n如果 compacion 阶段发生内存溢出导致**compaction失败**，会使得flink任务重启，hudi会对失败的compaction阶段生成的部分parquet文件进行清理，对应的就在文件目录下可以发现时间戳在失败的compaction instant之后的parquet文件会被清理掉\n\n一个**compaction未完成**的时候，即使合并出部分parquet文件，使用hive来查询的时候，这部分新的parquet是不会被查询到的\n\n此外hudi写parquet文件的时候，是需要消耗一定的堆外内存的，参考：[调优 | Apache Hudi应用调优指南](https://www.cnblogs.com/leesf456/p/13055010.html)\n\n#### 8.Caused by: com.mongodb.MongoQueryException: Query failed with error code 286 and error message 'Error on remote shard 172.31.xx.xx:27017 :: caused by :: Resume of change stream was not possible, as the resume point may no longer be in the oplog.' on server xxxx:27017\n\n```\njava.lang.RuntimeException: One or more fetchers have encountered exception\n\tat org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)\n\tat org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)\n\tat org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)\n\tat org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:385)\n\tat org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)\n\tat org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)\n\tat org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)\n\tat org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)\n\tat org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)\n\tat org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)\n\tat org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)\n\tat org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)\n\tat org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)\n\tat org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records\n\tat org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150)\n\tat org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.flink.util.FlinkRuntimeException: Read split StreamSplit{splitId='stream-split', offset={resumeToken={\"_data\": \"826472E86F0000000A2B022C0100296E5A1004B807779924DA402AB13486B3F67B6102463C5F6964003C306D5A776877724A000004\"}, timestamp=7238103114576822282}, endOffset={resumeToken=null, timestamp=9223372034707292159}} error due to Query failed with error code 286 and error message 'Error on remote shard 172.31.xx.xx:27017 :: caused by :: Resume of change stream was not possible, as the resume point may no longer be in the oplog.' on server xxxx:27017.\n\tat com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceStreamFetcher.checkReadException(IncrementalSourceStreamFetcher.java:124)\n\tat com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceStreamFetcher.pollSplitRecords(IncrementalSourceStreamFetcher.java:106)\n\tat com.ververica.cdc.connectors.base.source.reader.IncrementalSourceSplitReader.fetch(IncrementalSourceSplitReader.java:73)\n\tat org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)\n\tat org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)\n\t... 6 more\nCaused by: com.mongodb.MongoQueryException: Query failed with error code 286 and error message 'Error on remote shard 172.31.xx.xx:27017 :: caused by :: Resume of change stream was not possible, as the resume point may no longer be in the oplog.' on server xxxx:27017\n\tat com.mongodb.internal.operation.QueryHelper.translateCommandException(QueryHelper.java:29)\n\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:282)\n\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:512)\n\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:270)\n\tat com.mongodb.internal.operation.QueryBatchCursor.tryHasNext(QueryBatchCursor.java:223)\n\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$tryNext$0(QueryBatchCursor.java:206)\n\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:397)\n\tat com.mongodb.internal.operation.QueryBatchCursor.tryNext(QueryBatchCursor.java:205)\n\tat com.mongodb.internal.operation.ChangeStreamBatchCursor$3.apply(ChangeStreamBatchCursor.java:102)\n\tat com.mongodb.internal.operation.ChangeStreamBatchCursor$3.apply(ChangeStreamBatchCursor.java:98)\n\tat com.mongodb.internal.operation.ChangeStreamBatchCursor.resumeableOperation(ChangeStreamBatchCursor.java:195)\n\tat com.mongodb.internal.operation.ChangeStreamBatchCursor.tryNext(ChangeStreamBatchCursor.java:98)\n\tat com.mongodb.client.internal.MongoChangeStreamCursorImpl.tryNext(MongoChangeStreamCursorImpl.java:78)\n\tat com.ververica.cdc.connectors.mongodb.source.reader.fetch.MongoDBStreamFetchTask.execute(MongoDBStreamFetchTask.java:116)\n\tat com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceStreamFetcher.lambda$submitTask$0(IncrementalSourceStreamFetcher.java:86)\n\t... 5 more\n\n```\n\n同时使用 timeline show incomplete 命令查看未完成的timeline，会发现有 deltacommit 一直卡在 INFLIGHT 状态，说明拉取增量数据出现了问题\n\n<img src=\"/images/517519-20230530110501820-1096710620.png\" alt=\"\" loading=\"lazy\" />\n\n这个原因可能是oplog过期被清理掉了，这时候只能重头开始进行snapshot，之后可以调大oplog的保存大小或者时间&nbsp;\n\n#### 9.unable to convert to rowtype from unexpected value 'bsonarray{values=[]}' of type array\n\n这是由于mongo是无强数据类型的，所以在实际Flink Mongo CDC的使用中，有时就会出现数据类型对不上的时候，比如mongo中一个字段，应该是一个array，但是实际上却出现了json类型，这时候就会出现以上报错\n\n解决方法是对 flink-connector-mongodb-cdc 项目中的 MongoDBConnectorDeserializationSchema 代码进行修改，对数据类型对不上时候的报错进行捕获，直接填充字符串NULL，这和MapReduce dump Mongodb时候的对于字段类型无法对上的处理方法是一致的\n\n#### 10.java.util.concurrent.TimeoutException: Heartbeat of TaskManager with id container_1685522036674_0015_01_000014(ip-172-31-xxx-xx.us.compute.internal:8041) timed out.\n\n```\njava.util.concurrent.TimeoutException: Heartbeat of TaskManager with id container_1685522036674_0015_01_000014(ip-172-31-xxx-xx.us.compute.internal:8041) timed out.\n\tat org.apache.flink.runtime.jobmaster.JobMaster$TaskManagerHeartbeatListener.notifyHeartbeatTimeout(JobMaster.java:1371)\n\tat org.apache.flink.runtime.heartbeat.HeartbeatMonitorImpl.run(HeartbeatMonitorImpl.java:155)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:443)\n\tat org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)\n\tat org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:443)\n\tat org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:213)\n\tat org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)\n\tat org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)\n\tat akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)\n\tat akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:123)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)\n\tat akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)\n\tat akka.actor.Actor.aroundReceive(Actor.scala:537)\n\tat akka.actor.Actor.aroundReceive$(Actor.scala:535)\n\tat akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:548)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:231)\n\tat akka.dispatch.Mailbox.exec(Mailbox.scala:243)\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)\n\n```\n\n这个报错问题是由于task manager所在机器内存被占满等机器层面的问题，排查方法同下面第10条\n\n#### 11.Caused by: java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(TaskExecutorGateway.sendOperatorEventToTask(ExecutionAttemptID, OperatorID, SerializedValue))] at recipient [akka.tcp://flink@ip-172-31-xxx-xx.us-.compute.internal:33139/user/rpc/taskmanager_0] timed out\n\n```\norg.apache.flink.util.FlinkException: An OperatorEvent from an OperatorCoordinator to a task was lost. Triggering task failover to ensure consistency. Event: 'SourceEventWrapper[com.ververica.cdc.connectors.base.source.meta.events.FinishedSnapshotSplitsRequestEvent@19b057ed]', targetTask: Source: mongo_cdc_test[1] -> Calc[2] (6/60) - execution #1\n\tat org.apache.flink.runtime.operators.coordination.SubtaskGatewayImpl.lambda$null$0(SubtaskGatewayImpl.java:90)\n\tat org.apache.flink.runtime.util.Runnables.lambda$withUncaughtExceptionHandler$0(Runnables.java:49)\n\tat org.apache.flink.runtime.util.Runnables.assertNoException(Runnables.java:33)\n\tat org.apache.flink.runtime.operators.coordination.SubtaskGatewayImpl.lambda$sendEvent$1(SubtaskGatewayImpl.java:88)\n\tat java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)\n\tat java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)\n\tat java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)\n\tat org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:443)\n\tat org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)\n\tat org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:443)\n\tat org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:213)\n\tat org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)\n\tat org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)\n\tat akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)\n\tat akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:123)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)\n\tat akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)\n\tat akka.actor.Actor.aroundReceive(Actor.scala:537)\n\tat akka.actor.Actor.aroundReceive$(Actor.scala:535)\n\tat akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:548)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:231)\n\tat akka.dispatch.Mailbox.exec(Mailbox.scala:243)\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)\nCaused by: java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(TaskExecutorGateway.sendOperatorEventToTask(ExecutionAttemptID, OperatorID, SerializedValue))] at recipient [akka.tcp://flink@ip-172-31-xxx-xx.us-.compute.internal:33139/user/rpc/taskmanager_0] timed out. This is usually caused by: 1) Akka failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase akka.ask.timeout.\n\tat org.apache.flink.runtime.jobmaster.RpcTaskManagerGateway.sendOperatorEventToTask(RpcTaskManagerGateway.java:127)\n\tat org.apache.flink.runtime.executiongraph.Execution.sendOperatorEvent(Execution.java:874)\n\tat org.apache.flink.runtime.operators.coordination.ExecutionSubtaskAccess.lambda$createEventSendAction$1(ExecutionSubtaskAccess.java:67)\n\tat org.apache.flink.runtime.operators.coordination.OperatorEventValve.callSendAction(OperatorEventValve.java:180)\n\tat org.apache.flink.runtime.operators.coordination.OperatorEventValve.sendEvent(OperatorEventValve.java:94)\n\tat org.apache.flink.runtime.operators.coordination.SubtaskGatewayImpl.lambda$sendEvent$2(SubtaskGatewayImpl.java:98)\n\t... 26 more\nCaused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@ip-172-31-xxx-xx.us-.compute.internal:33139/user/rpc/taskmanager_0#-116058576]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.\n\n```\n\n这需要去job manager报错日志里面的ip对应的task manager上查看具体日志，排查下来发现是访问s3时候有问题导致的\n\n```\n2023-05-30 10:18:59,460 ERROR org.apache.hudi.sink.compact.CompactFunction                 [] - Executor executes action [Execute compaction for instant 20230530095353629 from task 1] error\norg.apache.hudi.exception.HoodieIOException: Could not check if s3a://xxxx/hudi_test35 is a valid table\n\tat org.apache.hudi.exception.TableNotFoundException.checkTableValidity(TableNotFoundException.java:59) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\tat org.apache.hudi.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:128) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\tat org.apache.hudi.common.table.HoodieTableMetaClient.newMetaClient(HoodieTableMetaClient.java:642) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\tat org.apache.hudi.common.table.HoodieTableMetaClient.access$000(HoodieTableMetaClient.java:80) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\tat org.apache.hudi.common.table.HoodieTableMetaClient$Builder.build(HoodieTableMetaClient.java:711) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\tat org.apache.hudi.table.HoodieFlinkTable.create(HoodieFlinkTable.java:59) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\tat org.apache.hudi.client.HoodieFlinkWriteClient.getHoodieTable(HoodieFlinkWriteClient.java:607) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\tat org.apache.hudi.sink.compact.CompactFunction.reloadWriteConfig(CompactFunction.java:125) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\tat org.apache.hudi.sink.compact.CompactFunction.lambda$processElement$0(CompactFunction.java:95) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\tat org.apache.hudi.sink.utils.NonThrownExecutor.lambda$wrapAction$0(NonThrownExecutor.java:130) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_372]\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_372]\n\tat java.lang.Thread.run(Thread.java:750) [?:1.8.0_372]\nCaused by: org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a://xxxx/hudi_test35/.hoodie: com.amazonaws.SdkClientException: Unable to unmarshall response (Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.). Response Code: 200, Response Text: OK: Unable to unmarshall response (Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.). Response Code: 200, Response Text: OK\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:214) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3861) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getFileStatus$24(S3AFileSystem.java:3556) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499) ~[hadoop-common-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444) ~[hadoop-common-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3554) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hudi.common.fs.HoodieWrapperFileSystem.lambda$getFileStatus$17(HoodieWrapperFileSystem.java:402) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\tat org.apache.hudi.common.fs.HoodieWrapperFileSystem.executeFuncWithTimeMetrics(HoodieWrapperFileSystem.java:106) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\tat org.apache.hudi.common.fs.HoodieWrapperFileSystem.getFileStatus(HoodieWrapperFileSystem.java:396) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\tat org.apache.hudi.exception.TableNotFoundException.checkTableValidity(TableNotFoundException.java:51) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\t... 12 more\nCaused by: com.amazonaws.SdkClientException: Unable to unmarshall response (Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.). Response Code: 200, Response Text: OK\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleResponse(AmazonHttpClient.java:1818) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleSuccessResponse(AmazonHttpClient.java:1477) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1384) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5397) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:971) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$11(S3AFileSystem.java:2595) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499) ~[hadoop-common-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:2586) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3832) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getFileStatus$24(S3AFileSystem.java:3556) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499) ~[hadoop-common-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444) ~[hadoop-common-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3554) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hudi.common.fs.HoodieWrapperFileSystem.lambda$getFileStatus$17(HoodieWrapperFileSystem.java:402) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\tat org.apache.hudi.common.fs.HoodieWrapperFileSystem.executeFuncWithTimeMetrics(HoodieWrapperFileSystem.java:106) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\tat org.apache.hudi.common.fs.HoodieWrapperFileSystem.getFileStatus(HoodieWrapperFileSystem.java:396) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\tat org.apache.hudi.exception.TableNotFoundException.checkTableValidity(TableNotFoundException.java:51) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\t... 12 more\nCaused by: java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.\n\tat org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:164) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.loadClass(FlinkUserCodeClassLoaders.java:172) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.xml.sax.helpers.NewInstance.newInstance(NewInstance.java:82) ~[?:1.8.0_372]\n\tat org.xml.sax.helpers.XMLReaderFactory.loadClass(XMLReaderFactory.java:228) ~[?:1.8.0_372]\n\tat org.xml.sax.helpers.XMLReaderFactory.createXMLReader(XMLReaderFactory.java:191) ~[?:1.8.0_372]\n\tat com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.<init>(XmlResponsesSaxParser.java:135) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsV2Unmarshaller.unmarshall(Unmarshallers.java:135) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsV2Unmarshaller.unmarshall(Unmarshallers.java:125) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:62) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:31) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.http.response.AwsResponseHandlerAdapter.handle(AwsResponseHandlerAdapter.java:69) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleResponse(AmazonHttpClient.java:1794) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleSuccessResponse(AmazonHttpClient.java:1477) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1384) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5397) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:971) ~[aws-java-sdk-bundle-1.12.331.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$11(S3AFileSystem.java:2595) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499) ~[hadoop-common-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:2586) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3832) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getFileStatus$24(S3AFileSystem.java:3556) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499) ~[hadoop-common-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444) ~[hadoop-common-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3554) ~[hadoop-aws-3.3.3-amzn-1.jar:?]\n\tat org.apache.hudi.common.fs.HoodieWrapperFileSystem.lambda$getFileStatus$17(HoodieWrapperFileSystem.java:402) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\tat org.apache.hudi.common.fs.HoodieWrapperFileSystem.executeFuncWithTimeMetrics(HoodieWrapperFileSystem.java:106) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\tat org.apache.hudi.common.fs.HoodieWrapperFileSystem.getFileStatus(HoodieWrapperFileSystem.java:396) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\tat org.apache.hudi.exception.TableNotFoundException.checkTableValidity(TableNotFoundException.java:51) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\t... 12 more\n2023-05-30 10:18:59,461 ERROR org.apache.flink.runtime.util.ClusterUncaughtExceptionHandler [] - WARNING: Thread 'pool-17-thread-2' produced an uncaught exception. If you want to fail on uncaught exceptions, then configure cluster.uncaught-exception-handling accordingly\norg.apache.flink.runtime.execution.CancelTaskException: Buffer pool has already been destroyed.\n\tat org.apache.flink.runtime.io.network.buffer.LocalBufferPool.checkDestroyed(LocalBufferPool.java:404) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegment(LocalBufferPool.java:373) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestBufferBuilder(LocalBufferPool.java:316) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewBufferBuilderFromPool(BufferWritingResultPartition.java:394) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewUnicastBufferBuilder(BufferWritingResultPartition.java:377) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.appendUnicastDataForNewRecord(BufferWritingResultPartition.java:281) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.emitRecord(BufferWritingResultPartition.java:157) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:106) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:54) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:104) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:90) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:44) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.hudi.sink.compact.CompactFunction.lambda$processElement$1(CompactFunction.java:96) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\tat org.apache.hudi.sink.utils.NonThrownExecutor.handleException(NonThrownExecutor.java:146) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\tat org.apache.hudi.sink.utils.NonThrownExecutor.lambda$wrapAction$0(NonThrownExecutor.java:133) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_372]\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_372]\n\tat java.lang.Thread.run(Thread.java:750) [?:1.8.0_372]\n\n```\n\n#### 12.org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connection unexpectedly closed by remote task manager 'ip-172-31-xxx-xx.us.compute.internal/172.31.xxx.xx:39597'. This might indicate that the remote task manager was lost.\n\n```\norg.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connection unexpectedly closed by remote task manager 'ip-172-31-xxx-xx.us.compute.internal/172.31.xxx.xx:39597'. This might indicate that the remote task manager was lost.\n\tat org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.channelInactive(CreditBasedPartitionRequestClientHandler.java:127)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)\n\tat org.apache.flink.runtime.io.network.netty.NettyMessageClientDecoderDelegate.channelInactive(NettyMessageClientDecoderDelegate.java:94)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:831)\n\tat org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384)\n\tat org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\tat org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat java.lang.Thread.run(Thread.java:750)\n\n```\n\n这种报错通常发生在机器出现内存不足等问题的时候\n\n去对应ip的task manager上查看日志，日志最后显示这个task manager当时正在加载checkpoint\n\n```\n2023-05-31 03:19:00,920 INFO  com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem           [] - Opening 's3://xxx/hudi_test_checkpoint/131ca6b0b81354dfb130360ccf9e9d30/shared/a5edc84c-840c-4ccd-8038-7da256d08cbd' for reading\n2023-05-31 03:19:03,369 INFO  com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem           [] - Opening 's3://xxx/hudi_test_checkpoint/131ca6b0b81354dfb130360ccf9e9d30/chk-125/9d7e2217-10e5-436d-b678-6ca27ea1c6a0' for reading\n2023-05-31 03:19:03,424 INFO  com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem           [] - Opening 's3://xxx/hudi_test_checkpoint/131ca6b0b81354dfb130360ccf9e9d30/shared/ad969d73-a803-4e82-b4fa-38a57469d48a' for reading\n\n```\n\n对比报错发生的时间点和当时机器的内存监控指标\n\n<img src=\"/images/517519-20230531153224219-2115333833.png\" width=\"1500\" height=\"862\" loading=\"lazy\" />\n\n发现报错的时候对应ip机器的内存都几乎满了\n\n<img src=\"/images/517519-20230531154538085-2017072666.png\" width=\"1000\" height=\"259\" loading=\"lazy\" />\n\n<img src=\"/images/517519-20230531154155825-238606117.png\" width=\"1000\" height=\"249\" loading=\"lazy\" />\n\n<img src=\"/images/517519-20230531154446167-1421223842.png\" width=\"1000\" height=\"214\" loading=\"lazy\" />\n\n#### 13.Caused by: com.mongodb.MongoCommandException: Command failed with error 13 (Unauthorized): &lsquo;not authorized on admin to execute command { aggregate: 1, pipeline: [ { $changeStream: { allChangesForCluster: true } } ], cursor: { batchSize: 1 }, $db: &ldquo;admin&rdquo;, $clusterTime: { clusterTime: Timestamp(1680783775, 2), signature: { hash: BinData(0, 898FE82BE1837F4BDBDA1A12CC1D5F765527A25C), keyId: 7175829065197158402 } }, lsid: { id: UUID(&ldquo;9d99adfc-cc35-4619-b5c5-7ad71ec77df1&rdquo;) } }&rsquo; on server xxxx:27017\n\n这是由于使用的mongo账号没有admin库的读取权限导致的，需要添加一下权限，参考：[https://ververica.github.io/flink-cdc-connectors/release-2.3/content/connectors/mongodb-cdc.html?highlight=mongo#setup-mongodb](https://ververica.github.io/flink-cdc-connectors/release-2.3/content/connectors/mongodb-cdc.html?highlight=mongo#setup-mongodb)\n\n```\nCaused by: org.apache.flink.util.FlinkRuntimeException: Read split SnapshotSplit{tableId=xxx.xxx, splitId=&lsquo;xxx.xxx:224&rsquo;, splitKeyType=[`_id` INT], splitStart=[{&ldquo;_id&rdquo;: 1.0}, {&ldquo;_id&rdquo;: &ldquo;xxxx&rdquo;}], splitEnd=[{&ldquo;_id&rdquo;: 1.0}, {&ldquo;_id&rdquo;: &ldquo;xxxx\"}], highWatermark=null} error due to Command failed with error 13 (Unauthorized): &lsquo;not authorized on admin to execute command { aggregate: 1, pipeline: [ { $changeStream: { allChangesForCluster: true } } ], cursor: { batchSize: 1 }, $db: &ldquo;admin&rdquo;, $clusterTime: { clusterTime: Timestamp(1680783775, 2), signature: { hash: BinData(0, 898FE82BE1837F4BDBDA1A12CC1D5F765527A25C), keyId: 7175829065197158402 } }, lsid: { id: UUID(&ldquo;9d99adfc-cc35-4619-b5c5-7ad71ec77df1\") } }&rsquo; on server xxxx:27017. The full response is {&ldquo;ok&rdquo;: 0.0, &ldquo;errmsg&rdquo;: &ldquo;not authorized on admin to execute command { aggregate: 1, pipeline: [ { $changeStream: { allChangesForCluster: true } } ], cursor: { batchSize: 1 }, $db: \\&ldquo;admin\\&ldquo;, $clusterTime: { clusterTime: Timestamp(1680783775, 2), signature: { }}}.\n    at com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceScanFetcher.checkReadException(IncrementalSourceScanFetcher.java:181)\n    at com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceScanFetcher.pollSplitRecords(IncrementalSourceScanFetcher.java:128)\n    at com.ververica.cdc.connectors.base.source.reader.IncrementalSourceSplitReader.fetch(IncrementalSourceSplitReader.java:73)\n    at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)\n    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)\n    ... 6 more\nCaused by: com.mongodb.MongoCommandException: Command failed with error 13 (Unauthorized): &lsquo;not authorized on admin to execute command { aggregate: 1, pipeline: [ { $changeStream: { allChangesForCluster: true } } ], cursor: { batchSize: 1 }, $db: &ldquo;admin&rdquo;, $clusterTime: { clusterTime: Timestamp(1680783775, 2), signature: {  } }, lsid: {  }&rsquo; on server xxxx:27017. The full response is {&ldquo;ok&rdquo;: 0.0, &ldquo;errmsg&rdquo;: &ldquo;not authorized on admin to execute command { aggregate: 1, pipeline: [ { $changeStream: { allChangesForCluster: true } } ], cursor: { batchSize: 1 }, $db: \\&ldquo;admin\\&ldquo;, $clusterTime: { clusterTime: Timestamp(1680783775, 2), signature: {  }, lsid: {  }&ldquo;, &ldquo;code&rdquo;: 13, &ldquo;codeName&rdquo;: &ldquo;Unauthorized&rdquo;, &ldquo;operationTime&rdquo;: {&ldquo;$timestamp&rdquo;: {&ldquo;t&rdquo;: 1680783775, &ldquo;i&rdquo;: 2}}, &ldquo;$clusterTime&rdquo;: {&ldquo;clusterTime&rdquo;: {&ldquo;$timestamp&rdquo;: {&ldquo;t&rdquo;: 1680783775, &ldquo;i&rdquo;: 2}}, &ldquo;signature&rdquo;: {&ldquo;hash&rdquo;: {&ldquo;$binary&rdquo;: {&ldquo;base64\": &ldquo;&ldquo;, &ldquo;subType&rdquo;: &ldquo;00\"}}, &ldquo;keyId&rdquo;:xxx}}}\n```\n\n#### 14.Caused by: com.mongodb.MongoCommandException: Command failed with error 18 (AuthenticationFailed): 'Authentication failed.' on server xxx:27017.\n\n```\norg.apache.flink.util.FlinkException: Global failure triggered by OperatorCoordinator for 'Source: mongo_cdc_test[1] -> Calc[2]' (operator cbc357ccb763df2852fee8c4fc7d55f2).\n\tat org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder$LazyInitializedCoordinatorContext.failJob(OperatorCoordinatorHolder.java:556)\n\tat org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$QuiesceableContext.failJob(RecreateOnResetOperatorCoordinator.java:231)\n\tat org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext.failJob(SourceCoordinatorContext.java:316)\n\tat org.apache.flink.runtime.source.coordinator.SourceCoordinator.start(SourceCoordinator.java:201)\n\tat org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$DeferrableCoordinator.resetAndStart(RecreateOnResetOperatorCoordinator.java:394)\n\tat org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.lambda$resetToCheckpoint$6(RecreateOnResetOperatorCoordinator.java:144)\n\tat java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)\n\tat java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)\n\tat java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)\n\tat java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)\n\tat org.apache.flink.runtime.operators.coordination.ComponentClosingUtils.lambda$closeAsyncWithTimeout$0(ComponentClosingUtils.java:77)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.flink.util.FlinkRuntimeException: Failed to discover captured tables for enumerator\n\tat com.ververica.cdc.connectors.base.source.IncrementalSource.createEnumerator(IncrementalSource.java:151)\n\tat org.apache.flink.runtime.source.coordinator.SourceCoordinator.start(SourceCoordinator.java:197)\n\t... 8 more\nCaused by: com.mongodb.MongoSecurityException: Exception authenticating MongoCredential{mechanism=SCRAM-SHA-1, userName='xxxx', source='admin', password=<hidden>, mechanismProperties=<hidden>}\n\tat com.mongodb.internal.connection.SaslAuthenticator.wrapException(SaslAuthenticator.java:273)\n\tat com.mongodb.internal.connection.SaslAuthenticator.getNextSaslResponse(SaslAuthenticator.java:137)\n\tat com.mongodb.internal.connection.SaslAuthenticator.access$100(SaslAuthenticator.java:48)\n\tat com.mongodb.internal.connection.SaslAuthenticator$1.run(SaslAuthenticator.java:63)\n\tat com.mongodb.internal.connection.SaslAuthenticator$1.run(SaslAuthenticator.java:57)\n\tat com.mongodb.internal.connection.SaslAuthenticator.doAsSubject(SaslAuthenticator.java:280)\n\tat com.mongodb.internal.connection.SaslAuthenticator.authenticate(SaslAuthenticator.java:57)\n\tat com.mongodb.internal.connection.DefaultAuthenticator.authenticate(DefaultAuthenticator.java:55)\n\tat com.mongodb.internal.connection.InternalStreamConnectionInitializer.authenticate(InternalStreamConnectionInitializer.java:205)\n\tat com.mongodb.internal.connection.InternalStreamConnectionInitializer.finishHandshake(InternalStreamConnectionInitializer.java:79)\n\tat com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:170)\n\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.open(UsageTrackingInternalConnection.java:53)\n\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.open(DefaultConnectionPool.java:496)\n\tat com.mongodb.internal.connection.DefaultConnectionPool$OpenConcurrencyLimiter.openWithConcurrencyLimit(DefaultConnectionPool.java:865)\n\tat com.mongodb.internal.connection.DefaultConnectionPool$OpenConcurrencyLimiter.openOrGetAvailable(DefaultConnectionPool.java:806)\n\tat com.mongodb.internal.connection.DefaultConnectionPool.get(DefaultConnectionPool.java:155)\n\tat com.mongodb.internal.connection.DefaultConnectionPool.get(DefaultConnectionPool.java:145)\n\tat com.mongodb.internal.connection.DefaultServer.getConnection(DefaultServer.java:92)\n\tat com.mongodb.internal.binding.ClusterBinding$ClusterBindingConnectionSource.getConnection(ClusterBinding.java:141)\n\tat com.mongodb.client.internal.ClientSessionBinding$SessionBindingConnectionSource.getConnection(ClientSessionBinding.java:163)\n\tat com.mongodb.internal.operation.CommandOperationHelper.lambda$executeCommand$4(CommandOperationHelper.java:190)\n\tat com.mongodb.internal.operation.OperationHelper.withReadConnectionSource(OperationHelper.java:583)\n\tat com.mongodb.internal.operation.CommandOperationHelper.executeCommand(CommandOperationHelper.java:189)\n\tat com.mongodb.internal.operation.ListDatabasesOperation.execute(ListDatabasesOperation.java:201)\n\tat com.mongodb.internal.operation.ListDatabasesOperation.execute(ListDatabasesOperation.java:54)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:184)\n\tat com.mongodb.client.internal.MongoIterableImpl.execute(MongoIterableImpl.java:135)\n\tat com.mongodb.client.internal.MongoIterableImpl.iterator(MongoIterableImpl.java:92)\n\tat com.mongodb.client.internal.MongoIterableImpl.forEach(MongoIterableImpl.java:121)\n\tat com.mongodb.client.internal.MappingIterable.forEach(MappingIterable.java:59)\n\tat com.ververica.cdc.connectors.mongodb.source.utils.CollectionDiscoveryUtils.databaseNames(CollectionDiscoveryUtils.java:56)\n\tat com.ververica.cdc.connectors.mongodb.source.dialect.MongoDBDialect.lambda$discoverAndCacheDataCollections$0(MongoDBDialect.java:99)\n\tat java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)\n\tat com.ververica.cdc.connectors.mongodb.source.dialect.MongoDBDialect.discoverAndCacheDataCollections(MongoDBDialect.java:94)\n\tat com.ververica.cdc.connectors.mongodb.source.dialect.MongoDBDialect.discoverDataCollections(MongoDBDialect.java:75)\n\tat com.ververica.cdc.connectors.mongodb.source.dialect.MongoDBDialect.discoverDataCollections(MongoDBDialect.java:60)\n\tat com.ververica.cdc.connectors.base.source.IncrementalSource.createEnumerator(IncrementalSource.java:139)\n\t... 9 more\nCaused by: com.mongodb.MongoCommandException: Command failed with error 18 (AuthenticationFailed): 'Authentication failed.' on server xxx:27017. The full response is {\"operationTime\": {\"$timestamp\": {\"t\": 1686237336, \"i\": 24}}, \"ok\": 0.0, \"errmsg\": \"Authentication failed.\", \"code\": 18, \"codeName\": \"AuthenticationFailed\", \"$clusterTime\": {\"clusterTime\": {\"$timestamp\": {\"t\": 1686237336, \"i\": 24}}, \"signature\": {\"hash\": {\"$binary\": {\"base64\": \"xjwzreIFobScDm4vZ2UH319Iklo=\", \"subType\": \"00\"}}, \"keyId\": 7192072244963573763}}}\n\tat com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:195)\n\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:400)\n\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:324)\n\tat com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)\n\tat com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)\n\tat com.mongodb.internal.connection.SaslAuthenticator.sendSaslStart(SaslAuthenticator.java:228)\n\tat com.mongodb.internal.connection.SaslAuthenticator.getNextSaslResponse(SaslAuthenticator.java:135)\n\n```\n\n这也是由于使用的mongo账号没有权限导致的，除了admin库的读取权限之外，还需要如下权限，参考：[https://ververica.github.io/flink-cdc-connectors/release-2.3/content/connectors/mongodb-cdc.html?highlight=mongo#setup-mongodb](https://ververica.github.io/flink-cdc-connectors/release-2.3/content/connectors/mongodb-cdc.html?highlight=mongo#setup-mongodb)\n\n```\nuse admin;\ndb.createRole(\n    {\n        role: \"flinkrole\",\n        privileges: [{\n            // Grant privileges on all non-system collections in all databases\n            resource: { db: \"\", collection: \"\" },\n            actions: [\n                \"splitVector\",\n                \"listDatabases\",\n                \"listCollections\",\n                \"collStats\",\n                \"find\",\n                \"changeStream\" ]\n        }],\n        roles: [\n            // Read config.collections and config.chunks\n            // for sharded cluster snapshot splitting.\n            { role: 'read', db: 'config' }\n        ]\n    }\n);\n\ndb.createUser(\n  {\n      user: 'flinkuser',\n      pwd: 'flinkpw',\n      roles: [\n         { role: 'flinkrole', db: 'admin' }\n      ]\n  }\n);\n\n```\n\n#### 15.checkpoint超时\n\n如果发现checkpoint间歇性失败，可以看一下是否是checkpoint超过默认10分钟的限制，对于大表，可能checkpoint会达到30分钟，可以通过以下方式调整checkpoint超时时间\n\n```\n// checkpoint1小时超时\nenv.getCheckpointConfig().setCheckpointTimeout(3600000);\n```\n\n<img src=\"/images/517519-20230614164231567-1109743112.png\" alt=\"\" loading=\"lazy\" />\n\n#### 16.org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator``\n\n这是由于flink的tuple类型不支持包含null，比如[\"test1\",null,\"test2\"]，从而导致遍历数组的时候出现NullPointException\n\n```\n2023-07-04 14:53:47\njava.lang.NullPointerException\n\tat org.apache.flink.table.runtime.typeutils.StringDataSerializer.copy(StringDataSerializer.java:56)\n\tat org.apache.flink.table.runtime.typeutils.StringDataSerializer.copy(StringDataSerializer.java:34)\n\tat org.apache.flink.table.runtime.typeutils.ArrayDataSerializer.copyGenericArray(ArrayDataSerializer.java:128)\n\tat org.apache.flink.table.runtime.typeutils.ArrayDataSerializer.copy(ArrayDataSerializer.java:86)\n\tat org.apache.flink.table.runtime.typeutils.ArrayDataSerializer.copy(ArrayDataSerializer.java:47)\n\tat org.apache.flink.table.runtime.typeutils.RowDataSerializer.copyRowData(RowDataSerializer.java:170)\n\tat org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:131)\n\tat org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:48)\n\tat org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:80)\n\tat org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)\n\tat org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)\n\tat org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask$AsyncDataOutputToOutput.emitRecord(SourceOperatorStreamTask.java:196)\n\tat org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:110)\n\tat org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:101)\n\tat com.ververica.cdc.connectors.base.source.reader.IncrementalSourceRecordEmitter$OutputCollector.collect(IncrementalSourceRecordEmitter.java:166)\n\tat com.ververica.cdc.connectors.mongodb.table.MongoDBConnectorDeserializationSchema.emit(MongoDBConnectorDeserializationSchema.java:216)\n\tat com.ververica.cdc.connectors.mongodb.table.MongoDBConnectorDeserializationSchema.deserialize(MongoDBConnectorDeserializationSchema.java:143)\n\tat com.ververica.cdc.connectors.base.source.reader.IncrementalSourceRecordEmitter.emitElement(IncrementalSourceRecordEmitter.java:141)\n\tat com.ververica.cdc.connectors.mongodb.source.reader.MongoDBRecordEmitter.processElement(MongoDBRecordEmitter.java:79)\n\tat com.ververica.cdc.connectors.base.source.reader.IncrementalSourceRecordEmitter.emitRecord(IncrementalSourceRecordEmitter.java:86)\n\tat com.ververica.cdc.connectors.base.source.reader.IncrementalSourceRecordEmitter.emitRecord(IncrementalSourceRecordEmitter.java:55)\n\tat org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:143)\n\tat org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:351)\n\tat org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)\n\tat org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)\n\tat org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496)\n\tat org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)\n\tat org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)\n\tat org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)\n\tat org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)\n\tat org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)\n\tat org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)\n\tat org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)\n\tat java.lang.Thread.run(Thread.java:750)　　\n```\n\n参考：[https://stackoverflow.com/questions/45567505/apache-flink-nullpointerexception-caused-by-tupleserializer](https://stackoverflow.com/questions/45567505/apache-flink-nullpointerexception-caused-by-tupleserializer)\n\n#### 17.java.lang.NoClassDefFoundError: Could not initialize class com.ververica.cdc.connectors.mongodb.internal.MongoDBEnvelope\n\n```\n2023-07-13 08:08:55,200 ERROR org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Failed to create Source Enumerator for source Source: mongo_cdc_test[1]\njava.lang.NoClassDefFoundError: Could not initialize class com.ververica.cdc.connectors.mongodb.internal.MongoDBEnvelope\n\tat com.ververica.cdc.connectors.mongodb.source.utils.MongoUtils.buildConnectionString(MongoUtils.java:373) ~[flink-sql-connector-mongodb-cdc-2.5-SNAPSHOT.jar:2.5-SNAPSHOT]\n\tat com.ververica.cdc.connectors.mongodb.source.config.MongoDBSourceConfig.<init>(MongoDBSourceConfig.java:79) ~[flink-sql-connector-mongodb-cdc-2.5-SNAPSHOT.jar:2.5-SNAPSHOT]\n\tat com.ververica.cdc.connectors.mongodb.source.config.MongoDBSourceConfigFactory.create(MongoDBSourceConfigFactory.java:253) ~[flink-sql-connector-mongodb-cdc-2.5-SNAPSHOT.jar:2.5-SNAPSHOT]\n\tat com.ververica.cdc.connectors.mongodb.source.config.MongoDBSourceConfigFactory.create(MongoDBSourceConfigFactory.java:41) ~[flink-sql-connector-mongodb-cdc-2.5-SNAPSHOT.jar:2.5-SNAPSHOT]\n\tat com.ververica.cdc.connectors.base.source.IncrementalSource.createEnumerator(IncrementalSource.java:134) ~[flink-sql-connector-mongodb-cdc-2.5-SNAPSHOT.jar:2.5-SNAPSHOT]\n\tat org.apache.flink.runtime.source.coordinator.SourceCoordinator.start(SourceCoordinator.java:197) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$DeferrableCoordinator.resetAndStart(RecreateOnResetOperatorCoordinator.java:394) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.lambda$resetToCheckpoint$6(RecreateOnResetOperatorCoordinator.java:144) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_372]\n\tat java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_372]\n\tat java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_372]\n\tat java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) ~[?:1.8.0_372]\n\tat org.apache.flink.runtime.operators.coordination.ComponentClosingUtils.lambda$closeAsyncWithTimeout$0(ComponentClosingUtils.java:77) ~[flink-dist-1.15.2.jar:1.15.2]\n\tat java.lang.Thread.run(Thread.java:750) [?:1.8.0_372]\n\n```\n\n该报错是由于没有按照flink cdc文档来使用jar导致的\n\n在项目的pom.xml中使用的依赖应该如下\n\n```\n        <dependency>\n            <groupId>com.ververica</groupId>\n            <artifactId>flink-connector-mongodb-cdc</artifactId>\n            <version>2.5-SNAPSHOT</version>\n            <scope>provided</scope>\n        </dependency>\n\n```\n\n在${FLINK_HOME}/lib目录下部署的jar应该如下\n\n```\nflink-sql-connector-mongodb-cdc-2.5-SNAPSHOT.jar\n\n```\n\n参考文档：[https://github.com/apache/flink-cdc/blob/master/docs/content/docs/connectors/flink-sources/mongodb-cdc.md](https://github.com/apache/flink-cdc/blob/master/docs/content/docs/connectors/flink-sources/mongodb-cdc.md)\n\n### 5.Mongo CDC业界使用案例\n\n#### 1.[Flink CDC 系列 - Flink MongoDB CDC 在 XTransfer 的生产实践](https://developer.aliyun.com/article/845666)&nbsp;（XTransfer） \n\n&nbsp;\n","tags":["Flink"]},{"title":"Hive学习笔记——函数","url":"/Hive学习笔记——函数.html","content":"## 1.cast函数\n\n数据类型转换函数\n\n比如date的值为\n\n参考：[Hive中CAST（）函数用法](https://www.jianshu.com/p/999176fa2730)\n\n## 2.explode函数\n\nexplode() 函数接收一个 array 或 map 作为输入，然后将 array 或 map 里面的元素按照每行的形式输出。其可以配合 LATERAL VIEW 一起使用\n\n参考：[Hive应用：explode和lateral view ](https://cloud.tencent.com/developer/article/1408939)\n\n## 3.lateral view\n\nlateral view用于和split、explode等UDTF一起使用的，能将一行数据拆分成多行数据，在此基础上可以对拆分的数据进行聚合，lateral view首先为原始表的每行调用UDTF，UDTF会把一行拆分成一行或者多行，lateral view在把结果组合，产生一个支持别名表的虚拟表。\n\n## 4.json_tuple\n\n参考：[一文学会Hive解析Json数组（好文收藏）](https://cloud.tencent.com/developer/article/1819621)\n\n## 5.json UDF\n\n[**brickhouse的json udf**](https://github.com/klout/brickhouse/wiki/Json-udfs)\n\n1.brickhouse.udf.json.ToJsonUDF，将hive表转换成json\n\n```\nadd jar /path/brickhouse-0.7.0-SNAPSHOT.jar;\n\nCREATE TEMPORARY FUNCTION to_json AS 'brickhouse.udf.json.ToJsonUDF';\n\nselect to_json(named_struct( 'name', a.col_1, 'children' , array(named_struct('name', b.col_2, 'logins', b.col_3))))\nfrom table_a a join table_b b on a.col_1 = b.col_1;\n\n{\"name\":\"userLogins\",\"children\":[{\"name\":\"Site B\",\"logins\":20}]}\n{\"name\":\"userLogins\",\"children\":[{\"name\":\"Site A\",\"logins\":10}]}\n\n```\n\n参考\n\n```\nhttps://stackoverflow.com/questions/25188734/converting-data-from-multiple-hive-tables-to-complex-json\n\n```\n\n2.brickhouse.udf.json.JsonMapUDF，将json的map转换成hive的map\n\n```\nadd jar /path/brickhouse-0.7.0-SNAPSHOT.jar;\n\nCREATE TEMPORARY FUNCTION json_map AS 'brickhouse.udf.json.JsonMapUDF';\n\n```\n\n3.brickhouse.udf.json.JsonSplitUDF，将json的value切分成hive的array，比如 {\"a\":null,\"b\":\"40573\"} => [null,\"40573\"]\n\n```\nadd jar /path/brickhouse-0.7.0-SNAPSHOT.jar;\n\nCREATE TEMPORARY FUNCTION json_split AS 'brickhouse.udf.json.JsonSplitUDF';\n\n```\n\n## 6.开窗函数\n\nROW_NUMBER() OVER 顺序排序。\n\nRANK() OVER 跳跃排序，如果有两个第一级别时，接下来是第三级别。\n\nDENSE_RANK() OVER<!--more-->\n&nbsp;&nbsp;连续排序，如果有两个第一级别时，接下来是第二级别。\n\n比如：在每个departmentId下按salary从高到低排序的dense rank（如果有并列第一名，接下来还是第二名）\n\n```\nselect name as Employee,salary as Salary,departmentId,DENSE_RANK() over(partition by departmentId order by salary desc) as rk from Employee\n\n```\n\n参考：[hive取不同班级前三名问题](https://blog.csdn.net/qq_35884447/article/details/90054307)\n\nleetcode例子：[185. 部门工资前三高的所有员工](https://leetcode.cn/problems/department-top-three-salaries/)\n\n&nbsp;\n","tags":["Hive"]},{"title":"Spring MVC学习笔记——给Controller和视图传值","url":"/Spring MVC学习笔记——给Controller和视图传值.html","content":"**　　一、给Controller传值，值将显示在控制台**\n\n　　**1.第一种：使用@RequestParam，改HelloController.java**\n\n```\n//RequestMapping表示用哪一个url来对应\n\t@RequestMapping({\"/hello\",\"/\"})\n\tpublic String hello(@RequestParam(\"username\") String username){\n\t\tSystem.out.println(\"hello\");\n\t\tSystem.out.println(username);\n\t\treturn \"hello\";\n\t}\n\n```\n\n<!--more-->\n&nbsp;　　然后在浏览器中**输入请求**\n\n```\nhttp://localhost:8080/SpringMVC_hello/hello?username=123\n\n```\n\n&nbsp;　　**控制台**可以看到传的值\n\n<img src=\"/images/517519-20170101175958320-883641978.png\" alt=\"\" width=\"390\" height=\"83\" />\n\n　　但是使用了RequestParam，如果**在请求中不传值的话，会报400错误**，因为默认把参数作为了地址的一部分\n\n**　　2.第二种，把RequestParam删除，直接String username**\n\n```\n\t//RequestMapping表示用哪一个url来对应\n\t@RequestMapping({\"/hello\",\"/\"})\n\tpublic String hello(String username){\n\t\tSystem.out.println(\"hello\");\n\t\tSystem.out.println(username);\n\t\treturn \"hello\";\n\t}\n\n```\n\n&nbsp;　　这种可以不传值，不传值时候为null\n\n<img src=\"/images/517519-20170101180206195-294000579.png\" alt=\"\" width=\"408\" height=\"94\" />\n\n**　　二、再从Controller给视图传值**\n\n**　　1.用Map来传值**\n\n**　　修改HelloController.java文件，用Map<String,Object> context，不过不太建议用Map**\n\n```\npackage org.common.controller;\n\nimport java.util.Map;\n\nimport org.springframework.stereotype.Controller;\nimport org.springframework.web.bind.annotation.RequestMapping;\n\n@Controller\npublic class HelloController {\n\t\n\t//RequestMapping表示用哪一个url来对应，从Controller给视图传值\n\t@RequestMapping({\"/hello\",\"/\"})\n\tpublic String hello(String username,Map<String,Object> context){\n\t\tcontext.put(\"username\", username);\t\t//从Controller给视图传值\n\t\tSystem.out.println(username);\n\t\tSystem.out.println(\"hello\");\n\t\treturn \"hello\";\n\t}\n\t\n}\n\n```\n\n**　　hello.jsp文件**\n\n```\n<%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\"\n    pageEncoding=\"UTF-8\"%>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<html>\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n<title>Insert title here</title>\n</head>\n<body>\n\t<h1>hello!! ${username}!!</h1>\n</body>\n</html>\n\n```\n\n**&nbsp;输入**\n\n```\nhttp://localhost:8080/SpringMVC_hello/hello?username=123\n\n```\n\n&nbsp;<img src=\"/images/517519-20170101184854945-625741429.png\" alt=\"\" width=\"296\" height=\"207\" />\n\n&nbsp;**　　2.用Model来传值**\n\n**　　<strong>HelloController.java文件改为**</strong>\n\n```\npublic String hello(String username,Model model){\n\tmodel.addAttribute(\"String\", username);\t\t//使用Model从Controller给视图传值\n\n```\n\n&nbsp;　　或者**addAttribute只有一个参数**，这时是**默认使用username的类型（String）作为参数**\n\n```\n//等于model.addAttribute(\"String\",username);\nmodel.addAttribute(username);\n\n```\n\n&nbsp;<img src=\"/images/517519-20170101185832257-340528645.png\" alt=\"\" width=\"312\" height=\"243\" />\n\n　　**完整文件**\n\n```\npackage org.common.controller;\n\nimport org.springframework.stereotype.Controller;\nimport org.springframework.ui.Model;\nimport org.springframework.web.bind.annotation.RequestMapping;\n\n@Controller\npublic class HelloController {\n\t\n\t//RequestMapping表示用哪一个url来对应\n\t@RequestMapping({\"/hello\",\"/\"}) \n\tpublic String hello(String username,Model model){\n\t\tSystem.out.println(\"hello\");\n\t\tmodel.addAttribute(\"username\", username);\n\t\t\n\t\t//等于model.addAttribute(\"String\",username);\n\t\tmodel.addAttribute(username);\n\t\t//model.addAttribute(new User());等于model.addAttribute(\"user\",new User());\n\t\tSystem.out.println(username);\n\t\treturn \"hello\";\n\t}\n\t\n\t@RequestMapping(\"/welcome\")\n\tpublic String welcome(){\n\t\tSystem.out.println(\"welcome\");\n\t\treturn \"welcome\";\n\t}\n\t\n}\n\n```\n\n**&nbsp;　　hello.jsp**\n\n```\n<%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\"\n    pageEncoding=\"UTF-8\"%>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<html>\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n<title>Insert title here</title>\n</head>\n<body>\n\t<h1>hello!! ${username}!!</h1>\n\t${string}\n</body>\n</html>\n\n```\n\n&nbsp;\n\n注解式控制器参考：\n\n<img src=\"/images/517519-20160524205412600-156356202.png\" width=\"500\" height=\"520\" />\n\n<img src=\"/images/517519-20160524205456881-1790345206.png\" width=\"500\" height=\"310\" />\n\n<img src=\"/images/517519-20160524205532647-1120220824.png\" width=\"500\" height=\"164\" />\n\n<img src=\"/images/517519-20160524210958538-813348020.png\" width=\"450\" height=\"159\" />\n\n<img src=\"/images/517519-20160524211019147-609173778.png\" width=\"350\" height=\"95\" />\n\n&nbsp;\n","tags":["SpringMVC"]},{"title":"给ubuntu的docky添加可以直接打开的图标","url":"/给ubuntu的docky添加可以直接打开的图标.html","content":"在/usr/share/applications和/usr/share/app-install/desktop寻找需要的图标，没有就自己做一个\n\n**eclipse的图标**\n\n```\n[Desktop Entry]\n\nVersion=1.0\n\nName=eclipse\n\nExec=/home/common/software/eclipse/eclipse\n\nTerminal=false\n\nIcon=/home/common/software/eclipse/eclipse.png\n\nType=Application\n\nCategories=Development\n\n```\n\n**<!--more-->\n&nbsp;sublimetext的图标**\n\n**为了使得sublime能修改root的文件，安装gksu，然后在Exec前面加上gksu **\n\n```\n[Desktop Entry]\nName=SublimeText2\nType=Application\nTerminal=false\nComment=Edit text files\nExec=/home/common/software/SublimeText2/sublime_text %F\nIcon=/home/common/software/SublimeText2/sublime_text.png\n\n```\n\n&nbsp;注意末尾不要有空格\n\n**&nbsp;docky添加的图标的目录在**\n\n```\n~/.local/share/applications\n\n```\n\n&nbsp;\n","tags":["Linux"]},{"title":"英文面试词汇","url":"/英文面试词汇.html","content":"1.递归调用：recursive call\n\n2.回溯法：backtricking method\n\n3.动态规划：dynamic programming\n\n4.定义一个函数：define a function\n\n5.初始化含量：initialize<!--more-->\n&nbsp;a&nbsp;variable called ...\n\n6.贪心法：greedy&nbsp;method\n\n7.初始值：initial&nbsp;value\n\n8.入栈和出栈：When&nbsp;it&nbsp;is&nbsp;an&nbsp;operator,&nbsp;pop&nbsp;two&nbsp;Numbers&nbsp;from&nbsp;the&nbsp;stack,&nbsp;do&nbsp;the&nbsp;calculation,&nbsp;and push&nbsp;back&nbsp;the result\n\n9.定义一个类：I'm&nbsp;going&nbsp;to&nbsp;define&nbsp;a&nbsp;class\n\n10.成员变量：member variables\n\n11.参数：parameters/arguments\n\n12.颠倒字母而成的字：Anagrams\n\n13.回文：Palindrome\n\n14.计算xxx的平方：calculate the square of xxx\n\n15.正数和负数：positive and negative\n\n16.时间复杂度和空间复杂度：time&nbsp;complexity and&nbsp;space&nbsp;complexity\n\n17.并发和多线程：concurrency and multi-threading\n\n18.实现接口：implement interface\n\n19.构造函数：constructor\n\n20.括号：圆括号parenthesis；方括号brackets；花括号brace\n\n21.返回值：return a value\n\n22.遍历数组：iterate/loop/go through&nbsp;the&nbsp;array\n\n23.指针：pointer\n\n24.进位：carry\n\n25.个位和十位：ones，tens\n\n26.深度优先遍历/广度优先遍历：performs a&nbsp;depth-first/breadth-first walk&nbsp;of&nbsp;the&nbsp;input&nbsp;tree\n\n27.相邻元素：adjacent&nbsp;elements\n\n28.邻接矩阵：Adjacency&nbsp;matrix\n\n29.矩阵右上角：top right corner of the matrix\n\n30.全排列：permutations\n\n31.分号：SemiColon；\n\n32.末尾零：trailing zeros；前导零：leading zero\n\n33.阶乘：factorial\n\n34.被除数a：dividend、除数b：divisor a/b\n\n35.周长：perimeter\n\n35.水平地：**horizontally**、竖直地：**vertically**\n\n36.X的3次方：X to the power of 3\" 或者 \"X cubed\"；10的平方：ten squared\n\n37.\n","tags":["杂谈"]},{"title":"Hive学习笔记——常用SQL","url":"/Hive学习笔记——常用SQL.html","content":"## 1.查询第二高的值\n\n输入：Salary表\n\n```\n+-------------+------+\n| Column Name | Type |\n+-------------+------+\n| id          | int  |\n| salary      | int  |\n+-------------+------+\n```\n\n使用limit+offset语法来限制结果数量，其中 limit N,1 等于 limit 1 offset N\n\n```\nselect (select DISTINCT Salary from Employee order by Salary DESC limit 1 offset 1) as SecondHighestSalary\n\n```\n\n输出：\n\n```\n+---------------------+\n| SecondHighestSalary |\n+---------------------+\n| 200                 |\n+---------------------+\n```\n\n## 2.查询连续出现3次的数字\n\n输入：Logs表\n\n```\n+----+-----+\n| id | num |\n+----+-----+\n| 1  | 1   |\n| 2  | 1   |\n| 3  | 1   |\n| 4  | 2   |\n| 5  | 1   |\n| 6  | 2   |\n| 7  | 2   |\n+----+-----+\n```\n\n查询表3次，然后通过where条件来筛选\n\n```\nSELECT DISTINCT(t1.Num) as ConsecutiveNums\nFROM Logs t1, Logs t2, Logs t3 \nWHERE\n    t1.Id = t2.Id - 1\n    AND t2.Id = t3.Id - 1\n    AND t1.Num = t2.Num\n    AND t2.Num = t3.Num\n;\n\n```\n\n输出：\n\n```\n+-----------------+\n| ConsecutiveNums |\n+-----------------+\n| 1               |\n+-----------------+\n```\n\n## 3.查询每个部门下最高的值\n\n输入：Employee表和Department表\n\n```\nEmployee 表:\n+----+-------+--------+--------------+\n| id | name  | salary | departmentId |\n+----+-------+--------+--------------+\n| 1  | Joe   | 70000  | 1            |\n| 2  | Jim   | 90000  | 1            |\n| 3  | Henry | 80000  | 2            |\n| 4  | Sam   | 60000  | 2            |\n| 5  | Max   | 90000  | 1            |\n+----+-------+--------+--------------+\nDepartment 表:\n+----+-------+\n| id | name  |\n+----+-------+\n| 1  | IT    |\n| 2  | Sales |\n+----+-------+\n```\n\n先用group by+max算出每个departmentId下最多的salary，然后用where+in来进行过滤\n\n```\nselect t2.Name as Department,t1.Name as Employee,Salary  from (\n(select Name, DepartmentId,Salary from Employee) t1\nleft join \n(select Id,Name from Department) t2\non t1.DepartmentId = t2.Id\n)\nWHERE\n(t2.Id,t1.Salary)\nin\n(\n(select DepartmentId, MAX(Salary) from Employee\ngroup by DepartmentId)\n)\n\n```\n\n## 4.将分数转换成排名\n\n输入：Scores表\n\n```\n+----+-------+\n| id | score |\n+----+-------+\n| 1  | 3.50  |\n| 2  | 3.65  |\n| 3  | 4.00  |\n| 4  | 3.85  |\n| 5  | 4.00  |\n| 6  | 3.65  |\n+----+-------+\n```\n\n```\n# t2表中有多少个大于t1当前score的\nselect Score, \n(select count(distinct(Score)) from Scores t1 where t1.Score > t2.Score ) +1 as `Rank` \nfrom Scores t2\norder by Score DESC;\n\n```\n\n去重后的scores表中，比scores表的每一行大的有多少个\n\n输出：\n\n```\n+-------+------+\n| score | rank |\n+-------+------+\n| 4.00  | 1    |\n| 4.00  | 1    |\n| 3.85  | 2    |\n| 3.65  | 3    |\n| 3.65  | 3    |\n| 3.50  | 4    |\n+-------+------+\n```\n\n<!--more-->\n&nbsp;\n","tags":["Hive"]},{"title":"Flink学习笔记——窗口","url":"/Flink学习笔记——窗口.html","content":"Flink窗口（window）可以用于keyed streams和non-keyed streams\n\n参考官方文档：[https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/datastream/operators/windows/](https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/datastream/operators/windows/)\n\n## 1.一些概念\n\n### 1.时间语义\n\nflink支持3种时间语义，分别是Event time，Processing time和Ingestion time。\n\n参考：[https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/learn-flink/streaming_analytics/](https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/learn-flink/streaming_analytics/)\n\n#### 1.Event time\n\nEvent time的时间戳是每个单独事件在其产生设备上发生的时间。Event time通常在record进入Flink之前就嵌入到record中，这样就可以从每条record中提取事件时间戳。在Event time中，时间的进程取决于数据，而不是任何时钟。Event time程序必须指定如何生成Event time watermarks，这是一种以事件时间表示进度的机制。\n\n在理想情况下，Event time处理将产生完全一致和确定的结果，而不管事件何时到达或它们的顺序如何。然而，除非事件是按顺序到达的(按时间戳)，否则事件时间处理在等待乱序事件时会产生一些延迟。因为只能等待一段有限的时间，这就限制了Event time应用程序的确定性。\n\n#### 2.Processing time\n\nProcessing time是指执行相应operator的机器的系统时间。\n\n当流处理程序在Porcessing time上运行时，所有基于时间的操作(如时间窗口)将使用运行相应操作的机器的系统时钟。每小时的处理时间窗口将包括在系统时钟显示整个小时的时间之间到达特定操作员的所有记录。例如，如果应用程序在上午9点15分开始运行，第一个小时处理时间窗口将包括上午9点15分到10点之间处理的事件，下一个窗口将包括上午10点到11点之间处理的事件，以此类推。<!--more-->\n&nbsp;\n\nProcessing time是最简单的时间概念，不需要流和机器之间的协调。它提供了最好的性能和最低的延迟。然而，在分布式和异步环境中，处理时间不提供确定性，因为它容易受到记录到达系统中的速度(例如从消息队列)、记录在系统内部操作符之间流动的速度以及中断(计划或其他)的影响。\n\n#### 3.Ingestion time\n\nFlink在采集事件时记录的时间戳\n\n<img src=\"/images/517519-20231015165103138-778287363.png\" width=\"500\" height=\"237\" loading=\"lazy\" />\n\n参考：[https://nightlies.apache.org/flink/flink-docs-master/zh/docs/concepts/time/#notions-of-time-event-time-and-processing-time](https://nightlies.apache.org/flink/flink-docs-master/zh/docs/concepts/time/#notions-of-time-event-time-and-processing-time)\n\n### 2.watermarks水印\n\nwatermarks是一种用于在event time中衡量进度的机制。watermarks作为data stream的一部门，带有一个时间戳t，watermark(t)表示在这个数据流中，event time已经到了t时刻，或者说在数据流中不会再有时间戳小于t时刻的元素\n\n参考官方文档：[Event Time and Watermarks](https://nightlies.apache.org/flink/flink-docs-master/zh/docs/concepts/time/#event-time-and-watermarks)\n\n### 3.Lateness延迟\n\n某些元素可能会违反watermarks条件，也就是说，即使在watermark(t)发生之后，仍然会出现更多时间戳为t ' <= t的元素。事实上，在现实世界的许多设置中，某些元素可以任意延迟，因此不可能指定某个事件时间戳中所有元素的发生时间。此外，即使延迟时间是有限制的，但如果延迟太久，往往也会造成评估事件时间窗口的延迟。 \n\n因此，流程序可能会显式地期望一些迟到的元素。晚到元素是指在系统的事件时钟(由watermark表示)已经超过晚到元素的时间之后到达的元素。\n\n#### 允许延迟\n\n在使用 **event-time** 窗口时，数据可能会迟到，即 Flink 用来追踪 event-time 进展的 watermark 已经 越过了窗口结束的 timestamp 后，数据才到达。对于 Flink 如何处理 event time， [event time](https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/datastream/event-time/generating_watermarks/) 和 [late elements](https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/datastream/event-time/generating_watermarks/#late-elements) 有更详细的探讨。\n\n默认情况下，watermark 一旦越过窗口结束的 timestamp，迟到的数据就会被直接丢弃。 但是 Flink 允许指定窗口算子最大的 **allowed lateness**。 Allowed lateness 定义了一个元素可以在迟到多长时间的情况下不被丢弃，这个参数默认是 0。 在 watermark 超过窗口末端、到达窗口末端加上 allowed lateness 之前的这段时间内到达的元素， 依旧会被加入窗口。取决于窗口的 trigger，一个迟到但没有被丢弃的元素可能会再次触发窗口，比如 `EventTimeTrigger`。\n\n为了实现这个功能，Flink 会将窗口状态保存到 allowed lateness 超时才会将窗口及其状态删除 （如 [Window Lifecycle](https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/datastream/operators/windows/#window-lifecycle) 所述）。\n\n默认情况下，allowed lateness 被设为 `0`。即 watermark 之后到达的元素会被丢弃。\n\n参考：[https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/datastream/operators/windows/#allowed-lateness](https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/datastream/operators/windows/#allowed-lateness)\n\n## 2.窗口window\n\nflink window有4个比较重要的组件：\n\n- assigner（分配器）：如何将元素分配给窗口\n- function（计算函数）：为窗口定义的计算：其实是一个计算函数，完成窗口内容的计算。\n- triger（触发器）：在什么条件下触发窗口的计算\n- evictor（退出器）：定义从窗口中移除数据\n\n参考：[看完这篇再说你会用Flink的Window](https://zhuanlan.zhihu.com/p/338038730)\n\n### 1.分配器（Assigner）\n\nflink内置了几种window assigner，比如滚动窗口（Tumbling Windows），滑动窗口（Sliding Windows），会话窗口（Session Windows），全局窗口（Global Windows）。\n\n参考：[https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/datastream/operators/windows/#window-assigners](https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/datastream/operators/windows/#window-assigners)\n\n#### 1.滚动窗口（Tumbling Windows）\n\n滚动窗口的 assigner 分发元素到指定大小的窗口。滚动窗口的大小是固定的，且各自范围之间不重叠。 比如说，如果你指定了滚动窗口的大小为 5 分钟，那么每 5 分钟就会有一个窗口被计算，且一个新的窗口被创建。\n\n#### 2.滑动窗口（Sliding Windows）\n\n与滚动窗口类似，滑动窗口的 assigner 分发元素到指定大小的窗口，窗口大小通过 **window size** 参数设置。 滑动窗口需要一个额外的滑动距离（**window slide**）参数来控制生成新窗口的频率。 因此，如果 slide 小于窗口大小，滑动窗口可以允许窗口重叠。这种情况下，一个元素可能会被分发到多个窗口。\n\n比如说，你设置了大小为 10 分钟，滑动距离 5 分钟的窗口，你会在每 5 分钟得到一个新的窗口， 里面包含之前 10 分钟到达的数据。\n\n#### 3.会话窗口（Session Windows）\n\n会话窗口的 assigner 会把数据按活跃的会话分组。 与**滚动窗口**和**滑动窗口**不同，会话窗口不会相互重叠，且没有固定的开始或结束时间。 会话窗口在一段时间没有收到数据之后会关闭，即在一段不活跃的间隔之后。 会话窗口的 assigner 可以设置固定的会话间隔（session gap）或 用 **session gap extractor** 函数来动态地定义多长时间算作不活跃。 当超出了不活跃的时间段，当前的会话就会关闭，并且将接下来的数据分发到新的会话窗口。\n\n#### 4.全局窗口（Global Windows）\n\n全局窗口的 assigner 将拥有相同 key 的所有数据分发到一个**全局窗口**。 这样的窗口模式仅在你指定了自定义的 [trigger](https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/datastream/operators/windows/#triggers) 时有用。 否则，计算不会发生，因为全局窗口没有天然的终点去触发其中积累的数据。\n\n<img src=\"/images/517519-20231015202402379-691148748.png\" width=\"700\" height=\"311\" loading=\"lazy\" />\n\n### 2.窗口函数（Function）\n\n定义了 window assigner 之后，我们需要指定当窗口触发之后，我们如何计算每个窗口中的数据， 这就是 **window function** 的职责了。关于窗口如何触发，详见 [triggers](https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/datastream/operators/windows/#triggers)。\n\n窗口函数有三种：`ReduceFunction`、`AggregateFunction` 或 `ProcessWindowFunction`。 前两者执行起来更高效（详见 [State Size](https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/datastream/operators/windows/#%e5%85%b3%e4%ba%8e%e7%8a%b6%e6%80%81%e5%a4%a7%e5%b0%8f%e7%9a%84%e8%80%83%e9%87%8f)）因为 Flink 可以在每条数据到达窗口后 进行增量聚合（incrementally aggregate）。 而 `ProcessWindowFunction` 会得到能够遍历当前窗口内所有数据的 `Iterable`，以及关于这个窗口的 meta-information。\n\n使用 `ProcessWindowFunction` 的窗口转换操作没有其他两种函数高效，因为 Flink 在窗口触发前必须缓存里面的**所有**数据。 `ProcessWindowFunction` 可以与 `ReduceFunction` 或 `AggregateFunction` 合并来提高效率。 这样做既可以增量聚合窗口内的数据，又可以从 `ProcessWindowFunction` 接收窗口的 metadata。\n\n参考：[https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/datastream/operators/windows/#%e7%aa%97%e5%8f%a3%e5%87%bd%e6%95%b0window-functions](https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/datastream/operators/windows/#%e7%aa%97%e5%8f%a3%e5%87%bd%e6%95%b0window-functions)\n\n#### 1.ReduceFunction\n\n`ReduceFunction` 指定两条输入数据如何合并起来产生一条输出数据，输入和输出数据的类型必须相同。 Flink 使用 `ReduceFunction` 对窗口中的数据进行增量聚合。\n\n#### 2.AggregateFunction\n\n`ReduceFunction` 是 `AggregateFunction` 的特殊情况。 `AggregateFunction` 接收三个类型：输入数据的类型(`IN`)、累加器的类型（`ACC`）和输出数据的类型（`OUT`）。 输入数据的类型是输入流的元素类型，`AggregateFunction` 接口有如下几个方法： 把每一条元素加进累加器、创建初始累加器、合并两个累加器、从累加器中提取输出（`OUT` 类型）。\n\n与 `ReduceFunction` 相同，Flink 会在输入数据到达窗口时直接进行增量聚合。\n\n#### 3.ProcessWindowFunction\n\nProcessWindowFunction 有能获取包含窗口内所有元素的 Iterable， 以及用来获取时间和状态信息的 Context 对象，比其他窗口函数更加灵活。 ProcessWindowFunction 的灵活性是以性能和资源消耗为代价的， 因为窗口中的数据无法被增量聚合，而需要在窗口触发前缓存所有数据。\n\n### 3.触发器（Triggers）\n\nTrigger决定了一个窗口（由 **window assigner** 定义）何时可以被 **window function** 处理。 每个 `WindowAssigner` 都有一个默认的 `Trigger`。 如果默认 trigger 无法满足你的需要，你可以在 `trigger(...)` 调用中指定自定义的 trigger。\n\n参考：[https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/datastream/operators/windows/#triggers](https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/datastream/operators/windows/#triggers)\n\n1.Event-time window assigner 都默认使用 EventTimeTrigger。 这个 trigger 会在 watermark 越过窗口结束时间后直接触发。ProcessingTimeTrigger 根据 processing time 触发。\n\n2.GlobalWindow 的默认 trigger 是永远不会触发的 NeverTrigger。因此，使用 GlobalWindow 时，你必须自己定义一个 trigger。\n\n3.CountTrigger 在窗口中的元素超过预设的限制时触发。\n\n4.PurgingTrigger 接收另一个 trigger 并将它转换成一个会清理数据的 trigger。\n\n### 4.退出器（Evictors）\n\nFlink 的窗口模型允许在 `WindowAssigner` 和 `Trigger` 之外指定可选的 `Evictor`。 如本文开篇的代码中所示，通过 `evictor(...)` 方法传入 `Evictor`。 Evictor 可以在 trigger 触发后、调用窗口函数之前或之后从窗口中删除元素。\n","tags":["Flink"]},{"title":"Flink学习笔记——checkpoint","url":"/Flink学习笔记——checkpoint.html","content":"## 1.开启checkpoint\n\n默认情况下checkpoint是禁用的，需要手动进行开启，如下\n\n```\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nStreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);\nenv.getCheckpointConfig().setCheckpointTimeout(3600000); // checkpoint1小时超时\nenv.enableCheckpointing(180000, CheckpointingMode.EXACTLY_ONCE); // 3分钟做一次checkpoint\nenv.getCheckpointConfig().setMaxConcurrentCheckpoints(1); // 同一时间只允许一个 checkpoint 进行\nenv.getCheckpointConfig().setTolerableCheckpointFailureNumber(2); // 允许两个连续的 checkpoint 错误\nenv.setStateBackend(new EmbeddedRocksDBStateBackend(true)); // 使用rocksdb state backend\nenv.getCheckpointConfig().setCheckpointStorage(\"s3://xxxx/xxxx/test_checkpoint\"); // 指定gcheckpoint路径\nenv.getCheckpointConfig().setExternalizedCheckpointCleanup(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION); // 使用 externalized checkpoints，这样 checkpoint 在作业取消后仍就会被保留\n\n```\n\n其他配置参考官方文档\n\n```\nhttps://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/dev/datastream/fault-tolerance/checkpointing/\n```\n\n## 2.从保留的 checkpoint 中恢复状态[<br />](https://nightlies.apache.org/flink/flink-docs-master/zh/docs/ops/state/checkpoints/#%e4%bb%8e%e4%bf%9d%e7%95%99%e7%9a%84-checkpoint-%e4%b8%ad%e6%81%a2%e5%a4%8d%e7%8a%b6%e6%80%81)\n\n```\nbin/flink run -s checkpoint_path\n\n```\n\n参考：https://nightlies.apache.org/flink/flink-docs-master/zh/docs/ops/state/checkpoints/\n\n## 3.checkpoint相关问题\n\n做checkpoint很慢问题排查，参考：[2022年最新版 | Flink经典线上问题小盘点](https://cloud.tencent.com/developer/article/1954930)\n\n增量checkpoint，参考：[Flink 管理大型状态之增量 Checkpoint](https://cloud.tencent.com/developer/article/1853214)\n\n大状态与 Checkpoint 调优，参考：[大状态与 Checkpoint 调优](https://nightlies.apache.org/flink/flink-docs-master/zh/docs/ops/state/large_state_tuning/)\n\n<!--more-->\n&nbsp;\n\n## **4.checkpoint原理**\n\nCheckpoint 由 CheckpointCoordinator 发起、确认，通过Rpc 通知 Taskmanager 的具体算子完成 Checkpoint 操作，参考：[Flink Checkpoint 流程](https://www.cnblogs.com/Springmoon-venn/p/13565208.html)，具体步骤如下\n\n```\n1、CheckpointCoordicator tirgger checkpoint 到 source\n2、Source\n　　1、生成并广播 CheckpointBarrier\t\n　　2、Snapshot state（完成后 ack Checkpoint 到 CheckpointCoordicator）\n3、Map\t\n　　1、接收到 CheckpointBarrier\t\n　　2、广播 CheckpointBarrier\t\n　　3、Snapshot state（完成后 ack Checkpoint 到 CheckpointCoordicator）\n4、Sink\t\n　　1、接收到 CheckpointBarrier\t\n　　2、Snapshot state（完成后 ack Checkpoint 到 CheckpointCoordicator）\n5、CheckpointCoordicator 接收到 所有 ack\t\n　　1、给所有算子发 notifyCheckpointComplete\n6、Source、Map、Sink 收到 notifyCheckpointComplete\n\n```\n\n详细步骤参考：[Checkpoint 原理剖析与应用实践](https://blog.csdn.net/hhhhhhfq/article/details/123852724)\n\n## 5.checkpoint和savepoint区别\n|Savepoint|Checkpoint\n|Savepoint是由命令触发,&nbsp;由用户创建和删除|Checkpoint被保存在用户指定的外部路径中,&nbsp;flink自动触发\n\nCheckpoint被保存在用户指定的外部路径中,&nbsp;flink自动触发\n|保存点存储在标准格式存储中，并且可以升级作业版本并可以更改其配置。|当作业失败或被取消时，将保留外部存储的检查点。\n\n当作业失败或被取消时，将保留外部存储的检查点。\n|用户必须提供用于还原作业状态的保存点的路径。|用户必须提供用于还原作业状态的检查点的路径。\n\n用户必须提供用于还原作业状态的检查点的路径。\n\n&nbsp;\n","tags":["Flink"]},{"title":"Spring MVC学习笔记——Welcome","url":"/Spring MVC学习笔记——Welcome.html","content":"## **1**.SpringMVC概念\n\n<img src=\"/images/517519-20160522171559310-742313146.png\" alt=\"\" width=\"776\" height=\"473\" />\n\n## **2.**整个**springmvc运行的流程**\n\n<img src=\"/images/517519-20160523192955647-1252667134.png\" alt=\"\" width=\"511\" height=\"243\" /> <img src=\"/images/517519-20160523193914772-647441162.png\" alt=\"\" width=\"578\" height=\"274\" />\n\n　　请求--><1>Dispatcher servlet(controller控制器)--><2>HandlerMapping(这个在配置文件中写,可以通过这个找到url该给哪个具体的controller来处理)\n\n　　　　　　　　　　　　　　　　　　　　　　　--><3>controller(具体的控制器)--><4>Model and logical viewname(具体的控制器再将值传给dispatcher servlet)\n\n　　　　　　　　　　　　　　　　　　　　　　　--><5>view resolver(通过这来找到对应的视图)\n\n　　　　　　　　　　　　　　　　　　　　　　　--><6>view\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20160523200922975-667439447.png\" width=\"600\" height=\"415\" />\n\n## **3.创建SpringMVC项目**\n\n在eclipse下建立一个**动态web项目springmvc_hello**，记得勾选**web.xml**的生成按钮 \n\n<img src=\"/images/517519-20170101145851539-1801774476.png\" alt=\"\" width=\"373\" height=\"454\" /> <img src=\"/images/517519-20170101145944117-1445767473.png\" alt=\"\" width=\"360\" height=\"148\" /> <img src=\"/images/517519-20170101150018789-1361196820.png\" alt=\"\" width=\"212\" height=\"71\" />\n\n在生成的项目的**WEB-INF的lib文件夹**下面**导jar包**\n\n**　　要<strong>开发一个Welcome项目需要导两个包:**</strong>\n\n**　　一个是<strong>springmvc的包**，另一个是**Apache的commonslogging的包**</strong>\n\n　　两个包的官方下载地址分别是\n\n　　http://maven.springframework.org/release/org/springframework/spring/3.2.0.RELEASE/\n\n　　和http://commons.apache.org/ (bin的那个包，不是源代码包)\n\n**<img src=\"/images/517519-20170101150201836-1010782293.png\" alt=\"\" width=\"144\" height=\"225\" />**\n\n**1.在导包了之后，写web.xml，每一个应用有一个web.xml文件,发送请求之后,会在mapping里面寻找.html或者.jsp等,然后找到相应的hello,然后通过hello去Servlet的定义里面查找Servlet-name是hello的Servlet服务，即找到hello-servlet.xml（在配置文件web.xml写HandlerMapping）**\n\n**<img src=\"/images/517519-20160523215255631-1881170053.png\" alt=\"\" width=\"576\" height=\"290\" />**\n\n　　　　**web.xml启动Dispatcher Servlet**，写web.xml的时候把生成时候里面的display-name和welcome-file-list删除后开始写\n\n&nbsp;　　　　**注意**：<servlet-name>hello</servlet-name>和**hello-servlet.xml**的名字是对应的\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<web-app xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.jcp.org/xml/ns/javaee\" xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd\" id=\"WebApp_ID\" version=\"3.1\">\n    <!-- 配置DispatchServlet，截获特定的URL请求 -->\n    <!-- 默认自动加载/WEB-INF/simpleSpringMVC-servlet.xml -->\n    <!-- （即<servlet-name>-servlet.xml）的Spring配置文件，启动web层的Spring容器 -->\n  <servlet>\n        <servlet-name>hello</servlet-name>\n        <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>\n        <load-on-startup>1</load-on-startup>\n    </servlet>\n    <servlet-mapping>\n        <servlet-name>hello</servlet-name>\n        <url-pattern>/</url-pattern>\n    </servlet-mapping>\n    \n</web-app>\n\n```\n\n**2.然后写hello-servlet.xml，这个文件是写springmvc的配置文件，这个就是具体的控制器，在这里如果请求是/hello.html,则可以找到HelloController这个类，在这个类中处理过后，又向控制器返回视图，然后控制器**\n\n在WEB-INF文件夹下面新建Spring Bean Configuration File文件,要建立这个文件必须安装spring的插件,然后选中beans context和mvc这三个选项\n\n没有spring插件的话,从spring framework reference.html这个帮助文档中寻找例子\n\n&nbsp;<img src=\"/images/517519-20170101150929945-152675211.png\" alt=\"\" />\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n    xmlns:context=\"http://www.springframework.org/schema/context\"\n    xmlns:mvc=\"http://www.springframework.org/schema/mvc\"\n    xsi:schemaLocation=\"http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-3.1.xsd\n        http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.1.xsd\">\n    \n    <!-- /hello.html对应控制器HelloController -->\n    <bean name=\"/hello.html\" class=\"org.common.controller.HelloController\"></bean>\n    <!-- InternalResourceViewResolver视图的映射关系，还有其他很多视图 -->\n    <bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\">\n        <property name=\"prefix\" value=\"/WEB-INF/jsp/\"></property>\n        <property name=\"suffix\" value=\".jsp\"></property>\n    </bean>\n    \n    \n</beans>\n\n```\n\n**3.在src文件夹下建包org.common.controller，然后建HelloController类**\n\n**这个类必须继承org.springframework.web.servlet.mvc.AbstractController（注意是第二个，不是Portlet）**\n\n**在类中做处理，然后返回视图给控制器**\n\n**<img src=\"/images/517519-20170101151412414-1960068609.png\" alt=\"\" width=\"538\" height=\"431\" />**\n\n&nbsp;\n\n```\npackage org.common.controller;\n\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\n\nimport org.springframework.web.servlet.ModelAndView;\nimport org.springframework.web.servlet.mvc.AbstractController;\n\npublic class HelloController extends AbstractController {\n\n\t@Override\n\tprotected ModelAndView handleRequestInternal(HttpServletRequest arg0, HttpServletResponse arg1) throws Exception {\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"hello\");\t\t\t\t\t\t\t\t//可以输出一句话\n        return new ModelAndView(\"hello\");\t\t\t//返回一个视图hello，在hello-servelet.xml写对应的映射hello.jsp\n\t}\n\n}\n\n```\n\n**4.在WEB-INF文件夹下面的jsp文件夹里面的hello.jsp文件**\n\n```\n<%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\"\n    pageEncoding=\"UTF-8\"%>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<html>\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n<title>Insert title here</title>\n</head>\n<body>\n<h1>Welcome!!</h1>\n</body>\n</html>\n\n```\n\n**最后访问**&nbsp; http://localhost:8080/SpringMVC_hello/hello.html\n\n&nbsp;<img src=\"/images/517519-20170101153619492-562863150.png\" alt=\"\" width=\"473\" height=\"318\" />\n\n但是，上面使用的**java文件**中的**handler mapping**一般都不用\n\n```\nprotected ModelAndView handleRequestInternal(HttpServletRequest arg0, HttpServletResponse arg1) throws Exception {\n\n```\n\n&nbsp;一般使用的是**defaultAnnotationHandleMapping**\n\n&nbsp;**使用annotation的HelloWord，只需要在原来的hello-servlet.xml文件中加入**\n\n```\n<context:component-scan base-package=\"org.common.controller\"></context:component-scan>\n<mvc:annotation-driven></mvc:annotation-driven>\n\n```\n\n**去掉**\n\n```\n<bean name=\"/hello.html\" class=\"org.common.controller.HelloController\"></bean>\n\n```\n\n**　　1.hello-servlet.xml文件**\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n    xmlns:context=\"http://www.springframework.org/schema/context\"\n    xmlns:mvc=\"http://www.springframework.org/schema/mvc\"\n    xsi:schemaLocation=\"http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-3.1.xsd\n        http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.1.xsd\">\n    \n    <!-- 使用defaultAnnotationHandleMapping -->\n    <context:component-scan base-package=\"org.common.controller\"></context:component-scan>\n\t<mvc:annotation-driven></mvc:annotation-driven>\n    <!-- /hello.html对应控制器HelloController -->\n    <!--  <bean name=\"/hello.html\" class=\"org.common.controller.HelloController\"></bean>-->\n    <!-- InternalResourceViewResolver视图的映射关系，还有其他很多视图 -->\n    <bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\">\n        <property name=\"prefix\" value=\"/WEB-INF/jsp/\"></property>\n        <property name=\"suffix\" value=\".jsp\"></property>\n    </bean>\n    \n    \n</beans>\n\n```\n\n**接下来就能使用Annotation来配置控制器，完全修改<strong>HelloController.java**文件，加上@Controller注入</strong>\n\n**&nbsp;　　2.HelloController.java文件**\n\n```\npackage org.common.controller;\n\nimport org.springframework.stereotype.Controller;\nimport org.springframework.web.bind.annotation.RequestMapping;\n\n@Controller\npublic class HelloController {\n\t\n\t//RequestMapping表示用哪一个url来对应\n\t@RequestMapping({\"/hello\",\"/\"})\n\tpublic String hello(){\n\t\tSystem.out.println(\"hello\");\n\t\treturn \"hello\";\n\t}\n\t\n\t@RequestMapping(\"/welcome\")\n\tpublic String welcome(){\n\t\tSystem.out.println(\"welcome\");\n\t\treturn \"welcome\";\n\t}\n\t\n}\n\n```\n\n**&nbsp;　　3.在jsp文件夹下面新建welcome.jsp文件**\n\n**<img src=\"/images/517519-20170101160306773-2116561021.png\" alt=\"\" width=\"265\" height=\"142\" />　　<img src=\"/images/517519-20170101160317711-1367862679.png\" alt=\"\" width=\"205\" height=\"138\" />　　<img src=\"/images/517519-20170101160329898-446024784.png\" alt=\"\" width=\"221\" height=\"136\" /><br />**\n\n**参考:**\n\nhttp://blog.csdn.net/hehexiaoyou/article/details/23747617\n\nhttp://www.codingyun.com/article/47.html\n","tags":["SpringMVC"]},{"title":"Flink学习笔记——统一的source服务","url":"/Flink学习笔记——统一的source服务.html","content":"为了方便使用Flink对流式数据进行统一的读写，需要开发统一的source服务\n\n1. kafka source\n\n需要可配置的参数，参考flume的kafka source配置参数\n\n```\nhttps://flume.apache.org/FlumeUserGuide.html#kafka-source\n\n```\n\n定义KafkaSourceConstants，参考：\n\n```\nhttps://github.com/apache/flume/blob/trunk/flume-ng-sources/flume-kafka-source/src/main/java/org/apache/flume/source/kafka/KafkaSourceConstants.java\n\n```\n\n比如对于kafka source相关的配置，统一的前缀为 kafka.source.，比如\n\n```\n# kafka source\nkafka.source.bootstrap.servers=localhost:9092\nkafka.source.topics=thrift_log_test\nkafka.source.group.id=test\n\n```\n\n参考文章：[Flink学习笔记&mdash;&mdash;配置](https://www.cnblogs.com/tonglin0325/p/14115069.html)，对kafka source前缀的配置进行读取\n\n<!--more-->\n&nbsp;\n","tags":["Flink"]},{"title":"Java多线程——延迟队列DelayQueue","url":"/Java多线程——延迟队列DelayQueue.html","content":"`DelayQueue`是一个基于优先队列（`PriorityQueue`）实现的阻塞队列（`BlockingQueue`），队列中的消息的优先级是根据消息的TTL来决定的。\n\n参考：[Java延迟队列DelayQueue](https://biteeniu.github.io/java/java-delay-queue/)\n\n使用延迟队列实现定时任务调度器\n\n参考：[https://soulmachine.gitbooks.io/system-design/content/cn/task-scheduler.html](https://soulmachine.gitbooks.io/system-design/content/cn/task-scheduler.html)\n","tags":["多线程"]},{"title":"Spring MVC学习笔记——POJO和DispatcherServlet","url":"/Spring MVC学习笔记——POJO和DispatcherServlet.html","content":"使用P**OJO**名称是为了避免和**[EJB(Enterprise ](http://baike.baidu.com/view/3542.htm)[JavaBean](http://baike.baidu.com/view/28155.htm)[)](http://baike.baidu.com/view/3542.htm)**混淆起来, 而且简称比较直接. 其中有**一些属性**及其**getter setter方法的类**,**没有业务逻辑**，有时可以作为[VO](http://baike.baidu.com/view/1097386.htm)(value -object)或[dto](http://baike.baidu.com/view/160599.htm)(Data Transform Object)来使用.当然,如果你有一个简单的运算属性也是可以的,但不允许有业务方法,也**不能携带有connection之类的方法**\n\n<!--more-->\n&nbsp;\n\n## POJO与javabean的区别\n\n## DispatcherServlet作用\n\nDispatcherServlet是前端控制器设计模式的实现,提供Spring Web MVC的集中访问点,而且负责职责的分派,而且与Spring IoC容器无缝集成,从而可以获得Spring的所有好处。\n\n**在web.xnl文件中配置DispatchServlet**\n\n&nbsp;\n\n**DispatcherServlet主要用作职责调度工作,本身主要用于控制流程,主要职责如下：**\n\n**　　1、文件上传解析,如果请求类型是multipart将通过MultipartResolver进行文件上传解析;<br />　　2、通过HandlerMapping,将请求映射到处理器(返回一个 HandlerExecutionChain ,它包括一个处理器、多个HandlerInterceptor拦截器);<br />　　3、通过HandlerAdapter支持多种类型的处理器( HandlerExecutionChain中的处理器);**\n\n**　　4、通过ViewResolver解析逻辑视图名到具体视图实现;<br />　　5、本地化解析;<br />　　6、渲染具体的视图等;<br />　　7、如果执行过程中遇到异常将交给HandlerExceptionResolver来解析。**\n\n&nbsp;\n\n## 上下文关系，也是在web.xml中配置\n\n集成Web环境的通用配置:\n\n```\n\t<!-- Spring集成WEB环境的通用配置，一般用于加载除Web层的Bean(如DAO、Service等),以便于与其他任何Web框架集成 -->\n\t<!-- Spring 的监听器可以通过这个上下文参数来获取beans.xml的位置 -->\n\t<context-param>\n\t\t<param-name>contextConfigLocation</param-name>\n\t\t<param-value>\n\t\t\tclasspath:beans.xml\n\t\t</param-value>\n\t</context-param>\n\t<!-- 创建Spring的监听器 -->\n\t<listener>\n\t\t<listener-class>org.springframework.web.context.ContextLoaderListener</listener-class>\n\t</listener>\n\n```\n\n如上配置是**Spring集成Web环境的通用配置**;一般用于加载除Web层的Bean(如**DAO、Service等)**,以便于与其他任何Web框架集成。\n\n&nbsp;\n\n**重要的接口和类的简单说明：**\n\n**DispatcherServlet**：**前端控制器，用于接收请求**。\n\n**HandlerMapping接口**：**用于处理请求的映射**。\n\n　　DefaultAnnotationHandlerMapping：HandlerMapping接口的实现，用于把一个URL映射到具体的Controller类上。\n\n**HandlerAdapter接口**：**用于处理请求的映射**。\n\n　　AnnotationMethodHandlerAdapter：HandlerAdapter接口的试下，用于把一个URL映射到对应Controller类的某个方法上。\n\n**ViewResolver接口**：**用于解析View**。\n\n　　InternalResourceViewResolver：ViewResolver接口的实现，用于把ModelAndView的逻辑视图名解析为具体的View。\n\n&nbsp;\n\n**参考阿里巴巴Java开发手册**\n\n**<strong>应用分层：**</strong>\n\n<img src=\"/images/517519-20210629105955816-565378213.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n**分层领域模型规约：**\n\nDO/PO/entity（ Data Object）：与数据库表结构一一对应，通过DAO层向上传输数据源对象。\n\nDTO（ Data Transfer Object）：数据传输对象，Service或Manager向外传输的对象。\n\nBO（ Business Object）：业务对象。 由Service层输出的封装业务逻辑的对象。\n\nAO（ Application Object）：应用对象。 在Web层与Service层之间抽象的复用对象模型，极为贴近展示层，复用度不高。\n\nVO（ View Object）：显示层对象，通常是Web向模板渲染引擎层传输的对象。\n\nPOJO（ Plain Ordinary Java Object）：在本手册中， POJO专指只有setter/getter/toString的简单类，包括DO/DTO/BO/VO等。\n\nQuery：数据查询对象，各层接收上层的查询请求。 注意超过2个参数的查询封装，禁止使用Map类来传输。\n\n&nbsp;\n\n参考\n\n```\nhttps://zhuanlan.zhihu.com/p/102389552\n\n```\n\n**VO（Value Object）值对象**\n\nVO就是展示用的数据，不管展示方式是网页，还是客户端，还是APP，只要是这个东西是让人看到的，这就叫VO\n\nVO主要的存在形式就是js里面的对象（也可以简单理解成json）&nbsp;\n\nVO是可以第一个优化掉的，展示业务不复杂的可以压根儿不要，直接用DTO\n\n&nbsp;\n\n**PO（Persistant Object）持久对象**\n\n简单说PO就是数据库中的记录，一个PO的数据结构对应着库中表的结构，表中的一条记录就是一个PO对象\n\n通常PO里面除了get，set之外没有别的方法\n\n对于PO来说，数量是相对固定的，一定不会超过数据库表的数量\n\n等同于Entity，这俩概念是一致的\n\n&nbsp;\n\n**BO（Business Object）业务对象**\n\nBO就是PO的组合\n\n简单的例子比如说PO是一条交易记录，BO是一个人全部的交易记录集合对象\n\n复杂点儿的例子**PO1**是交易记录，**PO2**是登录记录，**PO3**是商品浏览记录，**PO4**是添加购物车记录，**PO5**是搜索记录，**BO**是个人网站行为对象\n\nBO是一个业务对象，一类业务就会对应一个BO，数量上没有限制，而且BO会有很多业务操作，也就是说除了get，set方法以外，BO会有很多针对自身数据进行计算的方法\n\n为什么BO也画成横跨两层呢？原因是现在很多持久层框架自身就提供了数据组合的功能，因此BO有可能是在业务层由业务来拼装PO而成，也有可能是在数据库访问层由框架直接生成\n\n很多情况下为了追求查询的效率，框架跳过PO直接生成BO的情况非常普遍，PO只是用来增删改使用\n\n&nbsp;\n\n**BO和DTO的区别**\n\n这两个的区别主要是：BO进行字段的删减，得到DTO\n\nBO对内，为了进行业务计算需要辅助数据，或者是一个业务有多个对外的接口，BO可能会含有很多接口对外所不需要的数据，因此DTO需要在BO的基础上，删减一些字段，只要自己需要的数据，然后对外提供\n\n在这个关系上，通常不会有数据内容的变化，内容变化要么在BO内部业务计算的时候完成，要么在解释VO的时候完成\n\n一些工具类的系统和一些业务不是很复杂的系统DTO是可以和BO合并成一个，当业务扩展的时候注意拆分就行\n\n&nbsp;\n\n**VO和DTO的区别**\n\n主要有两个区别\n\n一个是字段不一样，VO根据需要会删减一些字段\n\n另一个是值不一样，VO会根据需要对DTO中的值进行展示业务的解释\n\n举个简单的例子\n\nDTO可能是这样的\n\n```\n{\n    \"gender\":\"男\", \n    \"age\":35 \n} \n\n```\n\n对于业务一来说只需要性别，而且因为是一个古风聊天室，也不能直接展示男，因此经过业务解释业务一的VO是\n\n```\n{ \n    \"gender\":\"公子\" \n} \n\n```\n\n对于业务二来说只需要年龄，而且不需要精确的年龄，因此经过业务解释业务二的VO是\n\n```\n{ \n    \"age\":\"30~39\" \n} \n\n```\n\n&nbsp;\n\n**DAO设计模式**\n\n<img src=\"/images/517519-20160507230121500-1325508727.png\" alt=\"\" width=\"597\" height=\"250\" />\n\n<img src=\"/images/517519-20160507230215359-2121756476.png\" alt=\"\" width=\"636\" height=\"857\" />\n","tags":["SpringMVC"]},{"title":"使用JConsole排查Java线程死锁","url":"/使用JConsole排查Java线程死锁.html","content":"1.添加JMX端口\n\n```\nexport JMX_PORT=xxx\n\n```\n\n然后启动java进程\n\n<!--more-->\n&nbsp;\n\n2.命令行启动 jconsole，并连接remote JMX端口\n\n<img src=\"/images/517519-20220621164452296-1899428108.png\" width=\"700\" height=\"588\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["JVM"]},{"title":"K8S学习笔记——容器类型","url":"/K8S学习笔记——容器类型.html","content":"目前在Kubernetes 1.18 version中，已经支持4种不同类型的容器，分别为：**标准容器（主容器）、Sidecar容器、Init 容器以及Ephemeral 容器**。\n\n1、标准容器<br />标准容器是Kubernetes中最常见的容器类型，它们是Pod中的核心组件，用于运行应用程序或服务。标准容器使用Docker或containerd等容器运行时创建，并具有完整的操作系统隔离环境。这些容器包含了应用程序的运行时和代码，并且可以与宿主机的网络和存储系统进行交互。标准容器提供了运行应用程序所需的所有环境，并允许通过资源限制和策略来控制容器的行为。<br />2、Sidecar容器<br />Sidecar容器是一种特殊的容器，通常与主应用程序容器一起部署在同一Pod中。Sidecar容器用于提供额外的功能或扩展主应用程序容器的功能。例如，Sidecar容器可以用于提供日志收集、监控、网络代理或与外部服务的通信等功能。通过将Sidecar容器与主应用程序容器部署在同一Pod中，可以实现应用程序的可观察性、可扩展性和可靠性。<br />3、Init容器<br />Init容器是用于执行一次性任务的特殊容器，通常在Pod中的其他容器启动之前运行。这些任务可能包括设置环境变量、预加载数据或执行一次性的系统任务等。Init容器的运行顺序是在同一Pod中的所有其他容器之前，并且一旦完成它们的任务，它们就会退出。通过使用Init容器，可以在Pod初始化时执行必要的设置和配置任务。<br />4、Ephemeral容器<br />Ephemeral容器是一种临时性的容器，与标准容器的永久性相反。Ephemeral容器的生命周期非常短，通常只运行一次任务然后就退出。这种类型的容器非常适合执行一次性任务，如数据迁移、批处理作业或清理任务等。由于Ephemeral容器没有持久化的状态，因此它们不会对宿主机造成持久性的负担。但是，由于它们的短暂生命周期，它们不适合用于需要持续运行的应用程序或服务。\n","tags":["k8s"]},{"title":"JavaScript学习笔记——HTTP请求","url":"/JavaScript学习笔记——HTTP请求.html","content":"前端发起请求可以使用Ajax（使用XMLHttpRequest），fetch（基于Promise），Axios（基于Promise，内部使用 XMLHttpRequest 对象来实现发起和处理网络请求）等工具\n\n可以使用浏览器的开发者工具来查看前端的请求是xhr还是fetch，比如访问google网站，我们可以在发起者这一栏查看是xhr还是fetch\n\n<img src=\"/images/517519-20240328001815058-1376985998.png\" width=\"1000\" height=\"106\" />\n\n还可以使用 -cause:fetch 对请求进行过滤\n\n<img src=\"/images/517519-20240328002116897-1403685704.png\" width=\"300\" height=\"107\" />\n\n参考：[前端开发常用的几种请求方式](https://juejin.cn/post/6930903529712254989)\n\n## 1.Ajax\n\n**AJAX**（Asynchronous JavaScript and XML） 是一种使用 XMLHttpRequest 对象进行异步通信的技术，可以在不刷新整个页面的情况下更新部分网页内容。\n\n<img src=\"/images/517519-20160514200611218-211412239.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514200636687-64537236.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514200720077-2122398102.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514200754484-873038411.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514200816015-53076797.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514200907859-6480930.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514200929234-415649770.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514200950343-917257132.png\" alt=\"\" />\n\n## 2.fetch\n\nFetch 是用于发起网络请求的现代 API，它提供了一种更简洁和强大的方式来处理网络请求。\n\nFetch API 基于 Promise，使用起来更加简洁和直观，支持链式调用和流式处理响应数据。\n\nFetch API 更加灵活，同时支持请求和响应对象的处理，但在某些方面仍有一些不足之处，比如不能原生支持请求的取消。\n\n参考：[jq／fetch／axios／vue-resource／fly 对比一下主流的http库](https://liangyuqi.github.io/2018/04/20/fetch-axios/)\n\n## 3.Axios\n\nAxios 是一个流行的基于 Promise 的 HTTP 客户端，可以用于浏览器和 Node.js 环境。它使得在前端和后端与服务器进行 HTTP 数据交互变得更加简单和方便。\n\n以下是 Axios 的一些主要特点和优点：\n\n<li>\n**Promise 支持**：Axios 是基于 Promise 的，允许你以一种更优雅和便捷的方式处理异步请求和响应。\n</li>\n<li>\n**浏览器和 Node.js 兼容**：Axios 可以在浏览器和 Node.js 环境中运行，这使得它成为一个通用的 HTTP 客户端解决方案。\n</li>\n<li>\n**易用性**：Axios 提供了简洁一致的API，使用起来相对容易理解和学习。\n</li>\n<li>\n**拦截器支持**：可以通过拦截器在请求或响应被处理前对它们进行拦截、转换或进行其他操作。\n</li>\n<li>\n**取消请求**：Axios 提供了取消请求的功能，可以中断正在进行的请求。\n</li>\n<li>\n**客户端端与服务器端的转换**：Axios 自动将 JSON 数据进行转换，简化了数据交互的过程。\n</li>\n<li>\n**错误处理**：Axios提供了灵活的错误处理机制，可以很容易地捕获和处理请求或响应中的错误。\n</li>\n\n使用 Axios，可以发送各种类型的 HTTP 请求，如 GET、POST、PUT、DELETE 等，并且可以很容易地设置请求头、请求参数、认证信息等。\n\n```\n// 导入 Axios\nconst axios = require('axios');\n\n// 发送 GET 请求\naxios.get('https://api.example.com/data')\n  .then(response => {\n    console.log(response.data);\n  })\n  .catch(error => {\n    console.error(error);\n  });\n\n// 发送 POST 请求\naxios.post('https://api.example.com/data', { name: 'John Doe' })\n  .then(response => {\n    console.log(response.data);\n  })\n  .catch(error => {\n    console.error(error);\n  });\n\n```\n\n参考：[硬核知识点&mdash;&mdash;浏览器中的三类五种请求](https://www.51cto.com/article/631880.html)\n\n## 3.使用油猴脚本拦截前端请求\n\n以访问google.com网站为例，编写如下油猴脚本\n\n```\n// ==UserScript==\n// @name         New Userscript\n// @namespace    http://tampermonkey.net/\n// @version      2024-03-28\n// @description  try to take over the world!\n// @author       You\n// @match        https://*.google.com/*\n// @grant        none\n// ==/UserScript==\n\n(function() {\n    'use strict';\n\n    // xhr\n    const originOpen = XMLHttpRequest.prototype.open;\n    XMLHttpRequest.prototype.open = function(method, url) {\n\n        originOpen.apply(this, arguments); // 调用原始的 open 方法\n        console.log(\"xhr => \" + method + \" \" + url);\n    };\n\n    // fetch\n    const originalFetch = window.fetch;\n    window.fetch = function(url, options) {\n\n        console.log(\"fetch => \" + url);\n        return originalFetch.apply(this, arguments);\n    };\n\n})();\n\n```\n\n成功在控制台打印出了使用xhr和fetch API请求的url地址\n\n<img src=\"/images/517519-20240331154813098-1443753757.png\" alt=\"\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n","tags":["前端"]},{"title":"Java数据库——使用数据库连接池","url":"/Java数据库——使用数据库连接池.html","content":"<img src=\"/images/517519-20160514112231905-1422823522.png\" alt=\"\" width=\"660\" height=\"583\" />\n\n<img src=\"/images/517519-20160514112826843-1030143836.png\" alt=\"\" width=\"655\" height=\"554\" />\n\n<img src=\"/images/517519-20160514122043390-1014064991.png\" alt=\"\" width=\"656\" height=\"344\" />\n\n**server.xml配置数据帐号和密码等**\n\n<img src=\"/images/517519-20160514122114843-171861718.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514122241280-1179789037.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514122305405-1392988959.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514122327343-1876086295.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514122348187-1262021029.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n","tags":["JDBC"]},{"title":"rsync使用教程","url":"/rsync使用教程.html","content":"rsync和scp类似，可以用于在机器之间的数据拷贝\n\n<!--more-->\n&nbsp;\n\n&nbsp;\n\n[Rsync同步文件时如何排除指定文件夹或多个目录](https://www.linuxrumen.com/rmxx/1426.html)\n\n[What are the differences between the rsync delete options?](https://superuser.com/questions/156664/what-are-the-differences-between-the-rsync-delete-options)\n","tags":["Linux"]},{"title":"广告系统——广告形式分类","url":"/广告系统——广告形式分类.html","content":"广告按形式进行分类，可以分成：\n\n**1.激励广告（rewarded ad）**\n\n主要使用场景是小游戏，工具类的应用，比如观看激励视频广告后获得复活机会，工具免费使用的次数等\n\n**2.插屏广告（Interstitial ad）**\n\n主要使用场景是在页面切换的时候，比如应用开屏广告，界面切换广告\n\n**3.原生广告（native ad）**\n\n主要使用场景是信息流。这种广告与周围的内容完全融合，使其看起来像是发布内容的一部分，提供更好的用户体验。原生广告可以出现在文章、新闻、应用程序或社交媒体中。\n\n**4.横幅广告（banner ad）**\n\n主要使用场景通常是应用顶部、底部、栏目或频道间隔，表现形式多为纯图片、纯文字或者图片+文字\n\n参考：[常见广告形式与场景入门](https://www.adtiming.com/itiming/cn/article01.html)\n\n<!--more-->\n&nbsp;\n\n还有其他的类型，比如再营销广告等\n\n再营销广告的目的主要有2个：\n\n1、**再互动**（沉睡用户唤醒）：用户设备上仍装有相关应用，这时用户与再营销广告互动后打开该应用\n\n2、**重装激活**（流失用户召回，即再营销重装激活）：用户设备上已没有相关应用（即用户已卸载该应用），这时用户与再营销广告互动，促使其再次安装并激活该应用\n","tags":["广告系统"]},{"title":"Starrocks学习笔记","url":"/Starrocks学习笔记.html","content":"## 1.使用docker部署环境\n\n```\ndocker run -p 9030:9030 -p 8030:8030 -p 8040:8040 -itd starrocks/allin1-ubuntu\n\n```\n\n参考：[https://docs.starrocks.io/docs/quick_start/deploy_with_docker/](https://docs.starrocks.io/docs/quick_start/deploy_with_docker/)\n\n## 2.查询\n\n由于starrocks兼容mysql协议，所以可以使用mysql client连接starrocks\n\n<!--more-->\n&nbsp;\n\n或者使用datagrip来连接starrocks\n\n<img src=\"/images/517519-20240124235921706-1752116774.png\" width=\"700\" height=\"476\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["Doris"]},{"title":"CDH5.16 Cloudera Manager启用TLS","url":"/CDH5.16 Cloudera Manager启用TLS.html","content":"在集群中启用了认证和鉴权之后，Cloudera Manager Server 将会在网络中和集群的节点传输敏感信息，比如keytab和密码。为了加密这些数据，所以必须在Cloudera Manager Server和所有集群节点之间配置使用TLS加密。\n\nTLS 加密还用于使用 HTTPS 保护与 Cloudera Manager 管理界面的客户端连接。\n\n在CDH启用了kerberos之后，在Cloudera Manager界面中会有安全警告要求至少添加一级TLS加密\n\n<img src=\"/images/517519-20220721104753408-969673592.png\" width=\"800\" height=\"170\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n&nbsp;\n\n参考官方文档：[Configuring TLS Encryption for Cloudera Manager](https://docs.cloudera.com/documentation/enterprise/5-16-x/topics/how_to_configure_cm_tls.html)\n\n[0638-6.1.0-Cloudera Manager配置TLS](https://cloud.tencent.com/developer/article/1430883)\n\n[Cloudera Manager配置TLS加密](https://blog.csdn.net/weixin_43215250/article/details/83307894)\n\n[大数据平台部署------CDH启用TLS加密传输](https://blog.csdn.net/sinat_32176947/article/details/79605542)\n\n&nbsp;\n\n1.配置环境\n\n```\nexport JAVA_HOME=/usr/java/jdk1.8.0_121\n\n```\n\n2.创建目录\n\n```\nsudo mkdir -p /opt/cloudera/security/pki \n\n```\n\n3.生成JKS和CSR\n\n生成Java秘钥库（Java keystore）\n\n切记使用相同的keystore password 和 key password。Cloudera Manager不支持为密钥和密钥库使用不同的密码。\n\n```\nroot@master:/opt/cloudera/security/pki# $JAVA_HOME/bin/keytool -genkeypair -alias $(hostname -f) -keyalg RSA -keystore /opt/cloudera/security/pki/$(hostname -f).jks -keysize 2048 -dname \"CN=$(hostname -f),OU=Engineering,O=Cloudera,L=Palo Alto,ST=California,C=US\" -ext san=dns:$(hostname -f)\nEnter keystore password:\nRe-enter new password:\nEnter key password for <master>\n        (RETURN if same as keystore password):\nRe-enter new password:\n\n```\n\n生成证书签名请求（CSR）\n\n```\nroot@master:/opt/cloudera/security/pki# $JAVA_HOME/bin/keytool -certreq -alias $(hostname -f) -keystore /opt/cloudera/security/pki/$(hostname -f).jks -file /opt/cloudera/security/pki/$(hostname -f).csr -ext san=dns:$(hostname -f) -ext EKU=serverAuth,clientAuth\nEnter keystore password:\nroot@master:/opt/cloudera/security/pki# ls\nmaster.csr  master.jks\n\n```\n\n4.提交CSR给CA，获得数字签名证书，这里采用openssl生成\n\n生成私钥key，带密码\n\n```\nroot@master:/opt/cloudera/security/pki# openssl genrsa -des3 -out ca.key 2048\nGenerating RSA private key, 2048 bit long modulus\n..................................+++\n......................................................................................................+++\ne is 65537 (0x10001)\nEnter pass phrase for ca.key:\nVerifying - Enter pass phrase for ca.key:\n\n```\n\n生成证书请求文件，csr文件\n\n这个填写的信息需要和生成Java秘钥库的保持一致\n\n```\nroot@master:/opt/cloudera/security/pki# openssl req -new -key ca.key -out ca.csr\nEnter pass phrase for ca.key:\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [AU]:US\nState or Province Name (full name) [Some-State]:California\nLocality Name (eg, city) []:Palo Alto\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:Cloudera\nOrganizational Unit Name (eg, section) []:Engineering\nCommon Name (e.g. server FQDN or YOUR name) []:master\nEmail Address []:\n\nPlease enter the following 'extra' attributes\nto be sent with your certificate request\nA challenge password []:\nAn optional company name []:\n\n```\n\n用自己的私钥给自己签发根证书，生成crt文件\n\n```\nroot@master:/opt/cloudera/security/pki# openssl x509 -req -days 3650 -in ca.csr -signkey ca.key -out ca.crt\nSignature ok\nsubject=/C=US/ST=California/L=Palo Alto/O=Cloudera/OU=Engineering/CN=master\nGetting Private key\nEnter pass phrase for ca.key:\n\n```\n\n用CA根证书来签名服务器端的证书请求文件，生成pem文件\n\n参考：[OpenSSL生成并使用CA根证书签名Keytool生成的证书请求](https://blog.csdn.net/weixin_43215250/article/details/83347146)\n\n```\nroot@master:/opt/cloudera/security/pki# openssl ca -days 3650 -keyfile ca.key -cert ca.crt -in master.csr -out master.pem\nUsing configuration from /usr/lib/ssl/openssl.cnf\nEnter pass phrase for ca.key:\nCheck that the request matches the signature\nSignature ok\nCertificate Details:\n        Serial Number: 1 (0x1)\n        Validity\n            Not Before: Jul 19 06:03:33 2022 GMT\n            Not After : Jul 16 06:03:33 2032 GMT\n        Subject:\n            countryName               = US\n            stateOrProvinceName       = California\n            organizationName          = Cloudera\n            organizationalUnitName    = Engineering\n            commonName                = master\n        X509v3 extensions:\n            X509v3 Basic Constraints:\n                CA:FALSE\n            Netscape Comment:\n                OpenSSL Generated Certificate\n            X509v3 Subject Key Identifier:\n                7F:4E:EC:8E:6D:44:9F:E2:63:65:6A:DC:86:A8:1B:35:20:AB:63:89\n            X509v3 Authority Key Identifier:\n                DirName:/C=US/ST=California/L=Palo Alto/O=Cloudera/OU=Engineering/CN=master\n                serial:86:33:3F:52:47:19:6A:66\n\nCertificate is to be certified until Jul 16 06:03:33 2032 GMT (3650 days)\nSign the certificate? [y/n]:y\n\n\n1 out of 1 certificate requests certified, commit? [y/n]y\nWrite out database with 1 new entries\nData Base Updated\n\n```\n\n创建的时候报错提示需要创建文件夹和文件\n\n```\nmkdir -p ./demoCA/newcerts\ntouch ./demoCA/index.txt\necho \"01\" > ./demoCA/serial\n\n```\n\n5.获得签署的证书之后，将其放到以下路径\n\n```\n/opt/cloudera/security/pki/$(hostname -f).pem\n\n```\n\n6.检查签名证书以验证服务器和客户端身份验证选项是否存在，以及使用者备用名称\n\n```\nopenssl x509 -in /opt/cloudera/security/pki/$(hostname -f).pem -noout -text\n\n```\n\n7.CA证书复制&nbsp;\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n","tags":["CDH"]},{"title":"ubuntu16.04下生成证书crt和秘钥key","url":"/ubuntu16.04下生成证书crt和秘钥key.html","content":"生成key文件\n\n```\nopenssl genrsa -des3 -out dummy.key 2048\n\n```\n\n生成pem文件\n\n```\nopenssl rsa -inform PEM -outform PEM -in dummy.key -pubout -out dummy-nopass.pem\n\n```\n\n　　\n","tags":["计算机网络"]},{"title":"广告系统——专业用词","url":"/广告系统——专业用词.html","content":"## 1.投放相关\n\n1.CPI<!--more-->\n&nbsp;（Cost per install）获客成本以及渠道是否有效的衡量指标\n\n2.oCPM（Optimized Cost per Mille的缩写），即优化千次**展现**出价，本质还是按照cpm付费。采用更精准的点击率和转化率预估机制，将广告展现给最容易产生转化的用户，在获取流量的同时，提高转化率、降低转化成本，跑量提速更快。\n\n3.oCPC（Optimized Cost per Click的缩写），即优化**点击**付费，本质还是按照cpc付费。采用更科学的转化率预估机制的准确性，可帮助广告主在获取更多优质流 量的同时提高转化完成率。系统会在广告主出价基础上，基于多维度、实时反馈及历史积累的海量数据，并根据预估的转化率以及竞争环境智能化的动态调整出价，进而优化广告排序，帮助广告主竞得最适合的流量，并降低转化成本。\n\n4.oCPA（Optimized Cost per Action的缩写），即优化**行为**出价，本质还是按照cpa付费。当广告主在广告投放流程中选定特定的优化目标（例如：移动应用的**激活**，网站的**下单**），提供愿意为此投放目标而支付的平均价格，并及时、准确回传效果数据，我们将借助转化预估模型，实时预估每一次点击对广告主的转化价值，自动出价，最终按照点击扣费；同时，我们的转化预估模型会根据广告主的广告转化数据不断自动优化。\n\n参考：[一文详解oCPA、oCPM是什么](https://zhuanlan.zhihu.com/p/429752248)\n\n5.tCPA（Target Cost per Action的缩写），即**目标**每次**转化**出价。tCPA 是一种设定**目标成本**的出价策略，广告主设置一个目标每次转化费用，系统会努力在这个目标成本范围内优化转化。\n\n6.oCPX（Optimized Cost per X），oCPX 是一种针对效果广告的智能出价投放方式，**广告主选择明确的优化目标**（如下载、激活、注册、付费），并给出期望的转化成本，系统通过机器学习预估每一次投放机会的转化概率，并结合期望成本，自动出价，保障成本效果稳定。\n\n参考：[QCon-oCPX多目标多场景联合建模在OPPO的实践](https://juejin.cn/post/7047329123904585759)\n\n7.ROAS（Return on AD Spending，广告支出回报）：ROAS 出价策略基于**广告支出回报率**进行优化，目标是最大化广告投资回报。**ROAS = (可归因至广告的收入 / 广告成本) x 100**\n\n8.定向：在哪些流量上打广告\n\n9.出价：用什么钱去竞价\n\n10.创意：给用户展示什么内容\n\n\n\n|广告定价模型|优点|缺点|适用场景\n| ---- | ---- | ---- | ---- \n|CPC|适合以点击量为目标的广告活动。广告主只需为用户的点击付费。|点击不一定会带来转化，可能会有无效点击。|增加网站流量或应用下载。需要提高品牌知名度并吸引用户互动。\n\n广告主只需为用户的点击付费。\n\n需要提高品牌知名度并吸引用户互动。\n|CPM|适用于品牌曝光和广泛受众的广告投放|效果不确定|**品牌推广**：当品牌希望提高知名度和曝光率时，CPM 是一种有效的广告计费方式。**大规模活动**：如新品发布、促销活动等，需要在短时间内达到大量受众。**媒体购买**：一些高流量媒体或广告网络倾向于使用 CPM 模式，广告主需要适应这种计费方式。\n\n**品牌推广**：当品牌希望提高知名度和曝光率时，CPM 是一种有效的广告计费方式。\n\n**媒体购买**：一些高流量媒体或广告网络倾向于使用 CPM 模式，广告主需要适应这种计费方式。\n|oCPC|在 CPC 的基础上，系统会自动优化，将广告展示给更可能点击并转化的用户。提高点击和转化的质量。|比普通 CPC 模式略贵，但能带来更高质量的点击。|希望不仅增加点击量，还能提高转化率。有明确的转化目标，如应用安装、注册等。\n\n提高点击和转化的质量。\n\n有明确的转化目标，如应用安装、注册等。\n|oCPM|适合品牌推广和提高曝光率。广告主按展示次数付费，能确保广告被更多人看到。|展示不保证点击或转化，可能导致较低的投资回报率。|品牌知名度提升。想要最大化广告展示次数，特别是在大型活动或新品发布时。\n\n广告主按展示次数付费，能确保广告被更多人看到。\n\n品牌知名度提升。\n|oCPA|专注于优化每次转化的成本，系统会根据转化数据和用户行为自动调整出价。提高转化质量，通过精准投放吸引更有可能完成转化的用户。|可能需要较高的预算来收集足够的数据进行优化。|明确的转化目标，如应用安装、购买或注册等。广告主希望自动优化广告出价，提高转化率，而不特别关注单次转化成本。**成本控制较为灵活，系统更关注转化数量或价值的最大化。**\n\n提高转化质量，通过精准投放吸引更有可能完成转化的用户。\n\n广告主希望自动优化广告出价，提高转化率，而不特别关注单次转化成本。\n|tCPA|**固定目标成本**：广告主设置一个明确的目标每次转化费用，系统会根据这个目标进行优化。**成本控制**：系统会努力使每次转化的成本接近广告主设定的目标，提供更好的成本控制。|**需要足够数据支持**：初期可能需要较高的预算来收集数据，以便系统进行优化。&nbsp;|&nbsp;广告主希望严格控制每次转化的成本，同时提高转化率。适用于需要精确预算管理和成本控制的广告活动。**严格控制每次转化成本，更适合有明确预算目标的广告活动。**\n\n**成本控制**：系统会努力使每次转化的成本接近广告主设定的目标，提供更好的成本控制。\n\n适用于需要精确预算管理和成本控制的广告活动。\n|oCPX|- **多样化的目标优化**：<ul><li>**灵活性**：oCPx 可以根据不同的广告目标进行优化，如点击、观看视频、添加购物车、注册、购买等。这使广告主能够根据具体的业务需求进行精细化操作。- **阶段优化**：适用于希望在用户转化路径的不同阶段进行优化的广告活动，可以针对用户行为路径中的关键步骤进行优化。</li>- **自动化和智能化**：<li>**智能出价**：系统利用机器学习算法，根据用户行为和转化数据自动调整出价，以实现最佳效果。- **实时调整**：系统能够根据实时数据不断优化投放策略，提高广告效果。</li>- **提高转化率**：<li>**精准投放**：通过分析用户行为数据，系统能够更精准地投放广告，吸引更有可能完成特定行为的用户，提高整体转化率。</li>- **节省时间和精力**：<li>**自动化管理**：广告主无需手动设置每次行为的出价，系统会自动优化，减少了广告管理的复杂度和工作量。</li></ul>|- **需要足够的数据**：<ul><li>**数据依赖性**：oCPx 模型的效果依赖于足够的数据支持，尤其是在广告活动初期，可能需要较高的预算来收集数据进行优化。</li>- **预算控制**：<li>**成本波动**：由于系统自动调整出价，广告主对每次行为的实际成本控制相对较弱，可能会出现成本波动。</li>- **初期效果波动**：<li>**学习期**：在广告活动初期，系统需要一定的时间来学习和优化，效果可能会有波动。广告主需要有耐心，等待系统优化效果的显现。</li>- **复杂度**：<li>**多目标优化**：如果广告活动包含多个优化目标，可能需要更多的策略和监控来确保每个目标都能达到预期效果。</li></ul>|广告主希望优化特定的用户行为，而不仅仅是最终转化。这些行为可能是转化漏斗中的关键步骤，如点击广告、观看视频、添加购物车等。\n\n- **智能出价**：系统利用机器学习算法，根据用户行为和转化数据自动调整出价，以实现最佳效果。\n- **实时调整**：系统能够根据实时数据不断优化投放策略，提高广告效果。\n\n**提高转化率**：\n\n- **自动化管理**：广告主无需手动设置每次行为的出价，系统会自动优化，减少了广告管理的复杂度和工作量。\n\n**预算控制**：\n\n- **成本波动**：由于系统自动调整出价，广告主对每次行为的实际成本控制相对较弱，可能会出现成本波动。\n\n**复杂度**：\n\n- **多目标优化**：如果广告活动包含多个优化目标，可能需要更多的策略和监控来确保每个目标都能达到预期效果。\n|enhanced CPC|&nbsp;|&nbsp;|&nbsp;\n|ROAS|根据实际销售额和广告支出优化，直接提高投资回报。适用于电商和直接销售型广告活动。|需要准确的销售和广告支出数据。可能需要较长的优化时间。|以销售额和投资回报为主要目标的广告活动。\n\n适用于电商和直接销售型广告活动。\n\n可能需要较长的优化时间。\n|Maximize Conversions|&nbsp;|&nbsp;|&nbsp;\n|MaximizeClick|&nbsp;|&nbsp;|&nbsp;\n\nClick\n\n&nbsp;\n\n## 2.收入相关\n\n1.LTV（生命周期总价值，意为客户终生价值）LTV = LT * ARPU（如果用户LT（平均生命周期）是3个月，ARPU（平均用户收入）是10元/月，那么LTV = 3 * 10 = 30元）\n\n2.ARPU是指&ldquo;每用户平均收入&rdquo;，而且通常以&ldquo;月&rdquo;为单位。因此，其计算方法是每月收入/MAU（月活跃用户人数\n\n3.ARPDAU（Average Revenue Per Daily Active User）：平均每个活跃用户的收入\n\n4.ARPPU (average revenue per paying user)：平均每个付费用户的收入\n\n5.eCPM：**（effective cost per mile）**，千次展示收益，是媒体衡量自己广告投产效率的指标，它是指每千次广告的曝光，能够给媒体带来多少的广告收入。这个值对媒体越大越好。eCPM=CPC&times;CTR&times;1000\n\n6.CPC（Cost Per Click）：点击价值，即单次点击为广告产品带来的收益\n\n7.eCPC（effective Cost Per Click）：平均每次点击获得的收入\n\n8.CPA (Cost Per Action): 按转化计费（平均转化价格）\n\n9.CPV （Cost Per View）：按展示计费，独立IP展示一次就计费一次。有效播放指的是视频广告的观看时间大于等于3秒或广告被完整播放。CPV计费方式适用于视频广告。\n\n10.ROI： 投资回报率（ROI）是指通过投资而应返回的价值，它涵盖了企业的获利目标。利润和投入的经营所必备的财产相关，因为管理人员必须通过投资和现有财产获得利润。又称会计收益率、投资利润率。\n\n11.CPT(Cost Per Time)：主要通过时间进行收费，多数以包月，星期进行计算\n\n12.CPS（按照实际销售额付费）：即按照实际销售额的一定比例付费，这个比例一般在5%到20%之间。CPS计费方式适用于电商广告和联盟营销。\n\n## 3.展示&amp;点击相关\n\n1.CTR 点击率（**Click Through Rate**）= click（点击） / impression（曝光） * 100%\n\n2.CVR 转化率（Conversion Rate）：点击广告后，订单成交的成功率= **（广告转化次数 / 广告点击次数）x 100%**，**转化**又可以分成**激活**（打开app），**注册**（注册账号），**付费**（充值）等\n\n3.AIPU（人均广告展示次数，Average Impression Per Users），人均广告展示次数=总广告展示数/活跃用户数\n\n4.PCTR（广告点击率预测，Predict Click-Through Rate）\n\n5.ad placement（指广告可以展示的位置）发布商可以创建覆盖整个屏幕的广告展示位置，也可以创建仅占据用户界面特定部分的广告展示位置\n\n6.PV (Page View): 流量\n\n7.UV 独立访客，一台电脑24小时以内访问N次计为1次\n\n## 4.竞价相关\n\n1.Fill Rate（填充率）填充率是指实际填充的广告资源与收到的广告请求之间的比率。此比率用于衡量广告网络中广告资源的丰富程度以及投放广告的实际百分比。填充率的计算公式为：填充率 = 投放到应用的广告数量/应用的广告请求\n\n2.waterfall：Waterfall（瀑布流模型）是一种流量方请求广告的技术。这种请求策略的运行逻辑是将流量通过聚合平台向不同平台进行分发，各平台按权重排序自上而下进行请求。权重通常按各平台历史eCPM表现进行设置。当上层没有返回时则继续向下一层请求，直到有广告被展示。\n\n参考：[提升广告变现收益背后的技术（一）：Waterfall](https://mp.weixin.qq.com/s?__biz=MzI1MjI0MTM5MA==&amp;mid=2247494809&amp;idx=1&amp;sn=fcb56c0db744157891c65c6e3b5672e6&amp;scene=21)\n\n3.bidding/auction：Header Bidding（头部竞价，又称 Pre-Bidding 或 Advance Bidding）是一种程序化交易广告技术，其同时向多个需求方发起竞价请求，从而展开公平竞价，出价最高者赢得这次展现机会。\n\n参考：[提升广告变现收益背后的技术（二）：Header Bidding](https://mp.weixin.qq.com/s?__biz=MzI1MjI0MTM5MA==&amp;mid=2247494874&amp;idx=1&amp;sn=191f4e99bc1bad582adbc6423f1d3c72&amp;chksm=e9e4185bde93914dc3e21769b6675caed73807750080876e81e0f689022720b08a578afe1016&amp;scene=178&amp;cur_album_id=1635346657015873541#rd)\n\n4.Bid Request（竞价请求)：向广告平台发起竞价请求的次数\n\n5.Bid Response（竞价响应）：广告平台收到竞价请求后成功响应的次数\n\n6.Bid Rate（竞价响应率）：竞价响应数 / 竞价请求数\n\n7.Win Rate（竞价胜出率）：广告平台竞价响应后胜出的比率，计算公式：胜出次数/竞价响应数。在聚合广告变现中，一般认为得到广告请求机会即算胜出。\n\n8.Conversion Rate：转化率\n\n9.竞价点（Bidding Point）：竞价点是指广告主在广告拍卖过程中为广告展示或点击所设置的**最大出价**。这是广告主愿意支付的最高金额，用于参与广告竞价过程。\n\n10.计费点（Billing Point）：这是广告主实际支付费用的时刻或事件，决定了广告主因何种用户行为支付广告费用。在广告拍卖过程中，系统会根据预测的转化价值自动调整竞价点。比如，对于预期转化率较高的展示机会，系统可能会提高竞价点，以增加赢得展示的概率。\n\n11.出价点（Bid Point）：这是广告主为特定目标（如点击、展示或转化）设置的**具体出价金额**，指导广告平台的出价策略。系统会根据用户的行为和转化可能性，动态设置具体的出价金额。比如，系统会在转化可能性高的情况下提高出价点，以获得更多点击，而在转化可能性低的情况下降低出价点，节约广告预算。\n\n12.考核点（Evaluation Point）：考核点是用于评估广告活动绩效的关键指标或标准。通过考核点，广告主能够衡量广告的效果和投资回报。**常见的考核点**：**CTR（Click-Through Rate）**：点击率。**CVR（Conversion Rate）**：转化率。**ROAS（Return on Ad Spend）**：广告支出回报率。**CPA（Cost Per Acquisition）**：每次获取成本。**AOV（Average Order Value）**：平均订单价值。\n\n## 5.电商相关\n\n1.GMV（商品交易总额）：Gross Merchandise Volume\n\n## 6.其他\n\n1.&nbsp;广告主（Advertiser）：想为自己的品牌/产品做广告的人，宝马/Intel/蒙牛/贪吃游戏。\n\n2.&nbsp;媒体（Publisher）：提供广告位置的载体，今日头条/抖音/QQ浏览器。\n\n3.&nbsp;广告商（Agency）：中介，帮广告主找媒体广告位。\n\n4.&nbsp;受众（Audience）：消费广告的人，受众、消费者或用户。\n\n5.实时竞价（RTB，real time bidding）：是一种利用第三方技术在数以百万计的网站上针对每一个用户展示行为进行评估以及出价的竞价技术。\n\n6. DSP（demand side platform 需求方平台）：需求方平台允许广告主在平台上设置广告的目标受众、投放地域、广告出价等等，从而通过竞价的方式帮助广告主找到更合适是受众人群，所以DSP是广告主服务平台。它提供广告定向、投放、优化和报告等功能。\n\n7. Bidder：Bidder 即竞价者，在 PPC 广告范畴內，bidder 就是普通 SEM 的操作从业者。在程序化广告范畴內，bidder 一般就是DSP 服务提供商。\n\n8. SSP（supply side platform 供应方平台）：供应方平台能够让媒体方资源接入到广告交易平台，从而使得他们的广告曝光可以进行实时竞价，使得库存得到有效的利用，从而提升媒体方的收益，所以SSP是媒体方服务平台。\n\n9. ADX （Ad Exchange 广告交易平台）：就是广告进行交易和竞价的场所，是一个开放的、能够将媒体方和广告主联系在一起的在线广告交易市场(类似于股票交易所)。交易平台可以帮助媒体方实现收益最大化，帮助广告主找到目标受众人群。\n\n10. DMP（Data-management-platform 数据管理平台）：为广告投放投放提供人群画像进行广告的受众定向，并进行人群标签画像的管理。\n\n<img src=\"/images/517519-20230911171731370-987306361.png\" width=\"500\" height=\"273\" loading=\"lazy\" />\n\n11. Native Ad（原生广告）原生广告是指外观和感觉与周围内容一模一样的广告。虽然传统广告可能具有侵入性，但原生广告与界面自然融合。开发者可以在不破坏用户体验的情况下投放更多原生广告。\n\n12. Interstitial&nbsp;Ad（插页广告）插页式广告是一种常见的广告形式。这些类型的广告通常会在自然暂停点（例如用户打开、暂停或退出应用时）弹出。广告展示的时机非常理想，符合应用的自然用户流，避免对用户体验产生任何负面影响。插页式广告可分为两类：图片广告和带文字的图片广告。\n\n参考：[https://www.pangleglobal.com/zh/glossary](https://www.pangleglobal.com/zh/glossary)\n\n13.QoS（**Quality of service**）：服务质量（1.Bit rate &amp; Play rate；2. Buffer Fill；3. Lag length；4. Lag Ratio；5. Play Length）\n\n14.QoE（Quality of Experience）：体验质量\n\n15.ABR(Adaptive BitRate Streaming)或者MBR(Multiple BitRate Streaming)：码率自适应算法\n\n参考：[https://www.conviva.com/anatomy-of-a-metric/](https://www.conviva.com/anatomy-of-a-metric/)\n\n16.KA（Key Account）重要客户\n\n17.SMB（Small and Midsize Business）中小企业客户\n\n18.IAA（In-App Advertisement）广告变现模式、IAP（In-App Purchase）内购模式\n\n19.digit campaign：数字营销活动\n\n&nbsp;\n","tags":["广告系统"]},{"title":"Spark学习笔记——使用PySpark","url":"/Spark学习笔记——使用PySpark.html","content":"1.启动pyspark\n\n<img src=\"/images/517519-20220718154110213-991992454.png\" width=\"700\" height=\"358\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n2.读取文件\n\n```\n>>> from pyspark.sql import SparkSession\n>>>\n>>> spark = SparkSession.builder.appName(\"myjob\").getOrCreate()\n>>> df = spark.read.text(\"hdfs:///user/lintong/logs/test\")\n>>> df.show()\n+-----+\n|value|\n+-----+\n|    1|\n|    2|\n|    3|\n|    4|\n+-----+\n\n```\n\n　　\n\n3.退出pyspark使用exit()\n\n&nbsp;\n\n4.使用spark-submit提交pyspark任务pi.py\n\n```\nspark2-submit --master local[*] /opt/cloudera/parcels/SPARK2/lib/spark2/examples/src/main/python/pi.py\n\n```\n\n<img src=\"/images/517519-20220718155501302-1116360249.png\" width=\"700\" height=\"482\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["Spark"]},{"title":"thrift,protobuf,avro序列化对比","url":"/thrift,protobuf,avro序列化对比.html","content":"对比thrift使用TCompactProtocol协议，protobuf使用，以及avro使用AvroKeyOutputFormat格式进行序列化对数据进行序列化后数据量大小\n\n由于thrift的binary数据类型不能再次序列化化成二进制，所以测试的schema中没有binary类型的字段\n\n## 1.avro schema\n\n测试数据的avro schema定义如下\n\n```\n{\n  \"namespace\": \"com.linkedin.haivvreo\",\n  \"name\": \"test_serializer\",\n  \"type\": \"record\",\n  \"fields\": [\n    { \"name\":\"string1\", \"type\":\"string\" },\n    { \"name\":\"int1\", \"type\":\"int\" },\n    { \"name\":\"tinyint1\", \"type\":\"int\" },\n    { \"name\":\"smallint1\", \"type\":\"int\" },\n    { \"name\":\"bigint1\", \"type\":\"long\" },\n    { \"name\":\"boolean1\", \"type\":\"boolean\" },\n    { \"name\":\"float1\", \"type\":\"float\" },\n    { \"name\":\"double1\", \"type\":\"double\" },\n    { \"name\":\"list1\", \"type\":{\"type\":\"array\", \"items\":\"string\"} },\n    { \"name\":\"map1\", \"type\":{\"type\":\"map\", \"values\":\"int\"} },\n    { \"name\":\"struct1\", \"type\":{\"type\":\"record\", \"name\":\"struct1_name\", \"fields\": [\n          { \"name\":\"sInt\", \"type\":\"int\" }, { \"name\":\"sBoolean\", \"type\":\"boolean\" }, { \"name\":\"sString\", \"type\":\"string\" } ] } },\n    { \"name\":\"enum1\", \"type\":{\"type\":\"enum\", \"name\":\"enum1_values\", \"symbols\":[\"BLUE\",\"RED\", \"GREEN\"]} },\n    { \"name\":\"nullableint\", \"type\":[\"int\", \"null\"] }\n  ] }\n\n```\n\n## 2.Thrift schema\n\n测试数据的thrift schema定义如下\n\n```\nnamespace java com.linkedin.haivvreo\n\nstruct struct1_name{\n    1: required i32 sInt;\n    2: required bool sBoolean;\n    3: required string sString;\n}\n\nenum enum1_values {\n    BLUE,\n    RED,\n    GREEN\n}\n\nstruct union1{\n    1: optional double member0;\n    2: optional bool member1;\n    3: optional string member2;\n}\n\nstruct test_serializer{\n  1: required string string1;\n  2: required i32 int1;\n  3: required i32 tinyint1;\n  4: required i32 smallint1;\n  5: required i64 bigint1;\n  6: required bool boolean1;\n  7: required double float1;\n  8: required double double1;\n  9: required list<string> list1;\n  10: required map<string, i32> map1;\n  11: required struct1_name struct1;\n  12: required string enum1;\n  13: optional i32 nullableint\n}\n\n```\n\n## 3.protobuf schema\n\n<!--more-->\n&nbsp;\n\n```\nsyntax = \"proto3\";\npackage com.linkedin.haivvreo;\n\nmessage Struct1Name {\n  int32 sInt = 1;\n  bool sBoolean = 2;\n  string sString = 3;\n}\n\nenum Enum1Values\n{\n  BLUE = 0; //proto3版本中，首成员必须为0，成员不应有相同的值\n  RED = 1;\n  GREEN = 2;\n}\n\nmessage TestSerializer {\n  string string1 = 1;\n  int32 int1 = 2;\n  int32 tinyint1 = 3;\n  int32 smallint1 = 4;\n  int64 bigint1 = 5;\n  bool boolean1 = 6;\n  double float1 = 7;\n  double double1 = 8;\n  repeated string list1 = 9;\n  map<string, int32> map1 = 10;\n  Struct1Name struct1 = 11;\n  Enum1Values enum1 = 12;\n  int32 nullableint = 13;\n}\n\n```\n\n编译protobuf schema\n\n```\nprotoc -I=./ --java_out=src/main/java/ ./src/main/proto3/test_serializer.proto\n```\n\n## 4.测试过程\n\n数据内容如下，使用代码随机生成thrift object\n\n```\n      val obj = new test_serializer()\n      obj.setString1(RandomStringUtils.randomAlphanumeric(10))\n      obj.setInt1(new java.util.Random().nextInt(100000))\n      obj.setTinyint1(new java.util.Random().nextInt(100))\n      obj.setSmallint1(new java.util.Random().nextInt(10))\n      obj.setBigint1(new java.util.Random().nextLong())\n      obj.setBoolean1(new java.util.Random().nextBoolean())\n      obj.setFloat1(new java.util.Random().nextFloat())\n      obj.setDouble1(new java.util.Random().nextDouble())\n      val cs1 = RandomStringUtils.randomAlphanumeric(10): CharSequence\n      val cs2 = RandomStringUtils.randomAlphanumeric(10): CharSequence\n      obj.setList1(List(cs1, cs2).asJava)\n      val map: java.util.Map[CharSequence, Integer] = HashMap(cs1 -> new java.util.Random().nextInt(10000), cs2 -> new java.util.Random().nextInt(10000))\n        .map(line => (line._1, (Integer.valueOf(line._2)))).asJava\n      obj.setMap1(map)\n      val struct1 = new struct1_name\n      struct1.setSInt(new java.util.Random().nextInt(1000000))\n      struct1.setSBoolean(new java.util.Random().nextBoolean())\n      struct1.setSString(RandomStringUtils.randomAlphanumeric(10))\n      obj.setStruct1(struct1)\n      val enum1 = enum1_values.BLUE\n      obj.setEnum1(enum1)\n      obj.setNullableint(new java.util.Random().nextInt(10000))\n      obj\n\n```\n\n如下\n\n<img src=\"/images/517519-20211214150226271-1241721831.png\" width=\"1000\" height=\"309\" loading=\"lazy\" />\n\n如果是avro object的话，可以从avro java class生成\n\n```\n      val rdd = sc.parallelize(Seq(1,1,1,1,1))\n      val rdd2 = rdd.map{ line =>\n        val avroSchema = new Schema.Parser().parse(schemasStr)\n        val avroRecord = new GenericData.Record(avroSchema)\n        avroRecord.put(\"firstname\", \"hello\")\n        avroRecord.put(\"lastname\", \"world\")\n        avroRecord.put(\"age\", 20)\n        val childSchema = avroRecord.getSchema.getField(\"address\").schema\n        val childRecord = new GenericData.Record(childSchema)\n        childRecord.put(\"streetaddress\", \"haidian\")\n        childRecord.put(\"city\", \"beijing\")\n        avroRecord.put(\"address\", childRecord)\n        avroRecord\n\n```\n\n也可以从avro schema生成\n\n```\n    val rdd = spark.sparkContext.parallelize(Seq(1, 1, 1, 1, 1, 1, 1))\n    val rdd2 = rdd.map { line =>\n      val jsonFormatSchema = new String(Files.readAllBytes(Paths.get(\"src/main/resources/schema.avro\")))\n      val avroSchema = new Schema.Parser().parse(jsonFormatSchema)\n      val datum = new GenericData.Record(avroSchema)\n      datum.put(\"firstname\", \"xiao\")\n      datum.put(\"lastname\", \"ming\")\n      datum.put(\"age\", 24)\n      datum.put(\"test_field2\", \"test2\")\n      val childSchema = avroSchema.getField(\"address\").schema()\n      val childDatum = new GenericData.Record(childSchema)\n      childDatum.put(\"streetaddress\", \"xierqi\")\n      childDatum.put(\"city\", \"beijing\")\n      datum.put(\"address\", childDatum)\n      datum.put(\"test_field\", 222)\n      datum\n    }\n\n```\n\n　　\n\nspark rdd保存lzo文件\n\n```\nscala> import com.hadoop.compression.lzo.LzopCodec\nimport com.hadoop.compression.lzo.LzopCodec\n\nscala> df.rdd.saveAsTextFile(\"/user/hive/warehouse/json_lzo\", classOf[LzopCodec])\n\n```\n\nspark rdd保存snappy文件\n\n```\nscala> import org.apache.hadoop.io.compress.SnappyCodec\nimport org.apache.hadoop.io.compress.SnappyCodec\n\nscala> df.repartition(1).rdd.saveAsTextFile(\"/user/hive/warehouse/json_snappy\", classOf[SnappyCodec])\n\n```\n\n&nbsp;\n\n测试数据\n\n&nbsp;\n\n\n\n|序列化框架|格式|压缩/序列化方式|数据行数|文件数量|文件大小\n| ---- | ---- | ---- | ---- | ---- | ---- \n|avro|AvroKeyOutputFormat|null|5250987|1|587.9 MB\n|avro|AvroKeyOutputFormat|SNAPPY|5250987|1|453.2 MB\n|avro|AvroParquetOutputFormat|SNAPPY|5250987|1|553.7 MB\n|thrift|ParquetThriftOutputFormat|SNAPPY|5250987|1|570.5 MB\n|thrift|SequenceFileOutputFormat|TBinaryProtocol|5250987|1|1.19 GB\n|thrift|SequenceFileOutputFormat|TCompactProtocol|5250987|1|788.7 MB\n|thrift|SequenceFileOutputFormat|TCompactProtocol+DefaultCodec|5250987|1|487.1 MB\n|json|textfile|null|5250987|1|1.84 GB\n|json|textfile|gzip|5250987|1|570.8 MB\n|json|textfile|lzo|5250987|1|716MB\n|json|textfile|snappy|5250987|1|727M\n","tags":["avro","Thrift"]},{"title":"ubuntu下git安装及使用","url":"/ubuntu下git安装及使用.html","content":"**1.设置用户名和邮箱**\n\n```\ngit config --global user.name \"xxxx\"\ngit config --global user.email \"xxx@xxx.edu.cn\"\n\n```\n\n2.查看当前git的用户和邮箱\n\n```\ngit config user.name\ngit config user.email\n\n```\n\n3.生成秘钥，回车3下，不设置密码\n\n```\nssh-keygen -t rsa -C \"xxx@xxx.edu.cn\" -f ~/.ssh/id_rsa_github\n\n```\n\n**4. ssh目录在etc/ssh下**\n\n**~/.ssh/config配置文件如下**\n\n```\n#自己私人用的 GitHub 帳號，id_rsa 就是我自己原本用的 ssh key\nHost github.com\n        User xxx\n        Hostname ssh.github.com\n        PreferredAuthentications publickey\n        IdentityFile ~/.ssh/id_rsa_github\n        Port 443\n\n#公司工作用的 GitHub 帳號，此處的 COMPANY 你可以自行取名\nHost gitlab.xxx.com\n        Hostname gitlab.xxx.com\n        Port xxx\n        User xxx\n        IdentityFile ~/.ssh/id_rsa\n\nHost xx-*\n        HostName %h\n        User xxx\n        Port xxx\n\nHost xxx-*\n        HostName %h\n        User xxx\n        Port xxx\n\nHost xxx-*\n        HostName %h\n        User xxx\n        Port xxx\n\n```\n\n5.上传.pub公钥到github\n\n6.可以git clone了\n\n<!--more-->\n&nbsp;\n\n**如何在本地使用git**\n\n**http://jingyan.baidu.com/album/295430f1c62c900c7e0050fd.html?picindex=1**\n\n&nbsp;\n\n参考：1.　　[ubuntu下git安装及使用](http://www.cnblogs.com/jackge/p/3264801.html)\n\n　　　　http://www.cnblogs.com/jackge/archive/2013/08/17/3264801.html\n\n　　　2.　　[多 SSH Key 管理技巧与 Git 多账户登录问题 ](http://blog.csdn.net/forlong401/article/details/50902983)\n\n　　　　http://blog.csdn.net/forlong401/article/details/50902983\n\n　　　3.　　[执行ssh-add时出现Could not open a connection to your authentication agent](http://www.cnblogs.com/sheldonxu/archive/2012/09/17/2688281.html)\n\n　　　　http://www.cnblogs.com/sheldonxu/archive/2012/09/17/2688281.html　　　　****ssh-agent bash****\n\n****　　　4.　　转载：ubuntu下github connect错误解决****\n\n```\nssh git@github.com\nssh: connect to host github.com port 22: Connection timed out\n\n```\n\n&nbsp;****解决办法：（linux下）****\n\n```\ncd ~/.ssh\ntouch config\n\n```\n\n&nbsp;****在.ssh目录下创建一个config文件，输入如下内容：<br />****\n\n```\nHost github.com\nUser xxx@163.com （你注册github时的邮箱，这里使用注册的用户名也行）\nHostname ssh.github.com\nPreferredAuthentications publickey\nIdentityFile ~/.ssh/id_rsa\nPort 443\n\n```\n\n&nbsp;测试是否连通\n\n```\nssh -T git@github.com\nThe authenticity of host '[ssh.github.com]:443 ([207.97.227.248]:443)' can't be established.\nRSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.\nAre you sure you want to continue connecting (yes/no)? y\nPlease type 'yes' or 'no': yes\nWarning: Permanently added '[ssh.github.com]:443,[207.97.227.248]:443' (RSA) to the list of known hosts.\nHi zhou411424! You've successfully authenticated, but GitHub does not provide shell access.\n出现Hi xxx!......表示连接成功。\n\n```\n\n&nbsp;\n\nssh免密码登陆设置时Authentication refused: bad ownership or modes错误解决方法\n\nsshd为了安全，对属主的目录和文件权限有所要求。如果权限不对，则ssh的免密码登陆不生效。\n\n```\n用户目录权限为 755 或者 700，就是不能是77x。\n.ssh目录权限一般为755或者700。\nrsa_id.pub 及authorized_keys权限一般为644\nrsa_id权限必须为600\n\n```\n\n&nbsp;\n\ncentos用户重启ssh服务\n\n```\nsystemctl restart sshd\n\n```\n\n禁止远程root用户登录，修改 /etc/ssh/sshd_config\n\n```\nPermitRootLogin no\n\n```\n\n修改端口，将Port改成其他的端口\n\n参考：https://blog.csdn.net/tianlesoftware/article/details/6201898\n\n&nbsp;\n\n生成ssh秘钥\n\n1.生成私钥和公钥\n\n```\nssh-keygen -t rsa -C \"worker\" -f ~/.ssh/worker\n\n```\n\n2.然后将公钥添加到authorized_keys文件中\n\n```\ntouch ./.ssh/authorized_keys\ncat ./worker.pub >> ./.ssh/authorized_keys\n\n```\n\n3.将私钥改名成xxx.pem，并拷贝到别的机器上，然后就可以ssh了\n\n```\nmv worker worker.pem\nssh -i ~/下载/worker.pem worker@master\n\n```\n\n配置当前用户免密登录\n\n1.生成默认的秘钥，必须是id_rsa 和 id_rsa.pub\n\n```\nssh-keygen -t rsa\n\n```\n\n2.将公钥添加到authorized_keys文件中\n\n```\ncat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n\n```\n\n3.可以免密登录\n\n```\nssh localhost\n\n```\n\n&nbsp;\n","tags":["Git"]},{"title":"特征平台——feast","url":"/特征平台——feast.html","content":"feast是google开源的一个特征平台，其提供特征注册管理，以及和特征存储（feature store），离线存储（offline store）和在线存储（online store）交互的SDK，官网文档：\n\n```\nhttps://docs.feast.dev/\n\n```\n\n目前最新的v0.24版本支持的**离线存储**：File，Snowflake，BigQuery，Redshift，Spark，PostgreSQL，Trino，AzureSynapse等，参考：\n\n```\nhttps://docs.feast.dev/reference/offline-stores\n\n```\n\n**在线存储**：SQLite，Snowflake，Redis，Datastore，DynamoDB，PostgreSQL，Cassandra等，参考：\n\n```\nhttps://docs.feast.dev/reference/online-stores\n\n```\n\n**provider<!--more-->\n&nbsp;**用于定义feast运行的环境，其提供了feature store在不同平台组件上的实现，目前有4种：local, gcp，aws和azure\n\n\n\n|provider|支持的offline store|支持的online store\n| ---- | ---- | ---- \n|local|BigQuery，file|Redis，Datastore，Sqlite\n|gcp|BigQuery，file|Datastore，Sqlite\n|aws|Redshift，file|DynamoDB，Sqlite\n|azure|Mysql，file|Redis，Splite\n\n参考：\n\n```\nhttps://docs.feast.dev/getting-started/architecture-and-components/provider\n\n```\n\n**data source&nbsp;**用于定义特征的数据来源，每个batch data source都和一个offline store关联，比如SnowflakeSource只能和Snowflake offline store关联\n\ndata source的类型包括：file，Snowflake，bigquery，redshift，push，kafka，kinesis，spark，postgreSQL，Trino，AzureSynapse+AzureSQL\n\n\n\n|data source|offline store\n| ---- | ---- \n|FileSource|file\n|SnowflakeSource|Snowflake\n|BigQuerySource|BigQuery\n|RedshiftSource|Redshift\n|PushSource（可以同时将feature写入online和offline store）|&nbsp;\n|KafkaSource（仍然处于实验性）|&nbsp;\n|KinesisSource（仍然处于实验性）|&nbsp;\n|SparkSource（支持hive和parquet文件）|Spark\n|PostgreSQLSource|PostgreSQL\n|TrinoSource|Trino\n|MsSqlServerSource|AzureSynapse+AzureSQL&nbsp;\n\n&nbsp;\n\n**Batch Materialization Engines**&nbsp;用于将offline store的数据刷到online store，其配置位于feature_store.xml的batch_engine\n\n其默认实现是LocalMaterializationEngine，也基于aws lambda的LambdaMaterializaionEngine\n\n```\nhttps://docs.feast.dev/getting-started/architecture-and-components/batch-materialization-engine\n\n```\n\n也可以Bytewax（配合k8s使用）和Snowflake（当使用SnowflakeSource的时候）作为batch materialization engine\n\n此外，还可以自行实现engine，参考：\n\n```\nhttps://docs.feast.dev/how-to-guides/customizing-feast/creating-a-custom-materialization-engine\n\n```\n\n　　\n\n&nbsp;\n\n## 1.feast的安装\n\n```\nhttps://docs.feast.dev/getting-started/quickstart\n\n```\n\n下面的安装以v0.23版本为例，安装v0.23版本的时候建议使用python3.8，v0.22版本的时候建议使用python3.7\n\n```\npip install feast===0.23.0\n\n```\n\n由于选择的离线存储是hive，在线存储是cassandra，所以还需要安装离线存储和在线存储的插件\n\n```\npip install feast-cassandra==0.1.3\npip install feast-hive==0.17.0\n\n```\n\n如果安装feast-hive的时候遇到无法安装thriftpy，则需要先安装cython\n\n```\npip install cython\npip install thriftpy\n\n```\n\n　　\n\n## 2.创建一个feast项目\n\n```\nfeast init my_project\n\n\nCreating a new Feast repository in /Users/lintong/coding/python/my_project.\n\n(⎈ |docker-desktop:default)➜  /Users/lintong/coding/python $ tree -L 3 my_project\nmy_project\n├── __init__.py\n├── data\n│&nbsp;&nbsp; └── driver_stats.parquet\n├── example.py\n└── feature_store.yaml\n\n1 directory, 4 files\n\n```\n\n其中feature_store.yaml，可以在其中配置offline store和online store，该文件必须位于project的根目录，参考：\n\n```\nhttps://docs.feast.dev/reference/feature-repository\n\n```\n\n如下\n\n```\nproject: my_project\nregistry: data/registry.db\nprovider: local\nonline_store:\n    path: data/online_store.db\nentity_key_serialization_version: 2\n\n```\n\nexample.py定义了feast pipeline的流程，即feature的数据source，特征的entity，特征的view注册，特征的服务化，如下\n\n```\n# This is an example feature definition file\n\nfrom datetime import timedelta\n\nfrom feast import Entity, FeatureService, FeatureView, Field, FileSource\nfrom feast.types import Float32, Int64\n\n# Read data from parquet files. Parquet is convenient for local development mode. For\n# production, you can use your favorite DWH, such as BigQuery. See Feast documentation\n# for more info.\ndriver_hourly_stats = FileSource(\n    name=\"driver_hourly_stats_source\",\n    path=\"/Users/lintong/coding/python/my_project/data/driver_stats.parquet\",\n    timestamp_field=\"event_timestamp\",\n    created_timestamp_column=\"created\",\n)\n\n# Define an entity for the driver. You can think of entity as a primary key used to\n# fetch features.\ndriver = Entity(name=\"driver\", join_keys=[\"driver_id\"])\n\n# Our parquet files contain sample data that includes a driver_id column, timestamps and\n# three feature column. Here we define a Feature View that will allow us to serve this\n# data to our model online.\ndriver_hourly_stats_view = FeatureView(\n    name=\"driver_hourly_stats\",\n    entities=[driver],\n    ttl=timedelta(days=1),\n    schema=[\n        Field(name=\"conv_rate\", dtype=Float32),\n        Field(name=\"acc_rate\", dtype=Float32),\n        Field(name=\"avg_daily_trips\", dtype=Int64),\n    ],\n    online=True,\n    source=driver_hourly_stats,\n    tags={},\n)\n\ndriver_stats_fs = FeatureService(\n    name=\"driver_activity\", features=[driver_hourly_stats_view]\n)\n\n```\n\n　　\n\n## 3.配置注册store和feature\n\n**feature store**的配置文件默认是feature_store.xml，也可以自行添加\n\n**feature定义**的配置文件默认是exampl.xml，也可以自行添加\n\n写好配置文件后通过运行**feast apply命令**来注册store和feature，也可以使用**.feastignore**文件来排除store和feature\n\n&nbsp;\n\n如果feast apply遇到如下报错\n\n```\nimporterror: cannot import name 'soft_unicode' from 'markupsafe'\n\n```\n\n则解决方法如下\n\n```\npip install markupsafe==2.0.1\n\n```\n\n　　\n\n&nbsp;\n","tags":["ML Infra"]},{"title":"JavaWeb学习笔记——jsp基础语法","url":"/JavaWeb学习笔记——jsp基础语法.html","content":"## <!--more-->\n&nbsp;1.JSP文件\n\n**JSP文件的命名最好采用小写的形式，比如hello.jsp，且必须加上第一句以用来指定编码，否则会出现乱码**\n\n```\n<%@ page language=\"java\" import=\"java.util.*\" contentType=\"text/html; charset=UTF-8\" %>  \n<html>\n\t<head>\n\t\t<title>这是一个title</title>\n\t</head>\n\t\t\n\t<body>\n\t\t<%\n\t\t\tout.println(\"<h1>这是一个h1</h1>\");\n\t\t%>\n\t</body>\n</html>\n```\n\n&nbsp;<img src=\"/images/517519-20160401172528176-1975175900.png\" width=\"600\" height=\"667\" />\n\n## **2.JSP注释**\n\n显式注释　　\n\n```\n<!-- 注释内容 -->\n\n```\n\n隐式注释，隐式注释在客户端无法看见\n\n```\n//\n/* */\n<% 注释内容 %>&nbsp;\n```\n\n## **3.Scriptlet（小脚本程序）**\n\n**所有嵌入在HTML代码中的Java程序都必须使用<strong>Scriptlet**标记起来，在JSP中一共有3种Scriptlet代码</strong>\n\n**<img src=\"/images/517519-20160423162600820-1927022160.png\" alt=\"\" width=\"560\" height=\"368\" />**\n\n&nbsp;\n\n<img src=\"/images/517519-20160423162703570-1506322097.png\" alt=\"\" width=\"592\" height=\"769\" />\n\n<img src=\"/images/517519-20160423162813726-2118876011.png\" alt=\"\" width=\"592\" height=\"572\" />\n\n<img src=\"/images/517519-20160423163528991-1412817064.png\" alt=\"\" width=\"606\" height=\"254\" />\n\n## 4**.Scriptlet标签**\n\n**<img src=\"/images/517519-20160423164748413-1245463072.png\" alt=\"\" width=\"588\" height=\"565\" />**\n\n## **5.page指令**\n\n**　　page指令在JSP开发中较为重要，使用此属性，可以定义一个JSP页面的相关属性，包括设置MIME类型，定义需要导入的包，错误页的指定等**\n\n**<img src=\"/images/517519-20160424182745273-969320268.png\" alt=\"\" width=\"632\" height=\"687\" />**\n\n&nbsp;\n\n<img src=\"/images/517519-20160424183713132-2046711540.png\" alt=\"\" width=\"645\" height=\"117\" />\n\n<img src=\"/images/517519-20160424183812382-2025088741.png\" alt=\"\" width=\"634\" height=\"743\" />\n\n<img src=\"/images/517519-20160424185150757-1629636228.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160424185218851-1183876408.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160424191520554-541131790.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160424191942632-1411819748.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160424192916695-632462152.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160424200203288-1594160984.png\" alt=\"\" width=\"572\" height=\"112\" />\n\n## **6.包含指令**\n\n**<img src=\"/images/517519-20160424205734632-1564498736.png\" alt=\"\" width=\"639\" height=\"552\" />**\n\n<img src=\"/images/517519-20160424205805741-1755287247.png\" alt=\"\" width=\"650\" height=\"860\" />\n\n<img src=\"/images/517519-20160424205835929-1212488634.png\" alt=\"\" width=\"619\" height=\"859\" />\n\n<img src=\"/images/517519-20160424205907304-1323384592.png\" alt=\"\" width=\"639\" height=\"847\" />\n\n<img src=\"/images/517519-20160424205934570-1702643893.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160424205957991-22322794.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160424210023507-1367664779.png\" alt=\"\" />\n\n## **7.跳转指令**\n\n**<img src=\"/images/517519-20160424210103148-2083513703.png\" alt=\"\" width=\"651\" height=\"451\" />**\n\n&nbsp;\n\n<img src=\"/images/517519-20160424210129507-1514152250.png\" alt=\"\" />\n\n## **8.内置对象**\n\n<img src=\"/images/517519-20160424220512913-1665835130.png\" alt=\"\" width=\"598\" height=\"418\" />\n\n<img src=\"/images/517519-20160424224113710-1009727095.png\" alt=\"\" width=\"610\" height=\"617\" />\n\n<img src=\"/images/517519-20160424234135195-1586402381.png\" alt=\"\" width=\"618\" height=\"436\" />\n\n## 9.setproperty和getproperty\n\n<img src=\"/images/517519-20160507212231468-1235486025.png\" alt=\"\" width=\"575\" height=\"101\" />\n\n<img src=\"/images/517519-20160507212255484-1153693405.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160507212323843-924277034.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160507212357609-1034371864.png\" alt=\"\" />\n\n## 10.javabean与表单<br />\n\n&nbsp;<img src=\"/images/517519-20160505212917872-509429278.png\" alt=\"\" width=\"712\" height=\"398\" />\n\n&nbsp;\n\n<img src=\"/images/517519-20160505213142482-682807574.png\" alt=\"\" width=\"674\" height=\"417\" />\n\n<img src=\"/images/517519-20160505221100966-1186868135.png\" alt=\"\" width=\"731\" height=\"452\" />\n\n<img src=\"/images/517519-20160505221152497-1727595646.png\" alt=\"\" width=\"758\" height=\"750\" />\n\n<img src=\"/images/517519-20160505221244076-2145641863.png\" alt=\"\" width=\"763\" height=\"743\" />\n\n<img src=\"/images/517519-20160505232829919-1715587322.png\" alt=\"\" width=\"684\" height=\"282\" />\n\n<img src=\"/images/517519-20160505232916810-1135230715.png\" alt=\"\" width=\"672\" height=\"880\" />\n\n<img src=\"/images/517519-20160505232950951-1543148389.png\" alt=\"\" width=\"670\" height=\"769\" />\n\n## 11.javabean的删除\n\n<img src=\"/images/517519-20160507224547687-1071923840.png\" alt=\"\" width=\"652\" height=\"426\" />\n\n<img src=\"/images/517519-20160507224638218-1449210349.png\" alt=\"\" width=\"600\" height=\"116\" />\n\n<img src=\"/images/517519-20160507224704218-380170292.png\" alt=\"\" width=\"643\" height=\"423\" />\n\n## 12.JSP标准标签库JSTL\n\n&nbsp;\n\n<img src=\"/images/517519-20160514160142874-744373856.png\" alt=\"\" width=\"558\" height=\"458\" />\n\n<img src=\"/images/517519-20160514160344718-130068007.png\" alt=\"\" width=\"585\" height=\"774\" />\n\n<img src=\"/images/517519-20160514162323202-843746419.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514162345093-311365286.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514162801343-2080189681.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514162824734-2019177436.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514170859624-1044396926.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514170925984-1691243463.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514170959046-579856004.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514171146890-576970633.png\" alt=\"\" />\n\n&nbsp;\n\n<img src=\"/images/517519-20160514171521202-716753107.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514171554890-955910886.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514172832155-606438254.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514172857296-792509012.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514173108593-434500444.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514173201468-1378891090.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514173414530-1326782718.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514173537312-1200015128.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514173600077-1509146275.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514173641249-566800358.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160514173748593-25859039.png\" alt=\"\" />\n\n## 13.表达式语言\n\n使用**表达式语言**，可以方便地访问**标志位（JSP中有page(pageContext)、request、session和application4种标志位）中的属性内容**，可以避免出现许多的Scriptlet代码。 \n\n<img src=\"/images/517519-20160513225458624-1768216453.png\" alt=\"\" width=\"627\" height=\"361\" />\n\n<img src=\"/images/517519-20160513230003030-709204617.png\" alt=\"\" width=\"620\" height=\"557\" />\n","tags":["JavaWeb"]},{"title":"CDH5.16安装sentry","url":"/CDH5.16安装sentry.html","content":"参考：[CDH配置Kerberos和Sentry （超详细）](https://blog.csdn.net/summer089089/article/details/107369994)\n\n添加服务\n\n<img src=\"/images/517519-20211021213253451-1203394662.png\" width=\"400\" height=\"149\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n选择sentry\n\n<img src=\"/images/517519-20211021213344033-694869882.png\" width=\"400\" height=\"174\" loading=\"lazy\" />\n\n&nbsp;\n\n填写mysql账号密码\n\n<img src=\"/images/517519-20211021213433062-2137481290.png\" width=\"600\" height=\"242\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;添加成功\n\n<img src=\"/images/517519-20211021213619360-391224715.png\" width=\"800\" height=\"318\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;\n\nSentry与Hive/Impala集成\n\n1.配置Hive集成Sentry\n\n取消hiveserver2的用户模拟\n\n<img src=\"/images/517519-20211021213843541-914398325.png\" width=\"500\" height=\"242\" loading=\"lazy\" />\n\n确保hive用户能够提交MR任务\n\n#### <img src=\"/images/517519-20211021214053187-807627695.png\" width=\"500\" height=\"274\" loading=\"lazy\" />\n\n&nbsp;\n\n在Hive配置项中搜索&ldquo;启用数据库中的存储通知&rdquo;，勾选\n\n<img src=\"/images/517519-20211021214339240-218560964.png\" width=\"600\" height=\"78\" loading=\"lazy\" />\n\n在hive的配置中启用sentry\n\n<img src=\"/images/517519-20211021214231112-698371051.png\" width=\"400\" height=\"152\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;2.配置Impala集成Sentry\n\n<img src=\"/images/517519-20211021214512232-1214580742.png\" width=\"400\" height=\"119\" loading=\"lazy\" />\n\n&nbsp;\n\n3.配置HDFS权限与Sentry同步\n\n在HDFS配置项中搜索&ldquo;ACL&rdquo;，勾选。\n\n<img src=\"/images/517519-20211021215001178-1813082155.png\" width=\"600\" height=\"258\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;4.配置HUE支持Sentry\n\n<img src=\"/images/517519-20211021215227533-287952054.png\" width=\"500\" height=\"216\" loading=\"lazy\" />\n\n&nbsp;\n\n5.查看hive组是否已经添加到sentry的admin.group中，也可以添加自定义的组，但是这个组必须在linux上存在\n\n之后将admin用户添加到hive组中，admin用户就可以在HUE中给sentry添加role，否则将看不到添加role的按钮\n\n<img src=\"/images/517519-20211022224208510-2097233872.png\" width=\"600\" height=\"404\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;\n\n6.配置sentry权限\n\n此时hive账号登录HUE是无法查询数据的\n\n<img src=\"/images/517519-20211021232247260-636804578.png\" width=\"800\" height=\"226\" loading=\"lazy\" />\n\n&nbsp;\n\n需要使用admin账号登录HUE配置用户权限\n\n<img src=\"/images/517519-20211021232058328-591278909.png\" width=\"200\" height=\"126\" loading=\"lazy\" />\n\n&nbsp;\n\n新建hive用户和hive group，并将hive用户拉到hive组中，其中\n\nfilebrowser.access是hdfs文件浏览器按钮的权限\n\nsecurity.access是security按钮的权限\n\nbeeswax.access的query按钮的权限\n\n<img src=\"/images/517519-20211022225659274-316229823.png\" width=\"700\" height=\"250\" loading=\"lazy\" />\n\n&nbsp;之后就可以使用hive用户登录，就可以进行添加Group和添加Privileges\n\n<img src=\"/images/517519-20211022225904470-1758968909.png\" width=\"800\" height=\"205\" loading=\"lazy\" />\n\n&nbsp;\n\n赋上所有db的all权限之后，hive用户就可以增删改查所有的hive表了\n\n<img src=\"/images/517519-20211022230101170-1664552865.png\" width=\"800\" height=\"392\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["CDH"]},{"title":"Linux下htop的使用","url":"/Linux下htop的使用.html","content":"## linux top命令VIRT,RES,SHR,DATA的含义\n\n<img src=\"/images/517519-20160814174605656-2131427289.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n**第1行-第4行：**显示CPU当前的运行负载，有几核就有几行，我的是4核\n\n**Mem：**显示内存的使用情况，3887M大概是3.8G，此时的Mem不包含buffers和cached的内存，所以和free -m会不同\n\n**Swp：**显示交换空间的使用情况，交换空间是当内存不够和其中有一些长期不用的数据时，ubuntu会把这些暂时放到交换空间中\n\n&nbsp;\n\n<strong>VIRT：virtual memory usage 虚拟内存<br />\n</strong>1、进程&ldquo;需要的&rdquo;虚拟内存大小，包括进程使用的库、代码、数据等<br />\n2、假如进程申请100m的内存，但实际只使用了10m，那么它会增长100m，而不是实际的使用量\n\n**RES：resident memory usage 常驻内存**<br />\n1、进程当前使用的内存大小，但不包括swap out<br />\n2、包含其他进程的共享<br />\n3、如果申请100m的内存，实际使用10m，它只增长10m，与VIRT相反<br />\n4、关于库占用内存的情况，它只统计加载的库文件所占内存大小\n\n**SHR：shared memory 共享内存**<br />\n1、除了自身进程的共享内存，也包括其他进程的共享内存<br />\n2、虽然进程只使用了几个共享库的函数，但它包含了整个共享库的大小<br />\n3、计算某个进程所占的物理内存大小公式：RES &ndash; SHR<br />\n4、swap out后，它将会降下来\n\n**DATA**<br />\n1、数据占用的内存。如果top没有显示，按f键可以显示出来。<br />\n2、真正的该程序要求的数据空间，是真正在运行中要使用的。\n\n**top 运行中可以通过 top 的内部命令对进程的显示方式进行控制。内部命令如下：**<br />\ns &ndash; 改变画面更新频率<br />\nl &ndash; 关闭或开启第一部分第一行 top 信息的表示<br />\nt &ndash; 关闭或开启第一部分第二行 Tasks 和第三行 Cpus 信息的表示<br />\nm &ndash; 关闭或开启第一部分第四行 Mem 和 第五行 Swap 信息的表示<br />\nN &ndash; 以 PID 的大小的顺序排列表示进程列表<br />\nP &ndash; 以 CPU 占用率大小的顺序排列进程列表<br />\nM &ndash; 以内存占用率大小的顺序排列进程列表<br />\nh &ndash; 显示帮助<br />\nn &ndash; 设置在进程列表所显示进程的数量<br />\nq &ndash; 退出 top<br />\ns &ndash; 改变画面更新周期\n\n序号 列名 含义<br />\na PID 进程id<br />\nb PPID 父进程id<br />\nc RUSER Real user name<br />\nd UID 进程所有者的用户id<br />\ne USER 进程所有者的用户名<br />\nf GROUP 进程所有者的组名<br />\ng TTY 启动进程的终端名。不是从终端启动的进程则显示为 ?<br />\nh PR 优先级<br />\ni NI nice值。负值表示高优先级，正值表示低优先级<br />\nj P 最后使用的CPU，仅在多CPU环境下有意义<br />\nk %CPU 上次更新到现在的CPU时间占用百分比<br />\nl TIME 进程使用的CPU时间总计，单位秒<br />\nm TIME+ 进程使用的CPU时间总计，单位1/100秒<br />\nn %MEM 进程使用的物理内存百分比<br />\no VIRT 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES<br />\np SWAP 进程使用的虚拟内存中，被换出的大小，单位kb。<br />\nq RES 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA<br />\nr CODE 可执行代码占用的物理内存大小，单位kb<br />\ns DATA 可执行代码以外的部分(数据段+栈)占用的物理内存大小，单位kb<br />\nt SHR 共享内存大小，单位kb<br />\nu nFLT 页面错误次数<br />\nv nDRT 最后一次写入到现在，被修改过的页面数。<br />\nw S 进程状态。（D=不可中断的睡眠状态，R=运行，S=睡眠，T=跟踪/停止，Z=僵尸进程）<br />\nx COMMAND 命令名/命令行<br />\ny WCHAN 若该进程在睡眠，则显示睡眠中的系统函数名<br />\nz Flags 任务标志，参考 sched.h\n\n默认情况下仅显示比较重要的 PID、USER、PR、NI、VIRT、RES、SHR、S、%CPU、%MEM、TIME+、COMMAND 列。可以通过下面的快捷键来更改显示内容。\n\n通过 f 键可以选择显示的内容。按 f 键之后会显示列的列表，按 a-z 即可显示或隐藏对应的列，最后按回车键确定。<br />\n按 o 键可以改变列的显示顺序。按小写的 a-z 可以将相应的列向右移动，而大写的 A-Z 可以将相应的列向左移动。最后按回车键确定。<br />\n按大写的 F 或 O 键，然后按 a-z 可以将进程按照相应的列进行排序。而大写的 R 键可以将当前的排序倒转。\n","tags":["Linux"]},{"title":"Java数据结构与排序算法——堆和堆排序","url":"/Java数据结构与排序算法——堆和堆排序.html","content":"<img src=\"/images/517519-20160417213933270-1183946871.png\" alt=\"\" width=\"692\" height=\"76\" />\n\n<img src=\"/images/517519-20160417214053504-395546798.png\" alt=\"\" width=\"685\" height=\"184\" />\n\n<img src=\"/images/517519-20160417214118754-37238632.png\" alt=\"\" width=\"498\" height=\"186\" />\n\n<img src=\"/images/517519-20160417214745801-804846320.png\" alt=\"\" width=\"716\" height=\"540\" />\n\n<!--more-->\n&nbsp;\n\n```\nclass Node_Heap{\n\tpublic int iData;\n\n\tpublic Node_Heap(int iData) {\t//构造函数\n\t\tsuper();\n\t\tthis.iData = iData;\n\t}\n\n\tpublic int getiData() {\n\t\treturn iData;\n\t}\n\n\tpublic void setiData(int iData) {\n\t\tthis.iData = iData;\n\t}\n}\n\nclass Heap{\n\tprivate Node_Heap[] heapArray;\n\tpublic int maxSize;\n\tprivate int currentSize;\n\t\n\tpublic Heap(int maxSize) {\t//构造函数\n\t\tsuper();\n\t\tthis.maxSize = maxSize;\n\t\tthis.currentSize = 0;\n\t\theapArray = new Node_Heap[maxSize];\n\t}\n\t\n\tpublic boolean isEmpty(){\n\t\treturn currentSize ==0;\n\t}\n\t\n\tpublic boolean insert(int key){\n\t\tif(currentSize == maxSize){\n\t\t\treturn false;\n\t\t}\n\t\tNode_Heap newNode = new Node_Heap(key);\t\n\t\theapArray[currentSize] = newNode;\t//把插入的节点放在最后的位置\n\t\ttrickleUp(currentSize++);\t\t//插入节点并把currentSize加1\n\t\treturn true;\n\t}\n\t\n\t//用于插入，把父类节点下移，然后把插入的节点放到合适的位置\n\tpublic void trickleUp(int index){\n\t\tint parent = (index-1)/2;\n\t\tNode_Heap bottom = heapArray[index];\t\t//暂存新插入的节点，因为需要把父节点下移\n\t\twhile(index>0 &amp;&amp; heapArray[parent].getiData()<bottom.getiData()){\t//如果小，就下移\n\t\t\theapArray[index] = heapArray[parent];\t\t//把父类节点下移\n\t\t\tindex = parent;\t\t\t\t\t\t//用于递归\n\t\t\tparent = (parent-1)/2;\n\t\t}\n\t\theapArray[index] = bottom;\t//把插入的节点放到合适的位置\n\t}\n\t\n\tpublic Node_Heap remove(){\t//删除最大的节点\n\t\tNode_Heap root = heapArray[0];\n\t\theapArray[0]=heapArray[--currentSize];\n\t\ttrickleDown(0);\n\t\treturn root;\n\t}\n\t\n\t//用于删除，把子类节点上移\n\tpublic void trickleDown(int index){\n\t\tint largerChild;\n\t\tNode_Heap top = heapArray[index];\t\t//\n\t\twhile(index<currentSize/2){\t//如果小，就下移\n\t\t\tint leftChild = 2*index+1;\n\t\t\tint rightChild = leftChild+1;\n\t\t\tif(rightChild<currentSize &amp;&amp; heapArray[leftChild].getiData() < heapArray[rightChild].getiData())\n\t\t\t\tlargerChild = rightChild;\n\t\t\telse\n\t\t\t\tlargerChild = leftChild;\n\t\t\tif(top.getiData()>=heapArray[largerChild].getiData())\n\t\t\t\tbreak;\n\t\t\theapArray[index] = heapArray[largerChild];\n\t\t\tindex = largerChild;\n\t\t}\n\t\theapArray[index] = top;\t\n\t}\n\t\n\tpublic void displayHeap(){\n\t\tSystem.out.print(\"heapArray:\");\n\t\tfor(int i=0;i<heapArray.length;i++){\n\t\t\tif(heapArray[i] != null)\n\t\t\t\tSystem.out.print(heapArray[i].getiData()+\" \");\n\t\t\telse\n\t\t\t\tSystem.out.print(\" -- \");\n\t\t}\n\t\tSystem.out.println();\n\t\t\n\t\tint nBlanks = 32;\t\t\t//定义空格\n\t\tint itemsPerRow = 1;\n\t\tint column = 0;\n\t\tint j=0;\t\t\t\t//标记当前的数组下标，从0开始\n\t\tSystem.out.println(\"......................................................\");\n\t\twhile(currentSize > 0){\n\t\t\tif(column == 0){\n\t\t\t\tfor(int i=0;i<nBlanks;i++){\n\t\t\t\t\tSystem.out.print(\" \");\n\t\t\t\t}\n\t\t\t}\n\t\t\tSystem.out.print(heapArray[j].getiData());\n\t\t\tif(++j == currentSize){\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif(++column==itemsPerRow){\t\t//如果每一行计数等于这一行的上限，则换行\n\t\t\t\tnBlanks /= 2;\t\t\t\t//空格数减半\n\t\t\t\titemsPerRow *= 2;\t\t//每一行的上限\n\t\t\t\tcolumn = 0;\n\t\t\t\tSystem.out.println();\n\t\t\t}else{\n\t\t\t\tfor(int i=0;i<nBlanks*2-2;i++){\n\t\t\t\t\tSystem.out.print(\" \");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tSystem.out.println(\"\\n\"+\"......................................................\");\n\t}\n\t\n}\n\npublic class Heap_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tint anArrays[]={1,2,3,4,5,6,7,8,9,10};\n\t\tHeap theHeap = new Heap(31);\n//\t\ttheHeap.insert(1);\n//\t\ttheHeap.insert(2);\n//\t\ttheHeap.insert(3);\n//\t\ttheHeap.insert(4);\n//\t\ttheHeap.insert(5);\n//\t\ttheHeap.insert(6);\n\t\tfor(int i=0;i<10;i++){\n\t\t\ttheHeap.insert(anArrays[i]);\n\t\t}\n\t\ttheHeap.displayHeap();\n\t\t//theHeap.remove();\n\t\t//theHeap.displayHeap();\n\t\tfor(int i=0;i<10;i++){\n\t\t\tanArrays[i]=theHeap.remove().iData;\n\t\t}\n\t\tfor(int i=0;i<10;i++){\n\t\t\tSystem.out.print(anArrays[i]+\" \");\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160418151532445-595123485.png\" alt=\"\" />\n","tags":["算法","数据结构"]},{"title":"读《硅谷之谜》有感","url":"/读《硅谷之谜》有感.html","content":"\n#### 但是真正做好一件事没有捷径，需要一万小时的专业训练和努力。 ———— 《数学之美》<br/>\n\n<!--more-->\n\n### <font color=#ff0000><1>关于搜索引擎</font>\n\n#### <font color=#0000ff>1.搜索引擎的组成</font>\n\n<font face=\"微软雅黑\">\n在《数学之美》中，吴军博士把搜索引擎的关键提炼成了3个方面：\n\n1.**下载** ———— **通过网路爬虫实现** 。搜素引擎的网络爬虫问题应该定义成“如何在有限的时间内最多地爬下最重要的网页”。首先，各个网站中最重要的网页肯定是首页，所以在这个前提下，**广度搜索（BFS）**明显优于**深度搜索（DFS）**。而实际的网络爬虫都是由成千上万的服务器组成的，对于一个网站，需要一次性把这个网站的内容都下载下来，而不是先下载5%，然后到第二轮再继续下来，所以这个就有点类似于（DFS）。这个的原因和爬虫的分布式结构以及网络通信的握手成功成本有关。同时，爬虫对网页遍历的次序不是简单的BFS或者DFS，所以就需要一个相对复杂的**下载优先级排序**的方法，这个管理优先级的子系统一般成为**调度系统**。\n\n2.**索引**  ———— 搜索引擎能在极短的时间内找到成千上万的搜索结果，是通过索引的方法来实现的。**最简单的索引** 就是对于一个确定的关键字，用01来表示一篇文章或者一个网页**是否含有这个关键字**，但是这样有多少个网页或者多少篇文章，这个二进制数就有多少位。虽然计算机处理二进制数的速度很快，但是由于这个索引十分庞大，这就需要把索引分成很多份，分别存储在不同的服务器中。没当接受一个查询的时候，这个查询就被分发到这些不同的服务器上，并行的处理请求，然后再把结果发送到主服务器中进行合并处理，最后返回给用户。\n\n3.**排序**  ————**PageRank算法** 。Google的PageRanke算法由佩奇和布林发明。**这个算法的思想** 在于民主表决，Y的网页排名pagerank来源于指向这个网页的所有网页X1、X2...Xk的**权重之和**。其中**佩奇**认为，X1、X2...Xk的权重分别是多少，这个取决去这些网页本身的排名。但是这就成为了一个先有鸡，还是先有蛋的问题。而解决这个问题的是**布林**，他把这个问题变成了一个二维矩阵相乘的问题，并且用迭代的方法解决了这个问题。他们先假定所有网页的排名都是一样的，并且根据这个初始值进行不断的迭代。从理论上证明，无论初始值如何选取，这种算法都保证了网页排名的估计值能收敛到排名的真实值，而且这个排名是没有任何人工干预的。在理论上，这个二维矩阵的元素有网页数量的平方那么多，计算量是非常庞大的，而佩奇和布林利用**稀疏矩阵**计算的技巧，大大简化了计算量，实现了这个网页排名算法。计算量仍然十分庞大，所以Google的工程师发明了**MapReduce**这个并行计算工具，使得PageRank的并行计算完全自动化。\n\n#### <font color=#0000ff>2.搜索技术——有限状态机</font>\n\n<font face=\"微软雅黑\">\n在《数学之美》中还提到了本地搜索的最基本技术——**有限状态机**。\n举一个例子就是要**搜索一个地址**，这就需要判断一个地址的正确性，同时非常准确地提炼出相应的地理信息（省、市、街道等）。这就就需要一个有限状态机来完成，**有限状态机**是一个特殊的有向图，有一个开始状态和一个终止状态，就比如开始是“XX省”，然后一个箭头既可以指向“XX市”，也可以指向“XX县”，但是不能是从“XX市”指向“XX省”，这里面具体的实现算法书中也并没有提到。\n</font>\n\n#### <font color=#0000ff>3.搜索中的最短路径问题</font>\n\n<font face=\"微软雅黑\">\n在这之后还提到了一个就是**全球导航和动态规划**(Dynamic Programming)的问题。就是要找出两个点之间的**最短距离问题**。当然这个最短距离是**带有权重**的，比如说花费的时间，消耗的金钱等等。\n\n举一个例子就是从**北京到广州的最短路径**，这时候可以把**北京—>广州的最短路径**拆成从**北京—>石家庄**和**石家庄—>广州**这两条路径，当这两条路径**都是最短**的时候，整体的路径就都是最短的。但是可能会有疑问就是北京—>广州的路径就**一定经过石家庄吗？**这个问题可以变成图论中的**最小割问题**（吐槽图论老师1万字省略。。。），没学过的可以想像一下，把你双手的五个手指张开，并左右手按手指顺序把手指尖碰在一起，这时从你左手臂到右手臂的这条路径中，肯定都会经过大拇指、食指、中指、无名指、小拇指之中的其中一个，都由这**五个手指**组成的点或者边就是图论里面的**最小割集或者最小边集**（比喻不是很恰当）。这时候打个比方，北京到广州的最短路径，就肯定会经过石家庄、天津、太原、郑州和武汉这**五个城市**中的其中一个，然后继续下去寻找**局部的最短路径**，就可以大大降低最短路径的计算复杂度。</br>\n</font>\n\n### <font color=#ff0000><2>TF-IDF、聚类问题</font>\n\n#### <font color=#0000ff>1.TF-IDF是什么</font>\n\n<font face=\"微软雅黑\">\n在《数学之美》中提到了如何确定**一个网页和某个查询的相关性**，而这个搜索关键词权重的科学衡量指标就是**TF-IDF(Term-frequency/Inverse Document Frequency)，即词频/逆文本频率指数**。\n\n其中**TF指的是关键词的频率**，比如说一个“原子能”的词在一个共有1000个词的网页上出现了2词，那么“原子能”的词频就是0.002，所以是多个词组成的关键词，就是把这些关键词的TF相加\n其中**IDF指的是就是关键词的权重**，比如“原子能的应用”中，“原子能”的权重要大于“的”和“应用”的权重，**公式为log(D/Dw)**，其中D是全部网页的个数，如果“的”在所有网页中都出现了的话，那么“的”的权重就是log(1)=0，这样就能对于不同的关键词有不同的权重。\n</font>\n\n#### <font color=#0000ff>2.新闻分类（聚类问题）和余弦相似度</font>\n\n<font face=\"微软雅黑\">\n除了搜索中**评价网页和某个查询的相关性**中用到**TF-IDF**，在新闻的分类，或者说**聚类问题**中，也用到了TF-IDF。把某一则新闻中所有的词算出的TF-IDF，按它的在词汇表中出现顺序进行排列，就可以得到一个向量。比如说词汇表共有64000个词，那么没有出现的词的TF-IDF就是0,而这个向量就是这一则新闻的**特征向量(Feature Vector)**。这时候，就可以利用**余弦定理**来计算两则新闻之间的夹角，而这个余弦的值就是两则新闻的相关度，即**余弦相似性**。越小表示两则新闻越相关，就越可以分在一类；如果算出来夹角是90度，那么就说明这两则新闻完全不相关，**余弦公式**：</font>\n\n![](http://images2015.cnblogs.com/blog/517519/201608/517519-20160812150047812-1638991450.gif)\n\n#### <font color=#0000ff>3.奇异值（SVD）分解</font>\n\n<font face=\"微软雅黑\">\n这样就可以根据**余弦相似性**的值设定阈值来对**新闻进行分类**。然而在计算的过程中，由于计算量很大，这是需要借助矩阵论的知识对这些计算量进行优化。在成千上万的新闻和百万的词汇表的计算中（比如说矩阵的每一一行代表词汇表中的每一个词，矩阵的每一列代表一则新闻），要计算每一列新闻中两两之间的余弦相似度，就要进行**奇异值分解（SVD分解）**，如果有N个词和M篇文章，这就是一个M*N的矩阵，此时M=1000000,N=500000,就是100万*50万，总共5000亿个元素。通过SVD分解，就能把这个**大矩阵**分解成一个100万*100的**矩阵X**，一个100*100的**矩阵B**，和一个100*50万的**矩阵Y**，这三个矩阵相乘的结果。这三个矩阵的元素共和为不超过1.5亿，计算量和存储量会大大减小。而这三个分解出来的矩阵也有非常清晰的物理含义，举个例子:\n\n   —————————A矩阵———————————            X矩阵      —————                B矩阵              ——————                Y矩阵\n![](http://images2015.cnblogs.com/blog/517519/201608/517519-20160812145405734-440117510.gif)\n\n其中**X矩阵**的每一行表示一个词，每一列表示一个语义相近的词类\n其中**B矩阵**表示词的类和文章的类之间的相关性\n其中**Y矩阵**是对文本的分类结果，每一行对应一个主题，每一列对应一个文本\n</font>\n\n#### <font color=#0000ff>4.相似哈希和信息指纹</font>    \n\n<font face=\"微软雅黑\">\n当然**余弦相似度**还可以用于对比**信息指纹**的重复性，《数学之美》中还提到了**相似哈希**这个特殊的信息指纹，具体的计算方法可以去看《数学之美》P150页。\n相似哈希的特点就是，如果两个网页的相似哈希相差越小，这两个网页的相似性越高；如果两个网页相同，则相似哈希肯定相同；如果少数权重小的词不同，其余的都相同，相似哈希也会几乎相同。之所以看到相似哈希会有印象，也是由于上次参加的某个算法比赛中对**作弊**的代码进行筛选的时候，主办方就用的相似哈希这个评价标准，所以对于常见的跨语言作弊、替换变量名作弊的方法都能进行有效的识别。\n</font>\n\n#### <font color=#0000ff>5.K-mean算法</font>   \n\n<font face=\"微软雅黑\">\n《数学之美》中提到**聚类问题**的时候，还提到了**期望最大化算法**，也就是**Ｋ-means算法**。\n\nＫ-means算法是随机地挑出出一些类的中心，然后来优化这些中心，使得它们和真实的聚类中心尽可能一致。假设有N篇文章，我们想把它们分成K类，这时我们可以不知道K具体是多少，最终分成多少类就分成多少类。\n\n**K-means算法**的步骤是：\n1.随机挑选K个点作为起始的中心；\n2.计算所有点到这些聚类中心的距离（使用欧几里德公式），将这些点归到最近的一类中；\n3.重新计算每一类的中心；\n4.迭代，最终偏移非常小的时候得到收敛。\n这一类算法的过程中，根据现有的模型，计算各个观测数据输入到模型中的计算结果，这个过程成为期望值计算过程，即**E过程**；接下来，重新计算模型参数，以最大化期望值，这一过程称为最大化的成果，即**M过程**。这一类算法都称为**EM算法**。</br>\n</font>\n\n### <font color=#ff0000><3>关于最大熵原理</font>\n\n<font face=\"微软雅黑\">\n**最大熵原理**指出，需要对一个随机事件的概率分布进行预测的时候，我们的预测应当满足全部已知的条件，而对未知的情况不要做任何的主观假设。在这种情况下，概率分布最均匀，预测的风险最小。因为这时概率分布的信息熵最大，所以人们称这种模型叫做“最大熵模型”。即**“不要把所有的鸡蛋放在一个篮子里”**，因为当我们遇到不确定性的时候，就要保留各种可能性。</br>\n</font>\n\n### <font color=#ff0000><4>关于机器学习和自然语言处理</font>\n\n#### <font color=#0000ff>1.图灵和人工智能</font>  \n\n<font face=\"微软雅黑\">\n**机器学习（Machine Learning）**是一门专门研究计算机怎样模拟或者实现人类的学习，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能的学科。而最早提出机器智能设想的是计算机科学之父—————**阿兰·图灵（Alan Turing）**。他在1950年的《思想》杂志上发表了一篇题为“计算的机器和智能”的论文中，提出了一种验证机器是否有智能的方法：让人和机器进行交流，如果人无法判断自己交流的对象是人还是机器的时候，就说明这个机器有了智能。后来这种方法被成为**图灵测试（Turning Test）**。\n</font>  \n\n#### <font color=#0000ff>2.自然语言处理的历史</font> \n\n<font face=\"微软雅黑\">\n而在自然语言的机器处理（**自然语言处理**）的**历史**中，可以分成两个阶段：\n\n1.一个是**20世纪50年代到70年代**，也就是从大约1950-1970的这段时间，全世界的科学家对计算机处理自然语言的认识都被自己局限在人类学习语言的方式上，就是用计算机模拟人脑。后面吴军博士提到了这段时间对自然语言处理的研究主要是**基于规则的方式**，想让计算机完成翻译或者语音识别这样只有人类才能完成的事情，就必须先让计算机理解自然语言，即让计算机有类似人类这样的智能。比如学习语言的过程中要学习语法规则，词性和构词法等等，而这些规则有很容易用算法来描述，所以在当时大家对基于规则的自然语言处理很有信心。但是实际上，当用算法来描述这些规则的时候，所形成的语法分析树十分的复杂，而且规则的数量众多，耗时且有时会互相矛盾。所以在20世纪70时代，基于规则的句法分析走到了尽头，也使得这段时间的成果近乎为零。\n\n2.另一个阶段是**20世纪70年代后**，一些先驱认识到**基于数学模型和统计的方法**才是正确的道路，所以自然语言处理进入了第二个阶段。这种方式的思想在于**统计**，即**大多数样本怎么做**。\n举个**例子**就是：pen有两个意思，一个意思是钢笔，而另一个意思是围栏。对于两个句子“The pen is in the box.”和“The box is in the pen.”来说，正确的翻译应该是“钢笔在盒子里。”和“盒子在围栏里。”，因为正常来说，盒子是不可能在钢笔里的。</br>\n</font>\n\n在20世纪70年代，给予统计的方法的核心模型是**通信系统加隐含马尔可夫模型**。这个系统的输入和输出都是一维的符号序列，而且保持原有的次序。最高获得成功的是**语音识别**是如此，后来第二个获得成功的此词性分析也是如此。而在句法分析中，输入的是一维的句子，输出的是二维的分析树；在**机器翻译**中，虽然输出的依然是一维的句子，但是次序会有很大的变化，所以上述的方法就不太管用了。1988年，IBM的彼得·布朗(Peter Brown)等人提出了基于统计的机器翻译方法，框架是正确的，但是由于缺乏足够的统计数据和没有足够强大的模型来解决不同语言语序颠倒的问题，所以实际的效果很差，后来这些人去了文艺复兴公司发财去了。而只有出现了基于有向图的统计模型才能很好的解决复杂的句法分析这个问题。</br>\n</font>\n\n### <font color=#ff0000><5>提到的计算机领域的科学家</font>\n\n<font face=\"微软雅黑\">\n1.阿兰·图灵(Alan Turing)————计算机科学之父\n2.约翰·麦卡锡(John McCarthy)————图灵奖获得者，发明Lisp语言，斯坦福大学人工智能实验室的主任\n3.马文·明斯基(Marvin Minsky)————图灵奖获得者，创建麻省理工学院(MIT)人工智能实验室\n4.纳撒尼尔·罗切斯特(Nathaniel Rochester)————IBM的杰出计算机设计师\n以上四人于1956年在达特茅斯学院举办的夏季人工智能研讨会是计算机科学史上的一座里程碑，参加会议的包括下面两位，总共10人。讨论了当时计算机科学尚未解决的问题，包括人工智能、自然语言处理和神经网络等。\n5.赫伯特·西蒙(Herbert Simon)————图灵奖获得者,同时还是美国管理学家和社会科学家，经济组织决策管理大师，第十届诺贝尔经济学奖获奖者\n6.艾伦·纽维尔(Allen Newell)————图灵奖获得者\n7.高德纳·克努特(Donald Knuth)————图灵奖获得者，算法和程序设计技术的先驱者，计算机排版系统TEX和METAFONT的发明者，提出用计算复杂度来衡量算法的耗时\n8.弗里德里克·贾里尼克(Frederick Jelinek)————领导IBM华生实验室，提出采用统计的方法解决语音识别问题\n9.米奇·马库斯(Mitch Marcus)————设立和领导了LDC项目，采集和整理全世界主要语言的语料（其中著名的是Penn Tree Bank），并培养了一批世界级的科学家</br>\n</font>\n\n### <font color=#ff0000><6>统计语言模型</font>\n\n<font face=\"微软雅黑\">\n计算机处理自然预言，一个基本的问题就是为**自然语言这种上下文相关的特性**建立数学模型。这个数学模型就是在自然语言处理中常说的**统计语言模型**。\n</font>\n\n#### <font color=#0000ff>1.怎样求一个句子S合理的可能性</font> \n\n<font face=\"微软雅黑\">\n**贾里尼克**的思想就是：认为一个句子是否合理，就是看它的**可能性大小**如何。\n假定**S表示某一个有意义的句子**，由一连串特定顺序排列的**词w1、w2...wn组成**，这里n是句子的长度。其中**S出现概率**是：\n<img src=\"https://latex.codecogs.com/svg.image?chl=\\Large P\\left \\{ S \\right \\} = P\\left \\{ w_{1},w_{2},\\cdots,w_{n} \\right \\}\" style=\"border:none;\">\n</font>\n\n#### <font color=#0000ff>2.一个句子S合理的可能性和句中每个词都有关</font> \n\n<font face=\"微软雅黑\">\n利用**条件概率公式**：\n<img src=\"https://latex.codecogs.com/svg.image?chl=\\Large P\\left ( A\\mid B \\right ) = \\frac{P\\left ( A B \\right ) }{P\\left ( B \\right ) }\" style=\"border:none;\">\n得：\n<img src=\"https://latex.codecogs.com/svg.image?chl=\\Large P\\left \\{ w_{1},w_{2},\\cdots,w_{n} \\right \\} = P\\left \\{ w_{1} \\right \\} \\cdot  P\\left \\{ w_{2} \\mid w_{1} \\right \\} \\cdots P\\left \\{ w_{n} \\mid w_{1},w_{2}, \\cdots ,w_{n-1} \\right \\}\" style=\"border:none;\">\n其中，**P(w1)**是第一个词出现的概率，**P(w2|w1)**是在已知第一个词的前提下，第二个词出现的概率，到了第n个词，这个词出现的概率和前面所有的词出现的概率都有关。\n</font>\n\n#### <font color=#0000ff>3.马尔可夫假设：后一个出现的概率只和前一个词有关（条件概率）</font> \n\n<font face=\"微软雅黑\">\n计算第一个词出现的概率很简单，即P(w1)，第二个词的条件概率P(w2|w1)很是可以计算的，但是越到后面，计算量就越来越大。到了19世纪至20世纪初的这段时间，俄罗斯有一个数学家——**马尔可夫(Andrey Markov)**就提出了方法，就是遇到这种情况，就**假设任意一个词Wi出现的概率只和前面一个词Wi-1有关**。这种假设在数学上成为**马尔可夫假设**。这下公式变成：\n<img src=\"https://latex.codecogs.com/svg.image?chl=\\Large P\\left ( S \\right ) = P\\left \\{ w_{1},w_{2},\\cdots,w_{n} \\right \\} = P\\left \\{ w_{1} \\right \\} \\cdot  P\\left \\{ w_{2} \\mid w_{1} \\right \\} \\cdots P\\left \\{ w_{n} \\mid w_{n-1} \\right \\}\" style=\"border:none;\">\n以上这个公式对应了**统计语言模型中的二元模型(Bigram Model)**。\n</font>\n\n#### <font color=#0000ff>4.如何计算条件概率——分别计算联合概率和边缘概率</font> \n\n<font face=\"微软雅黑\">\n接下来的问题变成了**如何计算条件概率**\n<img src=\"https://latex.codecogs.com/svg.image?chl=\\Large P\\left \\{ w_{n} \\mid w_{n-1} \\right \\} = \\frac{P\\left \\{ w_{n} , w_{n-1} \\right \\}}{P\\left \\{ w_{n-1} \\right \\} }\" style=\"border:none;\">\n\n当要计算**联合概率P(Wi-1,Wi)**和**边缘概率P(Wi-1)的时候**，需要大量的机读文本，也就是**语料库**。其中\n<img src=\"https://latex.codecogs.com/svg.image?chl=\\Large N \\left \\{ w_{n} , w_{n-1} \\right \\}\">是Wi-1,Wi这个二元词组在整个语料库中**前后相邻出现的次数**，而<img src=\"https://latex.codecogs.com/svg.image?chl=\\Large N \\left \\{ w_{n-1} \\right \\} \" > 是Wi-1这个词在整个语料库中**出现的次数**，这两个比值成为这个二元词组或者词的**相对频度**。\n\n根据**大数定理**，当统计量足够充分的时候，**相对频度就等于概率**，即\n<img src=\"https://latex.codecogs.com/svg.image?chl=\\Large P\\left \\{ w_{n} , w_{n-1} \\right \\} \\approx  \\frac{N\\left \\{ w_{n} , w_{n-1} \\right \\}}{ N }\">\n<img src=\"https://latex.codecogs.com/svg.image?chl=\\Large P\\left \\{ w_{n-1} \\right \\} \\approx  \\frac{N\\left \\{ w_{n-1} \\right \\}}{ N }\">\n所以，要计算的**条件概率**就等于\n<img src=\"https://latex.codecogs.com/svg.image?chl=\\Large P\\left \\{ w_{n} \\mid w_{n-1} \\right \\} \\approx \\frac{N\\left \\{ w_{n} , w_{n-1} \\right \\}}{N\\left \\{ w_{n-1} \\right \\} }\" style=\"border:none;\">\n</font>\n\n#### <font color=#0000ff>5.高阶模型的计算量</font> \n\n<font face=\"微软雅黑\">\n对于**高阶语言模型**，是假定文本中的每个词都和前面的N-1个词有关，而和更前面的词无关。对于N元模型的**空间复杂度**，几乎是N的**指数函数O(|V|^N)**，**时间复杂度**也是一个指数函数，**O(|V|^N-1)**.所以N的大小决定了空间复杂度和时间复杂度的大小。当N从1到2，从2再到3的时候，模型的效果上升的很明显，但是从3到4的时候，效果的提升就不是很显著了。\n\n模型训练中的问题：\n**零概率问题**——假定要训练一个汉语的语言模型，汉语的词汇量大致是20万这个量级，训练一个3元模型就需要200000^3 = 8*10^15这么多的参数，但是由于训练数据量不可能达到这么多，所以如果用直接的比值计算概率，大部分的条件概率依然是零，这种模型我们称之为“不平滑”。对于这种问题，图灵提出了一种在统计中相信可靠的统计数据，而对不可信的统计数据大折扣的一种概率估计方法，同时将折扣出来的那一小部分概率给予未看见的事件。这个重新估算概率的公式后来被称为**古德-图灵估计**。</br>\n</font>\n\n### <font color=#ff0000><7>隐含马尔可夫模型</font>\n\n#### <font color=#0000ff>1.已经求出一个句子S出现的概率</font> \n\n<font face=\"微软雅黑\">\n当求出了**一个句子S出现概率**之后：\n<img src=\"https://latex.codecogs.com/svg.image?chl=\\Large P\\left \\{ S \\right \\} = P\\left \\{ w_{1},w_{2},\\cdots,w_{n} \\right \\}\" style=\"border:none;\">\n</font>\n\n#### <font color=#0000ff>2.在已知一个英文句子O出现概率的情况下，求汉语句子S出现概率最大的那个S，即O最有可能被翻译成S（机器翻译）</font> \n\n<font face=\"微软雅黑\">\n对于一个观测到的信号o1,o2,……,on来还原发送的信号s1,s2,……,sn，实际的例子可以是根据收到的英文，求原来汉语的意思，这就是**机器翻译**的原理。写成数学表达式就是求以下公式有最大值时候的那个s1,s2,s3……：\n<img src=\"https://latex.codecogs.com/svg.image?chl=\\Large P\\left \\{ s_{1},s_{2},s_{3},\\cdots \\mid o_{1},o_{2},o_{3},\\cdots \\right \\}\" style=\"border:none;\">\n</font>\n\n#### <font color=#0000ff>3.利用贝叶斯公式化简</font> \n\n<font face=\"微软雅黑\">\n利用**贝叶斯公式**，可以把上述公式转换成（把条件概率公式用了两次）：\n<img src=\"https://latex.codecogs.com/svg.image?chl=\\Large \\frac{P\\left \\{ o_{1} , o_{2} , o_{3} \\cdots | s_{1} , s_{2} , s_{3} \\cdots \\right \\}\\cdot P\\left \\{ s_{1} , s_{2}, s_{3} \\cdots \\right \\}}{P\\left \\{ o_{1} , o_{2},o_{3} \\cdots \\right \\} }\" style=\"border:none;\">\n\n其中P{s1,s2,s3,……|o1,o2,o3,……}表示信息s1,s2,s3,……在传输之后变成接收的信号o1,o2,o3,……的可能性；\n而P{s1,s2,s3,……}表示s1,s2,s3,……本身是一个在接收端合乎情理的信号的可能性；最后而P{o1,o2,o3,……}表示在发送端产生信息o1,o2,o3,……的可能性（注意s和o在经过了条件概率公式后交换了发送和接收的位置），这时由于P{o1,o2,o3,……}是一个可以忽略的常熟，所以上面公式等价于\n<img src=\"https://latex.codecogs.com/svg.image?chl=\\Large P\\left \\{ o_{1} , o_{2} , o_{3} \\cdots | s_{1} , s_{2} , s_{3} \\cdots \\right \\}\\cdot P\\left \\{ s_{1} , s_{2}, s_{3} \\cdots \\right \\}\" style=\"border:none;\">\n</font>\n\n#### <font color=#0000ff>4.再次利用马尔可夫假设（转移概率）</font> \n\n<font face=\"微软雅黑\">\n这时，可以用**隐含马尔可夫模型**来解决这个问题：隐含马尔可夫模型其实并不是19世纪俄罗斯数学家马尔可夫发明的，而是美国数学家**鲍姆等人**在20世纪六七十年代发表的一系列论文中提出的，隐含马尔可夫模型的训练方法（**鲍姆-韦尔奇算法**）也是以他的名字命名的。隐含马尔可夫模型之所以是隐含的，是因为它每一个时刻的状态是不可见的。而对于马尔可夫链来说，对于一个随机时间序列S1,S1,S3,……St中的每一个状态的概率，不只其本身是随机的，也就是说它可以是马尔可夫链中的任何一个状态，而且它还和前一个状态有关，这两点说明了St的概率有**两个维度的不确定性**。在任一时刻t的状态St是不可见的且是不确定的，因为这时候只知道输出的序列Ot，所以观察者不能通过观察到s1,s1,s3,……sr来推测转移概率等参数。但是可以通过Ot来推测这些参数。\n此时根据上节马尔可夫提出的只和前一个有关的假设：\n<img src=\"https://latex.codecogs.com/svg.image?chl=\\Large P\\left \\{ o_{1} , o_{2} , o_{3} \\cdots | s_{1} , s_{2} , s_{3} \\cdots \\right \\}\" style=\"border:none;\">\n<img src=\"https://latex.codecogs.com/svg.image?chl=\\Large  = \\prod_{t}^{1} P\\left \\{ o_{t} | s_{t} \\right \\} \" style=\"border:none;\">\n<img src=\"https://latex.codecogs.com/svg.image?chl=\\Large P\\left \\{ s_{1} , s_{2} , s_{3} \\cdots \\right \\}\" style=\"border:none;\">\n<img src=\"https://latex.codecogs.com/svg.image?chl=\\Large  = \\prod_{t}^{1} P\\left \\{ s_{t} | s_{t-1} \\right \\} \" style=\"border:none;\">\n所以，可得\n<img src=\"https://latex.codecogs.com/svg.image?chl=\\Large P\\left \\{ o_{1} , o_{2} , o_{3} \\cdots | s_{1} , s_{2} , s_{3} \\cdots \\right \\}\\cdot P\\left \\{ s_{1} , s_{2}, s_{3} \\cdots \\right \\}\" style=\"border:none;\">\n<img src=\"https://latex.codecogs.com/svg.image?chl=\\Large  = \\prod_{t}^{1} P\\left \\{ s_{t} | s_{t-1} \\right \\} \\cdot P\\left \\{ o_{t} | s_{t} \\right \\} \" style=\"border:none;\">\n其中**P{s1,s2,s3,……}**就是上一节求出的**每一个句子是否合理的可能性**，也就是从收到的英文{O1,O2,O3,……}求原来的汉语{S1,S2,S3,……}的过程中，这个汉语句子{s1,s2,s3,……}存在的合理性。可以理解为“The box is in the pen”被翻译成“盒子在钢笔中”和“盒子在围栏中”这两个句子哪个更合理。\n而**P{s1,s2,s3,……|o1,o2,o3,……}**根据应用的不同而有不同的名称，在语音识别中被成为“声学模型”，在机器翻译中是“翻译模型”，而在拼写校正中是“纠错模型”。\n</font>\n\n#### <font color=#0000ff>5.如何求转移概率和生成概率</font> \n\n<font face=\"微软雅黑\">\n此时，问题变成了**求转移概率和生成概率乘积的最大值**的问题\n**1.转移概率**\n<img src=\"https://latex.codecogs.com/svg.image?chl=\\Large P\\left \\{ s_{t} | s_{t-1} \\right \\} = \\frac{P\\left \\{ s_{t-1} , s_{t} \\right \\}}{P\\left \\{ s_{t-1} \\right \\}} \\approx \\frac{N\\left \\{ s_{t-1} , s_{t} \\right \\}}{N\\left \\{ s_{t-1} \\right \\}}\" style=\"border:none;\">\n其中N(St-1)就是在有了足够多的人工标记的数据后，经过状态St-1的次数（可以理解为在语料库中某个词St-1出现的次数），而N(St-1,St)是在经过St-1这个状态后转移到St这个状态的次数（可以理解为在语料库中词St-1后出现词St的次数），而这两个数的比值就是**转移概率P(St|St-1)**的值\n同理\n**2.生成概率**\n<img src=\"https://latex.codecogs.com/svg.image?chl=\\Large P\\left \\{ o_{t} | s_{t} \\right \\} = \\frac{P\\left \\{ o_{t} , s_{t} \\right \\}}{P\\left \\{ s_{t} \\right \\}} \\approx \\frac{N\\left \\{ o_{t} , s_{t} \\right \\}}{N\\left \\{ s_{t} \\right \\}} \" style=\"border:none;\">\n其中N(St)就是在有了足够多的人工标记的数据后，经过状态St的次数（可以理解为在语料库中某个词St出现的次数），而N(Ot,St)是在经过Ot这个状态后转移到St这个状态的次数（可以理解为在语料库中英文单词Ot被翻译成汉语词语St的次数），而这两个数的比值就是**生成概率P(Ot|St)**的值\n</font>\n\n#### <font color=#0000ff>6.如何求转移概率和生成概率乘积的最大值</font> \n\n<font face=\"微软雅黑\">\n当求出转移概率和生成概率之后，就能求出一个已知的英文句子{O1,O2,O3,……}被翻译成{S1,S2,S3,……}的所有可能性的大小，然后下一步就是要**求这所有的可能性中最大的时候的那个{S1,S2,S2,……}**，这需要用到**维特比算法**。维特比算法是一个**动态规划算法**，**在求所以可能性中最大的值的时候，也相当于求一个求概率最大的路径（或者最短路径）的问题**。在已知输出的英文{O1,O2,O3,……}的情况下，可能有很多种不同的路径能从而得到不同的中文{S1,S2,S3,……}，这些路径组成了一个成为**篱笆网络（Lattice）的图**，其中我们要求的是最可能的，也就是概率最大的那个中文{S1,S2,S3,……}。\n举个**例子**就是Bush可以翻译成布什，也可以翻译成灌木丛，但是当Bush的前一个词是总统的时候，Bush翻译成布什的概率最大。这时就可以参考从北京到广州的最短路径的问题，使用维特比算法对计算量进行化简。\n\n**有监督的训练**需要大量人工标记的数据，实际上很多应用不可能做到，就翻译模型的训练来说，这需要大量的中英文对照的语料，还要把中英文词语一一对应起来。所以在实际的训练中常常使用的是**无监督的训练算法，即鲍姆-韦尔奇算法**。鲍姆-韦尔奇算法的思想是首先寻找一组能够产生输出序列O的模型参数，这个模型是一定存在的，因为当转移概率P和输出概率Q为均匀分布的时候，模型可以产生任何输出。然后在这个模型的基础上算出这个模型产生输出序列O的概率，并根据这个模型可以找到一个更好的模型，这个新的模型输出序列O的概率大于原来的模型。经过了不断的迭代从而不断的估计新的模型参数，使得输出序列O的概率最大化。这个过程被称为**期望最大化(Expectation-Maximization)，简称EM过程**。\n具体的方法和公式《数学之美》中并没有细说，具体的过程就没能理解了。\n</br>\n</font>\n\n#### **结束，理解有错误的话望指出**\n","tags":["杂谈"]},{"title":"Java数据结构——哈希表","url":"/Java数据结构——哈希表.html","content":"<img src=\"/images/517519-20160413184136754-487510155.png\" alt=\"\" width=\"746\" height=\"376\" />\n\n## 1.开放地址法\n\n<!--more-->\n&nbsp;\n\n## 2.链地址法\n","tags":["数据结构"]},{"title":"Java数据结构——二叉树","url":"/Java数据结构——二叉树.html","content":"<img src=\"/images/517519-20160411164514457-1856536877.png\" alt=\"\" width=\"679\" height=\"399\" />\n\n<img src=\"/images/517519-20160411164542191-936775937.png\" alt=\"\" width=\"684\" height=\"168\" />\n\n<!--more-->\n&nbsp;\n\n<img src=\"/images/517519-20160412114532332-1735988698.png\" alt=\"\" width=\"762\" height=\"417\" />\n\n## 1.二叉树的遍历\n\n**前序遍历&mdash;&mdash;根 左 右**\n\n**中序遍历&mdash;&mdash;左 根 右**\n\n**后序遍历&mdash;&mdash;左 右 根**\n\n&nbsp;\n\n## 2.二叉树的实现\n\n```\nclass Stack_BinaryTree{\n\tprivate int maxSize;\t\t\t//栈的长度\n\tprivate Node[] stackArray;\t\t//创建栈的数组的引用\n\tprivate int top;\t\t\t//创建栈顶的引用\n\t\n\tpublic Stack_BinaryTree(int s) {\t//构造函数\n\t\tthis.maxSize = s;\n\t\tstackArray = new Node[maxSize]; //创建对象\n\t\ttop = -1;\t\t\t//栈顶等于-1\n\t}\n\t\n\tpublic void push(Node j){\t\t//入栈操作\n\t\tstackArray[++top] = j;\t\t//先把top=-1自加成0,再入栈\n\t}\n\t\n\tpublic Node pop(){\n\t\treturn stackArray[top--];\t//弹出当前栈顶的元素后，再自减\n\t}\n\t\n\tpublic Node peek(){\n\t\treturn stackArray[top];\t\t//返回当前栈顶的元素\n\t}\n\t\n\tpublic boolean isEmpty(){\t//栈顶为-1,即栈为空\n\t\treturn (top == -1);\n\t}\n\t\n\tpublic boolean isFull(){\t//栈顶为maxSize-1,即栈为满\n\t\treturn (top == maxSize-1);\n\t}\n\t\n}\n\nclass Node{\n\tint iData;\n\tdouble fData;\n\tNode leftChild;\n\tNode rightChild;\n\t\n\tpublic void display(){\n\t\tSystem.out.println('{');\n\t\tSystem.out.println(iData);\n\t\tSystem.out.println(',');\n\t\tSystem.out.println(fData);\n\t\tSystem.out.println('}');\n\t}\n}\n\nclass Tree{\n\tpublic Node root;\n\t\n\tpublic Tree(){\n\t\troot = null;\n\t}\n\t\n\tpublic Node find(int key){\t\t//查找关键字的节点\n\t\tNode current = root;\n\t\twhile(current.iData != key){\t//不等于就一直循环\n\t\t\tif(key<current.iData){\n\t\t\t\tcurrent = current.leftChild;\n\t\t\t}else{\n\t\t\t\tcurrent = current.rightChild;\n\t\t\t}\n\t\t\tif(current == null){\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t\treturn current;\t\n\t}\n\t\n\tpublic void insert(int id,double dd){\t//插入新的节点\n\t\tNode newNode = new Node();\n\t\tnewNode.iData = id;\n\t\tnewNode.fData = dd;\n\t\tif(root == null){\n\t\t\troot = newNode;\n\t\t}else{\n\t\t\tNode current = root;\n\t\t\tNode parent;\n\t\t\twhile(true){\n\t\t\t\tparent = current;\n\t\t\t\tif(newNode.iData<current.iData){\t//如果插入的节点的数据小于当前节点的数据\n\t\t\t\t\tcurrent = current.leftChild;\n\t\t\t\t\tif(current == null){\n\t\t\t\t\t\tparent.leftChild = newNode;\t//把父亲节点的左孩子设为新节点\n\t\t\t\t\t\treturn;\n\t\t\t\t\t}\n\t\t\t\t}else{\t\t//如果插入的节点的数据大于当前节点的数据\n\t\t\t\t\tcurrent = current.rightChild;\n\t\t\t\t\tif(current == null){\n\t\t\t\t\t\tparent.rightChild = newNode;\t//把父亲节点的右孩子设为新节点\n\t\t\t\t\t\treturn;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\t\n\tpublic void displayTree(){\t//显示整个二叉树\n\t\tStack_BinaryTree globalStack = new Stack_BinaryTree(128);\t//用来放置每一层的二叉树\n\t\tglobalStack.push(root);\t\t//入栈\n\t\tint nBlanks = 32;\n\t\tboolean isRowEmpty = false;\n\t\tSystem.out.println(\".............................................\");\n\t\twhile(isRowEmpty==false){\n\t\t\tStack_BinaryTree localStack = new Stack_BinaryTree(128);\t//用来放置下一层的二叉树\n\t\t\tisRowEmpty = true;\n\t\t\tfor(int j=0;j<nBlanks;j++){\n\t\t\t\tSystem.out.print(' ');\n\t\t\t}\n\t\t\t\n\t\t\twhile(globalStack.isEmpty()==false){\t\t\t//当globalStack不为空，就一直出栈\n\t\t\t\tNode temp = (Node)globalStack.pop();\t\t//temp等于globalStack出栈的节点\n\t\t\t\tif(temp!=null){\n\t\t\t\t\tSystem.out.print(temp.iData);\t\t//当当前的节点不为空的时候，输出节点的值\n\t\t\t\t\tlocalStack.push(temp.leftChild);\t//入栈globalStack的左孩子到下一层\n\t\t\t\t\tlocalStack.push(temp.rightChild);\t//入栈globalStack的右孩子到下一层\n\t\t\t\t\tif(temp.leftChild != null || temp.rightChild != null){\n\t\t\t\t\t\tisRowEmpty = false;\t\t//只要有一个子节点不为空，就把isRowEmpty置为false\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse{\t\t\t\t\t\t//否则输出--，并把下一层置为空\n\t\t\t\t\tSystem.out.print(\"--\");\n\t\t\t\t\tlocalStack.push(null);\n\t\t\t\t\tlocalStack.push(null);\n\t\t\t\t}\n\t\t\t\tfor(int j=0;j<nBlanks*2-2;j++){\n\t\t\t\t\tSystem.out.print(' ');\n\t\t\t\t}\n\t\t\t}\n\t\t\t\n\t\t\tSystem.out.println();\n\t\t\tnBlanks /= 2;\t\t\t\t\t//输出的空格数减半\n\t\t\twhile(localStack.isEmpty() == false){\n\t\t\t\tglobalStack.push(localStack.pop());\t//还原本来的层\n\t\t\t}\n\t\t}\n\t\tSystem.out.println(\".............................................\");\n\t}\n\t\n\tpublic void preOrder(Node localRoot){\t\t//前序遍历\n\t\tif(localRoot != null){\n\t\t\tSystem.out.print(localRoot.iData+\" \");\n\t\t\tpreOrder(localRoot.leftChild);\n\t\t\tpreOrder(localRoot.rightChild);\n\t\t}\n\t}\n\t\n\tpublic void inOrder(Node localRoot){\t\t//中序遍历\n\t\tif(localRoot != null){\n\t\t\tinOrder(localRoot.leftChild);\n\t\t\tSystem.out.print(localRoot.iData+\" \");\n\t\t\tinOrder(localRoot.rightChild);\n\t\t}\n\t}\n\t\n\tpublic void postOrder(Node localRoot){\t\t//后序遍历\n\t\tif(localRoot != null){\n\t\t\tpostOrder(localRoot.leftChild);\n\t\t\tpostOrder(localRoot.rightChild);\n\t\t\tSystem.out.print(localRoot.iData+\" \");\n\t\t}\n\t}\n\t\n\tpublic Node minimum(){\t\t//找到最小的节点\n\t\tNode current;\n\t\tNode last = null;\n\t\tcurrent = root;\n\t\twhile(current != null){\n\t\t\tlast = current;\n\t\t\tcurrent = current.leftChild;\n\t\t}\n\t\treturn last;\n\t}\n\t\n\tpublic Node maxmum(){\t\t//找到最大的节点\n\t\tNode current;\n\t\tNode last = null;\n\t\tcurrent = root;\n\t\twhile(current != null){\n\t\t\tlast = current;\n\t\t\tcurrent = current.rightChild;\n\t\t}\n\t\treturn last;\n\t}\n\t\n\tpublic boolean delete(int key){\n\t\tNode current = root;\n\t\tNode parent = root;\n\t\tboolean isLeftChild = true;\n\t\t\n\t\twhile(current.iData != key){//查找要删除的节点，并把其置为current，如果没有返回null\n\t\t\tparent = current;\n\t\t\tif(key<current.iData){\n\t\t\t\tcurrent = current.leftChild;\n\t\t\t\tisLeftChild = true;\t\t\t//当前current节点的parent节点有左孩子节点\n\t\t\t}else{\n\t\t\t\tcurrent = current.rightChild;\n\t\t\t\tisLeftChild = false;\t\t\t//当前current节点的parent节点没有左孩子节点\n\t\t\t}\n\t\t\tif(current == null){\n\t\t\t\treturn false;\n\t\t\t}\n\t\t}\n\t\t//进入以下的时候，说明current已经匹配到要删除的节点\n\t\t//如果匹配的current节点没有孩子节点\n\t\tif(current.leftChild == null &amp;&amp; current.rightChild == null){\n\t\t\tif(current == root){\t\t//如果这个节点就是根节点\n\t\t\t\troot = null;\n\t\t\t}else if(isLeftChild){\n\t\t\t\tparent.leftChild = null;\t\t//父节点断开左孩子节点\n\t\t\t}else{\n\t\t\t\tparent.rightChild = null;\t//父节点断开右孩子节点\n\t\t\t}\n\t\t}\n\t\t//如果current没有右孩子，则要把左子树上移\n\t\telse if(current.rightChild == null){\t\n\t\t\tif(current == root){\t\t\t\t\t\t\t//如果是根节点\n\t\t\t\troot = current.leftChild;\n\t\t\t}else if(isLeftChild){\t//如果current节点是左孩子，则把current的左孩子放到parent的左孩子位置\n\t\t\t\tparent.leftChild = current.leftChild;\n\t\t\t}else{\t\t\t\t\t\t\t\t//如果current节点是右孩子，则把current的左孩子放到parent的右孩子位置\n\t\t\t\tparent.rightChild = current.leftChild;\n\t\t\t}\n\t\t}\n\t\t//如果current没有左孩子，则要把右子树上移\n\t\telse if(current.leftChild == null){\t\n\t\t\tif(current == root){\t\t\t\t\t\t\t//如果是根节点\n\t\t\t\troot = current.rightChild;\n\t\t\t}else if(isLeftChild){\t//如果current节点是左孩子，则把current的右孩子放到parent的左孩子位置\n\t\t\t\tparent.leftChild = current.rightChild;\n\t\t\t}else{\t\t\t\t\t\t\t\t//如果current节点是右孩子，则把current的右孩子放到parent的右孩子位置\n\t\t\t\tparent.rightChild = current.rightChild;\n\t\t\t}\n\t\t}\n\t\t//如果current有左右孩子\n\t\telse{\n\t\t\tNode successor = getSuccessor(current);\n\t\t\tif(current == root){\t\t//如果要删除的节点是根节点\n\t\t\t\troot = successor;\n\t\t\t}else if(isLeftChild){\t\t//如果要删除的节点是左孩子\n\t\t\t\tparent.leftChild  = successor;\n\t\t\t}else{\t\t\t\t\t\t\t\t\t//如果要删除的节点是右孩子\n\t\t\t\tparent.rightChild  = successor;\n\t\t\t}\n\t\t\tsuccessor.leftChild = current.leftChild;//把最小的值的节点连接到要删除节点的左子树上\n\t\t}\n\t\treturn true;\n\t}\n\t\n\tprivate Node getSuccessor(Node delNode){\n\t\tNode successorParent = delNode;\n\t\tNode successor = delNode;\n\t\tNode current = delNode.rightChild;\n\t\twhile(current != null){\t\t\t\t\t\t\t//循环，直到返回右子树中最小的值\n\t\t\tsuccessorParent = successor;\n\t\t\tsuccessor = current;\n\t\t\tcurrent = current.leftChild;\n\t\t}\n\t\tif(successor != delNode.rightChild){\n\t\t\tsuccessorParent.leftChild = successor.rightChild;\t//把最小的值的右孩子放到该最小值的位置\n\t\t\tsuccessor.rightChild = delNode.rightChild;\t\t\t\t//把最小的值连接到要删除的节点的右子树上\n\t\t}\n\t\treturn successor;\n\t}\n\t\t\n}\n\npublic class BinaryTree {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tint value;\n\t\tTree theTree = new Tree();\n\t\ttheTree.insert(50, 1.5);\n\t\ttheTree.insert(25, 1.4);\n\t\ttheTree.insert(75, 1.3);\n\t\ttheTree.insert(12, 1.6);\n\t\ttheTree.insert(37, 1.7);\n\t\t//theTree.insert(43, 1.2);\n\t\ttheTree.insert(60, 1.1);\n\t\ttheTree.insert(85, 1.1);\n\t\ttheTree.insert(77, 1.1);\n\t\ttheTree.displayTree();\n\t\t\n\t\tSystem.out.println(\"查找节点的值：\"+theTree.find(77).iData);\n\t\t\n\t\ttheTree.preOrder(theTree.root);\n\t\tSystem.out.println();\n\t\ttheTree.inOrder(theTree.root);\n\t\tSystem.out.println();\n\t\ttheTree.postOrder(theTree.root);\n\t\tSystem.out.println();\n\t\tSystem.out.println(\"最小的节点：\"+theTree.minimum().iData);\n\t\tSystem.out.println(\"最大的节点：\"+theTree.maxmum().iData);\n\t\t\n\t\ttheTree.delete(75);\n\t\ttheTree.displayTree();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n## **3.树、森林和二叉树之间的转换**\n\n### **树转换为二叉树**\n\n1. 加线\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在所有兄弟结点之间加一条连线。\n\n2. 去线\n\n&nbsp;&nbsp;&nbsp;&nbsp; 树中的每个结点，只保留它与第一个孩子结点的连线，删除它与其它孩子结点之间的连线。\n\n3. 层次调整\n\n&nbsp;&nbsp;&nbsp; 以树的根节点为轴心，将整棵树顺时针旋转一定角度，使之结构层次分明。（注意第一个孩子是结点的左孩子，兄弟转换过来的孩子是结点的右孩子）\n\n<img src=\"/images/2012110416594150.jpg\" alt=\"\" style=\"display: block; margin-left: auto; margin-right: auto;\" />\n\n### **森林转换为二叉树**\n\n1. 把每棵树转换为二叉树。\n\n2. 第一棵二叉树不动，从第二棵二叉树开始，依次把后一棵二叉树的根结点作为前一棵二叉树的根结点的右孩子，用线连接起来。\n\n<img src=\"/images/2012110417004247.jpg\" alt=\"\" style=\"display: block; margin-left: auto; margin-right: auto;\" />\n\n### **二叉树转换为树**\n\n是树转换为二叉树的逆过程。\n\n1. 加线\n\n&nbsp;&nbsp;&nbsp;&nbsp;若某结点X的左孩子结点存在，则将这个左孩子的右孩子结点、右孩子的右孩子结点、右孩子的右孩子的右孩子结点&hellip;，都作为结点X的孩子。将结点X与这些右孩子结点用线连接起来。\n\n2. 去线\n\n&nbsp;&nbsp;&nbsp;&nbsp; 删除原二叉树中所有结点与其右孩子结点的连线。\n\n3. 层次调整。\n\n<img src=\"/images/2012110417011138.jpg\" alt=\"\" style=\"display: block; margin-left: auto; margin-right: auto;\" />\n\n### **二叉树转换为森林**\n\n假如一棵二叉树的根节点有右孩子，则这棵二叉树能够转换为森林，否则将转换为一棵树。\n\n1. 从根节点开始，若右孩子存在，则把与右孩子结点的连线删除。再查看分离后的二叉树，若其根节点的右孩子存在，则连线删除&hellip;。直到所有这些根节点与右孩子的连线都删除为止。\n\n2.&nbsp;将每棵分离后的二叉树转换为树。\n\n<img src=\"/images/2012110417014911.jpg\" alt=\"\" style=\"display: block; margin-left: auto; margin-right: auto;\" />\n\n&nbsp;\n\n## 4.平衡二叉树的平衡因子\n\n若向平衡二叉树中插入一个新结点后破坏了平衡二叉树的平衡性。首先要找出插入新结点后失去平衡的最小子树根结点的指针。然后再调整这个子树中有关结点之间的链接关系，使之成为新的平衡子树。当失去平衡的最小子树被调整为平衡子树后，原有其他所有不平衡子树无需调整，整个二叉排序树就又成为一棵平衡二叉树。\n\n失去平衡的最小子树是指以离插入结点最近，且平衡因子绝对值大于 1 的结点作为根的子树。假设用 A 表示失去平衡的最小子树的根结点，则调整该子树的操作可归纳为下列四种情况。\n\n（ 1 ） LL 型平衡旋转法 \n\n由于在 A 的左孩子 B 的左子树上插入结点 F ，使 A 的平衡因子由 1 增至 2 而失去平衡。故需进行一次顺时针旋转操作。 即将 A 的左孩子 B 向右上旋转代替 A 作为根结点， A 向右下旋转成为 B 的右子树的根结点。而原来 B 的右子树则变成 A 的左子树。\n\n（ 2 ） RR 型平衡旋转法 \n\n由于在 A 的右孩子 C&nbsp; 的右子树上插入结点 F ，使 A 的平衡因子由 -1 减至 -2 而失去平衡。故需进行一次逆时针旋转操作。即将 A 的右孩子 C 向左上旋转代替 A 作为根结点， A 向左下旋转成为 C 的左子树的根结点。而原来 C 的左子树则变成 A 的右子树。\n\n（ 3 ） LR 型平衡旋转法 \n\n由于在 A 的左孩子 B 的右子数上插入结点 F ，使 A 的平衡因子由 1 增至 2 而失去平衡。故需进行两次旋转操作（先逆时针，后顺时针）。即先将 A 结点的左孩子 B 的右子树的根结点 D 向左上旋转提升到 B 结点的位置，然后再把该 D 结点向右上旋转提升到 A 结点的位置。即先使之成为 LL 型，再按 LL 型处理 。 \n\n&nbsp;如图中所示，即先将圆圈部分先调整为平衡树，然后将其以根结点接到 A 的左子树上，此时成为 LL 型，再按 LL 型处理成平衡型。\n\n（ 4 ） RL 型平衡旋转法 \n\n由于在 A 的右孩子 C 的左子树上插入结点 F ，使 A 的平衡因子由 -1 减至 -2 而失去平衡。故需进行两次旋转操作（先顺时针，后逆时针），即先将 A 结点的右孩子 C 的左子树的根结点 D 向右上旋转提升到 C 结点的位置，然后再把该 D 结点向左上旋转提升到 A 结点的位置。即先使之成为 RR 型，再按 RR 型处理。\n\n如图中所示，即先将圆圈部分先调整为平衡树，然后将其以根结点接到 A 的左子树上，此时成为 RR 型，再按 RR 型处理成平衡型。\n\n平衡化靠的是旋转。 参与旋转的是 3 个节点（其中一个可能是外部节点 NULL ），旋转就是把这 3 个节点转个位置。注意的是，左旋的时候 p->right 一定不为空，右旋的时候 p->left 一定不为空，这是显而易见的。 \n\n如果从空树开始建立，并时刻保持平衡，那么不平衡只会发生在插入删除操作上，而不平衡的标志就是出现 bf == 2 或者 &nbsp;bf == -2 的节点。\n\n## 5.树的三种存储结构\n\n（转自http://blog.csdn.net/x1247600186/article/details/24670775）\n\n说到存储结构,我们就会想到常用的两种存储方式:顺序存储和链式存储两种。\n\n先来看看顺序存储，用一段地址连续的存储单元依次存储线性表中数据元素，这对于线性表来说是很自然的，但是对于树这种一对多的结构而言是否适合呢？\n\n树中某个结点的孩子可以有多个，这就意味着，无论用哪种顺序将树中所有的结点存储到数组中，结点的存储位置都无法直接反映逻辑关系，试想一下，数据元素挨个存储，那么谁是谁的双亲，谁是谁的孩子呢？所以简单的顺序存储是不能满足树的实现要求的。\n\n不过可以充分利用顺序存储和链式存储结构的特点，完全可以实现对树的存储结构的表示。\n\n下面介绍三种不同的树的表示法:双亲表示法,、孩子表示法,、孩子兄弟表示法。\n\n1、双亲表示法：\n\n&nbsp; &nbsp; &nbsp;我们假设**以一组连续空间存储树的结点，同时在每个结点中，附设一个指示器指向其双亲结点到链表中的位置**。也就是说每个结点除了知道自己之外还需要知道它的双亲在哪里。\n\n它的结构特点是如图所示： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <img src=\"/images/517519-20200507225139733-386166775.png\" alt=\"\" width=\"200\" height=\"65\" />\n\n以下是我们的双亲表示法的结构定义代码：\n\n```\n     /*树的双亲表示法结点结构定义  */  \n    #define MAXSIZE 100  \n    typedef int ElemType;       //树结点的数据类型，暂定为整形   \n    typedef struct PTNode       //结点结构  \n    {  \n        ElemType data;          //结点数据  \n        int parent;             //双亲位置  \n    }PTNode;  \n      \n    typedef struct  \n    {  \n        PTNode nodes[MAXSIZE];  //结点数组  \n        int r,n;                //根的位置和结点数  \n    }PTree;  \n\n```\n\n&nbsp;\n\n2、孩子表示法\n\n换一种不同的考虑方法。由于每个结点可能有多棵子树，可以考虑使用多重链表，即每个结点有多个指针域，其中每个指针指向一棵子树的根结点，我们把这种方法叫做多重链表表示法。不过树的每个结点的度，也就是它的孩子个数是不同的。所以可以设计两种方案来解决。\n\n方案一：\n\n一种是指针域的个数就等于树的度（树的度是树的各个结点度的最大值）\n\n其结构如图所示：\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <img src=\"/images/517519-20200507225215094-2103153695.png\" alt=\"\" width=\"300\" height=\"67\" />\n\n不过这种结构由于每个结点的孩子数目不同，当差异较大时，很多结点的指针域就都为空，显然是浪费空间的，不过若树的各结点度相差很小时，那就意味着开辟的空间都被利用了，这时这种缺点反而变成了优点。\n\n<br />\n\n\n方案二：\n\n第二种方案是每个结点指针域的个数等于该结点的度，我们专门取一个位置来存储结点指针域的个数。\n\n其结构如图所示：\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <img src=\"/images/517519-20200507225251072-515327194.png\" alt=\"\" width=\"300\" height=\"63\" />\n\n这种方法克服了浪费空间的缺点，对空间的利用率是很高了，但是由于各个结点的链表是不相同的结构，加上要维护结点的度的数值，在运算上就会带来时间上的损耗。\n\n能否有更好的方法呢，既可以减少空指针的浪费，又能是结点结构相同。\n\n说到这大家肯定就知道是有的麦，那就是孩子表示法。\n\n具体办法是，**把每个结点的孩子排列起来，以单链表做存储结构，则n个结点有n个孩子链表，如果是叶子结点则此单链表为空。然后n个头指针有组成一个线性表，采用顺序存储结构，存放进入一个一维数组中**。\n\n为此，设计两种结点结构，\n\n一个是孩子链表的孩子结点，如下所示：\n\n<img src=\"/images/517519-20200507231250422-724223363.png\" alt=\"\" width=\"200\" height=\"81\" />\n\n其中child是数据域，用来存储某个结点在表头数组中的下标。next是指针域，用来存储指向某结点的下一个孩子结点的指针。\n\n另一个是表头结点，如下所示：\n\n<img src=\"/images/517519-20200507231208768-1899209711.png\" alt=\"\" width=\"200\" height=\"72\" />\n\n&nbsp;\n\n&nbsp;\n\n其中data是数据域，存储某结点的数据信息。firstchild是头指针域，存储该结点的孩子链表的头指针。\n\n以下是孩子表示法的结构定义代码：\n\n```\n    /*树的孩子表示法结点结构定义  */  \n    #define MAXSIZE 100  \n    typedef int ElemType;       //树结点的数据类型，暂定为整形   \n    typedef struct CTNode       //孩子结点  \n    {  \n        int child;  \n        struct CTNode *next;  \n    }*ChildPtr;  \n      \n    typedef struct              //表头结构  \n    {  \n        ElemType data;  \n        ChildPtr firstchild;  \n    }CTBox;  \n    typedef struct              //树结构  \n    {  \n        CTBox nodes[MAXSIZE];   //结点数组  \n        int r,n;                //根结点的位置和结点数  \n    }CTree;  \n\n```\n\n&nbsp;\n\n 3、孩子兄弟表示法\n\n我们发现，任意一颗树，它的结点的第一个孩子如果存在就是的，它的右兄弟如果存在也是唯一的。因此，我们**设置两个指针，分别指向该结点的第一个孩子和此结点的右兄弟。**\n\n其结点结构如图所示：\n\n<img src=\"/images/517519-20200507231139137-1754603018.png\" alt=\"\" width=\"300\" height=\"83\" />\n\n以下是孩子兄弟表示法的结构定义代码：<br />\n\n\n```\n        /*树的孩子兄弟表示法结构定义 */  \n        typedef struct CSNode  \n        {  \n            ElemType  data;  \n            struct CSNode  *firstchild, *rightsib;  \n        }CSNode, *CSTree;\n\n```\n\n&nbsp;\n","tags":["数据结构"]},{"title":"Java排序算法——快速排序","url":"/Java排序算法——快速排序.html","content":"<img src=\"/images/517519-20160410161026640-571012398.png\" alt=\"\" width=\"664\" height=\"124\" />\n\n输入是List\n\n```\npackage com.interview.sort;\n\nimport java.util.ArrayList;\nimport java.util.Iterator;\nimport java.util.List;\n\npublic class QuickSort2 {        //输入是List<Integer>\n\n    public static void main(String[] args) {\n        // TODO 自动生成的方法存根\n        List<Integer> list = new ArrayList<Integer>();\n        list.add(-1);\n        list.add(3);\n        list.add(-0);\n        list.add(-2);\n        list.add(-7);\n        list.add(-5);\n        QuickSort2 qs = new QuickSort2();\n        qs.quickSort(list);\n        Iterator<Integer> i = list.iterator();\n        while (i.hasNext()) {\n            int num = (Integer) i.next();\n            System.out.println(num);\n        }\n\n    }\n\n    public void quickSort(List<Integer> list) {\n        if (list.size() > 1) {\n            List<Integer> smaller = new ArrayList<Integer>();\n            List<Integer> same = new ArrayList<Integer>();\n            List<Integer> larger = new ArrayList<Integer>();\n            Integer mid = list.get(list.size() >> 1);\n            for (Integer i : list) {\n                if (i < mid) {\n                    smaller.add(i);\n                } else if (i > mid) {\n                    larger.add(i);\n                } else {\n                    same.add(i);\n                }\n            }\n            quickSort(smaller);\n            quickSort(larger);\n            list.clear();\n            list.addAll(smaller);\n            list.addAll(same);\n            list.addAll(larger);\n        }\n    }\n\n}\n\n```\n\n输入是Array\n\n```\nimport java.util.Arrays;\n\nclass Arrays_Quick{\n\tprivate int[] arrays;\n\tprivate int curNum;\n\n\tpublic Arrays_Quick(int max) {\t\t\t//建立一个max长度的空数组\n\t\tsuper();\n\t\tarrays = new int[max];\n\t\tcurNum = 0;\n\t}\n\t\n\tpublic void insert(int value){\t\t\t\t\t//往空的数组里面增加元素\n\t\tarrays[curNum] = value;\n\t\tcurNum++;\n\t}\n\t\n\tpublic void display(){\t\t\t\t\t\t\t\t\t//显示数组\n\t\tSystem.out.println(Arrays.toString(arrays));\n\t}\n\t\n\tpublic void QuickSort(){\n\t\tint i=0;\n\t\tint j=arrays.length-1;\n\t\t\n\t\trecQuickSort(i, j);\n\t}\n\t\n\tpublic void recQuickSort(int i,int j){\n\t\t// 结束条件\n        if(i == j )\n        \treturn;\n         \n        int key = arrays[i];\n        int stepi = i; \t\t\t// 记录开始位置\n        int stepj = j; \t\t\t// 记录结束位置\n         \n        while(j > i){\n            // j <<-------------- 向前查找\n            if(arrays[j] >= key){\n                j--;\n            }else{\n                arrays[i] = arrays[j];\n                //i++ ------------>>向后查找\n                while(j > ++i){\n                    if(arrays[i] > key){\n                        arrays[j] = arrays[i];\n                        break;\n                    }\n                }\n            }\n        }\n         \n        // 如果第一个取出的 key 是最小的数\n        if(stepi == i){\n        \trecQuickSort(++i, stepj);\n            return;\n        }\n         \n        // 最后一个空位留给 key\n        arrays[i] = key;\n         \n        // 递归\n        recQuickSort(stepi, i);\n        recQuickSort(j, stepj);\n    }\n}\n\npublic class QuickSort {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tint maxSize = 5;\n\t\tArrays_Quick arrays_demo = new Arrays_Quick(maxSize);\n\t\tarrays_demo.insert(4);\n\t\tarrays_demo.insert(5);\n\t\tarrays_demo.insert(3);\n\t\tarrays_demo.insert(1);\n\t\tarrays_demo.insert(2);\n\t\tarrays_demo.display();\n\t\tarrays_demo.QuickSort();\n\t\tarrays_demo.display();\n\t}\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n\n<img src=\"/images/517519-20160410161421890-1228382695.jpg\" alt=\"\" width=\"556\" height=\"416\" />\n","tags":["算法"]},{"title":"Java排序算法——归并排序","url":"/Java排序算法——归并排序.html","content":"<img src=\"/images/517519-20160407141231047-151917151.png\" alt=\"\" width=\"789\" height=\"632\" />\n\n<img src=\"/images/517519-20160408224011781-893743672.jpg\" alt=\"\" width=\"811\" height=\"607\" />\n\n<!--more-->\n&nbsp;\n\n```\nimport java.util.Arrays;\n\nclass Arrays_Merge{\n    private int[] arrays;\n    private int curNum;\n    //int[] workSpace;\n \n    public Arrays_Merge(int max) {         //建立一个max长度的空数组\n        super();\n        arrays = new int[max];\n        curNum = 0;\n    }\n     \n    public void insert(int value){                  //往空的数组里面增加元素\n        arrays[curNum] = value;\n        curNum++;\n    }\n     \n    public void display(){                                  //显示数组\n        System.out.println(Arrays.toString(arrays));\n    }\n    \n    public void mergeSort(){\n    \tint[] arrays = new int[curNum];\n    \trecMergeSort(arrays, 0, curNum-1);\n    }\n    \n    public void recMergeSort(int[] workSpace,int lowerBound,int upperBound){\n    \tif(lowerBound == upperBound)\n    \t\treturn;\n    \telse\n    \t{\n    \t\tint mid = (lowerBound+upperBound)/2;\n    \t\trecMergeSort(workSpace,lowerBound,mid);\n    \t\trecMergeSort(workSpace,mid+1,upperBound);\n    \t\tmerge(workSpace,lowerBound,mid+1,upperBound);\n    \t\tSystem.out.println(\"mid=\"+mid);\n    \t}\n    }\n     \n    public void merge(int[] workSpace,int lowPtr,int highPtr,int upperBound){\n    \tint j=0;\n    \tint lowerBound = lowPtr;\n    \tint mid = highPtr-1;\n    \tint n = upperBound-lowerBound+1;\n    \t\n    \twhile(lowPtr<=mid &amp;&amp; highPtr<=upperBound){\n    \t\tif(arrays[lowPtr]<arrays[highPtr]){\t\t\t//如果后面的小，就先放进新的数组中\n    \t\t\tworkSpace[j++] = arrays[lowPtr++];\t\t//如果运行第一行，就运行下面的第二个\n    \t\t\tSystem.out.println(\"1lowPtr=\"+(lowPtr-1)+\" \"+workSpace[j-1]);\n    \t\t}else{\n    \t\t\tworkSpace[j++] = arrays[highPtr++];\t//如果运行第二行，就运行下面的第一个\n    \t\t\tSystem.out.println(\"1highPtr=\"+(highPtr-1)+\" \"+(j-1)+\"=\"+workSpace[j-1]);\n    \t\t}\n    \t}\n    \t\n    \twhile(lowPtr<=mid){\t\t\t\t\t\t//以下两个只可能运行一个\n    \t\tworkSpace[j++] = arrays[lowPtr++]; \n    \t\tSystem.out.println(\"lowPtr=\"+(lowPtr-1)+\" \"+workSpace[j-1]);\n    \t}\n    \t\n    \twhile(highPtr<=upperBound){\n    \t\tworkSpace[j++] = arrays[highPtr++];\n    \t\tSystem.out.println(\"highPtr=\"+(highPtr-1)+\" \"+workSpace[j-1]);\n    \t}\n    \t\n    \tfor(j=0;j<n;j++){\n    \t\tarrays[lowerBound+j] = workSpace[j];\n    \t}\n    }\n}\n\npublic class MergeSort {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tint maxSize=10;\n\t\tArrays_Merge arr;\n\t\tarr = new Arrays_Merge(maxSize);\n\t\tarr.insert(9);\n\t\tarr.insert(8);\n\t\tarr.insert(7);\n\t\tarr.insert(6);\n\t\tarr.insert(5);\n\t\tarr.insert(4);\n\t\tarr.insert(3);\n\t\tarr.insert(2);\n\t\tarr.insert(1);\n\t\tarr.display();\n\t\tarr.mergeSort();\n\t\tarr.display();\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["算法"]},{"title":"Java递归算法——二分查找","url":"/Java递归算法——二分查找.html","content":"<img src=\"/images/517519-20160407093817937-2033053522.png\" alt=\"\" width=\"710\" height=\"52\" />\n\n```\nimport java.lang.reflect.Array;\nimport java.nio.Buffer;\nimport java.util.Arrays;\nimport java.util.Random;\n\nclass Rec_Find{\n\tprivate int[] temp;\n\tprivate int searchKey;\n\t//private int lowerBound = 0;\t\t\t//下界\n\t//private int upperBound ;\t\t\t\t//上界\n\tprivate int nElement;\n\t\n\tpublic int[] getTemp() {\n\t\treturn temp;\n\t}\n\n\tpublic void setTemp(int[] temp) {\n\t\tthis.temp = temp;\n\t}\n\n\tpublic Rec_Find(int[] temp) {//构造函数\n\t\tthis.temp = temp;\n\t\t//this.upperBound = temp.length-1;\n\t}\n\n\tpublic int find(int searchKey,int lowerBound,int upperBound){\n\t\t\tint curNum;\n\t\t\tthis.searchKey = searchKey;\n\t\t\tcurNum = (lowerBound+upperBound)/2;\n\t\t\tif(temp[curNum]==this.searchKey){\n\t\t\t\treturn curNum;\t\t\t\t\t\t\t//find\n\t\t\t}\n\t\t\telse if(lowerBound>upperBound){\n\t\t\t\treturn -1;\t\t\t\t\t\t\t\t\t\t//没有find\n\t\t\t}\n\t\t\telse{\n\t\t\t\tif(temp[curNum]<this.searchKey){\n\t\t\t\t\treturn find(searchKey,curNum+1,upperBound);\n\t\t\t\t}\n\t\t\t\telse{\n\t\t\t\t\treturn find(searchKey,lowerBound,curNum-1);\n\t\t\t\t}\n\t\t\t}\n\t}\t\n}\n\nclass RandomArray{\t\t\t\t\t//生成随机数组，有Num个\n\t\n\tprivate int[] Arrays;\n\t\n\tpublic int[] getArrays(int Num){\n//\t\tint[] Arrays = new int[Num];\n\t\tArrays = new int[Num];\n\t\tRandom r = new Random();\n\t\t\n\t\tfor(int i=0;i<Num;i++){\n\t\t\tArrays[i] = r.nextInt(1000);\n//\t\t\tSystem.out.print(Arrays[i]+\"、\");\n\t\t}\n\t\treturn Arrays;\n\t}\n}\n\nclass OrderedArray{\t\t\t//生成有序数组，从0开始到Num\n\t\n\tpublic int[] getArrays(int Num){\n\t\tint[] Arrays = new int[Num];\n\t\t\n\t\tfor(int i=0;i<Num;i++){\n\t\t\tArrays[i] = i;\n//\t\t\tSystem.out.print(Arrays[i]+\"、\");\n\t\t}\n\t\treturn Arrays;\n\t}\n}\n\npublic class RecFind {\n\t\n\tpublic static void main(String[] args) {\n\n//\t\tRandomArrays array_demo = new RandomArrays();\n//\t\tBinarySearch_Find arrays = new BinarySearch_Find(array_demo.getArrays(100));\n\t\t\n\t\tOrderedArray array_demo = new OrderedArray();\n\t\tRec_Find arrays = new Rec_Find(array_demo.getArrays(100));\n\t\tSystem.out.println(Arrays.toString(arrays.getTemp()));\n\t\tSystem.out.println(arrays.find(55,0,100));\n\t}\n}\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["算法"]},{"title":"Java递归算法——汉诺塔问题","url":"/Java递归算法——汉诺塔问题.html","content":"<img src=\"/images/517519-20160407094337203-457236713.png\" alt=\"\" width=\"668\" height=\"295\" />\n\n<img src=\"/images/517519-20160407094839656-1142978274.png\" alt=\"\" width=\"688\" height=\"71\" />\n\n<img src=\"/images/517519-20160407140338703-615882960.jpg\" alt=\"\" width=\"611\" height=\"456\" />\n\n```\npublic class Tower_demo {\n\t\n\tstatic int nDisks = 3;\n\t\n\tpublic static void main(String[] args) {\n\t\tdoTower(nDisks, 'A', 'B', 'C');\n\t}\n\n\tpublic static void doTower(int topN,char from,char inter,char to){\n\t\tif(topN == 1)\n\t\t\tSystem.out.println(\"Disk 1 from \"+from+\" to \"+to);\n\t\telse{\n\t\t\tdoTower(topN-1, from, to, inter);\n\t\t\tSystem.out.println(\"Disk \"+topN+\" from \"+from+\" to \"+to);\n\t\t\tdoTower(topN-1, inter, from, to);\n\t\t}\n\t}\n\t\n}\n\n```\n\n<!--more-->\n&nbsp;\n\n**通过二叉树的中序遍历过程来分析汉诺塔问题：**\n\n**考虑A、B、C三根柱子，A上从上到下有1、2、3三个数，要把A上的数移动到C上，其过程应该是**\n\n**　　　　&nbsp; A　　B　　C**\n\n**初始　　123**\n\n**A->C　&nbsp; 23　　　　 1**\n\n**<strong>A->B　&nbsp; 3　　2　　 1**</strong>\n\n**C->B　&nbsp; 3　&nbsp; 12　　 <br />**\n\n**<strong>A->C　&nbsp; 　　 12　　3<br />**</strong>\n\n**B->A　&nbsp; 1　　2　　 3<br />**\n\n**<strong>B->C　&nbsp; 1　　　　&nbsp; 23<br />**</strong>\n\n**A->C　&nbsp;&nbsp;&nbsp; 　　　　 123**\n\n&nbsp;\n\n**可以写成下图二叉树的中序遍历**\n\n<img src=\"/images/517519-20161130112415006-1744560822.png\" alt=\"\" width=\"485\" height=\"313\" />\n\n在程序中有两个输出语句，\n\n**结束条件中的数据语句**输出的是二叉树中的**叶子结点**\n\n递归左子树和递归右子树**中间的输出语句**输出是所有的**非叶子结点**\n\n**所有左孩子和右孩子输出语句中得到的实际参数都是其父结点传递的，根节点输出语句得到的参数是初始传递的参数**\n\n\n&nbsp;\n\n&nbsp;<1>首先确定递归函数的形式\n\n```\ndoTower(int topN,char from,char inter,char to)\n\n```\n\n&nbsp;<2>在main中**给根节点传递的初始值为（A,B,C） **\n\n&nbsp;对于根节点的输出A->C\n\n可以确定递归左子树和递归右子树**中间的输出语句**应该输出 from->to，即\n\n```\nSystem.out.println(\"Disk \"+topN+\" from \"+from+\" to \"+to);\n\n```\n\n<img src=\"/images/517519-20161130131940287-10411643.png\" alt=\"\" width=\"475\" height=\"319\" />\n\n&nbsp;<3>步骤<2>确定了递归左子树和递归右子树**中间的输出语句**的格式，\n\n对于根节点（A->C）的左孩子（A->B）和右孩子（B->C），由于都是**非叶子结点**，所以它们使用的仍然是递归左子树和递归右子树**中间的输出语句 from->to**，\n\n所以对于**左孩子（A->B）**，要输出（A->B），可以确定出给其传递的参数应该是（A,C,B）\n\n对于右**孩子（B->C）**，要输出（B->C），可以确定出给其传递的参数应该是（B,A,C）\n\n又因为**所有左孩子和右孩子输出语句中得到的实际参数都是其父结点传递的，由于父节点得到的实参是(frmo=A,inter=B,to=C)**\n\n**左孩子**要想得到（A,C,B），给其传递的顺序应该是（from,to,inter）\n\n**右孩子**要想得到（B,A,C），给其传递的顺序应该是（inter,from,to）\n\n即确定了\n\n```\ndoTower(topN-1, from, to, inter);\n\ndoTower(topN-1, inter, from, to);\n\n```\n\n&nbsp;\n\n&nbsp;\n\n<img src=\"/images/517519-20161130132556131-1089130409.png\" alt=\"\" width=\"730\" height=\"286\" />\n\n<4>最后确定**结束条件中的输出语句**的输出顺序\n\n**由于上一步确定了**\n\n```\ndoTower(topN-1, from, to, inter);\n\ndoTower(topN-1, inter, from, to);\n\n```\n\n&nbsp;对于A->B的左孩子A->C，由于在A->B结点中，from=A，inter=C，to=B，所以执行了doTower(topN-1, from, to, inter)后，给A->C传递的参数应该是（A,B,C），想要输出A->C，可以确定结束条件中的输出语句为\n\n```\nSystem.out.println(\"Disk 1 from \"+from+\" to \"+to);\n\n```\n\n&nbsp;同理，对于A->B的右孩子C->B，由于在A->B结点中，from=A，inter=C，to=B，所以执行了doTower(topN-1, inter, from, to)后，给C->B传递的参数应该是（C,A,B），可以输出C->B\n\n<img src=\"/images/517519-20161130140135599-386560611.png\" alt=\"\" width=\"798\" height=\"330\" />\n","tags":["算法"]},{"title":"Java递归算法——变位字","url":"/Java递归算法——变位字.html","content":"<img src=\"/images/517519-20160406165643500-1138766408.png\" alt=\"\" width=\"481\" height=\"227\" />\n\n<img src=\"/images/517519-20160406165734578-1022220258.png\" alt=\"\" width=\"738\" height=\"152\" />\n\n<img src=\"/images/517519-20160406170728718-751519215.png\" alt=\"\" />\n\n**轮换的含义**\n\n1.c ats --> 2.ca st\n\n3.c tsa --> 4.ct as\n\n5.c sat --> 6.cs ta\n\n7. atsc\n\n<!--more-->\n&nbsp;\n\n```\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\n\npublic class Anagram_demo {\n\t\n\tstatic int Size;\n\tstatic int count=0;\n\tstatic char[] arrChar = new char[100];\n\t\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"请输入一个单词：\");\n\t\tString input = getString();\n\t\tSize = input.length();\n\t\tfor(int i=0;i<Size;i++){\n\t\t\tarrChar[i] = input.charAt(i);\n\t\t}\n\t\tdoAnagram(Size);\n\t}\n\t\n\tpublic static void doAnagram(int newSize){\n\t\tif(newSize == 1)\t\t//只有一个就不做任何处理\n\t\t\treturn;\n\t\tfor(int i=0;i<newSize;i++){\n\t\t\tdoAnagram(newSize - 1);\n\t\t\tif(newSize == 2){\n\t\t\t\tdisplayWord();\n\t\t\t}\n\t\t\trorate(newSize);\n\t\t}\n\t}\n\t\n\tpublic static void displayWord(){\n\t\tSystem.out.print(++count);\n\t\tfor(int i=0;i<Size;i++){\n\t\t\tSystem.out.print(arrChar[i]);\n\t\t}\n\t\tSystem.out.println(\"、\");\n\t}\n\t\n\tpublic static void rorate(int newSize){\t\t//轮换函数\n\t\t\n\t\tint position = Size - newSize;\n\t\tchar temp = arrChar[position];\n\t\tfor(int i=position;i<Size-1;i++){\n\t\t\tarrChar[i] = arrChar[i+1];\n\t\t}\n\t\tarrChar[Size-1] = temp;\n\t}\n\t\n\t//输出方法\n\t\t\tpublic static String getString() throws IOException{\n\t\t\t\tInputStreamReader isr = new InputStreamReader(System.in);\n\t\t\t\tBufferedReader br = new BufferedReader(isr);\n\t\t\t\tString s = br.readLine();\n\t\t\t\treturn s;\n\t\t\t}\n\t\t\t\n\t\t\t//输出方法\n\t\t\tpublic static int getInt() throws IOException{\n\t\t\t\tString s = getString();\n\t\t\t\treturn Integer.parseInt(s);\n\t\t\t\t\n\t\t\t}\n}\n\n```\n\n&nbsp;\n","tags":["算法"]},{"title":"Java递归算法——阶乘","url":"/Java递归算法——阶乘.html","content":"```\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\n\npublic class Factorial_demo {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"输入数字：\");\n\t\tint theNumber = getInt();\n\t\tint theAnswer = factorial(theNumber); \n\t\tSystem.out.println(\"阶乘：\"+theAnswer);\n\t}\n\t\n\tpublic static int factorial(int n){\t\t//递归\n\t\tif(n == 1)\n\t\t\treturn 1;\n\t\telse\n\t\t\treturn (n*factorial(n-1));\n\t}\n\t\n\t//输出方法\n\t\tpublic static String getString() throws IOException{\n\t\t\tInputStreamReader isr = new InputStreamReader(System.in);\n\t\t\tBufferedReader br = new BufferedReader(isr);\n\t\t\tString s = br.readLine();\n\t\t\treturn s;\n\t\t}\n\t\t\n\t\t//输出方法\n\t\tpublic static int getInt() throws IOException{\n\t\t\tString s = getString();\n\t\t\treturn Integer.parseInt(s);\n\t\t}\n}\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["算法"]},{"title":"Java递归算法——三角数字（递归和非递归）","url":"/Java递归算法——三角数字（递归和非递归）.html","content":"<img src=\"/images/517519-20160405234929093-1455385806.png\" alt=\"\" width=\"630\" height=\"180\" />\n\n## <!--more-->\n&nbsp;1.递归法\n\n```\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\n\npublic class triangle_demo {\n\t\n\t//static int theNumber;\n\t\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"输入数字：\");\n\t\tint theNumber = getInt();\n\t\tint theAnswer = triangle(theNumber); \n\t\tSystem.out.println(\"三角上每一行的数量：\"+theAnswer);\n\t}\n\t\n\tpublic static int triangle(int n){\t\t//递归输出1 3 6 10....\n\t\tif(n==1)\n\t\t\treturn 1;\n\t\telse\n\t\t\treturn (n + triangle(n-1));\n\t}\n\t\n\t\n\t//输出方法\n\tpublic static String getString() throws IOException{\n\t\tInputStreamReader isr = new InputStreamReader(System.in);\n\t\tBufferedReader br = new BufferedReader(isr);\n\t\tString s = br.readLine();\n\t\treturn s;\n\t}\n\t\n\t//输出方法\n\tpublic static int getInt() throws IOException{\n\t\tString s = getString();\n\t\treturn Integer.parseInt(s);\n\t\t\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160405235018500-1259335894.png\" width=\"630\" height=\"496\" />\n\n<img src=\"/images/517519-20160904204809231-988980326.png\" width=\"630\" height=\"278\" />\n\n<img src=\"/images/517519-20160904204835592-877122612.png\" width=\"630\" height=\"503\" />\n\n<img src=\"/images/517519-20160904204901609-967003424.png\" width=\"630\" height=\"150\" />\n\n## 2.非递归法\n\n```\nimport java.io.*;                 // for I/O\n\nclass Params     //这个类的对象被压入栈中\n   {\n   public int n;\t\t\t\t\t\t\t//用来存放键盘输入的数字\n   public int returnAddress;\t//返回的地址\n\n   public Params(int nn, int ra)\n      {\n      n=nn;\n      returnAddress=ra;\n      }\n   }  // end class Params\n\nclass StackX\n   {\n   private int maxSize;         // size of StackX array\n   private Params[] stackArray;\n   private int top;             // top of stack\n//--------------------------------------------------------------\n   public StackX(int s)         // constructor\n      {\n      maxSize = s;              // set array size\n      stackArray = new Params[maxSize];  // create array\n      top = -1;                 // no items yet\n      }\n//--------------------------------------------------------------\n   public void push(Params p)   // put item on top of stack\n      {\n      stackArray[++top] = p;    // increment top, insert item\n      }\n//--------------------------------------------------------------\n   public Params pop()          // take item from top of stack\n      {\n      return stackArray[top--]; // access item, decrement top\n      }\n//--------------------------------------------------------------\n   public Params peek()         // peek at top of stack\n      {\n      return stackArray[top];\n      }\n//--------------------------------------------------------------\n   }  // end class StackX\n\nclass stackTriangle\n   {\n   static int theNumber;\t\t//用于接收输入的int\n   static int theAnswer;\n   static StackX theStack;\n   static int codePart;\t\t\t\t//用于switch选择\n   static Params theseParams;\n\n   public static void main(String[] args) throws IOException\n      {\n      System.out.print(\"Enter a number: \");\n      theNumber = getInt();\t\t//接收键盘输入的int\n      recTriangle();\n      System.out.println(\"Triangle=\"+theAnswer);\n      }  // end main()\n\n   public static void recTriangle()\n      {\n      theStack = new StackX(10000);\n      codePart = 1;\n      while( step() == false)  // call step() until it's true\n         ;                     // null statement\n      }\n//-------------------------------------------------------------\n   public static boolean step()\n      {\n      switch(codePart)\n         {\n         case 1:                              // initial call\n        \t System.out.println(\"进入1\");\n            theseParams = new Params(theNumber, 6);\t\n            theStack.push(theseParams);\n            codePart = 2;\n            break;\n            \n         case 2:                              // method entry\n        \t System.out.println(\"进入2\");\n            theseParams = theStack.peek();\t//对输入的数字一直减1，直到等于1，如果大于1就跳到3中，压入栈中\n            if(theseParams.n == 1)            \t\t// n是键盘输入的数字，如果是1，结果是1，codePart跳到5\n               {\n               theAnswer = 1;\n               codePart = 5;   // exit\n               }\n            else\t\t\t\t\t\t\t\t\t\t\t\t\t\t//如果大于1，就跳到3\n               codePart = 3;   // recursive call\n            break;\n            \n         case 3:                              \n        \t System.out.println(\"进入3\");\n            Params newParams = new Params(theseParams.n - 1, 4);\n            theStack.push(newParams);\t\t\t//把输入的数字减去1，并压入栈中\n            codePart = 2;  \t\t\t\t\t\t\t\t\t\t//回到2中判断数组减去1后，是否等于1\n            break;\n            \n         case 4:                              // calculation\n        \t System.out.println(\"进入4\");\n            theseParams = theStack.peek();\t\t\t//取得2\n            theAnswer = theAnswer + theseParams.n;\t//1+2\n            codePart = 5;\n            break;\n            \n         case 5:                              // method exit\n        \t System.out.println(\"进入5\");\n            theseParams = theStack.peek();\n            codePart = theseParams.returnAddress; //在2和3中交替跳转后，结束时跳到5，此时栈中codePart除了栈底是6，其他都是4\n            theStack.pop();\t\t\t\t//在取得了下次跳转的位置后，出栈，第一次出栈的是（1,4）\n            break;\n            \n         case 6:                              // return point\n        \t System.out.println(\"进入6\");\n            return true;\n         }  // end switch\n      return false;\n      }  // end triangle\n\n   public static String getString() throws IOException\n      {\n      InputStreamReader isr = new InputStreamReader(System.in);\n      BufferedReader br = new BufferedReader(isr);\n      String s = br.readLine();\n      return s;\n      }\n\n   public static int getInt() throws IOException\n      {\n      String s = getString();\n      return Integer.parseInt(s);\n      }\n\n   }  \n\n```\n\n&nbsp;\n","tags":["算法"]},{"title":"ubuntu下增加中文编码","url":"/ubuntu下增加中文编码.html","content":"## 1.查看系统的语言环境\n\n在Ubuntu中，利用locale命令\n\n**运行locale指令**得到当前系统编码设置的详细资料。\n\n一、locale的五脏六腑\n\n1、 语言符号及其分类(LC_CTYPE)<br />2、 数字(LC_NUMERIC)<br />3、 比较和排序习惯(LC_COLLATE)<br />4、 时间显示格式(LC_TIME)<br />5、 货币单位(LC_MONETARY)<br />6、 信息主要是提示信息,错误信息, 状态信息, 标题, 标签, 按钮和菜单等(LC_MESSAGES)<br />7、 姓名书写方式(LC_NAME)<br />8、 地址书写方式(LC_ADDRESS)<br />9、 电话号码书写方式(LC_TELEPHONE)<br />10、度量衡表达方式(LC_MEASUREMENT)<br />11、默认纸张尺寸大小(LC_PAPER)<br />12、对locale自身包含信息的概述(LC_IDENTIFICATION)。\n\n二、理解locale的设置\n\n设定locale就是设定12大类的locale分类属性，即 12个LC_*。除了这12个变量可以设定以外，为了简便起见，还有两个变量：LC_ALL和LANG。\n\n它们之间有一个优先级的关系：LC_ALL > LC_* > LANG\n\n可以这么说，LC_ALL是最上级设定或者强制设定，而LANG是默认设定值。\n\n三 具体设定locale的方法（zh_CN.UTF-8、zh_CN.GBK）\n\nfreebsd的设置：\n\n1.GDM登录改为终端登录后startx启动图形桌面\n\n2.在~/.cshrc中增加如下语句,（根据自己使用的shell进行相应设置）\n\nsetenv LANG zh_CN.GBK<br />setenv LC_ALL zh_CN.GBK<br />setenv LC_CTYPE zh_CN.GBK\n\n3.修改/etc/fstab的默认值：\n\nlinux 设置：\n\n1.修改/etc/sysconfig/i18n文件，LANG=\"zh_CN.UTF-8\"或LANG=\"zh_CN.GBK\"\n\n普通用户修改~/.profile\n\n...<br />export LANG zh_CN.GBK<br />...\n\n2.修改/etc/fstab的默认值\n\n参考：http://blog.chinaunix.net/uid-94449-id-2002589.html\n\n## 2.修改语言环境\n\n**方法1：**\n\n```\nvim /etc/sysconfig/i18n\n\n```\n\n**默认为:**\n\n```\nLANG=\"en_US.UTF-8\"\nSYSFONT=\"latarcyrheb-sun16\"\n\n```\n\n**修改为:**\n\n```\nLANG=\"zh_CN.GBK\"\nSUPPORTED=\"zh_CN.UTF-8:zh_CN:zh\"\nSYSFONT=\"latarcyrheb-sun16\"\n\n```\n\n**方法2：**\n\n```\nvim /etc/profile\n\nexport LC_ALL=\"zh_CN.GBK\"\nexport LANG=\"zh_CN.GBK\"\n\n```\n\n**Windows的默认编码为GBK，Linux的默认编码为UTF-8。在Windows下编辑的中文，在Linux下显示为乱码。**\n\n**为了解决此问题，修改Linux的默认编码为GBK。方法如下：**\n\n**ubuntu 16.04可以使用方法3：**\n\n**Ubuntu默认的中文字符编码为zh_CN.UTF-8，这个可以在/etc/environment中看到：<br />**\n\n```\nsudo cat /etc/environment\n\nPATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games\"\nLANG=\"zh_CN.UTF-8\"\nLANGUAGE=\"zh_CN:zh:en_US:en\"\n\n```\n\n第二行即是默认的中文字符编码。注：可以通过这里修改默认的中文编码字符，比如修改为：zh_CN.GBK。\n\n<!--more-->\n&nbsp;\n","tags":["Linux"]},{"title":"使用python操作Amazon S3","url":"/使用python操作Amazon S3.html","content":"1.判断s3 object是否存在\n\n```\nimport boto3\n\ns3 = boto3.resource('s3')\nbucket = s3.Bucket('my-bucket')\nkey = 'dootdoot.jpg'\nobjs = list(bucket.objects.filter(Prefix=key))\nif any([w.key == path_s3 for w in objs]):\n    print(\"Exists!\")\nelse:\n    print(\"Doesn't exist\")\n\n```\n\n2.读取s3 object文件内容\n\n```\nimport boto3\n\ns3 = boto3.resource('s3')\nobject_content = s3.Object('my_bucket_name', object_dir)\nfile_content = object_content.get()['Body'].read().decode('utf-8')\n\n```\n\n3.列出s3 object目录\n\n```\nimport boto3\n\ns3 = boto3.resource('s3')\nmy_bucket = s3.Bucket('my_bucket_name')\ndirs = sorted([_.key for _ in my_bucket.objects.filter(Prefix=\"aaa/bbb/ccc/\")], reverse=True)\nfor dir in dirs:\n    print(dir)\n\n```\n\n4.查看emr集群信息，参考：https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/emr.html#EMR.Client.describe_cluster\n\n```\nimport boto3\n\nemr_client = boto3.client('emr',region_name='us-west-2')\nresponse = emr_client.describe_cluster(ClusterId=\"j-xxxxxx\")\nprint(response['Cluster']['MasterPublicDnsName'])\n\n```\n\n　　\n\n　　\n","tags":["AWS"]},{"title":"CDH5.16集成ldap","url":"/CDH5.16集成ldap.html","content":"集成ldap之前请参考安装好openldap：[Ubuntu16.04安装openldap和phpldapadmin](https://www.cnblogs.com/tonglin0325/p/13661323.html)\n\n**1.hadoop集成ldap**\n\nHDFS 的文件权限与 Linux/Unix 系统类似，也是采用UGO模型，分成用户、组和其他权限。其权限you两种实现方式：1.基于Linux/Unix系统的用户和用户组；2.基于使用LDAP协议的数据库\n\n参考网易数帆的文章：[HDFS权限管理实践](https://sq.sf.163.com/blog/article/178584865073586176)\n\n使用基于Linux/Unix系统的用户和用户组，即 hadoop.security.group.mapping 的值为 org.apache.hadoop.security.ShellBasedUnixGroupsMapping\n\n使用基于使用LDAP协议的数据库，即 hadoop.security.group.mapping 的值为<!--more-->\n&nbsp;org.apache.hadoop.security.LdapGroupsMapping\n\n首先在phpldapadmin上配置好user和group\n\n<img src=\"/images/517519-20211107203010714-1416797422.png\" width=\"250\" height=\"68\" />\n\n1.给用户创建一个新的ou，叫person\n\n<img src=\"/images/517519-20211107203123300-1565447791.png\" width=\"250\" height=\"38\" />\n\n<img src=\"/images/517519-20211107203936358-1341537169.png\" width=\"400\" height=\"253\" loading=\"lazy\" />\n\n&nbsp;\n\n并在这个person ou下面创建一个名为xiaoming的person\n\n<img src=\"/images/517519-20211107204026179-2134922818.png\" width=\"200\" height=\"70\" loading=\"lazy\" />\n\n&nbsp;\n\n选择默认\n\n<img src=\"/images/517519-20211107203347173-1305530919.png\" width=\"200\" height=\"73\" />\n\n选择person\n\n<img src=\"/images/517519-20211107204218886-1987310162.png\" width=\"400\" height=\"411\" loading=\"lazy\" />\n\n&nbsp;\n\n<img src=\"/images/517519-20211107204340022-795409868.png\" width=\"400\" height=\"547\" loading=\"lazy\" />\n\n&nbsp;\n\n2.给组创建新的ou，叫hadoopGroup\n\n<img src=\"/images/517519-20211107203207875-1230897968.png\" width=\"400\" height=\"217\" loading=\"lazy\" />\n\n&nbsp;\n\n<img src=\"/images/517519-20211107203308089-1590031165.png\" width=\"250\" height=\"79\" />\n\n然后在这个hadoopGroup ou下面创建一个groupOfNames的object，名为hive\n\n&nbsp;<img src=\"/images/517519-20211107203509120-270152052.png\" width=\"400\" height=\"325\" loading=\"lazy\" />\n\n&nbsp;\n\n并把刚刚的xiaoming用户添加到hive用户组中\n\n<img src=\"/images/517519-20211107205311056-2103666697.png\" width=\"400\" height=\"612\" loading=\"lazy\" />\n\n&nbsp;\n\n在CDH上配置hadoop\n\n<img src=\"/images/517519-20211108003038319-587182920.png\" width=\"600\" height=\"716\" loading=\"lazy\" />\n\n&nbsp;\n\nhadoop.security.group.mapping.ldap.search.filter.user 的值为\n\n(|(&amp;(objectClass=person)(cn={0}))(&amp;(objectClass=account)(uid={0})))\n\n将会同时支持os用户和外部用户的权限，需要注意外部用户和os用户不能同名，否则hue会无法登录\n\n然后重启集群，接下来验证权限\n\n可以看到外部用户xiaoming属于hive组，和ldap上配置的一样，即表示CDH hadoop集成ldap成功\n\n```\nroot@master:~# kinit -kt /var/lib/hadoop-hdfs/hdfs.keytab hdfs/master@HADOOP.COM\nroot@master:~# hdfs groups xiaoming\nxiaoming : hive\n\n```\n\n再将os用户hive也拉到hive组中\n\n<img src=\"/images/517519-20211108003322048-1183840094.png\" width=\"400\" height=\"332\" loading=\"lazy\" />\n\n```\nroot@master:~# hdfs groups hive\nhive : hive\n\n```\n\nos用户hive也拥有了hive组的权限\n\n参考：[Setting Up Hadoop Group Mapping for LDAP/AD](https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.6.1/bk_security/content/setting_up_hadoop_group_mappping_for_ldap_ad.html)\n\n[hdfs+hive+impala+hue+sentry与ldap整合全套方案](https://blog.51cto.com/xiaolanlan/2378004)\n\n[CDH5.10.0基于OpenLDAP配置 Hadoop GroupMapping](https://blog.csdn.net/biplusplus/article/details/81603803)\n\n&nbsp;\n\n如果你参考下面的配置来集成ldap的话，只能对linux上的用户生效，而对于外部在os上不存在的用户，及时在ldap上添加了组，也不能拿到访问hdfs的权限\n\n```\nHadoop 用户组映射实现 \nhadoop.security.group.mapping \n\norg.apache.hadoop.security.LdapGroupsMapping\n----------------------------------------------------------------\nHadoop 用户组 进程ping LDAP URL \nhadoop.security.group.mapping.ldap.url\n\nldap://master\n----------------------------------------------------------------\nHadoop 用户组映射 LDAP 绑定用户可分辨名称\nhadoop.security.group.mapping.ldap.bind.user\n\nuid=admin,ou=people,dc=hadoop,dc=com\n----------------------------------------------------------------\nHadoop 用户组 进程ping 搜索基础\nhadoop.security.group.mapping.ldap.base\n\ndc=hadoop,dc=com\n----------------------------------------------------------------\nHadoop 用户组 进程ping LDAP 用户搜索筛选器\nhadoop.security.group.mapping.ldap.search.filter.user\n\n(&amp;(objectClass=posixAccount)(uid={0}))\n-----------------------------------------------------------------\nHadoop 用户组 进程ping LDAP 组搜索筛选器\nhadoop.security.group.mapping.ldap.search.filter.group\n\n(objectClass=posixGroup)\n-----------------------------------------------------------------\nHadoop 用户组 进程ping LDAP 组成员身份属性\nhadoop.security.group.mapping.ldap.search.attr.member\n\nmemberUid\n-----------------------------------------------------------------\nHadoop 用户组 进程ping LDAP 组名称属性\nhadoop.security.group.mapping.ldap.search.attr.group.name\n\ncn\n-----------------------------------------------------------------\nHadoop 用户组 进程ping LDAP 绑定用户密码\nhadoop.security.group.mapping.ldap.bind.password\n\npassword\n-----------------------------------------------------------------\nhdfs-site.xml的HDFS服务高级配置代码段（安全阀）\n\nhadoop.security.group.mapping.ldap.posix.attr.uid.name\nuid\n\nhadoop.security.groups.cache.secs\n10\n-----------------------------------------------------------------\ncore-site.xml的群集范围高级配置代码段（安全阀）\n\nhadoop.security.group.mapping.ldap.posix.attr.uid.name\nuid\n-----------------------------------------------------------------\nhadoop-policy.xml的HDFS服务高级配置代码段（安全阀）\n\nhadoop.security.group.mapping.ldap.posix.attr.uid.name\nuid\n\n```\n\n&nbsp;\n\n**2.HUE集成ldap**\n\nhue也同时支持os用户和外部用户登录\n\n<img src=\"/images/517519-20211108003533793-1131194944.png\" width=\"400\" height=\"517\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;\n\n<img src=\"/images/517519-20211108003555796-1950619300.png\" width=\"400\" height=\"156\" loading=\"lazy\" />\n\n参考：[6.如何为Hue配置OpenLDAP认证](https://cloud.tencent.com/developer/article/1078615)\n","tags":["CDH","ldap"]},{"title":"mac安装chronograf","url":"/mac安装chronograf.html","content":"安装chronograf来使用UI对InfluxDB进行操作\n\n参考：[mac 安装influxdb + Chronograf<!--more-->\n&nbsp;](https://www.jianshu.com/p/912464e4c92f)\n\n```\nbrew install chronograf\n\n```\n\n启动\n\n```\nsudo chronograf\n\n```\n\n添加influxDB连接\n\n<img src=\"/images/517519-20211027145228217-2050346392.png\" width=\"600\" height=\"128\" loading=\"lazy\" />\n\n在InfluxDB Admin里面去添加database\n\n<img src=\"/images/517519-20211027145907779-1322975511.png\" width=\"700\" height=\"178\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["InfluxDB"]},{"title":"Github workflows教程","url":"/Github workflows教程.html","content":"## Github workflows教程\n","tags":["开发工具"]},{"title":"ubuntu下JDK的安装","url":"/ubuntu下JDK的安装.html","content":"## 1.deb安装方法\n\n安装Oracle的jdk8\n\n```\nwget https://s3.cn-north-1.amazonaws.com.cn/tfssa/packages/oracle-java/oracle-java8-jdk_8u181_amd64.deb\nsudo dpkg -i ./oracle-java8-jdk_8u181_amd64.deb\n\n```\n\n如何依赖不满足\n\n```\nsudo apt-get install -f\n\n```\n\n如果依赖的包找不到，可以尝试\n\n```\nsudo mv /etc/apt/sources.list.d/backports.list /etc/apt/sources.list.d/backports.list.bak\n\n```\n\n选择使用rpm或者apt-get来安装java-8-oracle，这时的配置将会在 /etc/profile.d/jdk.sh 中\n\n```\ncat /etc/profile.d/jdk.sh\n\n```\n\n<!--more-->\n&nbsp;使用下面命令来使得这些配置生效\n\n```\nsource /etc/profile\n```\n\n　　\n\n其他安装包地址：\n\naws jdk_8u181_amd64下载地址\n\n```\nhttps://s3.cn-north-1.amazonaws.com.cn/tfssa/packages/oracle-java/oracle-java8-jdk_8u181_amd64.deb\n\n```\n\n## 2.tar包安装方法\n\n参考：[Debian 上安装 java | openjdk8的方法 使用Amazon Correo解决Package 'openjdk-8-jdk' has no installation candidate](https://blog.csdn.net/weixin_42917830/article/details/94406566)\n\n下载好JDK，解压在/usr/lib/jvm/目录下\n\n然后在 /etc/profile 文件中添加环境变量\n\n```\nexport JAVA_HOME=/usr/lib/jvm/java-8-oracle   \nexport JRE_HOME=${JAVA_HOME}/jre  \nexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib  \nexport PATH=${JAVA_HOME}/bin:$PATH\n\n```\n\n让配置生效\n\n```\nsource /etc/profile\n\n```\n\n最后通过 java -version 查看是否生效\n\n参考：[Ubuntu安装Oracle Java8以及环境变量的正确设置方法](https://www.linuxdashen.com/ubuntu%E5%AE%89%E8%A3%85oracle-java8%E4%BB%A5%E5%8F%8A%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E7%9A%84%E6%AD%A3%E7%A1%AE%E8%AE%BE%E7%BD%AE%E6%96%B9%E6%B3%95)\n\n&nbsp;\n\n## 3.Mac系统dmg安装方法\n\n1.x86芯片\n\n安装jdk-8u211-macosx-x64.dmg\n\n```\nhttps://www.oracle.com/java/technologies/javase/javase8u211-later-archive-downloads.html\n\n```\n\n2.arm芯片\n\nm1 macbook pro jdk下载地址，使用原生arm的jdk编译速度更快，30s->11s\n\n```\nhttps://cdn.azul.com/zulu/bin/zulu8.50.0.1017-ca-jdk8.0.275-macos_aarch64.dmg\n\n```\n\nmac jdk11 下载地址\n\n```\nhttps://repo.huaweicloud.com/java/jdk/11.0.2+9/jdk-11.0.2_osx-x64_bin.dmg\n\n```\n\n同时安装jdk8和jdk11，在~/.bash_profile中添加\n\n```\n# java\nexport JAVA_8_HOME=$(/usr/libexec/java_home -v1.8)\nexport JAVA_11_HOME=$(/usr/libexec/java_home -v11)\n\nalias java8='export JAVA_HOME=$JAVA_8_HOME'\nalias java11='export JAVA_HOME=$JAVA_11_HOME'\n\n# default to Java 8\njava8\n\n```\n\n输入java8或者java11可以进行切换，参考：[M1 macOS安装java8/java11并动态切换](https://www.955code.com/19519.html)\n\n&nbsp;\n","tags":["Linux"]},{"title":"Flink学习笔记——读写hdfs","url":"/Flink学习笔记——读写hdfs.html","content":"Flink自带Exactly Once语义，对于支持事务的存储，可以做到数据的不重不丢。\n\n当使用Flink来写hdfs的时候，因为hdfs文件只能在末尾进行append，如果要做到数据不重不丢，hdfs在2.7.0及其以上的版本中提供了truncate功能，可以根据valid-length长度对hdfs文件中的无效数据进行截断操作，从而保证数据不重复。\n\n参考：[Flink exactly-once 实战笔记](https://www.huaweicloud.com/articles/1b84394b589e493b4e3d8af1d567e40f.html)\n\n关于hdfs的truncate功能可以参考：[HDFS Truncate文件截断](https://blog.csdn.net/Androidlushangderen/article/details/52651995)\n","tags":["Flink"]},{"title":"Kafka学习笔记——Kafka Connect","url":"/Kafka学习笔记——Kafka Connect.html","content":"**kafka connect**是kafka提供的一个用于在kafka和其他数据系统之间传输数据的工具\n\n```\nhttps://kafka.apache.org/documentation/#connect\n\n```\n\n## 1.Kafka Connect组件\n\n```\nhttps://docs.confluent.io/platform/current/connect/concepts.html\n\n```\n\n1. [Connectors](https://docs.confluent.io/platform/current/connect/concepts.html#connect-connectors) &ndash; the high level abstraction that coordinates data streaming by managing tasks\n\nConnectors定义了数据是如何拷贝进kafka以及如何复制出kafka的\n\n包含2种形式的connector，source connector和sink connector\n\n- Source connector<!--more-->\n&nbsp;&ndash; Ingests entire databases and streams table updates to Kafka topics. A source connector can also collect metrics from all your application servers and store these in Kafka topics, making the data available for stream processing with low latency.\n- Sink connector&nbsp;&ndash; Delivers data from Kafka topics into secondary indexes such as Elasticsearch, or batch systems such as Hadoop for offline analysis.\n\nconfluent公司提供了很多现成的connector，常用的包括\n\n```\nhttps://www.confluent.io/product/connectors/\n```\n\nsource connector：\n\n```\nhttps://docs.confluent.io/kafka-connect-spooldir/current/\n\n```\n\nsink connector：\n\n```\nhttps://docs.confluent.io/kafka-connect-http/current/connector_config.html\nhttps://docs.confluent.io/kafka-connect-s3-sink/current/overview.html\n```\n\n&nbsp;\n\n2. [Tasks](https://docs.confluent.io/platform/current/connect/concepts.html#connect-tasks) &ndash; the implementation of how data is copied to or from Kafka\n\nTasks是 Connect 的数据模型中的主要执行组件。每个连接器实例协调一组实际复制数据的任务。通过允许连接器将单个作业分解为许多任务，Kafka Connect 提供了对并行性和可扩展数据复制的内置支持，且只需很少的配置。\n\n这些任务中没有存储任何状态。任务状态存储在 Kafka 的特殊topic config.storage.topic 和 status.storage.topic 中，并由关联的连接器管理。因此，可以随时启动、停止或重新启动任务，以提供弹性、可缩放的数据管道。\n\n&nbsp;\n\n3. [Workers](https://docs.confluent.io/platform/current/connect/concepts.html#connect-workers) &ndash; the running processes that execute connectors and tasks\n\n4. [Converters](https://docs.confluent.io/platform/current/connect/concepts.html#connect-converters) &ndash; the code used to translate data between Connect and the system sending or receiving data\n\n5. [Transforms](https://docs.confluent.io/platform/current/connect/concepts.html#connect-transforms) &ndash; simple logic to alter each message produced by or sent to a connector\n\n6. [Dead Letter Queue](https://docs.confluent.io/platform/current/connect/concepts.html#dead-letter-queues) &ndash; how Connect handles connector errors\n\n&nbsp;\n\nkafka connect配置，kafka connect使用的唯一前置条件就是需要有一些kafka brokers，broker的版本不一定要相同\n\n```\nhttps://docs.confluent.io/home/connect/self-managed/userguide.html#\n\n```\n\n同时，kafka connect还可以和schema registry集成，以支持avro，protobuf和JSON schema\n\n```\nhttps://docs.confluent.io/platform/current/schema-registry/connect.html\n\n```\n\n以及\n\n```\nhttps://docs.confluent.io/home/connect/self-managed/userguide.html#connect-configuring-converters\n\n```\n\n&nbsp;\n\n## 2.Kafka Connect部署&mdash;&mdash;Standalone单机模式\n\nkafka connet的worker有2种运行模式：**Standalone** 和 **Distributed Mode**\n\n单机模式可以用于收集web日志到kafka\n\nStandalone部署文档：[Kafka Connect的部署和使用详解1（安装配置、基本用法）](https://www.hangge.com/blog/cache/detail_3102.html)&nbsp;和&nbsp;[Kafka Connect的部署和使用详解2（添加connector）](https://www.hangge.com/blog/cache/detail_3103.html)　　\n\n部署步骤：\n\n去kafka官网\n\n```\nhttps://kafka.apache.org/downloads.html\n\n```\n\n下载kafka安装包\n\n```\nwget https://dlcdn.apache.org/kafka/3.2.0/kafka_2.12-3.2.0.tgz\ntar -zxvf kafka_2.12-3.2.0.tgz\n\n```\n\nkafka自带2个source和2个sink，用于文件和终端数据的生产和消费\n\n```\n➜  /Users/lintong/software/kafka_2.12-3.2.0/config $ ls | grep sink\nconnect-console-sink.properties\nconnect-file-sink.properties\n➜  /Users/lintong/software/kafka_2.12-3.2.0/config $ ls | grep source\nconnect-console-source.properties\nconnect-file-source.properties\n\n```\n\n### 1.kafka2file，消费kafka写文件\n\n修改 conf/connect-standalone.properties 配置文件，添加kafka集群的地址参数 bootstrap.servers 等配置\n\n```\n# 如果消费的时候报 If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration，添加如下配置\nkey.converter.schemas.enable=false \nvalue.converter.schemas.enable=false\n\n# kafka 地址\nbootstrap.servers=xx:9092\n\n# lib 地址\nplugin.path=/Users/lintong/software/kafka_2.12-3.2.0/libs\n\n```\n\n修改 conf/connect-file-sink.properties 配置文件，如下\n\n```\nname=local-file-sink\nconnector.class=FileStreamSink\ntasks.max=1\nfile=/Users/lintong/Downloads/test.sink.txt\ntopics=connect-test\n\n```\n\n使用connect-file-sink.properties配置来启动一个kafka connect单机的worker\n\n```\n./bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-sink.properties\n\n```\n\n就会在kafka manager上看到名为&nbsp;connect-local-file-sink&nbsp;的消费者\n\n&nbsp;\n\n### 2.file2kafka，采集文件发送到kafka\n\n修改 conf/connect-standalone.properties 配置文件，添加kafka集群的地址参数 bootstrap.servers 等配置，同上\n\n修改 conf/connect-file-source.properties&nbsp;配置文件，如下\n\n```\nname=local-file-source\nconnector.class=FileStreamSource\ntasks.max=1\nfile=/Users/lintong/Downloads/test1.txt\ntopic=lintong_test\n\n```\n\n使用connect-file-source.properties配置来启动一个kafka connect的单机的worker\n\n```\n./bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties\n\n```\n\n文件中的数据就采集并发送到kafka topic当中\n\n```\necho \"123\" >> /Users/lintong/Downloads/test1.txt\n\n```\n\n当前采集文件的offset将会默认记录在&nbsp;/tmp/connect.offsets 文件中\n\n```\ncat /tmp/connect.offsets\n��srjava.util.HashMap���`�F\nloadFactorI\tthresholdxp?@\n                             ur[B��T�xpG[\"local-file-source\",{\"filename\":\"/Users/lintong/Downloads/test1.txt\"}]uq~{\"position\":4}x%\n\n```\n\n　　\n\n### 3.kafka connect REST API\n\nconnect默认的端口为8083，也可以添加rest.port=8083配置进行修改\n\n1.查看运行的connect任务\n\n```\ncurl localhost:8083/connectors\n[\"local-file-source\"]\n\n```\n\n2.获得指定connect任务的信息\n\n```\ncurl localhost:8083/connectors/local-file-source | jq\n\n{\n  \"name\": \"local-file-source\",\n  \"config\": {\n    \"connector.class\": \"FileStreamSource\",\n    \"file\": \"/Users/lintong/Downloads/test1.txt\",\n    \"tasks.max\": \"1\",\n    \"name\": \"local-file-source\",\n    \"topic\": \"lintong_test\"\n  },\n  \"tasks\": [\n    {\n      \"connector\": \"local-file-source\",\n      \"task\": 0\n    }\n  ],\n  \"type\": \"source\"\n}\n\n```\n\n3.获得运行connect任务的配置\n\n```\ncurl localhost:8083/connectors/local-file-source/config | jq\n\n{\n  \"connector.class\": \"FileStreamSource\",\n  \"file\": \"/Users/lintong/Downloads/test1.txt\",\n  \"tasks.max\": \"1\",\n  \"name\": \"local-file-source\",\n  \"topic\": \"lintong_test\"\n}\n\n```\n\n4.获得运行connect任务的状态\n\n```\ncurl localhost:8083/connectors/local-file-source/status | jq\n\n{\n  \"name\": \"local-file-source\",\n  \"connector\": {\n    \"state\": \"RUNNING\",\n    \"worker_id\": \"127.0.0.1:8083\"\n  },\n  \"tasks\": [\n    {\n      \"id\": 0,\n      \"state\": \"RUNNING\",\n      \"worker_id\": \"127.0.0.1:8083\"\n    }\n  ],\n  \"type\": \"source\"\n}\n\n```\n\n5.获得运行的connect任务的task\n\n```\ncurl localhost:8083/connectors/local-file-source/tasks | jq\n\n[\n  {\n    \"id\": {\n      \"connector\": \"local-file-source\",\n      \"task\": 0\n    },\n    \"config\": {\n      \"topic\": \"lintong_test\",\n      \"file\": \"/Users/lintong/Downloads/test1.txt\",\n      \"task.class\": \"org.apache.kafka.connect.file.FileStreamSourceTask\",\n      \"batch.size\": \"2000\"\n    }\n  }\n]\n\n```\n\n6.获得运行的connect任务的某个task的运行状态\n\n```\ncurl localhost:8083/connectors/local-file-source/tasks/0/status | jq\n\n{\n  \"id\": 0,\n  \"state\": \"RUNNING\",\n  \"worker_id\": \"127.0.0.1:8083\"\n}\n\n```\n\n7.暂停一个connect任务\n\n```\ncurl -X PUT localhost:8083/connectors/local-file-source/pause\n\ncurl localhost:8083/connectors/local-file-source/status | jq\n\n{\n  \"name\": \"local-file-source\",\n  \"connector\": {\n    \"state\": \"PAUSED\",\n    \"worker_id\": \"127.0.0.1:8083\"\n  },\n  \"tasks\": [\n    {\n      \"id\": 0,\n      \"state\": \"PAUSED\",\n      \"worker_id\": \"127.0.0.1:8083\"\n    }\n  ],\n  \"type\": \"source\"\n}\n\n```\n\n8.恢复一个被暂停的&nbsp;connector 任务\n\n```\ncurl -X PUT localhost:8083/connectors/local-file-source/resume\n\ncurl localhost:8083/connectors/local-file-source/status | jq\n\n{\n  \"name\": \"local-file-source\",\n  \"connector\": {\n    \"state\": \"RUNNING\",\n    \"worker_id\": \"127.0.0.1:8083\"\n  },\n  \"tasks\": [\n    {\n      \"id\": 0,\n      \"state\": \"RUNNING\",\n      \"worker_id\": \"127.0.0.1:8083\"\n    }\n  ],\n  \"type\": \"source\"\n}\n\n```\n\n9.重启connect任务，一般在任务失败的时候会重启\n\n```\ncurl -X POST localhost:8083/connectors/local-file-source/restart\n\n```\n\n10.重启connect任务的task，一般在task失败的时候会重启\n\n```\ncurl -X POST localhost:8083/connectors/local-file-source/tasks/0/restart\n\n```\n\n11.删除一个connect任务\n\n```\ncurl -X DELETE localhost:8083/connectors/local-file-source\n\ncurl localhost:8083/connectors/local-file-source\n\n{\"error_code\":404,\"message\":\"Connector local-file-source not found\"}\n\n```\n\n12.创建一个connect任务，需要2个字段，一个是name，一个是config\n\n```\ncurl -X POST localhost:8083/connectors -H 'Content-Type: application/json' -d '{\"name\": \"local-file-source\",\"config\": {\"connector.class\": \"FileStreamSource\",\"file\": \"/Users/lintong/Downloads/test1.txt\",\"tasks.max\": \"1\",\"topic\": \"lintong_test\"}}'\n\n{\"name\":\"local-file-source\",\"config\":{\"connector.class\":\"FileStreamSource\",\"file\":\"/Users/lintong/Downloads/test1.txt\",\"tasks.max\":\"1\",\"topic\":\"lintong_test\",\"name\":\"local-file-source\"},\"tasks\":[{\"connector\":\"local-file-source\",\"task\":0}],\"type\":\"source\"}\n\n```\n\n13.更新一个connect任务的配置\n\n```\ncurl -X PUT localhost:8083/connectors/local-file-source/config -H 'Content-Type: application/json' -d '{\"connector.class\": \"FileStreamSource\",\"file\": \"/Users/lintong/Downloads/test.txt\",\"tasks.max\": \"1\",\"topic\": \"lintong_test\"}'\n\n{\"name\":\"local-file-source\",\"config\":{\"connector.class\":\"FileStreamSource\",\"file\":\"/Users/lintong/Downloads/test.txt\",\"tasks.max\":\"1\",\"topic\":\"lintong_test\",\"name\":\"local-file-source\"},\"tasks\":[{\"connector\":\"local-file-source\",\"task\":0}],\"type\":\"source\"}\n\n```\n\n14.查看当前的插件\n\n```\ncurl localhost:8083/connector-plugins | jq\n\n[\n  {\n    \"class\": \"org.apache.kafka.connect.file.FileStreamSinkConnector\",\n    \"type\": \"sink\",\n    \"version\": \"3.2.0\"\n  },\n  {\n    \"class\": \"org.apache.kafka.connect.file.FileStreamSourceConnector\",\n    \"type\": \"source\",\n    \"version\": \"3.2.0\"\n  },\n  {\n    \"class\": \"org.apache.kafka.connect.mirror.MirrorCheckpointConnector\",\n    \"type\": \"source\",\n    \"version\": \"3.2.0\"\n  },\n  {\n    \"class\": \"org.apache.kafka.connect.mirror.MirrorHeartbeatConnector\",\n    \"type\": \"source\",\n    \"version\": \"3.2.0\"\n  },\n  {\n    \"class\": \"org.apache.kafka.connect.mirror.MirrorSourceConnector\",\n    \"type\": \"source\",\n    \"version\": \"3.2.0\"\n  }\n]　&nbsp;\n```\n\n## 3.Kafka Connect部署&mdash;&mdash;Distributed Mode分布式模式\n\n运行Connect集群上将有更高的容错性，添加添加和删除节点。分布式的Connect集群可以运行在Kubernetes, Apache Mesos, Docker Swarm, or Yarn上面\n\n### **1.Docker部署方式**\n\n可以参考文档，confleunt公司提供了现成的docker镜像用于构建kafka connect集群\n\nDocker镜像地址：[cp-kafka-connect-base](https://hub.docker.com/r/confluentinc/cp-kafka-connect-base) 或者&nbsp;[cp-kafka-connect](https://hub.docker.com/r/confluentinc/cp-kafka-connect)&nbsp;，两者几乎是相同的，区别是在confluentic 6.0之前cp-kafka-connect预装了几个connect\n\n官方Docker安装文档：[Kafka Connect configuration](https://docs.confluent.io/platform/current/installation/docker/config-reference.html#kconnect-long-configuration)&nbsp;\n\n使用docker run来启动kafka connect服务\n\n```\ndocker run -d \\\n  --name=kafka-connect \\\n  --net=host \\\n  -e CONNECT_BOOTSTRAP_SERVERS=localhost:29092 \\\n  -e CONNECT_REST_PORT=28082 \\\n  -e CONNECT_GROUP_ID=\"quickstart\" \\\n  -e CONNECT_CONFIG_STORAGE_TOPIC=\"quickstart-config\" \\\n  -e CONNECT_OFFSET_STORAGE_TOPIC=\"quickstart-offsets\" \\\n  -e CONNECT_STATUS_STORAGE_TOPIC=\"quickstart-status\" \\\n  -e CONNECT_KEY_CONVERTER=\"org.apache.kafka.connect.json.JsonConverter\" \\\n  -e CONNECT_VALUE_CONVERTER=\"org.apache.kafka.connect.json.JsonConverter\" \\\n  -e CONNECT_INTERNAL_KEY_CONVERTER=\"org.apache.kafka.connect.json.JsonConverter\" \\\n  -e CONNECT_INTERNAL_VALUE_CONVERTER=\"org.apache.kafka.connect.json.JsonConverter\" \\\n  -e CONNECT_REST_ADVERTISED_HOST_NAME=\"localhost\" \\\n  -e CONNECT_PLUGIN_PATH=/usr/share/java \\\n  confluentinc/cp-kafka-connect:7.1.1\n\n```\n\n如果想要自行添加connector的话，需要自行编写Dockerfile，如下\n\n```\nFROM confluentinc/cp-kafka-connect-base:7.1.1 as base\n\nRUN confluent-hub install --no-prompt confluentinc/kafka-connect-s3:10.0.0\n\n```\n\nbuild\n\n```\ndocker build -f ./Dockerfile . -t xx:0.0.1\n\n```\n\n如果遇到&nbsp;java.net.unknownhostexception: api.hub.confluent.io\n\n可以通过在DNS中添加8.8.8.8来解决，如下\n\n<img src=\"/images/517519-20220622104245699-122588776.png\" width=\"500\" height=\"222\" loading=\"lazy\" />\n\n&nbsp;&nbsp;\n\n### **2.K8S部署方式**\n\n可以参考文档\n\n```\nhttps://docs.confluent.io/operator/current/overview.html\n```\n\n## 3.Kafka Connect配置\n\n参考：[Kafka Connect配置](https://www.orchome.com/537)\n\n## 4.Kafka Connect异常处理\n\n参考：https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues/\n\n&nbsp;\n","tags":["kafka"]},{"title":"SpringBoot学习笔记——spring-boot-configuration-processor作用","url":"/SpringBoot学习笔记——spring-boot-configuration-processor作用.html","content":"spring-boot-configuration-processor的作用是生成配置的元数据信息，即META-INF目录下的spring-configuration-metadata.json文件，从而告诉spring这个jar包中有哪些自定义的配置\n\n1.其中spring-configuration-metadata.json文件是在编译的时候自动生成的\n\n<img src=\"/images/517519-20210531171156443-1931882474.png\" alt=\"\" loading=\"lazy\" />\n\n2.还可以在resources目录下手动添加META-INF/additional-spring-configuration-metadata.json文件，这个文件是手动添加的，用于对spring-configuration-metadata.json进行补充，编译后会合并到spring-configuration-metadata.json中\n\n<img src=\"/images/517519-20210531171828658-1248690128.png\" alt=\"\" loading=\"lazy\" />\n\n参考：[spring 自动加载配置](https://cloud.tencent.com/developer/article/1432121)\n\n<!--more-->\n&nbsp;\n\n步骤：\n\n1.添加依赖\n\n```\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-configuration-processor</artifactId>\n    <optional>true</optional>\n</dependency>\n\n```\n\n2.在配置类上添加注解&nbsp;ConfigurationProperties\n\n```\nimport lombok.Data;\nimport org.springframework.boot.context.properties.ConfigurationProperties;\n\n@Data\n@ConfigurationProperties(prefix = \"xxxx\")\npublic class XxxxProperties {\n\n    private boolean aaa = true;\n    private String bbb;\n\n}\n\n```\n\n参考：[springboot之spring-configuration-metadata自定义提示](https://zhuanlan.zhihu.com/p/80706620)\n\n3.编译后即可在application.yml中使用配置提示\n\n<img src=\"/images/517519-20210601142522968-1060511276.png\" alt=\"\" loading=\"lazy\" />\n\n如果没有提示可能需要刷新一下maven依赖\n\n&nbsp;<img src=\"/images/517519-20210601142425519-850887474.png\" alt=\"\" loading=\"lazy\" />\n","tags":["SpringBoot"]},{"title":"confluent学习笔记——schema registry","url":"/confluent学习笔记——schema registry.html","content":"schema registry是confluent公司开发的一个集中式管理和验证kafka消息schema的组件。官方网站如下\n\n```\nhttps://docs.confluent.io/platform/current/schema-registry/index.html\n\n```\n\n其支持3种格式的schema：JSON，AVRO和protobuf\n\ndocker镜像地址如下\n\n```\nhttps://hub.docker.com/r/confluentinc/cp-schema-registry\n\n```\n\n其提供了schema registry的后端服务，提供API用于管理，验证和存储schema，保证了kafka的生产者和消费者可以使用schema来保证数据的一致性和兼容性。\n\n有开源的schema registry前端UI，比如\n\nschema-registry-ui，只支持AVRO的schema\n\n```\nhttps://hub.docker.com/r/landoop/schema-registry-ui/\n\n```\n\nkafka-ui，支持JSON，AVRO和protobuf（pb嵌套schema需要等0.8版本发布）格式的schema\n\n```\nhttps://hub.docker.com/r/provectuslabs/kafka-ui\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["confluent"]},{"title":"Java排序算法——表插入排序","url":"/Java排序算法——表插入排序.html","content":"<img src=\"/images/517519-20160331103029723-1758410325.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n```\npublic class ListInsertSort_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tint Size = 10;\n\t\tLink_long[] linkArray = new Link_long[Size];\n\t\t\n\t\tfor(int i =0;i<Size;i++){\n\t\t\tlong n = (long)(java.lang.Math.random()*99);\n\t\t\tlinkArray[i] = new Link_long(n);\n\t\t}\n\t\tSystem.out.println(\"未排序小于100的数组：\");\n\t\tfor(int i=0;i<Size;i++){\n\t\t\tSystem.out.print(linkArray[i].dData+\"、\");\n\t\t}\n\t\t\n\t\tSortedList theSortedList = new SortedList();\n\t\tfor(int i=0;i<Size;i++){\n\t\t\ttheSortedList.insert(linkArray[i].dData);\n\t\t}\n\t\tfor(int i=0;i<Size;i++){\n\t\t\tlinkArray[i].dData = theSortedList.remove();\n\t\t}\n\t\tSystem.out.println();\n\t\tSystem.out.println(\"已经排序的小于100的数组：\");\n\t\tfor(int i=0;i<Size;i++){\n\t\t\tSystem.out.print(linkArray[i].dData+\"、\");\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["算法"]},{"title":"Java数据结构——双向链表","url":"/Java数据结构——双向链表.html","content":"<img src=\"/images/517519-20160331110125566-1122381444.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160331110144816-702423933.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n```\nclass DoublyLinkedList{\t\t\t\t//双向链表\n\tprivate Link_long first;\n\tprivate Link_long last;\n\t\n\tpublic DoublyLinkedList(){\t\t//构造函数\n\t\tthis.first = null;\n\t\tthis.last = null;\n\t}\n\t\n\tpublic boolean isEmpty(){\n\t\treturn first==null;\n\t}\n\t\n\tpublic void insertFirst(long dd){\t\t//从链表的头开始插入\n\t\tLink_long newLink = new Link_long(dd);\n\t\tif(isEmpty()){\n\t\t\tlast = newLink;\t\t\t\t//不用改变first\n\t\t}else{\n\t\t\tfirst.previous = newLink;\t\t//插入新的元素\n\t\t}\n\t\tnewLink.next = first;\t\t\t\t\t//插入新的元素\n\t\tfirst = newLink;\t\t\t\t\t\t\t//修改first的位置\n\t}\n\t\n\tpublic void insertLast(long dd){\t\t//从链表的尾开始插入\n\t\tLink_long newLink = new Link_long(dd);\n\t\tif(isEmpty()){\n\t\t\tfirst = newLink;\t\t\t//不用改变last\n\t\t}else{\n\t\t\tlast.next = newLink;\t\t//在last后面添加新元素，并修改last的位置\n\t\t\tnewLink.previous = last;\n\t\t}\n\t\tlast = newLink;\t\t\t\t//注意：只有一个元素的时候，插入要把last也赋为newLink\n\t}\n\t\n\tpublic Link_long deleteFirst(){\t\t\t\t//从链表的头删除一个元素\n\t\tLink_long temp = first;\t\t//暂存first\n\t\tif(first.next == null){\t\t\t//如果只有一个元素，把last也赋为null\n\t\t\tlast = null;\n\t\t}else{\n\t\t\tfirst.next.previous = null;\n\t\t}\n\t\tfirst = first.next;\t\t\t//把next设为first\n\t\treturn temp;\t\t\t\t//返回原来的first\n\t}\n\t\n\tpublic Link_long deleteLast(){\t\t\t\t//从链表的头删除一个元素\n\t\tLink_long temp = last;\t\t//暂存last\n\t\tif(first.next == null){\t\t\t//如果只有一个元素，把first也赋为null\n\t\t\tfirst = null;\n\t\t}else{\n\t\t\tlast.previous.next = null;\n\t\t}\n\t\tlast = last.previous;\t\t\t//把previous设为last\n\t\treturn temp;\t\t\t\t\t\t//返回原来的last\n\t}\n\t\n\tpublic boolean insertAfter(long key,long dd){\t\t//在特定元素后面插入\n\t\tLink_long current = first;\n\t\twhile(current.dData != key){\n\t\t\tcurrent = current.next;\n\t\t\tif(current == null){\n\t\t\t\treturn false;\n\t\t\t}\n\t\t}\n\t\tLink_long newLink = new Link_long(dd);\n\t\t\n\t\tif(current == last){\n\t\t\tnewLink.next = null;\n\t\t\tlast = newLink;\n\t\t}else{\n\t\t\tnewLink.next = current.next;\n\t\t\tcurrent.next.previous = newLink;\n\t\t}\n\t\tnewLink.previous = current;\n\t\tcurrent.next = newLink;\n\t\treturn true;\n\t}\n\t\n\tpublic Link_long deleteKey(long key){\t\t//删除指定元素\n\t\tLink_long current = first;\n\t\twhile(current.dData != key){\n\t\t\tcurrent = current.next;\n\t\t\tif(current == null){\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t\tif(current == first){\t\t//如果第一个元素匹配，则删除，并把first向后移动\n\t\t\tfirst = current.next;\n\t\t}else{\n\t\t\tcurrent.previous.next = current.next;\t//改变current上一个元素的next的指向\n\t\t}\n\t\tif(current == last){\n\t\t\tlast = current.previous;\t//如果最后一个元素匹配，则删除，并把previous向前移动\n\t\t}else{\n\t\t\tcurrent.next.previous = current.previous;\t//改变current后一个元素的previous的指向\n\t\t}\n\t\treturn current;\n\t}\n\t\n\tpublic void displayForward(){\n\t\tSystem.out.println(\"List(first-->last):\");\n\t\tLink_long current = first;\t\t\t//用于不断改变位置实现遍历\n\t\twhile(current != null){\n\t\t\tcurrent.displayLink();\n\t\t\tcurrent = current.next;\n\t\t}\n\t}\n\t\n\tpublic void displayBackward(){\n\t\tSystem.out.println(\"List(last-->first):\");\n\t\tLink_long current = last;\t\t\t//用于不断改变位置实现遍历\n\t\twhile(current != null){\n\t\t\tcurrent.displayLink();\n\t\t\tcurrent = current.previous;\n\t\t}\n\t}\n\t\n}\n\npublic class DoublyLinked_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tDoublyLinkedList theList = new DoublyLinkedList();\n\t\t\n\t\ttheList.insertFirst(40);\n\t\ttheList.insertFirst(30);\n\t\ttheList.insertFirst(20);\n\t\ttheList.insertFirst(10);\n\t\ttheList.displayForward();\n\t\ttheList.displayBackward();\n\t\t\n\t\ttheList.insertAfter(20, 25);\n\t\ttheList.displayForward();\n\t\ttheList.displayBackward();\n\t\t\n\t\ttheList.deleteFirst();\n\t\ttheList.displayForward();\n\t\t\n\t\ttheList.deleteKey(20);\n\t\ttheList.displayForward();\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160331161645691-817061824.jpg\" alt=\"\" width=\"411\" height=\"545\" />\n\n<img src=\"/images/517519-20160331161810644-919841019.jpg\" alt=\"\" width=\"758\" height=\"1008\" />\n","tags":["数据结构"]},{"title":"OpenTSDB API基本操作","url":"/OpenTSDB API基本操作.html","content":"1.OpenTSDB CLI，参考\n\n```\nhttps://www.docs4dev.com/docs/zh/opentsdb/2.3/reference/user_guide-cli-index.html\n\n```\n\n<!--more-->\n&nbsp;\n\n&nbsp;\n\n2.OpenTSDB HTTP API，参考\n\n```\nhttps://www.docs4dev.com/docs/zh/opentsdb/2.3/reference/api_http-index.html\n\n```\n\n&nbsp;\n","tags":["OpenTSDB"]},{"title":"使用java原生API，DOM4J，JDOM和SAX解析XML文件","url":"/使用java原生API，DOM4J，JDOM和SAX解析XML文件.html","content":"<img src=\"/images/517519-20160331214007488-1382884137.png\" alt=\"\" width=\"640\" height=\"233\" />\n\n解析<!--more-->\n&nbsp;XML&nbsp;有两种方式：&nbsp;SAX&nbsp;和&nbsp;DOM&nbsp;。它们各有利弊。\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DOM&nbsp;是把&nbsp;XML&nbsp;文档全部装载到内存中，然后当成一树进行处理。其好处是当成树处理起来比较方便，但弊端是如果&nbsp;XML&nbsp;文件比较大时，会对内存消耗比较大；\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SAX&nbsp;是逐行扫描&nbsp;XML&nbsp;文档，逐行解析，而且可以在处理&nbsp;XML&nbsp;文档过程中的任意时刻中止处理过程，比如找到我们的目标节点，剩下的&nbsp;XML&nbsp;文档内容就可以不读了，直接结束。其弊端是操作起来相对不方便，而且对&nbsp;XML&nbsp;文档进行处理，如果修改、新增、删除等操作比较不方便。\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SAX&nbsp;是事件驱动型&nbsp;XML&nbsp;解析的一个标准接口。它的工作原理是读到文档的开始与结束、标签元素的开始与结束、内容实体等地方时，触发相应的函数，我们就可以在相应的函数中进行我们所要进行的处理。\n\n&nbsp;\n\n**1.使用Java API解析DOM解析**\n\n**<img src=\"/images/517519-20160331214105348-629654565.png\" alt=\"\" width=\"593\" height=\"302\" />**\n\n<img src=\"/images/517519-20160331222736894-450660668.png\" alt=\"\" />\n\n&nbsp;\n\n<img src=\"/images/517519-20160331215103598-65001493.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160331215632394-1287122074.png\" alt=\"\" />\n\n&nbsp;\n\n**只在跟节点<addresslist>下面建立一个子节点<name>**\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<addresslist>\n\t<linkman>\n\t\t<name>张三</name>\n\t\t<email>www.baidu.com</email>\n\t</linkman>\n\t\n\t<linkman>\n\t\t<name>李四</name>\n\t\t<email>www.sina.com</email>\n\t</linkman>\n</addresslist>\n\n```\n\n&nbsp;\n\n```\nimport java.io.IOException;\n\nimport javax.xml.parsers.DocumentBuilder;\nimport javax.xml.parsers.DocumentBuilderFactory;\n\nimport org.w3c.dom.Document;\nimport org.w3c.dom.NodeList;\nimport org.xml.sax.SAXException;\n\n//Function        : \tDOM_demo\npublic class DOM_demo {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tDocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();\t//建立DocumentBuilderFactory\n\t\tDocumentBuilder builder = factory.newDocumentBuilder();\t\t//建立DocumentBuilder\n\t\tDocument doc = null;\n\t\ttry{\n\t\t\tdoc = builder.parse(\"/home/common/software/coding/HelloWord/JavaWeb/bin/dom_name.xml\");\n\t\t}catch(SAXException e){\n\t\t\te.printStackTrace();\n\t\t}catch(IOException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\tNodeList nl = doc.getElementsByTagName(\"name\");\t\t//查找name节点\n\t\tSystem.out.println(\"姓名：\"+nl.item(1).getFirstChild().getNodeValue());\t\t//输出第1个节点的内容\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**一些DOM操作，循环输出节点信息**\n\n```\nimport java.io.IOException;\n\nimport javax.xml.parsers.DocumentBuilder;\nimport javax.xml.parsers.DocumentBuilderFactory;\n\nimport org.w3c.dom.Element;\nimport org.w3c.dom.Document;\nimport org.w3c.dom.NodeList;\nimport org.xml.sax.SAXException;\n\n//Function        : \tDOM_demo\npublic class DOM_demo {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tDocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();\t//建立DocumentBuilderFactory\n\t\tDocumentBuilder builder = factory.newDocumentBuilder();\t\t//建立DocumentBuilder\n\t\tDocument doc = null;\n\t\ttry{\n\t\t\tdoc = builder.parse(\"/home/common/software/coding/HelloWord/JavaWeb/bin/dom_name.xml\");\n\t\t}catch(SAXException e){\n\t\t\te.printStackTrace();\n\t\t}catch(IOException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\t\n//\t\tNodeList nl = doc.getElementsByTagName(\"name\");\t\t//查找name节点\n//\t\tSystem.out.println(\"姓名：\"+nl.item(1).getFirstChild().getNodeValue());\t\t//输出第1个节点的内容\n\t\t\n\t\tNodeList lm = doc.getElementsByTagName(\"linkman\");\t\t//查找linkman节点\n\t\tfor(int i=0;i<lm.getLength();i++){\n\t\t\tElement e = (Element)lm.item(i);\t\t//取得每一个元素\n\t\t\tSystem.out.println(\"姓名：\"+e.getElementsByTagName(\"name\").item(0).getFirstChild().getNodeValue());\n\t\t\tSystem.out.println(\"邮箱：\"+e.getElementsByTagName(\"email\").item(0).getFirstChild().getNodeValue());\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**生成XML文件**\n\n**<img src=\"/images/517519-20160331231725285-527728677.png\" alt=\"\" />**\n\n```\nimport java.io.File;\nimport java.io.IOException;\n\nimport javax.xml.parsers.DocumentBuilder;\nimport javax.xml.parsers.DocumentBuilderFactory;\nimport javax.xml.transform.OutputKeys;\nimport javax.xml.transform.Transformer;\nimport javax.xml.transform.TransformerConfigurationException;\nimport javax.xml.transform.TransformerException;\nimport javax.xml.transform.TransformerFactory;\nimport javax.xml.transform.dom.DOMSource;\nimport javax.xml.transform.stream.StreamResult;\n\nimport org.w3c.dom.Document;\nimport org.w3c.dom.Element;\nimport org.xml.sax.SAXException;\n\n//Function        : \tDOM_demo\npublic class DOM_demo {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\t//建立DocumentBuilderFactory\n\t\tDocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();\t\n\t\t//建立DocumentBuilder\n\t\tDocumentBuilder builder = factory.newDocumentBuilder();\n\t\t//定义Document接口对象，通过DocumentBuilder类进行DOM树的转换操作\n\t\tDocument doc = null;\n\t\tdoc = builder.newDocument();\n\n\t\t//建立各个操作节点\n\t\tElement addresslist = doc.createElement(\"addresslist\");\n\t\tElement linkman = doc.createElement(\"linkman\");\n\t\tElement name = doc.createElement(\"name\");\n\t\tElement email = doc.createElement(\"email\");\n\t\t//设置节点的文本内容，即为每一个节点添加文本节点\n\t\tname.appendChild(doc.createTextNode(\"王五\"));\n\t\temail.appendChild(doc.createTextNode(\"www.soho.com\"));\n\t\t//设置节点关系\n\t\tlinkman.appendChild(name);\t\t\t\t\t//子节点\n\t\tlinkman.appendChild(email);\t\t\t\t\t//子节点\n\t\taddresslist.appendChild(linkman);\t\t//子节点\n\t\tdoc.appendChild(addresslist);\t\t\t\t//文档上保存节点\n\t\t//输出文档到文件中\n\t\tTransformerFactory tf = TransformerFactory.newInstance();\n\t\tTransformer t = null;\n\t\ttry{\n\t\t\tt = tf.newTransformer();\n\t\t}catch(TransformerConfigurationException e1){\n\t\t\te1.printStackTrace();\n\t\t}\n\t\tt.setOutputProperty(OutputKeys.ENCODING, \"UTF-8\");\t\t//设置编码\n\t\tDOMSource source = new DOMSource(doc);\t\t\t\t\t\t\t//输出文档\n\t\tStreamResult result = new StreamResult(new File(\"/home/common/software/coding/HelloWord/JavaWeb/bin/dom_name_output.xml\")) ;\n\t\ttry{\n\t\t\tt.transform(source, result); \t\t\t\t\t//输出\n\t\t}catch(TransformerException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**2.使用DOM4J解析XML**\n\n<img src=\"/images/517519-20160401143456004-315242633.png\" alt=\"\" />\n\n&nbsp;\n\n**<img src=\"/images/517519-20160401143526738-727583693.png\" alt=\"\" />**\n\n<img src=\"/images/517519-20160401143547926-68266586.png\" alt=\"\" />\n\n```\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\n\nimport org.dom4j.Document;\nimport org.dom4j.DocumentHelper;\nimport org.dom4j.Element;\nimport org.dom4j.io.OutputFormat;\nimport org.dom4j.io.XMLWriter;\n\n//Function        : \tDOM4JWriter\npublic class DOM4JWriter {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tDocument doc = DocumentHelper.createDocument();\t\t//创建文档\n\t\t\n\t\tElement addresslist = doc.addElement(\"addresslist\");\t\t//定义节点\n\t\tElement linkman = addresslist.addElement(\"linkman\");\t//定义子节点\n\t\tElement name = addresslist.addElement(\"name\");\t\t\t\t//定义子节点\n\t\tElement email = addresslist.addElement(\"email\");\t\t\t\t//定义子节点\n\t\tname.setText(\"张三\");\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//设置name节点内容\n\t\temail.setText(\"www.baidu.com\");\t\t\t\t\t\t\t\t\t\t\t\t//设置email节点内容\n\t\t//设置输出格式\n\t\tOutputFormat format = OutputFormat.createPrettyPrint();\n\t\t//设置输出编码\n\t\tformat.setEncoding(\"UTF-8\");\n\t\ttry{\n\t\t\tXMLWriter writer = new XMLWriter(new FileOutputStream(new File(\"/home/common/software/coding/HelloWord/JavaWeb/bin/name_out.xml\")),format);\n\t\t\twriter.write(doc);\t\t//输出内容\n\t\t\twriter.close();\t\t\t\t//关闭输出流\n\t\t}catch(IOException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**解析输出文件**\n\n```\nimport java.io.File;\nimport java.util.Iterator;\n\nimport org.dom4j.Document;\nimport org.dom4j.DocumentException;\nimport org.dom4j.Element;\nimport org.dom4j.io.SAXReader;\n\n//Function        : \tDOM4JReader\npublic class DOM4JReader {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tFile file = new File(\"/home/common/software/coding/HelloWord/JavaWeb/bin/name_out.xml\");\n\t\tSAXReader reader = new SAXReader();\t\t\t//建立SAX解析读取\n\t\tDocument doc = null;\t\t\t\t\n\t\ttry{\n\t\t\tdoc = reader.read(file);\t\t\t\t\t\t\t\t\t\t//读取文档\n\t\t}catch(DocumentException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\tElement root = doc.getRootElement();\t\t\t//取得根元素\n\t\tIterator iter = root.elementIterator();\t\t\t\t//取得全部的子节点\n\t\twhile(iter.hasNext()){\n\t\t\tElement linkman = (Element) iter.next();\t//取得每一个linkman\n\t\t\tSystem.out.println(\"姓名：\"+linkman.elementText(\"name\"));\n\t\t\tSystem.out.println(\"邮件：\"+linkman.elementText(\"email\"));\n\t\t}\n\t\t\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**3.使用JDOM解析XML文件**\n\n<img src=\"/images/517519-20160401113744957-1560320449.png\" alt=\"\" />\n\n&nbsp;\n\n**JavaDOC的网址：http://www.jdom.org/docs/apidocs/index.html**\n\n**<img src=\"/images/517519-20160401113838301-734859933.png\" alt=\"\" />**\n\n&nbsp;\n\n```\nimport java.io.FileOutputStream;\n\nimport org.jdom2.Attribute;\nimport org.jdom2.Document;\nimport org.jdom2.Element;\nimport org.jdom2.output.XMLOutputter;\nimport org.xml.sax.Attributes;\n\npublic class WriteXML {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t//建立各个操作节点\n\t\tElement addresslist = new Element(\"addresslist\");\n\t\tElement linkman = new Element(\"linkman\");\n\t\tElement name = new Element(\"name\");\n\t\tElement email = new Element(\"email\");\n\t\t//定义属性\n\t\tAttribute id = new Attribute(\"id\",\"zs\"); \n\t\t//声明一个Document对象\n\t\tDocument doc = new Document(addresslist);\n\t\t//设置元素的内容\n\t\tname.setText(\"张三\");\n\t\tname.setAttribute(id);\t\t\t\t//设置name的属性\n\t\temail.setText(\"www.baidu.com\");\n\t\t//设置linkman的子节点\n\t\tlinkman.addContent(name);\n\t\tlinkman.addContent(email);\n\t\t//将linkman加入根节点中\n\t\taddresslist.addContent(linkman);\n\t\t//用来输出XML文件\n\t\tXMLOutputter out = new XMLOutputter();\n\t\t//设置输出的编码\n\t\tout.setFormat(out.getFormat().setEncoding(\"UTF-8\"));\n\t\t//输出XML文件\n\t\ttry{\n\t\t\tout.output(doc, new FileOutputStream(\"/home/common/software/coding/HelloWord/JavaWeb/bin/address.xml\"));\n\t\t}catch(Exception e){\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160401121042144-866826089.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160401121110551-2062385045.png\" alt=\"\" />\n\n```\nimport java.util.List;\n\nimport org.jdom2.Document;\nimport org.jdom2.Element;\nimport org.jdom2.input.SAXBuilder;\n\n//Function        : \tReadXML\npublic class ReadXML {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\t//建立SAX解析\n\t\tSAXBuilder builder = new SAXBuilder();\n\t\t//找到Document\n\t\tDocument read_doc = builder.build(\"/home/common/software/coding/HelloWord/JavaWeb/bin/address.xml\");\n\t\t//读取根元素\n\t\tElement stu = read_doc.getRootElement();\n\t\t//得到全部linkman子元素\n\t\tList list = stu.getChildren(\"linkman\");\n\t\tfor(int i=0;i<list.size();i++){\n\t\t\tElement e = (Element)list.get(i);\t\t//取得全部的linkman子元素\n\t\t\tString name = e.getChildText(\"name\");\n\t\t\tString id = e.getChild(\"name\").getAttribute(\"id\").getValue();\n\t\t\tString email = e.getChildText(\"email\");\n\t\t\t\n\t\t\tSystem.out.println(\"---------联系人---------\");\n\t\t\tSystem.out.println(\"姓名：\"+name+\"，编号：\"+id);\n\t\t\tSystem.out.println(\"邮箱：\"+email);\n\t\t\tSystem.out.println(\"------------------\");\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160401143031535-1066721012.png\" alt=\"\" />\n\n**4.使用SAX解析XML文件**\n\n<img src=\"/images/517519-20160331232219769-334342557.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160401104431473-1681781544.png\" alt=\"\" />\n\n```\nimport org.xml.sax.Attributes;\nimport org.xml.sax.SAXException;\nimport org.xml.sax.helpers.DefaultHandler;\n\npublic class MySAX extends DefaultHandler{\t\t\t\t//定义SAX解析器\n\tpublic void startDocument() throws SAXException{\t\t//文档开始\n\t\tSystem.out.println(\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\");\n\t}\n\t\n\tpublic void endDocument() throws SAXException{\t\t//文档结束\n\t\tSystem.out.println(\"\\n 文档读取结束。。。\");\n\t}\n\t\n\tpublic void startElement(String uri,String localName,String name,Attributes attributes) throws SAXException{\n\t\tSystem.out.print(\"<\");\n\t\tSystem.out.print(name);\n\t\tif(attributes != null){\n\t\t\tfor(int i=0;i<attributes.getLength();i++){\n\t\t\t\tSystem.out.print(\" \"+attributes.getQName(i)+\"=\\\"\"+attributes.getValue(i)+\"\\\"\");\n\t\t\t}\n\t\t\tSystem.out.print(\">\");\n\t\t}\n\t}\n\t\n\tpublic void character(char[] ch,int start,int lenght) throws SAXException{\n\t\tSystem.out.print(new String(ch,start,lenght));\n\t}\n\t\n\tpublic void endElement(String uri,String localName,String name) throws SAXException{\n\t\tSystem.out.print(\"</\");\n\t\tSystem.out.print(name);\n\t\tSystem.out.print(\">\");\n\t}\n\t\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160401104522754-347615793.png\" alt=\"\" />\n\n```\nimport javax.xml.parsers.SAXParser;\nimport javax.xml.parsers.SAXParserFactory;\n\npublic class testSAX {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\t//建立SAX解析工厂\n\t\tSAXParserFactory factory = SAXParserFactory.newInstance();\n\t\t//构造解析器\n\t\tSAXParser parser = factory.newSAXParser();\n\t\t//解析XML，使用HANDLER\n\t\tparser.parse(\"/home/common/software/coding/HelloWord/JavaWeb/bin/dom_name.xml\", new MySAX());\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["开发工具"]},{"title":"Java数据结构——哈夫曼树（Huffman Tree）","url":"/Java数据结构——哈夫曼树（Huffman Tree）.html","content":"## 1.哈夫曼树\n\n给定N个权值作为N个叶子节点，构造一棵二叉树，若该树的**带权路径长度****（WPL）**达到**最小**，称这样的二叉树为最优二叉树，也称为**哈夫曼树(Huffman Tree)**。\n\n哈夫曼树是带权路径长度最短的树，权值较大的结点离根较近。\n\n参考：[霍夫曼树](https://oi-wiki.org/ds/huffman-tree/)（参考该文章中的概念，其中代码不对）\n\n## 2.树的带权路径长度（WPL，Weighted Path Length of Tree）\n\n树的**带权路径长度**，就是树中所有的**叶结点的权值**乘上**其到根结点的路径长度**（若根结点为0层，叶结点到根结点的路径长度为叶结点的层数）\n\n如下图中的哈夫曼树，其WPL的值 = (2+3) * 3 + 4 * 2 + 6 * 1 = 29<br />\n\n<img src=\"/images/517519-20240512150425132-1418014129.png\" width=\"350\" height=\"367\" loading=\"lazy\" />\n\n上图参考：[数据结构&mdash;&mdash;哈夫曼树（Huffman Tree）](https://zhuanlan.zhihu.com/p/415467000)\n\n## 3.哈夫曼树的应用\n\n### 1.哈夫曼编码\n\n<img src=\"/images/517519-20240512154023785-835108801.png\" width=\"500\" height=\"210\" loading=\"lazy\" />\n\n## 4.哈夫曼编码的实现\n\n### 1.哈夫曼编码的步骤：\n\n<li class=\"para_wER59 content_XrPG6 MARK_MODULE\" data-tag=\"paragraph\" data-uuid=\"go1fc1wosk\" data-idx=\"2-2\">根据输入的字符构建哈夫曼树<ol>\n1. 统计原始数据中各字符出现的频率；\n1. 所有字符按频率降序排列；\n1. 建立哈夫曼树：\n\n\n\n\n1. 将哈夫曼树存入结果数据；\n1. 重新编码原始数据到结果数据\n\n\n\n\n### 2.建立哈夫曼树的步骤：\n\n1. **初始化**：由给定的 N<img src=\"/images/517519-20240512160759128-417111981.gif\" title=\"n\" /> 个权值构造 N<img src=\"/images/517519-20240512160759128-417111981.gif\" title=\"n\" /> 棵只有一个根节点的二叉树，得到一个二叉树集合 <img src=\"/images/517519-20240512160759128-417111981.gif\" title=\"F\" />。\n1. **选取与合并**：从二叉树集合 PriorityQueue<img src=\"/images/517519-20240512160759128-417111981.gif\" title=\"F\" /> 中选取根节点权值 **最小的两棵** 二叉树分别作为左右子树构造一棵新的二叉树，这棵新二叉树的根节点的权值为其左、右子树根结点的权值和。\n1. **删除与加入**：从 <img src=\"/images/517519-20240512160759128-417111981.gif\" title=\"F\" />PriorityQueue<img src=\"/images/517519-20240512160759128-417111981.gif\" title=\"F\" /> 中删除作为左、右子树的两棵二叉树，并将新建立的二叉树加入到 <img src=\"/images/517519-20240512160759128-417111981.gif\" title=\"F\" />PriorityQueue<img src=\"/images/517519-20240512160759128-417111981.gif\" title=\"F\" /> 中。\n1. 重复 2、3 步，当集合中只剩下一棵二叉树时，这棵二叉树就是霍夫曼树。\n\n\n\n\n\n\n\n\n\n\n### 3.代码实现：\n\n下面的2个网站中的实现都差不多，都借助了优先队列来进行排序（java是PriorityQueue，python是heapq）\n","tags":["数据结构"]},{"title":"Java数据结构——双端链表","url":"/Java数据结构——双端链表.html","content":"<img src=\"/images/517519-20160330164623441-351850391.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n```\nclass FirstLastList{\n\tprivate Link first;\n\tprivate Link last;\n\t\n\tpublic FirstLastList() {\t//构造函数\n\t\tthis.first = null;\n\t\tthis.last = null;\n\t}\n\t\n\tpublic boolean isEmpty(){\n\t\treturn (first == null);\n\t}\n\t\n\tpublic void insertFirst(int id,double dd){\t\t//从链表的头开始插入\n\t\tLink newLink = new Link(id,dd);\n\t\tif(isEmpty()){\n\t\t\tlast = newLink;\t\t\t\t//不用改变first\n\t\t}\n\t\tnewLink.next = first;\n\t\tfirst = newLink;\n\t}\n\t\n\tpublic void insertLast(int id,double dd){\t//从链表的尾开始插入\n\t\tLink newLink = new Link(id,dd);\n\t\tif(isEmpty()){\n\t\t\tfirst = newLink;\t\t//不用改变last\n\t\t}else{\n\t\t\tlast.next = newLink;\t\t//在last后面添加新元素，并修改last的位置\n\t\t}\n\t\tlast = newLink;\t\t\t\t//注意：只有一个元素的时候，插入要把last也赋为newLink\n\t}\n\t\n\tpublic Link deleteFirst(){\n\t\tLink temp = first;\t\t//暂存first\n\t\tif(first.next == null){\t\t//如果只有一个元素，把last也赋为null\n\t\t\tlast = null;\n\t\t}\n\t\tfirst = first.next;\t\t\t//把next设为first\n\t\treturn temp;\t\t\t\t//返回原来的first\n\t}\n\t\n\tpublic void displayList(){\n\t\tSystem.out.println(\"List(first-->last):\");\n\t\tLink current = first;\t\t\t//用于不断改变位置实现遍历\n\t\twhile(current != null){\n\t\t\tcurrent.displayLink();\n\t\t\tcurrent = current.next;\n\t\t}\n\t}\n\t\n}\n\npublic class FirstLastList_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tFirstLastList theList = new FirstLastList();\n\t\ttheList.insertFirst(11, 11.1);\n\t\ttheList.insertFirst(22, 22.2);\n\t\ttheList.insertFirst(33, 33.3);\n\t\ttheList.insertFirst(44, 44.4);\n\t\ttheList.insertFirst(55, 55.5);\n\t\ttheList.displayList();\n\t\t\n\t\ttheList.deleteFirst();\n\t\ttheList.deleteFirst();\n\t\ttheList.deleteFirst();\n\t\ttheList.displayList();\n\t\t\n\t}\n\n}\n\n```\n\n&nbsp;\n\n&nbsp;\n\n<img src=\"/images/517519-20160330174501473-717774414.png\" alt=\"\" />\n","tags":["数据结构"]},{"title":"Java数据结构——有序链表","url":"/Java数据结构——有序链表.html","content":"<img src=\"/images/517519-20160330220913754-944314032.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160331100455051-453414364.jpg\" alt=\"\" width=\"595\" height=\"790\" />\n\n```\nclass SortedList{\n\tprivate Link_long first;\n\t\n\tpublic SortedList(){\t\t\t//构造函数\n\t\tfirst = null;\n\t}\n\t\n\tpublic void insert(long key){\n\t\tLink_long newLink = new Link_long(key);\n\t\tLink_long previous = null;\t\t//上一次插入的值\n\t\tLink_long current = first;\t\t\t//每插入一次,current就重新赋为表头的值\n\t\twhile(current != null &amp;&amp; key > current.dData){\t//没进入这里，pre就是null，也就只进入下面if的上一层\n\t\t\tprevious = current;\n\t\t\tcurrent = current.next;\t\t\t//current的位置往后移动\n\t\t}\n\t\tif(previous == null){\t\t\t\t//最开始的情况，给first赋值为newLink，即key\n\t\t\tfirst = newLink;\n\t\t}else{\t\t\t\t\t\t\t\t\t\t\t//\n\t\t\tprevious.next = newLink;\n\t\t}\n\t\tnewLink.next = current;\n\t}\n\t\n\tpublic long remove(){\n\t\tLink_long temp = first;\n\t\tfirst = first.next;\n\t\treturn temp.dData;\n\t}\n\t\n\tpublic void displayList(){\n\t\tSystem.out.println(\"List (first-->last)\");\n\t\tLink_long current = first;\n\t\twhile(current != null){\n\t\t\tcurrent.displayLink();\n\t\t\tcurrent = current.next;\n\t\t}\n\t}\n\t\n}\n\npublic class SortedList_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tSortedList theSortedList = new SortedList();\n\t\ttheSortedList.insert(10);\n\t\ttheSortedList.insert(20);\n\t\ttheSortedList.insert(30);\n\t\ttheSortedList.displayList();\n\t\ttheSortedList.remove();\n\t\ttheSortedList.remove();\n\t\ttheSortedList.displayList();\n\t}\n\n}\n\n```\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20160331101941144-1285573142.png\" alt=\"\" />\n","tags":["数据结构"]},{"title":"Java数据结构——用双端链表实现队列","url":"/Java数据结构——用双端链表实现队列.html","content":"<img src=\"/images/517519-20160330211553207-299463171.png\" alt=\"\" />\n\n```\nclass FirstLastList_long{\n\tprivate Link_long first;\n\tprivate Link_long last;\n\t\n\tpublic FirstLastList_long() {\t\t\t//构造函数\n\t\tthis.first = null;\n\t\tthis.last = null;\n\t}\n\t\n\tpublic boolean isEmpty(){\n\t\treturn (first == null);\n\t}\n\t\n\tpublic void insertFirst(long dd){\t\t//从链表的头开始插入\n\t\tLink_long newLink = new Link_long(dd);\n\t\tif(isEmpty()){\n\t\t\tlast = newLink;\t\t\t\t//不用改变first\n\t\t}\n\t\tnewLink.next = first;\n\t\tfirst = newLink;\n\t}\n\t\n\tpublic void insertLast(long dd){\t\t//从链表的尾开始插入\n\t\tLink_long newLink = new Link_long(dd);\n\t\tif(isEmpty()){\n\t\t\tfirst = newLink;\t\t\t//不用改变last\n\t\t}else{\n\t\t\tlast.next = newLink;\t\t//在last后面添加新元素，并修改last的位置\n\t\t}\n\t\tlast = newLink;\t\t\t\t\t//注意：只有一个元素的时候，插入要把last也赋为newLink\n\t}\n\t\n\tpublic long deleteFirst(){\n\t\tLink_long temp = first;\t\t//暂存first\n\t\tif(first.next == null){\t\t//如果只有一个元素，把last也赋为null\n\t\t\tlast = null;\n\t\t}\n\t\tfirst = first.next;\t\t\t//把next设为first\n\t\treturn temp.dData;\t\t\t\t//返回原来的first\n\t}\n\t\n\tpublic void displayList(){\n\t\tSystem.out.println(\"List(first-->last):\");\n\t\tLink_long current = first;\t\t\t//用于不断改变位置实现遍历\n\t\twhile(current != null){\n\t\t\tcurrent.displayLink();\n\t\t\tcurrent = current.next;\n\t\t}\n\t}\n\t\n}\n\nclass LinkQueue{\n\tprivate FirstLastList_long theList;\n\n\tpublic LinkQueue() {\t\t\t\t\t\t//构造函数\n\t\ttheList = new FirstLastList_long();\t\t//创建一个双端链表对象\n\t}\n\t\n\tpublic void push(long j){\t\t\t\t//从链表的尾开始插入，新来的元素在尾部\n\t\ttheList.insertLast(j);\n\t}\n\t\n\tpublic long pop(){\n\t\treturn theList.deleteFirst();\t\t//从链表的头开始弹出，先进的元素先被弹出\n\t}\n\t\n\tpublic boolean idEmpty(){\n\t\treturn theList.isEmpty();\n\t}\n\t\n\tpublic void displayQueue(){\n\t\tSystem.out.println(\"Queue (front-->rear)\");\n\t\ttheList.displayList();\n\t}\n}\n\npublic class LinkQueue_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tLinkQueue theQueue = new LinkQueue();\n\t\ttheQueue.push(10);\n\t\ttheQueue.push(20);\n\t\ttheQueue.push(30);\n\t\ttheQueue.displayQueue();\n\t\t\n\t\ttheQueue.pop();\n\t\ttheQueue.pop();\n\t\ttheQueue.displayQueue();\n\t}\n\n}\n\n```\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20160330220131848-60326052.png\" alt=\"\" />\n","tags":["数据结构"]},{"title":"Java数据结构——用链表实现栈","url":"/Java数据结构——用链表实现栈.html","content":"<img src=\"/images/517519-20160330211128691-384615933.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160330211153566-1310001821.png\" alt=\"\" />\n\n```\nclass Link_long{\t//链节点类\n\tpublic long dData;\n\tpublic Link_long next;\t\t//链表中下一个节点的引用\n\t\n\tpublic Link_long(long dData) {\n\t\tsuper();\n\t\tthis.dData = dData;\n\t}\n\t\n\tpublic void displayLink(){\t//显示当前节点的值\n\t\tSystem.out.println(\"dData\"+dData);\n\t}\n\t\n}\n\nclass LinkList_long{\n\tprivate Link_long first;\t//只需要第一个节点，从第一个节点出发即可定位所有节点\n\n\tpublic LinkList_long() {\t//构造函数\n\t\tthis.first = null;\n\t}\n\t\n\tpublic boolean isEmpty(){\n\t\treturn (first == null);\n\t}\n\t\n\tpublic void insertFirst(long dd){\t//插入元素是从链表的头开始插入\n\t\tLink_long newLink = new Link_long(dd);\n\t\tnewLink.next = first;\n\t\tfirst = newLink;\n\t}\n\t\n\tpublic long deleteFirst(){\t\t//删除temp.dData\n\t\tLink_long temp = first;\t\t//暂存first\n\t\tfirst = first.next;\t\t//把next设为first\n\t\treturn temp.dData;\t\t//返回原来的first\n\t}\n\t\n\tpublic void displayList(){\n\t\tSystem.out.println(\"List(first-->last):\");\n\t\tLink_long current = first;\t//用于不断改变位置实现遍历\n\t\twhile(current != null){\n\t\t\tcurrent.displayLink();\n\t\t\tcurrent = current.next;\n\t\t}\n\t}\n\t\n\tpublic Link_long find(int key){\t\t//查找指定的关键字\n\t\tLink_long current = first;\n\t\twhile(current.dData != key){\n\t\t\tif(current.next == null)\n\t\t\t\treturn null;\n\t\t\telse\n\t\t\t\tcurrent = current.next;\n\t\t}\n\t\treturn current;\n\t}\n\t\n\tpublic Link_long delete(int key){\t//如果current的值匹配，则删除\n\t\tLink_long current = first;\t\t\t\t\n\t\tLink_long previous = first;\n\t\t//没有匹配到值\n\t\twhile(current.dData != key){\n\t\t\tif(current.next == null)\n\t\t\t\treturn null;\n\t\t\telse{\t\t\t//pre和cur向后移动\n\t\t\t\tprevious = current;\n\t\t\t\tcurrent = current.next;\n\t\t\t}\n\t\t}\n\t\t//匹配到值\n\t\tif(current == first)\t\t//只有一个first，并匹配，则把first设成first.next\n\t\t\tfirst = first.next;\n\t\telse\t\t\t\t//current的值匹配，则删除，并把cur的next赋给pre的next\n\t\t\tprevious.next = current.next;\n\t\treturn current;\n\t}\n}\n\nclass LinkStack{\t//用链表实现栈，链表从First开始插入，新的元素将变成First，先删除后来元素\n\t\n\tprivate LinkList_long theList;\n\n\tpublic LinkStack() {\t\t//构造函数\n\t\ttheList = new LinkList_long();\n\t}\n\t\n\tpublic void push(long j){\t\t//入栈，即在链表的First插入元素\n\t\ttheList.insertFirst(j);\n\t}\n\t\n\tpublic long pop(){\t\t\t\t\t//出栈，即删除链表的First元素\n\t\treturn theList.deleteFirst();\n\t}\n\t\n\tpublic boolean isEmpty(){\t//判断栈是否为空，即判断链表是否为空\n\t\treturn (theList.isEmpty());\n\t}\n\t\n\tpublic void displayStack(){\n\t\tSystem.out.println(\"Stack(top-->bottom):\");\n\t\ttheList.displayList();\n\t}\n\t\n}\n\npublic class LinkStack_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tLinkStack theStack = new LinkStack();\n\t\ttheStack.push(10);\n\t\ttheStack.push(20);\n\t\ttheStack.push(30);\n\t\ttheStack.displayStack();\n\t\t\n\t\ttheStack.pop();\n\t\ttheStack.pop();\n\t\ttheStack.displayStack();\n\t}\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["数据结构"]},{"title":"Java数据结构——链表-单链表","url":"/Java数据结构——链表-单链表.html","content":"**<1>链表**\n\n<img src=\"/images/517519-20160330141250098-69970065.png\" alt=\"\" />\n\n**<2>引用和基本类型**\n\n<img src=\"/images/517519-20160330144244910-916670416.png\" alt=\"\" />\n\n**<3>单链表**\n\n<img src=\"/images/517519-20160330150410504-2065872703.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n```\nclass Link{\t\t\t//链节点类\n\tpublic int iData;\n\tpublic double dData;\n\tpublic Link next;\t//链表中下一个节点的引用\n\t\n\tpublic Link(int iData, double dData) {\n\t\tsuper();\n\t\tthis.iData = iData;\n\t\tthis.dData = dData;\n\t}\n\t\n\tpublic void displayLink(){\t//显示当前节点的值\n\t\tSystem.out.println(\"iData=\"+iData+\",\"+\"dData\"+dData);\n\t}\n\t\n}\n\nclass LinkList{\n\tprivate Link first;\t\t//只需要第一个节点，从第一个节点出发即可定位所有节点\n\n\tpublic LinkList() {\t\t//构造函数\n\t\tthis.first = null;\n\t}\n\t\n\tpublic boolean isEmpty(){\n\t\treturn (first == null);\n\t}\n\t\n\tpublic void insertFirst(int id,double dd){\t//插入元素是从链表的头开始插入\n\t\tLink newLink = new Link(id,dd);\n\t\tnewLink.next = first;\n\t\tfirst = newLink;\n\t}\n\t\n\tpublic Link deleteFirst(){\n\t\tLink temp = first;\t\t//暂存first\n\t\tfirst = first.next;\t\t//把next设为first\n\t\treturn temp;\t\t\t//返回原来的first\n\t}\n\t\n\tpublic void displayList(){\n\t\tSystem.out.println(\"List(first-->last):\");\n\t\tLink current = first;\t\t//用于不断改变位置实现遍历\n\t\twhile(current != null){\n\t\t\tcurrent.displayLink();\n\t\t\tcurrent = current.next;\n\t\t}\n\t}\n}\n\npublic class LinkList_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tLinkList theList = new LinkList();\n\t\ttheList.insertFirst(11, 11.1);\n\t\ttheList.insertFirst(22, 22.2);\n\t\ttheList.insertFirst(33, 33.3);\n\t\ttheList.insertFirst(44, 44.4);\n\t\ttheList.insertFirst(55, 55.5);\n\t\ttheList.displayList();\n\t\twhile(!theList.isEmpty()){\n\t\t\tLink aLink = theList.deleteFirst();\n\t\t\tSystem.out.print(\"delete:\");\n\t\t\taLink.displayLink();\n\t\t}\n\t\ttheList.displayList();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n```\nclass Link{\t\t\t//链节点类\n\tpublic int iData;\n\tpublic double dData;\n\tpublic Link next;\t//链表中下一个节点的引用\n\t\n\tpublic Link(int iData, double dData) {\n\t\tsuper();\n\t\tthis.iData = iData;\n\t\tthis.dData = dData;\n\t}\n\t\n\tpublic void displayLink(){\t//显示当前节点的值\n\t\tSystem.out.println(\"iData=\"+iData+\",\"+\"dData\"+dData);\n\t}\n\t\n}\n\nclass LinkList{\n\tprivate Link first;\t\t//只需要第一个节点，从第一个节点出发即可定位所有节点\n\n\tpublic LinkList() {\t\t//构造函数\n\t\tthis.first = null;\n\t}\n\t\n\tpublic boolean isEmpty(){\n\t\treturn (first == null);\n\t}\n\t\n\tpublic void insertFirst(int id,double dd){\t//插入元素是从链表的头开始插入\n\t\tLink newLink = new Link(id,dd);\n\t\tnewLink.next = first;\n\t\tfirst = newLink;\n\t}\n\t\n\tpublic Link deleteFirst(){\n\t\tLink temp = first;\t\t//暂存first\n\t\tfirst = first.next;\t\t//把next设为first\n\t\treturn temp;\t\t\t//返回原来的first\n\t}\n\t\n\tpublic void displayList(){\n\t\tSystem.out.println(\"List(first-->last):\");\n\t\tLink current = first;\t\t//用于不断改变位置实现遍历\n\t\twhile(current != null){\n\t\t\tcurrent.displayLink();\n\t\t\tcurrent = current.next;\n\t\t}\n\t}\n\t\n\tpublic Link find(int key){\t\t//查找指定的关键字\n\t\tLink current = first;\n\t\twhile(current.iData != key){\n\t\t\tif(current.next == null)\n\t\t\t\treturn null;\n\t\t\telse\n\t\t\t\tcurrent = current.next;\n\t\t}\n\t\treturn current;\n\t}\n\t\n\tpublic Link delete(int key){\t\t//如果current的值匹配，则删除\n\t\tLink current = first;\t\t\t\t\n\t\tLink previous = first;\n\t\t//没有匹配到值\n\t\twhile(current.iData != key){\n\t\t\tif(current.next == null)\n\t\t\t\treturn null;\n\t\t\telse{\t\t\t//pre和cur向后移动\n\t\t\t\tprevious = current;\n\t\t\t\tcurrent = current.next;\n\t\t\t}\n\t\t}\n\t\t//匹配到值\n\t\tif(current == first)\t\t//只有一个first，并匹配，则把first设成first.next\n\t\t\tfirst = first.next;\n\t\telse\t\t\t\t//current的值匹配，则删除，并把cur的next赋给pre的next\n\t\t\tprevious.next = current.next;\n\t\treturn current;\n\t}\n}\n\npublic class LinkList_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tLinkList theList = new LinkList();\n\t\ttheList.insertFirst(11, 11.1);\n\t\ttheList.insertFirst(22, 22.2);\n\t\ttheList.insertFirst(33, 33.3);\n\t\ttheList.insertFirst(44, 44.4);\n\t\ttheList.insertFirst(55, 55.5);\n\t\ttheList.displayList();\n\t\t\n\t\tLink f = theList.find(22);\n\t\tif(f != null){\n\t\t\tSystem.out.print(\"找到：\");\n\t\t\tf.displayLink();\n\t\t}\n\t\telse\n\t\t\tSystem.out.print(\"没有找到\");\n\t\t\n\t\tLink d = theList.delete(32);\n\t\tif(d != null){\n\t\t\tSystem.out.print(\"删除：\");\n\t\t\td.displayLink();\n\t\t}\n\t\telse\n\t\t\tSystem.out.print(\"没有找到匹配的删除\");\n\t\t\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["数据结构"]},{"title":"Ubuntu16.04离线安装python3.8","url":"/Ubuntu16.04离线安装python3.8.html","content":"1.下载python3.8\n\n```\ncd ~/Download\nwget https://www.python.org/ftp/python/3.8.11/Python-3.8.11.tgz\n\n```\n\n解压\n\n```\ntar -zxvf Python-3.8.11.tgz\n\n```\n\n2.创建目录\n\n```\ncd /usr/local\nsudo mkdir python\ncd ./python\nsudo mkdir python3.8\n\n```\n\n3.编译安装\n\n```\n./configure prefix=/usr/local/python/python3.8 --enable-optimizations\nmake -j 2\nmake altinstall  >&amp;1|tee make.log\n\n```\n\n4.配置环境变量\n\n```\n# python\nexport PATH=$PATH:/usr/local/python/python3.8/bin\n\n```\n\n参考：[Ubuntu16.04安装Python3.8，3.7，3.9(含卸载方法，支持多版本共存)](https://blog.csdn.net/qq_35743870/article/details/125903040)\n","tags":["Python"]},{"title":"Java排序算法——插入排序","url":"/Java排序算法——插入排序.html","content":"```\nimport java.util.Arrays;\n\nclass Arrays_Insert{\n\tprivate int[] arrays;\n\tprivate int curNum;\n\n\tpublic Arrays_Insert(int max) {\t\t\t//建立一个max长度的空数组\n\t\tsuper();\n\t\tarrays = new int[max];\n\t\tcurNum = 0;\n\t}\n\t\n\tpublic void insert(int value){\t\t\t\t\t//往空的数组里面增加元素\n\t\tarrays[curNum] = value;\n\t\tcurNum++;\n\t}\n\t\n\tpublic void display(){\t\t\t\t\t\t\t\t\t//显示数组\n\t\tSystem.out.println(Arrays.toString(arrays));\n\t}\n\t\n\tprivate void swap(int one,int two){\t\t//交换\n\t\tint temp = arrays[one];\n\t\tarrays[one] = arrays[two];\n\t\tarrays[two] = temp;\n\t}\n\t\n\tpublic void InsertSort(){\n\t\tint out,in;\n\t\t\n\t\tfor(out=1;out<curNum;out++){\t\t\t\t\t\t//从第2个开始，和第1个比较\n\t\t\tint temp = arrays[out];\n\t\t\tin = out;　　　　　　　　　　　　　　　　　　　　　　　　//in等于out，比较从in-1开始\n\t\t\twhile(in>0 &amp;&amp; arrays[in-1] >= temp){\t\t//如果大于temp，就往右移动\n\t\t\t\tarrays[in] = arrays[in-1];　　　　　　　　例如：2 3 1 temp=1 -> 2 3 3 temp=1 -> 2 2 3 temp=1 -> 1 2 3 temp=1\n\t\t\t\t--in;\n\t\t\t}\n\t\t\tarrays[in] = temp;\n\t\t}\n\t}\n\t\n}\n\npublic class Insert_Sort {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tint maxSize = 100;\n\t\tArrays_Insert arrays_demo = new Arrays_Insert(maxSize);\n\t\tarrays_demo.insert(58);\n\t\tarrays_demo.insert(57);\n\t\tarrays_demo.insert(56);\n\t\tarrays_demo.insert(60);\n\t\tarrays_demo.insert(59);\n\t\tarrays_demo.display();\n\t\tarrays_demo.InsertSort();\n\t\tarrays_demo.display();\n\t}\n\n}\n\n```\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20160329094252660-78657633.png\" width=\"700\" height=\"673\" />\n\n**冒泡**&nbsp; 　　N^2/2比较　　N^2/4交换\n\n**选择**　　&nbsp; N^2/2比较　　比冒泡少的交换\n\n**插入**　　 &nbsp;N^2/4比较　　N^2/4复制\n\n复制是交换的3倍\n","tags":["算法"]},{"title":"Java数据结构——优先级队列","url":"/Java数据结构——优先级队列.html","content":"<img src=\"/images/517519-20160329212158894-1558850086.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160329215621691-1002445254.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160329215641160-3107436.png\" alt=\"\" />\n\n```\nclass PriorityQueue{\n\tprivate int maxSize;\t\t\t//队列的长度\n\tprivate long[] queueArray;\t\t//创建队列的数组的引用\n\tprivate int curNum;\t\t\t//创建当前元素的个数\n\t\n\tpublic PriorityQueue(int s) {\t\t//构造函数\n\t\tthis.maxSize = s;\n\t\tqueueArray = new long[maxSize]; //创建对象\n\t\tcurNum = 0;\t\t\t//当前的元素的个数是0\n\t}\n\t\n\tpublic void insert(long item){\n\t\tint j;\n\t\tif(curNum == 0){\t\t//输入1个数，就是1个数\n\t\t\tqueueArray[curNum++] = item;\t\t\t\t\n\t\t}else{\t\t\t\t\t//左边是rear，右边是front，左边总比右边大\n\t\t\tfor(j=curNum-1;j>=0;j--){\t//当有2个数时，若第2个数比第1个数大，则交换\n\t\t\t\tif(item>queueArray[j]){\t//j在-1的时候停止，rear=0，front=curNum-1\n\t\t\t\t\tqueueArray[j+1] = queueArray[j];\t// 1 2 -> 1 1 -> 2 1 \t\t\t\n\t\t\t\t}else{break;}\t\t\t\t\t// 2 1 3 -> 2 1 1 -> 2 2 1 -> 3 2 1\n\t\t\t}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\tqueueArray[j+1] = item;\n\t\t\tcurNum++;\n\t\t}\n\n\t}\n\t\n\tpublic long remove(){\n\t\treturn queueArray[--curNum];\n\t}\n\t\n\tpublic long peekFront(){\n\t\treturn queueArray[curNum-1];\t\t\n\t}\n\t\n\tpublic boolean isEmpty(){\n\t\treturn (curNum==0);\n\t}\n\t\n\tpublic boolean isFull(){\n\t\treturn (curNum==maxSize);\n\t}\n\t\n\t\n}\n\npublic class PriorityQueue_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tPriorityQueue queue_demo = new PriorityQueue(5);\n\t\tqueue_demo.insert(30);\n\t\tqueue_demo.insert(10);\n\t\tqueue_demo.insert(50);\n\t\tqueue_demo.insert(40);\n\t\tqueue_demo.insert(20);\n\t\twhile( !queue_demo.isEmpty()){\n\t\t\tlong value = queue_demo.remove();\n\t\t\tSystem.out.println(value+\"、\");\n\t\t}\n\t}\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["数据结构"]},{"title":"Java数据结构——栈","url":"/Java数据结构——栈.html","content":"<img src=\"/images/517519-20160329112254723-734210911.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n```\nclass Stack{\n\tprivate int maxSize;\t\t\t//栈的长度\n\tprivate long[] stackArray;\t\t//创建栈的数组的引用\n\tprivate int top;\t\t\t//创建栈顶的引用\n\t\n\tpublic Stack(int s) {\t\t\t//构造函数\n\t\tthis.maxSize = s;\n\t\tstackArray = new long[maxSize]; //创建对象\n\t\ttop = -1;\t\t\t//栈顶等于-1\n\t}\n\t\n\tpublic void push(long j){\t\t//入栈操作\n\t\tstackArray[++top] = j;\t\t//先把top=-1自加成0,再入栈\n\t}\n\t\n\tpublic long pop(){\n\t\treturn stackArray[top--];\t//弹出当前栈顶的元素后，再自减\n\t}\n\t\n\tpublic long peek(){\n\t\treturn stackArray[top];\t\t//返回当前栈顶的元素\n\t}\n\t\n\tpublic boolean isEmpty(){\t//栈顶为-1,即栈为空\n\t\treturn (top == -1);\n\t}\n\t\n\tpublic boolean isFull(){\t\t//栈顶为maxSize-1,即栈为满\n\t\treturn (top == maxSize-1);\n\t}\n\t\n}\n\npublic class Stack_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tStack stack_demo = new Stack(10);\n\t\tstack_demo.push(50);\n\t\tstack_demo.push(60);\n\t\tstack_demo.push(70);\n\t\tstack_demo.push(80);\n\t\t\n\t\twhile(!stack_demo.isEmpty()){\t\t//当栈不为空\n\t\t\tlong value = stack_demo.pop();\n\t\t\tSystem.out.print(value+\"、\");\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160329122048613-366891597.png\" alt=\"\" />\n\n```\nclass Stack_Char{\n\tprivate int maxSize;\t\t\t//栈的长度\n\tprivate char[] stackArray;\t\t//创建栈的数组的引用\n\tprivate int top;\t\t\t//创建栈顶的引用\n\t\n\tpublic Stack_Char(int s) {\t\t//构造函数\n\t\tthis.maxSize = s;\n\t\tstackArray = new char[maxSize]; \t//创建对象\n\t\ttop = -1;\t\t\t\t//栈顶等于-1\n\t}\n\t\n\tpublic void push(char j){\t\t//入栈操作\n\t\tstackArray[++top] = j;\t\t//先把top=-1自加成0,再入栈\n\t}\n\t\n\tpublic char pop(){\n\t\treturn stackArray[top--];\t//弹出当前栈顶的元素后，再自减\n\t}\n\t\n\tpublic char peek(){\n\t\treturn stackArray[top];\t\t//返回当前栈顶的元素\n\t}\n\t\n\tpublic boolean isEmpty(){\t//栈顶为-1,即栈为空\n\t\treturn (top == -1);\n\t}\n\t\n\tpublic boolean isFull(){\t\t//栈顶为maxSize-1,即栈为满\n\t\treturn (top == maxSize-1);\n\t}\n\t\n}\n\nclass Reverse{\n\tprivate String input;\n\tprivate String output;\n\t\n\tpublic Reverse(String input) {\t\t//构造函数\n\t\tsuper();\n\t\tthis.input = input;\n\t}\n\t\n\tpublic String DoReverse(){\n\t\tint stackSize = input.length();\n\t\tStack_Char stack_demo = new Stack_Char(stackSize);\n\t\t\n\t\tfor(int i=0;i<stackSize;i++){\t\t//把每一个字母入栈\n\t\t\tchar ch = input.charAt(i);\n\t\t\tstack_demo.push(ch);\n\t\t}\n\t\toutput = \"\";\t\t\t\t//给output赋值，否则会变成nullXXX\n\t\twhile( !stack_demo.isEmpty() ){\t\t//把每一个字母出栈\n\t\t\tchar ch = stack_demo.pop();\n\t\t\tthis.output += ch;\n\t\t}\n\t\treturn this.output;\n\t}\n\t\n}\n\npublic class Word_Reverse {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tString input,output;\n\t\t\tinput = \"part\";\t\n\t\t\tSystem.out.println(\"输入的字符串：\"+input);\n\t\t\tReverse rev = new Reverse(input);\n\t\t\toutput = rev.DoReverse();\n\t\t\tSystem.out.println(\"输出的字符串：\"+output);\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160329142814660-411463878.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160329142834988-1613307503.png\" alt=\"\" />\n\n```\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\n\nclass BracketsChecker{\n\tprivate String input;\n\n\tpublic BracketsChecker(String input) {\t\t//构造函数\n\t\tsuper();\n\t\tthis.input = input;\n\t}\n\t\n\tpublic void Check(){\t\t\t\t\t\t\t\t\t\t\t\n\t\tint stackSize = input.length();\n\t\tStack_Char stack_demo = new Stack_Char(stackSize);\n\t\t\n\t\tfor(int i=0;i<input.length();i++){\n\t\t\tchar ch = input.charAt(i);\t\t\t//遍历每一个字符\n\t\t\tswitch(ch){\n\t\t\t\tcase '{':\t\t\t\t\n\t\t\t\tcase '[':\t\t\t\t\n\t\t\t\tcase '(':\t\t\t\t\n\t\t\t\t\tstack_demo.push(ch);\t\t\t//遇到'{[('就入栈\n\t\t\t\t\tbreak;\n\t\t\t\t\t\n\t\t\t\tcase '}':\n\t\t\t\tcase ']':\n\t\t\t\tcase ')':\n\t\t\t\t\tif( !stack_demo.isEmpty()){\n\t\t\t\t\t\t char chx = stack_demo.pop();\t\t//遇到'}])'弹出堆栈\n\t\t\t\t\t\t if(\t(chx=='{' &amp;&amp; ch!='}')\t||\t(chx=='[' &amp;&amp; ch!=']')\t||\t(chx=='(' &amp;&amp; ch!=')')){\n\t\t\t\t\t\t\t System.out.println(\"Error:右括号不应该是\"+ch+\" at \"+i);\n\t\t\t\t\t\t }\n\t\t\t\t\t}\t \n\t\t\t\t\telse{\n\t\t\t\t\t\t System.out.println(\"Error:只有右括号\"+ch+\" at \"+i);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\t\tdefault:break;\n\t\t\t}\n\t\t}\n\t\tif( !stack_demo.isEmpty()){\t\t\t\t//如果栈不为空的话，证明缺少右括号\n\t\t\tSystem.out.println(\"Error:缺少右括号\");\n\t\t}\n\t}\n}\n\npublic class Brackets_demo {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tString input;\n//\t\tinput = \"{[(]}\";\n\t\tSystem.out.print(\"输入字符串：\");\n\t\tinput = getString();\n\t\tSystem.out.println(\"输入的字符串为：\"+input);\n\t\tBracketsChecker che = new BracketsChecker(input);\n\t\tche.Check();\n\t}\n\n\tpublic static String getString() throws IOException{\n\t\tInputStreamReader isr = new InputStreamReader(System.in); //把输入的字节流转换成字符流\n\t\tBufferedReader br = new BufferedReader(isr); //只能接收字符输入流的实例\n\t\tString str = br.readLine();\t\t\t\t//一次性从缓冲区中读取内容\n\t\treturn str;\n\t}\n\t\n}\n\n```\n\n&nbsp;\n","tags":["数据结构"]},{"title":"Java数据结构——队列","url":"/Java数据结构——队列.html","content":"<img src=\"/images/517519-20160329200625113-1427366909.png\" alt=\"\" />\n\n```\nclass Queue{\n\tprivate int maxSize;\t\t\t//队列的长度\n\tprivate long[] queueArray;\t\t//创建队列的数组的引用\n\tprivate int front;\t\t\t//创建队头的引用\n\tprivate int rear;\t\t\t//创建队尾的引用\n\tprivate int curNum;\t\t\t//创建当前元素的个数\n\t\n\tpublic Queue(int s) {\t\t//构造函数\n\t\tthis.maxSize = s;\n\t\tqueueArray = new long[maxSize]; //创建对象\n\t\tfront = 0;\t\t\t//队头等于0\n\t\trear = -1;\t\t\t//队尾等于-1\n\t\tcurNum = 0;\t\t\t//当前的元素的个数是0\n\t}\n\t\n\tpublic void insert(long j){\n\t\tif(rear == maxSize-1){\t\t//如果队尾是队列的最大值-1,则队尾等于-1\n\t\t\trear = -1;\n\t\t}\n\t\tqueueArray[++rear] = j;\t\t//先自加为0,然后给队尾的元素赋值\n\t\tcurNum++;\t\t\t//当前的元素的个数加1\n\t}\n\t\n\tpublic long remove(){\n\t\tlong temp = queueArray[front++];\t//取得队头的元素，然后自加\n\t\tif(front == maxSize){\n\t\t\tfront =0;\n\t\t}\n\t\tcurNum--;\t\t//当前的元素的个数减1\n\t\treturn temp;\n\t}\n\t\n\tpublic long peekFront(){\n\t\treturn queueArray[front];\t\t\n\t}\n\t\n\tpublic boolean isEmpty(){\n\t\treturn (curNum==0);\n\t}\n\t\n\tpublic boolean isFull(){\n\t\treturn (curNum==maxSize);\n\t}\n\t\n\tpublic int Size(){\t//返回当前队列元素的个数\n\t\treturn curNum;\n\t}\n\t\n}\n\npublic class Queue_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tQueue queue_demo = new Queue(5);\n\t\tqueue_demo.insert(50);\n\t\tqueue_demo.insert(60);\n\t\tqueue_demo.insert(70);\n\t\tqueue_demo.insert(80);\n\t\tqueue_demo.remove();\n\t\tqueue_demo.insert(50);\n\t\t\n\t\twhile( !queue_demo.isEmpty()){\n\t\t\tlong value = queue_demo.remove();\n\t\t\tSystem.out.print(value+\"、\");\n\t\t}\n\t}\n}\n\n```\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20160329211920098-996496402.png\" alt=\"\" />\n","tags":["数据结构"]},{"title":"Java排序算法——冒泡排序","url":"/Java排序算法——冒泡排序.html","content":"<img src=\"/images/517519-20160328163653113-2060011406.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n```\npackage com.interview.sort;\n\nimport java.util.Arrays;\n\nclass ArraysBubble {\n    private int[] arrays;\n    private int curNum;\n\n    public ArraysBubble(int max) {            //建立一个max长度的空数组\n        super();\n        arrays = new int[max];\n        curNum = 0;\n    }\n\n    public void insert(int value) {                    //往空的数组里面增加元素\n        arrays[curNum] = value;\n        curNum++;\n    }\n\n    public void display() {                                    //显示数组\n        System.out.println(Arrays.toString(arrays));\n    }\n\n    private void swap(int one, int two) {        //交换\n        int temp = arrays[one];\n        arrays[one] = arrays[two];\n        arrays[two] = temp;\n    }\n\n    public void BubbleSort() {\n        for (int i = 0; i < curNum; i++) {\n            for (int j = i + 1; j < curNum; j++) {\n                if (arrays[j] < arrays[i]) {\n                    swap(i, j);\n                }\n            }\n        }\n    }\n\n//    public void BubbleSort() {\n//        int out, in;\n//        for (out = curNum - 1; out > 0; out--) {        //curNum自加为下一次插入做做准备，在此应减去1\n//            for (in = 0; in < out; in++) {                        //从最后一个数开始，和前面每一个数比较，把最大的移到最后\n//                if (arrays[in] > arrays[out]) {\n//                    swap(in, in + 1);\n//                }\n//            }\n//        }\n//\n//\n//    }\n}\n\npublic class BubbleSort {\n\n    public static void main(String[] args) {\n        // TODO 自动生成的方法存根\n        int maxSize = 100;\n        ArraysBubble arrays_demo = new ArraysBubble(maxSize);\n        arrays_demo.insert(58);\n        arrays_demo.insert(57);\n        arrays_demo.insert(56);\n        arrays_demo.insert(60);\n        arrays_demo.insert(59);\n        arrays_demo.display();\n        arrays_demo.BubbleSort();\n        arrays_demo.display();\n    }\n\n}\n\n```\n\n&nbsp;输出\n\n```\n[58, 57, 56, 60, 59, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[56, 57, 58, 59, 60, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n```\n\n&nbsp; <img src=\"/images/517519-20160329091249910-1552751438.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160328163910863-643787420.png\" alt=\"\" />\n","tags":["算法"]},{"title":"Java排序算法——选择排序","url":"/Java排序算法——选择排序.html","content":"<img src=\"/images/517519-20160328164456644-987492407.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n```\nimport java.util.Arrays;\n\nclass Arrays_Select{\n\tprivate int[] arrays;\n\tprivate int curNum;\n\n\tpublic Arrays_Select(int max) {\t\t\t//建立一个max长度的空数组\n\t\tsuper();\n\t\tarrays = new int[max];\n\t\tcurNum = 0;\n\t}\n\t\n\tpublic void insert(int value){\t\t\t\t\t//往空的数组里面增加元素\n\t\tarrays[curNum] = value;\n\t\tcurNum++;\n\t}\n\t\n\tpublic void display(){\t\t\t\t\t\t\t\t\t//显示数组\n\t\tSystem.out.println(Arrays.toString(arrays));\n\t}\n\t\n\tprivate void swap(int one,int two){\t\t//交换\n\t\tint temp = arrays[one];\n\t\tarrays[one] = arrays[two];\n\t\tarrays[two] = temp;\n\t}\n\t\n\tpublic void SelectSort(){\n\t\tint out,in,min;\n\t\t\n\t\tfor(out=0;out<curNum-1;out++){\n\t\t\tmin = out;\t\t\t\t\t//out从0开始，设第一个为最小\n\t\t\tfor(in=out+1;in<curNum;in++){　　　　　　　　　　　//搜索之后的元素，从1开始，如果有比第0个小，交换\n\t\t\t\tif(arrays[in]<arrays[min]){　　　　　　　//都是找到未排序中最小的数\n\t\t\t\t\tmin = in;\t\t\t//找到最小数的标号\n\t\t\t\t}\n\t\t\t\tswap(out,min);\t//交换\n\t\t\t}\n\t\t}\n\t}\n}\n\npublic class Select_Sort {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tint maxSize = 100;\n\t\tArrays_Select arrays_demo = new Arrays_Select(maxSize);\n\t\tarrays_demo.insert(58);\n\t\tarrays_demo.insert(57);\n\t\tarrays_demo.insert(56);\n\t\tarrays_demo.insert(60);\n\t\tarrays_demo.insert(59);\n\t\tarrays_demo.display();\n\t\tarrays_demo.SelectSort();\n\t\tarrays_demo.display();\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160328173931160-1881265620.png\" alt=\"\" />\n","tags":["算法"]},{"title":"Java查找算法——二分查找","url":"/Java查找算法——二分查找.html","content":"<img src=\"/images/517519-20160328134909973-1775785493.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n```\nimport java.lang.reflect.Array;\nimport java.nio.Buffer;\nimport java.util.Arrays;\nimport java.util.Random;\n\nclass BinarySearch_Find{\n\tprivate int[] temp;\n\tprivate int searchKey;\n\tprivate int lowerBound = 0;\t\t\t//下界\n\tprivate int upperBound ;\t\t\t\t//上界\n\tprivate int curNum;\n\t\n\tpublic int[] getTemp() {\n\t\treturn temp;\n\t}\n\n\tpublic void setTemp(int[] temp) {\n\t\tthis.temp = temp;\n\t}\n\n\tpublic BinarySearch_Find(int[] temp) {//构造函数\n\t\tthis.temp = temp;\n\t\tthis.upperBound = temp.length-1;\n\t}\n\n\tpublic int find(int searchKey){\n\t\tthis.searchKey = searchKey;\n\t\twhile(true){\n\t\t\tcurNum = (lowerBound+upperBound)/2;\n\t\t\tif(temp[curNum]==this.searchKey){\n\t\t\t\treturn curNum;\t\t\t\t\t\t\t//find\n\t\t\t}\n\t\t\telse if(lowerBound>upperBound){\n\t\t\t\treturn -1;\t\t\t\t\t\t\t\t\t\t//没有find\n\t\t\t}\n\t\t\telse{\n\t\t\t\tif(temp[curNum]<this.searchKey){\n\t\t\t\t\tlowerBound = curNum+1;\n\t\t\t\t}\n\t\t\t\telse{\n\t\t\t\t\tupperBound = curNum-1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\t\n}\n\nclass RandomArrays{\t\t\t\t\t//生成随机数组，有Num个\n\t\n\tpublic int[] getArrays(int Num){\n\t\tint[] Arrays = new int[Num];\n\t\tRandom r = new Random();\n\t\t\n\t\tfor(int i=0;i<Num;i++){\n\t\t\tArrays[i] = r.nextInt(1000);\n//\t\t\tSystem.out.print(Arrays[i]+\"、\");\n\t\t}\n\t\treturn Arrays;\n\t}\n}\n\nclass OrderedArrays{\t\t\t\t\t//生成有序数组，从0开始到Num\n\t\n\tpublic int[] getArrays(int Num){\n\t\tint[] Arrays = new int[Num];\n\t\t\n\t\tfor(int i=0;i<Num;i++){\n\t\t\tArrays[i] = i;\n//\t\t\tSystem.out.print(Arrays[i]+\"、\");\n\t\t}\n\t\treturn Arrays;\n\t}\n}\n\npublic class Binary_Search {\n\t\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\n//\t\tRandomArrays array_demo = new RandomArrays();\n//\t\tBinarySearch_Find arrays = new BinarySearch_Find(array_demo.getArrays(100));\n\t\t\n\t\tOrderedArrays array_demo = new OrderedArrays();\n\t\tBinarySearch_Find arrays = new BinarySearch_Find(array_demo.getArrays(100));\n\t\tSystem.out.println(Arrays.toString(arrays.getTemp()));\n\t\tSystem.out.println(arrays.find(1000));\n\t\t\n\t}\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160328140256004-119925651.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160328143628832-408260470.png\" alt=\"\" />\n","tags":["算法"]},{"title":"SpringBoot学习笔记——filter和interceptor","url":"/SpringBoot学习笔记——filter和interceptor.html","content":"Servlet API中提供了一个**Filter接口**，Filter接口在javax.servlet.Filter包下面。开发web应用时，如果编写的**Java类实现了这个接口**，则把这个**java类称之为过滤器Filter**。\n\n通过Filter技术，开发人员可以实现用户在访问某个目标资源之前，对访问的请求和响应进行拦截，如下所示：\n\n<img src=\"/images/517519-20170103213238816-1583662914.png\" alt=\"\" width=\"592\" height=\"181\" />\n\nWEB开发人员通过Filter技术，对web服务器管理的所有web资源：例如Jsp, Servlet, 静态图片文件或静态 html 文件等**进行拦截**，从而实现一些特殊的功能。\n\n例如**实现URL级别的权限访问控制、乱码问题**、**过滤敏感词汇**、**压缩响应信息**等一些高级功能。\n\nfilter在开发中的常见应用： <br /><!--more-->\n&nbsp;&nbsp; &nbsp;&nbsp;　　 1.filter可以目标资源执行之前，进行权限检查，检查用户有无权限，如有权限则放行，如没有，则拒绝访问 <br />&nbsp;&nbsp; &nbsp;&nbsp;　　 2.filter可以放行之前，对request和response进行预处理，从而实现一些全局性的设置。 <br />&nbsp;&nbsp; &nbsp;&nbsp;　　 3.filter在放行之后，可以捕获到目标资源的输出，从而对输出作出类似于压缩这样的设置&nbsp; \n\n<img src=\"/images/517519-20170103213540394-830100257.png\" alt=\"\" />\n\n<img src=\"/images/517519-20170103213617300-841127662.png\" alt=\"\" />\n\n<img src=\"/images/517519-20170103213653519-1017272505.png\" alt=\"\" />\n\n在filter中可以对request请求进行拦截，并对response进行修改\n\n实现Filter接口的话，需要重写3个方法init，doFilter和destroy\n\n```\npublic interface Filter {\n    void init(FilterConfig var1) throws ServletException;\n\n    void doFilter(ServletRequest var1, ServletResponse var2, FilterChain var3) throws IOException, ServletException;\n\n    void destroy();\n}\n```\n\n而实现 OncePerRequestFilter 接口只用重写1个方法，其和 Filter 的区别参考：[OncePerRequestFilter的作用](https://www.cnblogs.com/shanshouchen/archive/2012/07/31/2617412.html)\n\n```\npublic class JwtAuthenticationFilter extends OncePerRequestFilter {\n    @Override\n    protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException {\n\n    }\n}\n```\n\n下面使用&nbsp;OncePerRequestFilter 来实现一个java web token认证的功能\n\n1. 定义&nbsp;JwtAuthenticationFilter\n\n```\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.web.filter.OncePerRequestFilter;\n\nimport javax.servlet.FilterChain;\nimport javax.servlet.ServletException;\nimport javax.servlet.http.Cookie;\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\nimport java.io.IOException;\n\n@Slf4j\npublic class JwtAuthenticationFilter extends OncePerRequestFilter {\n    @Override\n    protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException {\n\n        Cookie[] cookies = request.getCookies();\n        int len = cookies.length;\n        for (int i = 0; i < len; i++) {\n            Cookie cookie = cookies[i];\n            if (\"token\".equals(cookie.getName())) {\n                log.info(\"has token\");\n                break;\n            }\n            if (i == len - 1) {\n                log.info(\"no token\");\n            }\n        }\n\n        filterChain.doFilter(request, response);\n    }\n}\n\n```\n\n2. 配置filter，使该filter生效\n\n```\nimport com.example.demo.jwt.JwtAuthenticationFilter;\nimport org.springframework.boot.web.servlet.FilterRegistrationBean;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\n@Configuration\npublic class FilterConfig {\n\n    @Bean\n    public FilterRegistrationBean<JwtAuthenticationFilter> jwtFilter() {\n        FilterRegistrationBean<JwtAuthenticationFilter> registration = new FilterRegistrationBean<>();\n        registration.setFilter(new JwtAuthenticationFilter());\n        registration.addUrlPatterns(\"/*\");\n        registration.setOrder(1);\n        return registration;\n    }\n}\n\n```\n\n3.请求接口，cookie中没有携带token的话，将会打印出no token\n\n&nbsp;<img src=\"/images/517519-20210610152238815-2116161918.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n使用interceptor实现认证，参考：[后端接口幂等性校验（数据库唯一索引 + 自定义注解和拦截器的token校验）](https://blog.csdn.net/LO_YUN/article/details/100917750)\n\n1. 定义interceptor\n\n```\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.web.servlet.HandlerInterceptor;\nimport org.springframework.web.servlet.ModelAndView;\n\nimport javax.servlet.http.Cookie;\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\n\n@Slf4j\npublic class CheckTokenInterceptor implements HandlerInterceptor {\n\n    @Override\n    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {\n\n        Cookie[] cookies = request.getCookies();\n        int len = cookies.length;\n        for (int i = 0; i < len; i++) {\n            Cookie cookie = cookies[i];\n            if (\"token\".equals(cookie.getName())) {\n                log.info(\"has token\");\n                break;\n            }\n            if (i == len - 1) {\n                log.info(\"no token\");\n            }\n        }\n        return true; // 是否放行\n    }\n\n\n    @Override\n    public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception {\n\n    }\n\n    @Override\n    public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception {\n\n    }\n\n}\n\n```\n\n2. 配置config\n\n```\nimport com.example.demo.jwt.CheckTokenInterceptor;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.web.servlet.config.annotation.InterceptorRegistry;\nimport org.springframework.web.servlet.config.annotation.WebMvcConfigurer;\n\n@Configuration\npublic class LoginConfig implements WebMvcConfigurer {\n\n    @Override\n    public void addInterceptors(InterceptorRegistry registry) {\n        // 加载check token interceptor\n        registry.addInterceptor(new CheckTokenInterceptor())\n                .addPathPatterns(\"/**\");\n//                .excludePathPatterns(\"/admin\")\n//                .excludePathPatterns(\"/admin/login\");\n    }\n}\n\n```\n\n3.请求接口，cookie中没有携带token的话，将会打印出no token\n\n&nbsp;<img src=\"/images/517519-20210610152238815-2116161918.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\nfilter，interceptor和aop的区别参考：[过滤器、拦截器和AOP的分析与对比](https://www.shuzhiduo.com/A/D854PWnw5E/)\n","tags":["SpringBoot"]},{"title":"使用jstack找到java进程中cpu占用最高的线程","url":"/使用jstack找到java进程中cpu占用最高的线程.html","content":"1. 使用top命令查看进程的cpu占用，找到进程的pid\n\n<img src=\"/images/517519-20210806185107378-210582.png\" alt=\"\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n2.查看进程中cpu占用最高的线程\n\n```\ntop -Hp ${pid} -d 1 -n 1\n\n```\n\n<img src=\"/images/517519-20210806185458422-2052054195.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n该进程的pid是279，其十六进制表示为0x117\n\n3.打印进程的堆栈信息到文件\n\n```\njstack -l ${pid} > jstack.log\n\n```\n\n4.在堆栈信息中查看0x117，即可看到具体代码的信息\n\n<img src=\"/images/517519-20210806185823861-304646303.png\" alt=\"\" loading=\"lazy\" />\n\n参考：[Linux下JAVA线程占用CPU高一点分享](https://www.iteye.com/blog/lxiaodao-1413774)\n","tags":["JVM"]},{"title":"广告系统当中的ID","url":"/广告系统当中的ID.html","content":"在广告系统当中，ID是标识用户比较重要的手段，\n\n## 1.安卓端\n\n**AdID**：又称为Google advertising ID，海外安卓平台常用的设备标识符，每一台 Android设备都会被分配一个唯一的标识符，海外安卓平台的广告投放归因也主要依赖AdID。在国内，因google play在大陆地区无法使用，故google推动的adid也无法使用。且在android12开始，所有在设备设置中选择退出个性化广告的用户，其广告 ID 会变为一串零 (0)，这将会影响android广告的归因。参考：[移动广告流量中那些ID的坑](https://zhuanlan.zhihu.com/p/122470581) 和 [Google 宣布移除限制广告跟踪用户的安卓广告 ID](https://www.adjust.com/zh/blog/android-advertising-id-announcement/)\n\n**IMEI**（International Mobile EquipmentIdentity，移动设备国际识别码，又称为国际移动设备标识），即手机的卡槽号ID，可见这种ID是用户无法关闭或重置的，严格意义上来说在个人信息隐私保护方面存在不合规的高风险，所以采集时要经过用户的授权同意，存储使用时也一定要进行加密处理（MD5摘要加密），匿名化处理。也正是IMEI对个人信息隐私保护方面存在不合规性的高风险，所以从Android Q 开始，IMEI等ID的获取将受到非常大的安全限制，需用户每次授权。故国产手机纷纷开始推广OAID体系。参考：[穿山甲&mdash;&mdash;如何获取设备ID](https://www.csjplatform.com/supportcenter/5425)\n\n**OAID**：OAID全称是Open Anonymous Device Identifier，中文名是匿名设备标识符。 OAID是**一种非永久性设备标识符，最长64位，在系统首次启动的时候生成**。 因此OAID可在保护用户个人数据隐私安全的前提下，用于向用户提供个性化广告，用户统计，同时三方监测平台也可以向广告主提供转化归因分析。支持OAID的条件：1)安卓设备系统版本10及以上; 2)设备品牌：HUAWEI/OPPO/VIVO/XIAOMI\n\n**ANID**：ANDROID_ID android设备的唯一识别码，在设备首次启动时，系统会随机生成一个64位的数字，并把这个数字以16进制字符串的形式保存下来，这个16进制的字符串就是ANDROID_ID，当设备被wipe后该值会被重置。\n\n## 2.IOS端\n\n**IDFA**：IOS系统下的广告流量主ID相对于Android的情况要好很多了，基本统一使用IDFA（Identifier For Advertising）\n\n<!--more-->\n&nbsp;\n","tags":["广告系统"]},{"title":"Hive学习笔记——UDF开发","url":"/Hive学习笔记——UDF开发.html","content":"实现一个UDF函数可以继承 org.apache.hadoop.hive.ql.exec.UDF，也可以继承 org.apache.hadoop.hive.ql.udf.generic.GenericUDF\n\n1.继承UDF，参考\n\n```\nhttps://docs.microsoft.com/en-us/azure/hdinsight/hadoop/apache-hadoop-hive-java-udf\n\n```\n\n引入依赖\n\n```\n<dependency>\n    <groupId>org.apache.hive</groupId>\n    <artifactId>hive-exec</artifactId>\n    <version>1.1.0-cdh5.16.2</version>\n</dependency>\n<dependency>\n    <groupId>org.apache.hadoop</groupId>\n    <artifactId>hadoop-common</artifactId>\n    <version>2.6.0-cdh5.16.2</version>\n</dependency>\n\n```\n\n代码\n\n```\nimport org.apache.hadoop.hive.ql.exec.Description;\nimport org.apache.hadoop.hive.ql.exec.UDF;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n@Description(\n        name = \"hello\",\n        value = \"_FUNC_(str) - from the input string\"\n                + \"returns the value that is \\\"Hello $str\\\" \",\n        extended = \"Example:\\n\"\n                + \" > SELECT _FUNC_(str) FROM src;\"\n)\npublic class MyUDF extends UDF {\n\n    private static final Logger logger = LoggerFactory.getLogger(MyUDF.class);\n\n    public String evaluate(String str){\n        try {\n            return \"Hello \" + str;\n        } catch (Exception e) {\n            // TODO: handle exception\n            e.printStackTrace();\n            return \"ERROR\";\n        }\n    }\n\n}\n\n```\n\n打包的时候需要注意，需要把所以依赖的jar都打进去，然后将jar包上传到HDFS上或者s3上\n\n```\nhive> add jar hdfs:///user/hive/udf/bigdata-1.0-SNAPSHOT-jar-with-dependencies.jar;\nconverting to local hdfs:///user/hive/udf/bigdata-1.0-SNAPSHOT-jar-with-dependencies.jar\nAdded [/tmp/5aa66ab6-35ab-45d5-bef1-5acc79d16b23_resources/bigdata-1.0-SNAPSHOT-jar-with-dependencies.jar] to class path\nAdded resources: [hdfs:///user/hive/udf/bigdata-1.0-SNAPSHOT-jar-with-dependencies.jar]\nhive> create temporary function my_lower as \"com.bigdata.hive.MyUDF\";\nOK\nTime taken: 0.073 seconds\nhive> select my_lower(\"123\");\nOK\nHello 123\nTime taken: 0.253 seconds, Fetched: 1 row(s)\n\n```\n\n查看jar包\n\n```\nhive> list jar;\n/tmp/5aa66ab6-35ab-45d5-bef1-5acc79d16b23_resources/bigdata-1.0-SNAPSHOT-jar-with-dependencies.jar\n\n```\n\n删除jar包\n\n```\nhive> delete jar /tmp/5aa66ab6-35ab-45d5-bef1-5acc79d16b23_resources/bigdata-1.0-SNAPSHOT-jar-with-dependencies.jar;\n\n```\n\n查看function\n\n```\nhive> show functions like '*lower';\nOK\nlower\nTime taken: 0.016 seconds, Fetched: 1 row(s)\n\n```\n\n删除function\n\n```\nhive> drop function if exists my_lower;\n\n```\n\n<!--more-->\n&nbsp;\n\n2.继承GenericUDF，参考：[Hive- UDF&amp;GenericUDF](https://www.jianshu.com/p/ca9dce6b5c37) 以及 [guide-to-writing-hive-udfs](https://blog.matthewrathbone.com/2013/08/10/guide-to-writing-hive-udfs.html)\n\n&nbsp;\n","tags":["Hive"]},{"title":"Hudi学习笔记——同步hive metastore","url":"/Hudi学习笔记——同步hive metastore.html","content":"## 1.使用Flink SQL\n\n如果使用的是flink sql的话，如果想要同步表到hive metastore的话，只需要在flink sql的建表语句中添加<!--more-->\n&nbsp;hive_sync 相关的一些配置即可，如下\n\n```\n'hive_sync.enable' = 'true',\n'hive_sync.mode' = 'hms',\n'hive_sync.metastore.uris' = 'thrift://xxx:9083',\n'hive_sync.table'='hudi_xxxx_table',\n'hive_sync.db'='default',\n\n```\n\n如果遇到不能正常建表，或者只能建出ro表的情况，报错如下\n\n```\norg.apache.hudi.exception.HoodieException: Got runtime exception when hive syncing hudi_xxxx_table\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:145) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat org.apache.hudi.sink.StreamWriteOperatorCoordinator.doSyncHive(StreamWriteOperatorCoordinator.java:335) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat org.apache.hudi.sink.utils.NonThrownExecutor.lambda$wrapAction$0(NonThrownExecutor.java:130) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_372]\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_372]\n\tat java.lang.Thread.run(Thread.java:750) [?:1.8.0_372]\nCaused by: org.apache.hudi.hive.HoodieHiveSyncException: Failed to sync partitions for table hudi_xxxx_table_ro\n\tat org.apache.hudi.hive.HiveSyncTool.syncPartitions(HiveSyncTool.java:341) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:232) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:158) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:142) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\t... 5 more\nCaused by: org.apache.hudi.hive.HoodieHiveSyncException: Failed to get all partitions for table default.hudi_xxxx_table_ro\n\tat org.apache.hudi.hive.HoodieHiveSyncClient.getAllPartitions(HoodieHiveSyncClient.java:180) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat org.apache.hudi.hive.HiveSyncTool.syncPartitions(HiveSyncTool.java:317) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:232) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:158) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:142) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\t... 5 more\nCaused by: org.apache.hadoop.hive.metastore.api.NoSuchObjectException: @hive#default.hudi_xxxx_table_ro table not found\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.read(ThriftHiveMetastore.java) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.read(ThriftHiveMetastore.java) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result.read(ThriftHiveMetastore.java) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_partitions(ThriftHiveMetastore.java:2958) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_partitions(ThriftHiveMetastore.java:2943) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listPartitions(HiveMetaStoreClient.java:1368) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listPartitions(HiveMetaStoreClient.java:1362) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat sun.reflect.GeneratedMethodAccessor125.invoke(Unknown Source) ~[?:?]\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_372]\n\tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_372]\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat com.sun.proxy.$Proxy92.listPartitions(Unknown Source) ~[?:?]\n\tat sun.reflect.GeneratedMethodAccessor125.invoke(Unknown Source) ~[?:?]\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_372]\n\tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_372]\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2773) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat com.sun.proxy.$Proxy92.listPartitions(Unknown Source) ~[?:?]\n\tat org.apache.hudi.hive.HoodieHiveSyncClient.getAllPartitions(HoodieHiveSyncClient.java:175) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat org.apache.hudi.hive.HiveSyncTool.syncPartitions(HiveSyncTool.java:317) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:232) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:158) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:142) ~[blob_p-8126a52f21d07b448344e4277f6fb0837c921987-71810e5577eef736251ddb420010bd50:?]\n\t... 5 more\n\n```\n\n原因是没有使用正确的hudi-flink-bundle jar包的原因，可用的jar需要自行编译打包hudi项目来得到，参考：[Flink SQL操作Hudi并同步Hive使用总结](https://dongkelun.com/2022/10/31/hudiFlinkSql/)\n\n自行编译hudi-flink-bundle jar包的步骤：\n\n1.git clone hudi项目，并且切到使用的hudi版本的分支上，比如0.13.0\n\n```\ngit clone git@github.com:apache/hudi.git\ngit checkout release-0.13.0\n\n```\n\n2.编译hudi-flink-hundle jar包，这里使用的hive metastore是hive2，flink版本是1.16.0\n\n```\nmvn clean package -DskipTests -Drat.skip=true -Pflink-bundle-shade-hive2 -Dflink1.16 -Dscala-2.12 -Dspark3\n\n```\n\n3.拷贝编译好的jar包到集群的/usr/lib/flink/lib目录下\n\n<img src=\"/images/517519-20230712150301061-1502548028.png\" width=\"500\" height=\"419\" loading=\"lazy\" />\n\n参考：[https://hudi.apache.org/cn/docs/syncing_metastore/](https://hudi.apache.org/cn/docs/syncing_metastore/)\n\n&nbsp;\n","tags":["Hudi"]},{"title":"Java——NIO","url":"/Java——NIO.html","content":"<img src=\"/images/517519-20160326215416808-786433959.png\" alt=\"\" />\n\n## **1.缓冲区Buffer**\n\n**<img src=\"/images/517519-20160326215624433-605360240.png\" alt=\"\" />**\n\n<!--more-->\n&nbsp;\n\n<img src=\"/images/517519-20160326215652120-387254333.png\" alt=\"\" />\n\n```\nimport java.nio.IntBuffer;\n\n//主类\n//Function        : \tIntBuffer_demo\npublic class IntBuffer_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tIntBuffer buf = IntBuffer.allocate(10);\t\t\t//开辟10个大小的缓冲区\n\t\tSystem.out.print(\"1.写入数据之前的position、limit和capacity\");\n\t\tSystem.out.println(\"position=\"+buf.position()+\"、limit=\"+buf.limit()+\"、capacity=\"+buf.capacity());\n\t\tint temp[] = {3,5,7};\t\t\t\t\t//定义整型数组\n\t\tbuf.put(3);\t\t\t\t\t\t\t\t\t//向缓冲区写入数据\n\t\tbuf.put(temp);\t\t\t\t\t\t\t//向缓冲区中写入一组数据\n\t\tSystem.out.print(\"2.写入数据之后的position、limit和capacity\");\n\t\tSystem.out.println(\"position=\"+buf.position()+\"、limit=\"+buf.limit()+\"、capacity=\"+buf.capacity());\n\t\tbuf.flip();\t\t\t\t\t\t//重设缓冲区，改变指针\n\t\tSystem.out.print(\"3.准备输出数据时的position、limit和capacity\");\n\t\tSystem.out.println(\"position=\"+buf.position()+\"、limit=\"+buf.limit()+\"、capacity=\"+buf.capacity());\n\t\twhile(buf.hasRemaining()){\n\t\t\tint x = buf.get();\n\t\t\tSystem.out.print(x+\"、\");\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160326221712761-1607095157.png\" alt=\"\" />\n\n&nbsp;<img src=\"/images/517519-20160326221850979-1027403906.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160326221917354-1936117076.png\" alt=\"\" />\n\n**创建子缓冲区**\n\n```\nimport java.nio.IntBuffer;\n\n//主类\n//Function        : \tIntBuffer_demo\npublic class IntBuffer_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tIntBuffer buf = IntBuffer.allocate(10);\t\t\t//开辟10个大小的缓冲区\n\t\tIntBuffer sub = null;\t\t\t\t\t\t\t\t\t\t\t//定义缓冲区对象\n\t\tfor(int i=0;i<10;i++){\n\t\t\tbuf.put(2*i+1);\n\t\t}\n\t\tbuf.position(2);\n\t\tbuf.limit(6);\n\t\tsub = buf.slice();\t\t\t\t//开辟子缓冲区\n\t\tfor(int i=0;i<sub.capacity();i++){\n\t\t\tint temp = sub.get(i);\n\t\t\tsub.put(temp-1);\n\t\t}\n\t\tbuf.flip();\t\t\t//重设缓冲区\n\t\tbuf.limit(buf.capacity());\t//设置limit\n\t\tSystem.out.println(\"主缓冲区中的内容：\");\n\t\twhile(buf.hasRemaining()){\n\t\t\tint x = buf.get();\t\t\t\t\t//取出当前内容\n\t\t\tSystem.out.print(x+\"、\");\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160326224214620-1309007878.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160326224237933-463025799.png\" alt=\"\" />\n\n&nbsp;\n\n```\nimport java.nio.IntBuffer;\n\n//主类\n//Function        : \tIntBuffer_demo\npublic class IntBuffer_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tIntBuffer buf = IntBuffer.allocate(10);\t\t\t//开辟10个大小的缓冲区\n\t\tIntBuffer read = null;\t\t\t\t\t\t\t\t\t\t\t//定义缓冲区对象\n\t\tfor(int i=0;i<10;i++){\n\t\t\tbuf.put(2*i+1);\n\t\t}\n\t\tread = buf.asReadOnlyBuffer();\t\t\t//创建只读缓冲区\n\t\tbuf.flip();\t\t\t//重设缓冲区\n\t\tSystem.out.println(\"主缓冲区中的内容：\");\n\t\twhile(buf.hasRemaining()){\n\t\t\tint x = buf.get();\t\t\t\t\t//取出当前内容\n\t\t\tSystem.out.print(x+\"、\");\n\t\t}\n\t\tSystem.out.println();\n\t\tread.put(30);\t\t\t//错误，不可写\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160326224642511-22857013.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160326224727542-1438121961.png\" alt=\"\" />\n\n## **2.通道Channel**\n\n<img src=\"/images/517519-20160326224905276-1782427384.png\" alt=\"\" />\n\n```\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.FileChannel;\n\n//主类\n//Function        : \tFileChannel_demo\npublic class FileChannel_demo {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tString info[] = {\"123\",\"456\",\"789\"};\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/out.txt\");//路径\n\t\tFileOutputStream output = null;\n\t\toutput = new FileOutputStream(f);\t\n\t\tFileChannel fout = null;\t\t\t\t\t\t\t//声明输出的通道\n\t\tfout = output.getChannel();\t\t\t\t\t\t//得到输出的文件通道\n\t\tByteBuffer buf = ByteBuffer.allocate(1024);\t\t//开辟缓冲\n\t\tfor(int i=0;i<info.length;i++){\n\t\t\tbuf.put(info[i].getBytes());\n\t\t}\n\t\tbuf.flip();\t\t\t\t//重设缓冲区，准备输出\n\t\tfout.write(buf);\t//输出\n\t\tfout.close();\n\t\toutput.close();\n\t\t\n\t}\n\n}\n\n```\n\n**&nbsp;读写文件**\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.FileChannel;\n\n//主类\n//Function        : \tFileChannel_demo\npublic class FileChannel_demo {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tFile f1 = new File(\"/home/common/software/coding/HelloWord/HelloWord/out.txt\");//路径\n\t\tFile f2 = new File(\"/home/common/software/coding/HelloWord/HelloWord/outnote.txt\");//路径\n\t\tFileInputStream input = null;\n\t\tFileOutputStream output = null;\n\t\tinput = new FileInputStream(f1);\n\t\toutput = new FileOutputStream(f2);\t\n\t\tFileChannel fin = null;\t\t\t\t\t\t\t//声明输入的通道\n\t\tFileChannel fout = null;\t\t\t\t\t\t//声明输出的通道\n\t\tfin = input.getChannel();\t\t\t\t\t\t//得到输入的文件通道\n\t\tfout = output.getChannel();\t\t\t\t\t\t//得到输出的文件通道\n\t\tByteBuffer buf = ByteBuffer.allocate(1024);\t\t//开辟缓冲\n\t\tint temp = 0;\t\t\t\t\t\t\t\t\t\t\t//声明变量接收内容\n\t\twhile((temp=fin.read(buf)) != -1){\n\t\t\tbuf.flip();\n\t\t\tfout.write(buf);\n\t\t\tbuf.clear();\n\t\t}\n\t\tfin.close();\n\t\tfout.close();\n\t\tinput.close();\n\t\toutput.close();\n\t\t\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160326233803542-992195594.png\" alt=\"\" />\n\n&nbsp;\n\n<img src=\"/images/517519-20160326233852964-1007188044.png\" alt=\"\" />\n\n## 3.Selector\n\n<img src=\"/images/517519-20160326234154245-250110520.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160326234330823-1777848443.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160326234349636-771283869.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160326234409401-1351061590.png\" alt=\"\" />\n","tags":["Java"]},{"title":"Spark学习笔记——读写Avro","url":"/Spark学习笔记——读写Avro.html","content":"**1.DataFrame API读取avro文件**\n\n```\nhttps://sparkbyexamples.com/spark/read-write-avro-file-spark-dataframe/\n\n```\n\npom引入，spark2.4.0之后可以使用apache的spark-avro包，之前需要使用databricks的spark-avro包\n\n```\n<!--avro-->\n<dependency>\n    <groupId>org.apache.avro</groupId>\n    <artifactId>avro</artifactId>\n    <version>1.11.0</version>\n    <exclusions>\n        <exclusion>\n            <artifactId>jackson-databind</artifactId>\n            <groupId>com.fasterxml.jackson.core</groupId>\n        </exclusion>\n    </exclusions>\n</dependency>\n<dependency>\n    <groupId>org.apache.parquet</groupId>\n    <artifactId>parquet-avro</artifactId>\n    <version>1.12.1</version>\n</dependency>\n<!--spark-->\n<dependency>\n    <groupId>org.apache.spark</groupId>\n    <artifactId>spark-core_2.11</artifactId>\n    <version>2.4.0.cloudera2</version>\n    <exclusions>\n        <exclusion>\n            <artifactId>avro</artifactId>\n            <groupId>org.apache.avro</groupId>\n        </exclusion>\n    </exclusions>\n</dependency>\n<dependency>\n    <groupId>org.apache.spark</groupId>\n    <artifactId>spark-sql_2.11</artifactId>\n    <version>2.4.0.cloudera2</version>\n    <exclusions>\n        <exclusion>\n            <artifactId>avro</artifactId>\n            <groupId>org.apache.avro</groupId>\n        </exclusion>\n        <exclusion>\n            <artifactId>parquet-jackson</artifactId>\n            <groupId>com.twitter</groupId>\n        </exclusion>\n    </exclusions>\n</dependency>\n<dependency>\n    <groupId>org.apache.spark</groupId>\n    <artifactId>spark-avro_2.11</artifactId>\n    <version>2.4.0.cloudera2</version>\n</dependency>\n\n```\n\n读取avro文件，得到DataFrame\n\n```\nimport java.io.File\n\nimport org.apache.avro.Schema\nimport org.apache.spark.sql.SparkSession\n\nobject SparkAvroDemo {\n\n  def main(args: Array[String]) {\n\n    val sparkSession = SparkSession.builder()\n      .master(\"local\")\n      .appName(\"spark session example\")\n      .getOrCreate()\n\n    val schemaAvro = new Schema.Parser().parse(new File(\"src/main/avro/kst.avsc\"))\n\n    val df = sparkSession.read\n      .format(\"avro\")\n      .option(\"avroSchema\", schemaAvro.toString)\n      .load(\"file:///Users/lintong/coding/java/interview/bigdata/data/000000_0\")\n\n    df.show()\n\n  }\n\n}\n\n```\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20211207221327312-1395173222.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n**DataFrame API写入avro文件**&nbsp;\n\n```\ndf.write.format(\"avro\").save(\"person.avro\")\n\n```\n\n&nbsp;\n\n**2.spark使用Hadoop API读取avro文件**\n\n```\nval sparkSession = SparkSession.builder()\n  .master(\"local\")\n  .appName(\"spark session example\")\n  .getOrCreate()\n\nval sc = sparkSession.sparkContext\nval conf = sc.hadoopConfiguration\nval path = \"file:///Users/lintong/coding/java/interview/bigdata/data/000000_0\"\nval rdd = sc.newAPIHadoopFile(\n  path,\n  classOf[AvroKeyInputFormat[test_serializer]],\n  classOf[AvroKey[test_serializer]],\n  classOf[NullWritable],\n  conf\n).map(_._1.datum())\nrdd.foreach(println(_))\n\n```\n\n输出\n\n```\n{\"string1\": \"test\", \"int1\": 2, \"tinyint1\": 1, \"smallint1\": 2, \"bigint1\": 1000, \"boolean1\": true, \"float1\": 1.111, \"double1\": 2.22222, \"list1\": [\"test1\", \"test2\", \"test3\"], \"map1\": {\"test123\": 2222, \"test321\": 4444}, \"struct1\": {\"sInt\": 123, \"sBoolean\": true, \"sString\": \"London\"}, \"union1\": 0.2, \"enum1\": \"BLUE\", \"nullableint\": 12345, \"bytes1\": \"00008DAC\", \"fixed1\": [49, 49, 49]}\n\n```\n\n参考：\n\n```\nhttps://github.com/subprotocol/spark-avro-example/blob/master/src/main/scala/com/subprotocol/AvroUtil.scala\n\n```\n\n&nbsp;\n\n**spark使用Hadoop API写入avro文件**\n\n```\nval writeJob = new Job()\nAvroJob.setOutputKeySchema(writeJob, test_serializer.SCHEMA$)\nwriteJob.setOutputFormatClass(classOf[AvroKeyOutputFormat[test_serializer]])\n\nrdd\n  .map { s => (new AvroKey(s), NullWritable.get) }\n  .saveAsNewAPIHadoopFile(\n    \"file:///Users/lintong/coding/java/interview/bigdata/data/avro_container\",\n    classOf[AvroKey[test_serializer]],\n    classOf[NullWritable],\n    classOf[AvroKeyOutputFormat[test_serializer]],\n    writeJob.getConfiguration\n  )\n\n```\n\n参考：\n\n```\nhttps://mail-archives.apache.org/mod_mbox/spark-user/201411.mbox/%3CCAKz4c0S_cuo90q2JXudvx9WC4FWU033kX3-FjUJYTXxhr7PXOw@mail.gmail.com%3E\n\n```\n\n&nbsp;\n\n**3.spark使用Hadoop API读取avro parquet文件<br />**\n\n```\nimport org.apache.avro.specific.SpecificRecord\nimport org.apache.hadoop.mapred.JobConf\nimport org.apache.parquet.avro.AvroParquetInputFormat\nimport org.apache.spark.SparkContext\nimport org.apache.spark.rdd.RDD\n\nimport scala.reflect.{ClassTag, classTag}\n\nobject HDFSIO {\n\n  def readAvroParquetFile[T <: SpecificRecord : ClassTag](\n                                                           sc: SparkContext,\n                                                           path: String,\n                                                           avroClass: Class[T]): RDD[T] = {\n    val jobConf = new JobConf(sc.hadoopConfiguration)\n    sc.newAPIHadoopFile(path, classOf[AvroParquetInputFormat[T]], classOf[Void], avroClass, jobConf)\n      .map {\n        case (_, obj) => obj\n      }\n  }\n\n}\n\n```\n\n&nbsp;\n\n然后调用HDFSIO.readAvroParquetFile\n\n```\nval rdd = HDFSIO.readAvroParquetFile(sc, \"file:///Users/lintong/coding/java/interview/bigdata/data/avro_parquet\", classOf[test_serializer])\nrdd.foreach(line => println(line))\n\n```\n\n输出\n\n```\n{\"string1\": \"test\", \"int1\": 2, \"tinyint1\": 1, \"smallint1\": 2, \"bigint1\": 1000, \"boolean1\": true, \"float1\": 1.111, \"double1\": 2.22222, \"list1\": [\"test1\", \"test2\", \"test3\"], \"map1\": {\"test123\": 2222, \"test321\": 4444}, \"struct1\": {\"sInt\": 123, \"sBoolean\": true, \"sString\": \"London\"}, \"union1\": 0.2, \"enum1\": \"BLUE\", \"nullableint\": 12345, \"bytes1\": \"00008DAC\", \"fixed1\": [49, 49, 49]}\n{\"string1\": \"test\", \"int1\": 2, \"tinyint1\": 1, \"smallint1\": 2, \"bigint1\": 1000, \"boolean1\": true, \"float1\": 1.111, \"double1\": 2.22222, \"list1\": [\"test1\", \"test2\", \"test3\"], \"map1\": {\"test123\": 2222, \"test321\": 4444}, \"struct1\": {\"sInt\": 123, \"sBoolean\": true, \"sString\": \"London\"}, \"union1\": 0.2, \"enum1\": \"BLUE\", \"nullableint\": 12345, \"bytes1\": \"00008DAC\", \"fixed1\": [49, 49, 49]}\n\n```\n\n&nbsp;\n\n**spark使用Hadoop API写入avro parquet文件**\n\n&nbsp;\n\n```\nAvroWriteSupport.setSchema(sc.hadoopConfiguration, test_serializer.SCHEMA$)\n\nrdd\n  .map((null, _)).saveAsNewAPIHadoopFile(\n  \"file:///Users/lintong/coding/java/interview/bigdata/data/avro_parquet\",\n  classOf[Void],\n  classOf[test_serializer],\n  classOf[AvroParquetOutputFormat[test_serializer]],\n  writeJob.getConfiguration\n)\n\n```\n\n&nbsp;\n","tags":["avro","Spark"]},{"title":"scala报Exception in thread \"main\" java.lang.NoSuchMethodError：scala.Product.$init$(Lscala/Product;)V","url":"/scala报Exception in thread \"main\" java.lang.NoSuchMethodError：scala.Product.$init$(Lscala／Product;)V.html","content":"IDEA中运行spark报如下错误\n\n```\nException in thread \"main\" java.lang.NoSuchMethodError: scala.Product.$init$(Lscala/Product;)V\n\tat org.apache.spark.SparkConf$DeprecatedConfig.<init>(SparkConf.scala:799)\n\tat org.apache.spark.SparkConf$.<init>(SparkConf.scala:596)\n\tat org.apache.spark.SparkConf$.<clinit>(SparkConf.scala)\n\tat org.apache.spark.SparkConf.set(SparkConf.scala:94)\n\tat org.apache.spark.SparkConf.set(SparkConf.scala:83)\n\tat org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$1(SparkSession.scala:916)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:130)\n\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:916)\n\n```\n\n是由于本应该使用scala 2.12，但是在IDEA中配置的却是Scala 2.11导致的\n\n<img src=\"/images/517519-20220623173814330-1687225066.png\" width=\"800\" height=\"498\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n将其修改成2.12就解决了\n\n<img src=\"/images/517519-20220623173900514-2033703258.png\" width=\"800\" height=\"276\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["Scala"]},{"title":"将parquet schema转换成avro schema","url":"/将parquet schema转换成avro schema.html","content":"1.引入依赖\n\n```\n        <!--parquet-->\n        <dependency>\n            <groupId>org.apache.parquet</groupId>\n            <artifactId>parquet-avro</artifactId>\n            <version>1.10.0</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.parquet</groupId>\n            <artifactId>parquet-hadoop</artifactId>\n            <version>1.10.0</version>\n        </dependency>\n        <!--hadoop-->\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-client</artifactId>\n            <version>2.7.3</version>\n        </dependency>\n\n```\n\n2.从parquet文件的footer读取parquet schema\n\n```\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.parquet.hadoop.ParquetFileReader;\nimport org.apache.parquet.hadoop.util.HadoopInputFile;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.parquet.schema.MessageType;\n\nConfiguration config = new Configuration();\n        Path parquetPath = new Path(\"file:///Users/lintong/Downloads/xxxx.snappy.parquet\");\n        ParquetFileReader reader = ParquetFileReader.open(HadoopInputFile.fromPath(parquetPath, config));\n        MessageType parquetSchema = reader.getFooter().getFileMetaData().getSchema();\n        System.out.println(parquetSchema);\n\n```\n\n输出\n\n```\nmessage TestSerializer {\n  optional binary string1 (UTF8);\n  optional int32 int1;\n  optional int32 tinyint1;\n  optional int32 smallint1;\n  optional int64 bigint1;\n  optional boolean boolean1;\n  optional double float1;\n  optional double double1;\n  optional group list1 (LIST) {\n    repeated binary array (UTF8);\n  }\n  optional group map1 (LIST) {\n    repeated group array {\n      optional binary key (UTF8);\n      optional int32 value;\n    }\n  }\n  optional group struct1 {\n    optional int32 sInt;\n    optional boolean sBoolean;\n    optional binary sString (UTF8);\n  }\n  optional binary enum1 (UTF8);\n  optional int32 nullableint;\n}\n\n```\n\n3.将parquet schema转换成avro schema\n\n```\nimport org.apache.parquet.avro.AvroSchemaConverter;\nimport org.apache.avro.Schema;\n\nSchema avroSchema = new AvroSchemaConverter(config).convert(parquetSchema);\nSystem.out.println(avroSchema);\n\n```\n\n输出\n\n```\n{\n   \"type\":\"record\",\n   \"name\":\"TestSerializer\",\n   \"fields\":[\n      {\n         \"name\":\"string1\",\n         \"type\":[\n            \"null\",\n            \"string\"\n         ],\n         \"default\":null\n      },\n      {\n         \"name\":\"int1\",\n         \"type\":[\n            \"null\",\n            \"int\"\n         ],\n         \"default\":null\n      },\n      {\n         \"name\":\"tinyint1\",\n         \"type\":[\n            \"null\",\n            \"int\"\n         ],\n         \"default\":null\n      },\n      {\n         \"name\":\"smallint1\",\n         \"type\":[\n            \"null\",\n            \"int\"\n         ],\n         \"default\":null\n      },\n      {\n         \"name\":\"bigint1\",\n         \"type\":[\n            \"null\",\n            \"long\"\n         ],\n         \"default\":null\n      },\n      {\n         \"name\":\"boolean1\",\n         \"type\":[\n            \"null\",\n            \"boolean\"\n         ],\n         \"default\":null\n      },\n      {\n         \"name\":\"float1\",\n         \"type\":[\n            \"null\",\n            \"double\"\n         ],\n         \"default\":null\n      },\n      {\n         \"name\":\"double1\",\n         \"type\":[\n            \"null\",\n            \"double\"\n         ],\n         \"default\":null\n      },\n      {\n         \"name\":\"list1\",\n         \"type\":[\n            \"null\",\n            {\n               \"type\":\"array\",\n               \"items\":\"string\"\n            }\n         ],\n         \"default\":null\n      },\n      {\n         \"name\":\"map1\",\n         \"type\":[\n            \"null\",\n            {\n               \"type\":\"array\",\n               \"items\":{\n                  \"type\":\"record\",\n                  \"name\":\"array\",\n                  \"fields\":[\n                     {\n                        \"name\":\"key\",\n                        \"type\":[\n                           \"null\",\n                           \"string\"\n                        ],\n                        \"default\":null\n                     },\n                     {\n                        \"name\":\"value\",\n                        \"type\":[\n                           \"null\",\n                           \"int\"\n                        ],\n                        \"default\":null\n                     }\n                  ]\n               }\n            }\n         ],\n         \"default\":null\n      },\n      {\n         \"name\":\"struct1\",\n         \"type\":[\n            \"null\",\n            {\n               \"type\":\"record\",\n               \"name\":\"struct1\",\n               \"fields\":[\n                  {\n                     \"name\":\"sInt\",\n                     \"type\":[\n                        \"null\",\n                        \"int\"\n                     ],\n                     \"default\":null\n                  },\n                  {\n                     \"name\":\"sBoolean\",\n                     \"type\":[\n                        \"null\",\n                        \"boolean\"\n                     ],\n                     \"default\":null\n                  },\n                  {\n                     \"name\":\"sString\",\n                     \"type\":[\n                        \"null\",\n                        \"string\"\n                     ],\n                     \"default\":null\n                  }\n               ]\n            }\n         ],\n         \"default\":null\n      },\n      {\n         \"name\":\"enum1\",\n         \"type\":[\n            \"null\",\n            \"string\"\n         ],\n         \"default\":null\n      },\n      {\n         \"name\":\"nullableint\",\n         \"type\":[\n            \"null\",\n            \"int\"\n         ],\n         \"default\":null\n      }\n   ]\n}\n\n```\n\n参考：[https://stackoverflow.com/questions/54159454/how-to-convert-parquet-schema-to-avro-in-java-scala](https://stackoverflow.com/questions/54159454/how-to-convert-parquet-schema-to-avro-in-java-scala)\n","tags":["avro","parquet"]},{"title":"CDH5.16安装lzo","url":"/CDH5.16安装lzo.html","content":"1.在CDH管理页面进入parcels，下载GPLEXTRAS\n\n```\nlintong@master:/opt/cloudera/parcel-repo$ ls | grep GPLEXTRAS\nGPLEXTRAS-5.16.2-1.cdh5.16.2.p0.8-xenial.parcel\nGPLEXTRAS-5.16.2-1.cdh5.16.2.p0.8-xenial.parcel.sha1\n\n```\n\n将sha1改成sha\n\n```\nsudo mv GPLEXTRAS-5.16.2-1.cdh5.16.2.p0.8-xenial.parcel.sha1 GPLEXTRAS-5.16.2-1.cdh5.16.2.p0.8-xenial.parcel.sha\n\n```\n\n如果parcels的哈希文件不存在，可以这样生成\n\n```\nsha1sum ./SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012-xenial.parcel | cut -d ' ' -f 1 > SPARK2-2.4..cloudera2-1.cdh5.13.3.p0.1041012-xenial.parcel.sha1\n\n```\n\n2.在界面上分配并激活\n\n<img src=\"/images/517519-20211112223614078-2124251515.png\" width=\"800\" height=\"188\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n&nbsp;\n\n3.在HDFS配置的 io.compression.codecs 参数添加上\n\n```\ncom.hadoop.compression.lzo.LzoCodec\ncom.hadoop.compression.lzo.LzopCodec\n\n```\n\n&nbsp;<img src=\"/images/517519-20211112223912744-2008568322.png\" width=\"800\" height=\"583\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;\n\n参考文档\n\n```\nhttps://www.cloudera.com/documentation/enterprise/latest/topics/cm_mc_gpl_extras.html\n\n```\n\n4.在节点上安装lzo\n\n```\nsudo apt-get install liblzo2-2\n\n```\n\n参考文档\n\n```\nhttps://docs.cloudera.com/documentation/enterprise/latest/topics/cm_ig_install_gpl_extras.html#xd_583c10bfdbd326ba-3ca24a24-13d80143249--7ec6\n\n```\n\n4.YARN配置，在mapreduce.application.classpath中添加\n\n```\n/opt/cloudera/parcels/GPLEXTRAS/lib/hadoop/lib/*\n\n```\n\n&nbsp;<img src=\"/images/517519-20211112224951834-1353261980.png\" width=\"600\" height=\"274\" loading=\"lazy\" />\n\n&nbsp;\n\n5.重启，验证\n\n```\ncreate table test_table(id int,name string)\n\nset mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.LzoCodec;\nset hive.exec.compress.output=true;\nset mapreduce.output.fileoutputformat.compress=true;\nset mapreduce.output.fileoutputformat.compress.type=BLOCK; \n\ninsert overwrite table test_table select * from test_table;\n\n```\n\n&nbsp;<img src=\"/images/517519-20211113113749183-1072929800.png\" width=\"300\" height=\"227\" loading=\"lazy\" />\n\n参考：[0003-如何在CDH中使用LZO压缩](https://blog.51cto.com/u_14049791/2316621)\n\n&nbsp;\n","tags":["CDH"]},{"title":"Flink学习笔记——Flink State","url":"/Flink学习笔记——Flink State.html","content":"Flink有两种基本类型的状态：**托管状态（Managed State）**和**原生状态（Raw State）**。两者的区别：Managed State是由Flink管理的，Flink帮忙存储、恢复和优化，Raw State是开发者自己管理的，需要自己序列化。\n|<!--more-->\n&nbsp;|Managed&nbsp;State|Raw&nbsp;State\n\nRaw&nbsp;State\n|状态管理方式|&nbsp;Flink&nbsp;Runtime托管,&nbsp;自动存储,&nbsp;自动恢复,&nbsp;自动伸缩|&nbsp;用户自己管理\n|状态数据结构|&nbsp;Flink提供多种常用数据结构,&nbsp;例如:ListState, MapState等|&nbsp;字节数组:&nbsp;byte[]\n|使用场景|&nbsp;绝大数Flink算子|&nbsp;所有算子\n\nListState使用案例：[使用flink实现一个topN的程序](https://www.cnblogs.com/huangqingshi/p/12041032.html)\n\nManaged State又有两种类型：Keyed State和Operator State。\n\n<img src=\"/images/517519-20230804205606458-1771588050.png\" width=\"600\" height=\"391\" />\n\n在Flink 中，Keyed State 是**绑定到数据流中的特定键（Key）的**。 这意味着Flink 维护了数据流中每一个key 的状态。 当处理一个新的元素时，Flink 会将状态的键设置为该元素的键，然后调用对应的处理函数。 Keyed State 的一个典型应用场景是计算每个键的滚动聚合。参考：[Flink系列 13. 介绍Flink中的Operator State 和 Keyed State](https://www.hnbian.cn/posts/1dadc314.html)\n\n**Operator State**可以被所有的operators使用。它通常适用于source，例如FlinkKafkaConsumer。每个operator状态都绑定到一个并行operator实例。Kafka consumer的每个并行实例都维护着一个topic分区和offset的映射，作为它的operator状态。官方文档：[https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/datastream/fault-tolerance/state/#operator-state](https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/datastream/fault-tolerance/state/#operator-state)\n|&nbsp;|Operator&nbsp;State|Keyed&nbsp;State\n\nKeyed&nbsp;State\n|适用算子类型|&nbsp;可用于所有算子: 常用于source, 例如 FlinkKafkaConsumer|只适用于KeyedStream上的算子\n\n只适用于KeyedStream上的算子\n|状态分配|一个算子的子任务对应一个状态|一个Key对应一个State: 一个算子会处理多个Key, 则访问相应的多个State\n\n一个算子的子任务对应一个状态\n|创建和访问方式|实现CheckpointedFunction或ListCheckpointed(已经过时)接口|重写RichFunction, 通过里面的RuntimeContext访问\n\n实现CheckpointedFunction或ListCheckpointed(已经过时)接口\n|横向扩展|并发改变时有多重重写分配方式可选:&nbsp;均匀分配和合并后每个得到全量|并发改变,&nbsp;State随着Key在实例间迁移\n|支持的数据结构|ListState和BroadCastState&nbsp;|ValueState, ListState,MapState ReduceState, AggregatingState\n\n参考：[Apache Flink State Types](https://medium.com/big-data-processing/apache-flink-state-types-b5a76f36926d) 和 [【Flink】Flink 状态管理](https://www.jianshu.com/p/5d71455cc578)\n\n&nbsp;\n\n&nbsp;\n\n参考：[Apache Flink&mdash;&mdash;状态编程](https://www.jianshu.com/p/00d49e7ab972)\n","tags":["Flink"]},{"title":"Java——Java网络","url":"/Java——Java网络.html","content":"## 1.IP和InetAddress\n\n<img src=\"/images/517519-20160325211944683-2070008841.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160907100634269-1145144898.png\" alt=\"\" width=\"462\" height=\"278\" />\n\n<img src=\"/images/517519-20160907101809019-1824750390.png\" alt=\"\" width=\"585\" height=\"446\" />\n\n<!--more-->\n&nbsp;\n\n<img src=\"/images/517519-20160325212305198-1380934598.png\" alt=\"\" />\n\n&nbsp;\n\n```\nimport java.net.InetAddress;\n\npublic class InetAddress_demo {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tInetAddress locAdd = null;\t\t\t//声明InetAddress对象\n\t\tInetAddress remAdd = null;\t\t\t//声明InetAddress对象\n\t\t\n\t\tlocAdd = InetAddress.getLocalHost();\t\t//得到本地的InetAddress对象\n\t\tremAdd = InetAddress.getByName(\"www.baidu.com\");\n\t\t\n\t\tSystem.out.println(\"本机的IP地址：\"+locAdd.getHostAddress());\n\t\tSystem.out.println(\"远程的IP地址：\"+remAdd.getHostAddress());\n\t\tSystem.out.println(\"本机是否可达：\"+locAdd.isReachable(5000));\n\t}\n\n}\n```\n\n## 2.URLConnection,URLEncoder和URLDecoder\n\n<img src=\"/images/517519-20160325213059901-882332090.png\" alt=\"\" />\n\n### **1.使用URL读取内容**\n\n```\nimport java.awt.im.InputContext;\nimport java.io.InputStream;\nimport java.net.MalformedURLException;\nimport java.net.URL;\nimport java.util.Scanner;\n\npublic class URL_demo {\n\n\tpublic static void main(String[] args) throws Exception {\n\t\t// TODO 自动生成的方法存根\n\t\tURL url = new URL(\"http\",\"10.108.84.220\",8080,\"/Hello/Hello.html\");\t//指定操作的URL\n\t\tInputStream input = url.openStream();\t\t\t//打开输入流，读取URL内容\n\t\tScanner scan = new Scanner(input);\t\t\t\t//实例化读取分隔符\n\t\tscan.useDelimiter(\"\\n\");\t\t\t\t//设置读取分隔符\n\t\twhile(scan.hasNext()){\n\t\t\tSystem.out.println(scan.next());\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160325221451542-989653471.png\" alt=\"\" />\n\n### **2.取得URL的基本信息**\n\n```\nimport java.net.URL;\nimport java.net.URLConnection;\n\npublic class URL_demo {\n\n\tpublic static void main(String[] args) throws Exception {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tURL url = new URL(\"http://www.baidu.com\");\n\t\tURLConnection urlCon = url.openConnection();\t//建立连接\n\t\tSystem.out.println(\"内容大小：\"+urlCon.getContentLength()); //取得内容大小\n\t\tSystem.out.println(\"内容类型：\"+urlCon.getContentType());\t\t//取得内容类型\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160325222118495-2069154522.png\" alt=\"\" />\n\n```\nimport java.net.URLDecoder;\nimport java.net.URLEncoder;\n\npublic class URL_demo {\n\n\tpublic static void main(String[] args) throws Exception {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tString keyWord = \"张三\";\n\t\tString encod = URLEncoder.encode(keyWord,\"UTF-8\");\t//对内容进行编码\n\t\tSystem.out.println(\"编码之后的内容：\"+encod);\n\t\tString decod = URLDecoder.decode(keyWord,\"UTF-8\");\t//对内容进行编码\n\t\tSystem.out.println(\"解码之后的内容：\"+decod);\n\t}\n\n}\n\n```\n\n&nbsp;输出\n\n```\n编码之后的内容：%E5%BC%A0%E4%B8%89\n解码之后的内容：张三\n　　\n```\n\n## 3.TCP\n\n<img src=\"/images/517519-20160326161839308-1946713452.png\" alt=\"\" />\n\n&nbsp;\n\n<img src=\"/images/517519-20160326161912667-2007337776.png\" alt=\"\" />\n\n&nbsp;\n\n```\nimport java.io.PrintStream;\nimport java.net.ServerSocket;\nimport java.net.Socket;\n\npublic class HelloServer_demo {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tServerSocket server = null; \t\t\t//声明ServerSocket对象\n\t\tSocket client = null;\t\t\t\t\t\t\t//一个Socket对象表示一个客户端\n\t\tPrintStream out = null;\t\t\t\t\t//声明打印流对象，以向客户端输出\n\t\tserver = new ServerSocket(8888);\t//表示服务器在8888端口上等待客户端的访问\n\t\tSystem.out.println(\"服务器运行，等待客户端连接\");\n\t\tclient = server.accept();\t\t\t\t\t//程序阻塞，等待客户端连接\n\t\tString str = \"HelloWord\";\t\t\t\t\t//要向客户端输出信息\n\t\tout = new PrintStream(client.getOutputStream());\t//实例化打印流对象，输出信息\n\t\tout.println(str);\n\t\tout.close();\n\t\tclient.close();\n\t\tserver.close();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n```\nimport java.io.BufferedReader;\nimport java.io.InputStreamReader;\nimport java.net.Socket;\n\npublic class HelloClient_demo {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tSocket client = null;\t\t\t\t\t\t\t\t\t//声明Socket对象\n\t\tclient = new Socket(\"localhost\",8888);\t//指定连接的主机和端口\n\t\tBufferedReader buf = null;\t\t\t\t\t//声明BufferedReader对象，接受信息\n\t\t//指定客户端的输入流\n\t\tbuf = new BufferedReader(new InputStreamReader(client.getInputStream()));\n\t\tString str = buf.readLine();\n\t\tSystem.out.println(\"服务器输出的内容是\"+str);\n\t\tclient.close();\n\t\tbuf.close();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160326185705276-306692122.png\" alt=\"\" />\n\n```\nimport java.io.BufferedReader;\nimport java.io.InputStreamReader;\nimport java.io.PrintStream;\nimport java.net.ServerSocket;\nimport java.net.Socket;\n\npublic class EchoServer_demo {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tServerSocket server = null; \t\t\t//声明ServerSocket对象\n\t\tSocket client = null;\t\t\t\t\t\t\t//一个Socket对象表示一个客户端\n\t\tPrintStream out = null;\t\t\t\t\t//声明打印流对象，以向客户端输出\n\t\tBufferedReader buf = null;\t\t\t\t\t//声明BufferedReader对象，接受信息\n\t\tserver = new ServerSocket(8888);\t//表示服务器在8888端口上等待客户端的访问\n\t\tboolean f = true;\n\t\twhile(f){\n\t\t\tSystem.out.println(\"服务器运行，等待客户端连接：\");\n\t\t\tclient = server.accept();\t\t\t\t//接收客户端连接\n\t\t\t//得到客户端的输入信息\n\t\t\tbuf = new BufferedReader(new InputStreamReader(client.getInputStream()));\n\t\t\t//实例化客户端的输出流\n\t\t\tout = new PrintStream(client.getOutputStream());\n\t\t\tboolean flag = true;\n\t\t\twhile(flag){\n\t\t\t\tString str = buf.readLine();\t\t\t\t//在此处不断地接收信息\n\t\t\t\tif(str == null || \"\".equals(str)){\n\t\t\t\t\tflag = false;\n\t\t\t\t}else{\n\t\t\t\t\tif(\"bye\".equals(str)){\n\t\t\t\t\t\tflag = false;\n\t\t\t\t\t}else{\n\t\t\t\t\t\tout.print(\"ECHO\"+str);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tout.close();\n\t\t\tclient.close();\n\t\t}\n\t\tserver.close();\n\t}\n}\n\n```\n\n&nbsp;\n\n```\nimport java.io.BufferedReader;\nimport java.io.InputStreamReader;\nimport java.io.PrintStream;\nimport java.net.Socket;\n\npublic class EchoClient_demo {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tSocket client = null;\t\t\t\t\t\t\t\t\t//声明Socket对象\n\t\tclient = new Socket(\"localhost\",8888);\t//指定连接的主机和端口\n\t\tBufferedReader buf = null;\t\t\t\t\t//声明BufferedReader对象，接受信息\n\t\tPrintStream out = null;\t\t\t\t\t\t\t//输出流，向服务器端发送信息\n\t\tBufferedReader input = null;\t\t\t\t//声明BufferedReader对象\n\t\t//从键盘接收数据\n\t\tinput = new BufferedReader(new InputStreamReader(System.in));\n\t\t//向服务器端输出信息\n\t\tout = new PrintStream(client.getOutputStream());\n\t\t//接收服务器端输入信息\n\t\tbuf = new BufferedReader(new InputStreamReader(client.getInputStream()));\n\t\tboolean flag = true;\n\n\t\twhile(flag){\n\t\t\tSystem.out.print(\"输入信息：\");\n\t\t\tString str = input.readLine();\t\t\t\t//从键盘接收数据\n\t\t\tout.println(str);\n\n\t\t\tif(\"bye\".equals(str)){\n\t\t\t\tflag = false;\n\t\t\t}else{\n\t\t\t\tString echo = buf.readLine();\n\t\t\t\tSystem.out.println(echo);\n\t\t\t}\n\t\t}\n\t\tclient.close();\n\t\tbuf.close();\n\t}\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160326185810667-1138400024.png\" alt=\"\" />\n\n```\nimport java.io.BufferedReader;\nimport java.io.InputStreamReader;\nimport java.io.PrintStream;\nimport java.net.ServerSocket;\nimport java.net.Socket;\n\nclass EchoThreadServer implements Runnable{\n\tprivate Socket client = null;\n\t\n\tpublic EchoThreadServer(Socket client){\n\t\tthis.client = client;\n\t}\n\t\n\t@Override\n\tpublic void run() {\n\t\t// TODO 自动生成的方法存根\n\t\tPrintStream out = null;\t\t\t\t\t//声明打印流对象，以向客户端输出\n\t\tBufferedReader buf = null;\t\t\t\t\t//声明BufferedReader对象，接受信息\n\t\t\n\t\ttry{\n\t\t\t//得到客户端的输入信息\n\t\t\tbuf = new BufferedReader(new InputStreamReader(client.getInputStream()));\n\t\t\t//实例化客户端的输出流\n\t\t\tout = new PrintStream(client.getOutputStream());\n\t\t\tboolean flag = true;\n\t\t\n\t\t\twhile(flag){\n\t\t\t\tString str = buf.readLine();\t\t\t\t//在此处不断地接收信息\n\t\t\t\tif(str == null || \"\".equals(str)){\n\t\t\t\t\tflag = false;\n\t\t\t\t}else{\n\t\t\t\t\tif(\"bye\".equals(str)){\n\t\t\t\t\t\tflag = false;\n\t\t\t\t\t}else{\n\t\t\t\t\t\tout.print(\"ECHO\"+str);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t}\n\t\tout.close();\n\t\tclient.close();\n\t}catch(Exception e){\n\t\t}\n\t}\n}\n\npublic class EchoThreadServer_demo {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tServerSocket server = null;\n\t\tSocket client = null;\n\t\tserver = new ServerSocket(8888);\n\t\tboolean f = true;\n\t\twhile(f){\n\t\t\tSystem.out.println(\"服务器运行，等待客户端连接：\");\n\t\t\tclient = server.accept();\n\t\t\tnew Thread(new EchoThreadServer(client)).start();\n\t\t}\n\t\tserver.close();\n\t}\n\n}\n\n```\n\n## 4.UDP\n\n<img src=\"/images/517519-20160326203450542-261207160.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160326203543464-1456365531.png\" alt=\"\" />\n\n&nbsp;UDP server\n\n```\nimport java.net.DatagramPacket;\nimport java.net.DatagramSocket;\nimport java.net.InetAddress;\n\npublic class UDPServer_demo {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tDatagramSocket ds = null;\t\t\t\t\t//声明DatagramSocket对象\n\t\tDatagramPacket dp = null;\t\t\t\t\t//声明DatagramPacket对象\n\t\tds = new DatagramSocket(3000);\n\t\tString str = \"HelloWord\";\n\t\tdp = new DatagramPacket(str.getBytes(),str.length(),InetAddress.getByName(\"localhost\"),9000);\n\t\tSystem.out.println(\"发送信息\");\n\t\tds.send(dp);\n\t\tds.close();\n\t}\n\n}\n\n```\n\n&nbsp;UDP client\n\n```\nimport java.net.DatagramPacket;\nimport java.net.DatagramSocket;\n\npublic class UDPClient_demo {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tDatagramSocket ds = null;\t\t\t\t\t//声明DatagramSocket对象\n\t\tbyte[] buf = new byte[1024];\t\t\t//定义接收数据的字节数据\n\t\tDatagramPacket dp = null;\t\t\t\t\t//声明DatagramPacket对象\n\t\tds = new DatagramSocket(9000);\t\t//此客户端在9000端口监听\n\t\tdp = new DatagramPacket(buf,1024);\t\t\t//指定接收数据的长度为1024\n\t\tSystem.out.println(\"等待接收数据\");\n\t\tds.receive(dp); \t\t\t\t//接收数据\n\t\tString str = new String(dp.getData(),0,dp.getLength())+\" from\"\n\t\t+dp.getAddress().getHostAddress()+\" : \"+dp.getPort();\t//接收数据\n\t\tSystem.out.println(str);\t\t\t//输出数据\n\t\tds.close();\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["Java"]},{"title":"Mybatis学习笔记——缓存","url":"/Mybatis学习笔记——缓存.html","content":"Mybatis默认情况下，只开启一级缓存，一级缓存只是相对于同一个SqlSession而言。\n\n如果想要开启二级缓存，则需要在xml配置文件中添加\n\n```\n<cache/>\n\n```\n\n此外，还要求返回的POJO对象要实现Serializable接口\n\n<!--more-->\n&nbsp;\n","tags":["mybatis"]},{"title":"SpringBoot学习笔记——统一的异常处理","url":"/SpringBoot学习笔记——统一的异常处理.html","content":"可以使用 @RestControllerAdvice 拦截异常并进行统一处理\n\n1.首先定义统一的异常码<!--more-->\n&nbsp;ResultCode，\n\n其中code以HTTP code status为前缀，后缀为具体异常编号\n\nmessage为异常消息，前端可以直接拿来显示给用户\n\n```\nimport lombok.AllArgsConstructor;\nimport lombok.Getter;\n\n@Getter\n@AllArgsConstructor\npublic enum ResultCode {\n\n    // 200\n    SUCCESS(200, \"成功\"),\n\n    // 401\n    UN_AUTH(40101, \"该用户未认证\"),\n\n    // 403\n    ACCESS_DENIED(40301, \"该用户无权限\"),\n\n    // 404\n    API_NOT_FOUND(40401, \"请求的API不存在\"),\n    RESOURCE_NOT_FOUND(40402, \"请求的资源不存在\"),\n\n    // 500\n    SYS_UNKNOWN_ERROR(50001, \"未知系统错误，请稍后再试\"),\n\n    // 状态码\n    private Integer code;\n    // 提示信息\n    private String message;\n\n}\n\n```\n\n2. 定义全局的异常处理\n\n比如在其中对API接口不存在抛出的异常&nbsp;NoHandlerFoundException 进行了拦截，并返回统一的异常码\n\n```\nimport com.example.demo.common.ControllerResponseT;\nimport com.example.demo.common.ResultCode;\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.http.HttpStatus;\nimport org.springframework.web.bind.annotation.ExceptionHandler;\nimport org.springframework.web.bind.annotation.ResponseStatus;\nimport org.springframework.web.bind.annotation.RestControllerAdvice;\nimport org.springframework.web.servlet.NoHandlerFoundException;\n\nimport javax.servlet.http.HttpServletRequest;\n\n@Slf4j\n@RestControllerAdvice\npublic class GlobalExceptionHandler {\n\n    @ResponseStatus(HttpStatus.NOT_FOUND)\n    @ExceptionHandler(NoHandlerFoundException.class)\n    public ControllerResponseT apiNotFoundException(final Throwable e, final HttpServletRequest request) {\n        String message = ResultCode.API_NOT_FOUND.getMessage();\n        log.error(message + \" => {}\", e.getMessage());\n        return ControllerResponseT.ofFail(ResultCode.API_NOT_FOUND.getCode(), message, \" API [\" + request.getRequestURI() + \"] 不存在\");\n    }\n\n}\n```\n\n其中对于404的异常捕获不到的话，需要在application.properties中添加配置来解决\n\n```\n# mvc\nspring.mvc.throw-exception-if-no-handler-found=true\nspring.mvc.static-path-pattern=/resources/**\n\n```\n\n参考：[Spring Boot之全局异常处理：404异常为何捕获不到？](https://blog.csdn.net/w1014074794/article/details/106038996)\n\n请求不存在的接口返回结果\n\n<img src=\"/images/517519-20210624202313448-383607770.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n日志信息\n\n```\n2021-06-24 20:18:16.188  WARN 19926 --- [io-18080-exec-4] o.s.web.servlet.PageNotFound             : No mapping for GET /fail123\n2021-06-24 20:18:16.196 ERROR 19926 --- [io-18080-exec-4] c.e.demo.handler.GlobalExceptionHandler  : 请求的API不存在 => No handler found for GET /fail123\n\n```\n\n　　\n","tags":["SpringBoot"]},{"title":"SpringBoot学习笔记——缓存","url":"/SpringBoot学习笔记——缓存.html","content":"Springboot可以使用Ehcache或者redis作为缓存\n\n**1.Ehcache缓存**\n\n参考：[SpringBoot学习－（十八）SpringBoot整合EhCache](https://blog.csdn.net/qq_28988969/article/details/78210873)\n\n添加依赖，starter+ehcache\n\n```\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-cache</artifactId>\n</dependency>\n<dependency>\n    <groupId>net.sf.ehcache</groupId>\n    <artifactId>ehcache</artifactId>\n</dependency>\n```\n\n添加配置文件 ecache.xml\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<ehcache xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"http://ehcache.org/ehcache.xsd\">\n\n    <!--timeToIdleSeconds 当缓存闲置n秒后销毁 -->\n    <!--timeToLiveSeconds 当缓存存活n秒后销毁 -->\n    <!-- 缓存配置\n        name:缓存名称。\n        maxElementsInMemory：缓存最大个数。\n        eternal:对象是否永久有效，一但设置了，timeout将不起作用。\n        timeToIdleSeconds：设置对象在失效前的允许闲置时间（单位：秒）。仅当eternal=false对象不是永久有效时使用，可选属性，默认值是0，也就是可闲置时间无穷大。\n        timeToLiveSeconds：设置对象在失效前允许存活时间（单位：秒）。最大时间介于创建时间和失效时间之间。仅当eternal=false对象不是永久有效时使用，默认是0.，也就是对象存活时间无穷大。\n        overflowToDisk：当内存中对象数量达到maxElementsInMemory时，Ehcache将会对象写到磁盘中。 diskSpoolBufferSizeMB：这个参数设置DiskStore（磁盘缓存）的缓存区大小。默认是30MB。每个Cache都应该有自己的一个缓冲区。\n        maxElementsOnDisk：硬盘最大缓存个数。\n        diskPersistent：是否缓存虚拟机重启期数据 Whether the disk\n        store persists between restarts of the Virtual Machine. The default value\n        is false.\n        diskExpiryThreadIntervalSeconds：磁盘失效线程运行时间间隔，默认是120秒。  memoryStoreEvictionPolicy：当达到maxElementsInMemory限制时，Ehcache将会根据指定的策略去清理内存。默认策略是\nLRU（最近最少使用）。你可以设置为FIFO（先进先出）或是LFU（较少使用）。\n        clearOnFlush：内存数量最大时是否清除。 -->\n\n    <!-- 磁盘缓存位置 -->\n    <diskStore path=\"java.io.tmpdir\"/>\n    <!-- 默认缓存 -->\n    <defaultCache\n            maxElementsInMemory=\"10000\"\n            eternal=\"false\"\n            timeToIdleSeconds=\"120\"\n            timeToLiveSeconds=\"120\"\n            maxElementsOnDisk=\"10000000\"\n            diskExpiryThreadIntervalSeconds=\"120\"\n            memoryStoreEvictionPolicy=\"LRU\">\n\n        <persistence strategy=\"localTempSwap\"/>\n    </defaultCache>\n\n    <!-- 测试 -->\n    <cache name=\"GoodsType\"\n           eternal=\"false\"\n           timeToIdleSeconds=\"2400\"\n           timeToLiveSeconds=\"2400\"\n           maxEntriesLocalHeap=\"10000\"\n           maxEntriesLocalDisk=\"10000000\"\n           diskExpiryThreadIntervalSeconds=\"120\"\n           overflowToDisk=\"false\"\n           memoryStoreEvictionPolicy=\"LRU\">\n    </cache>\n\n</ehcache>\n```\n\n添加 @EnableCaching注解 开启缓存\n\n<!--more-->\n&nbsp;\n\n&nbsp;\n\n**2.Redis缓存**\n\n&nbsp;\n","tags":["SpringBoot"]},{"title":"SpringBoot学习笔记——连接池","url":"/SpringBoot学习笔记——连接池.html","content":"SpringBoot学习笔记&mdash;&mdash;连接池\n","tags":["SpringBoot"]},{"title":"Flink学习笔记——Flink MySQL CDC","url":"/Flink学习笔记——Flink MySQL CDC.html","content":"## 1.Flink CDC介绍\n\nFlink CDC提供了一系列connector，用于从其他数据源获取变更数据（change data capture）\n\n官方文档\n\n```\nhttps://ververica.github.io/flink-cdc-connectors/release-2.3/content/about.html\n\n```\n\n官方github\n\n```\nhttps://github.com/ververica/flink-cdc-connectors\n\n```\n\n各种数据源使用案例，参考：\n\n[基于 AWS S3、EMR Flink、Presto 和 Hudi 的实时数据湖仓 &ndash; 使用 EMR 迁移 CDH](https://aws.amazon.com/cn/blogs/china/use-emr-to-migrate-cdh-based-on-aws-s3-emr-flink-presto-and-hudi-real-time-data-lake/)\n\n[Flink CDC关于source和sink全调研及实践](http://junyao.tech/posts/748d18ad.html)\n\n## 2.Flink MySQL CDC\n\n### 原理\n\nFlink MySQL CDC官方文档：[https://github.com/apache/flink-cdc/blob/master/docs/content/docs/connectors/flink-sources/mysql-cdc.md](https://github.com/apache/flink-cdc/blob/master/docs/content/docs/connectors/flink-sources/mysql-cdc.md)\n\nMySQL binlog可以参考：<!--more-->\n&nbsp;[MySQL学习笔记&mdash;&mdash;binlog](https://www.cnblogs.com/tonglin0325/p/5265788.html)\n\nFlink MySQL CDC在2.0版本之后有比较大的性能提升：\n\n1.无锁\n\n2.并发同步（在大表同步的时候可以加速）\n\n3.支持snapshot阶段checkpoint（在snapshot同步阶段失败后可以继续同步）\n\n参考：[Flink CDC 2.0 实现原理剖析](https://zhjwpku.com/2022/01/16/flink-cdc-2-0-analysis.html#flink-cdc-20-%E5%85%A8%E9%87%8F%E5%A2%9E%E9%87%8F%E8%AF%BB%E5%8F%96%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86)\n\n### 使用datastream api\n\n可以参考：[https://github.com/apache/flink-cdc/blob/master/docs/content/docs/connectors/flink-sources/datastream-api-package-guidance.md](https://github.com/apache/flink-cdc/blob/master/docs/content/docs/connectors/flink-sources/datastream-api-package-guidance.md)\n\n```\npackage com.bigdata.flink;\n\nimport org.apache.flink.api.common.eventtime.WatermarkStrategy;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n\nimport org.apache.flink.cdc.connectors.mysql.source.MySqlSource;\nimport org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema;\n\npublic class MysqlCdcExample {\n\n    public static void main(String[] args) throws Exception {\n\n        MySqlSource<String> mySqlSource = MySqlSource.<String>builder()\n                .hostname(\"localhost\")\n                .port(3306)\n                .databaseList(\"test\") // set captured database, If you need to synchronize the whole database, Please set tableList to \".*\".\n                .tableList(\"test.user\") // set captured table\n                .username(\"root\")\n                .password(\"123456\")\n                .serverTimeZone(\"America/Danmarkshavn\")\n                .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String\n                .build();\n\n        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n        // enable checkpoint\n        env.enableCheckpointing(3000);\n\n        env\n                .fromSource(mySqlSource, WatermarkStrategy.noWatermarks(), \"MySQL Source\")\n                // set 1 parallel source tasks\n                .setParallelism(1)\n                .print().setParallelism(1); // use parallelism 1 for sink\n\n        env.execute(\"Print MySQL Snapshot + Binlog\");\n    }\n\n}\n\n```\n\ninsert一条MySQL数据，输出如下\n\n```\n{\n   \"before\":null,\n   \"after\":{\n      \"id\":\"AQ==\",\n      \"username\":\"test\",\n      \"email\":\"test@test.com\"\n   },\n   \"source\":{\n      \"version\":\"1.9.8.Final\",\n      \"connector\":\"mysql\",\n      \"name\":\"mysql_binlog_source\",\n      \"ts_ms\":0,\n      \"snapshot\":\"false\",\n      \"db\":\"test\",\n      \"sequence\":null,\n      \"table\":\"user\",\n      \"server_id\":0,\n      \"gtid\":null,\n      \"file\":\"\",\n      \"pos\":0,\n      \"row\":0,\n      \"thread\":null,\n      \"query\":null\n   },\n   \"op\":\"r\",\n   \"ts_ms\":1723951109745,\n   \"transaction\":null\n}\n\n```\n\nupdate这条数据，输出如下\n\n```\n{\n   \"before\":{\n      \"id\":\"AQ==\",\n      \"username\":\"test\",\n      \"email\":\"test@test.com\"\n   },\n   \"after\":{\n      \"id\":\"AQ==\",\n      \"username\":\"test123\",\n      \"email\":\"test@test.com\"\n   },\n   \"source\":{\n      \"version\":\"1.9.8.Final\",\n      \"connector\":\"mysql\",\n      \"name\":\"mysql_binlog_source\",\n      \"ts_ms\":1723951151000,\n      \"snapshot\":\"false\",\n      \"db\":\"test\",\n      \"sequence\":null,\n      \"table\":\"user\",\n      \"server_id\":1,\n      \"gtid\":null,\n      \"file\":\"mysql-bin.000005\",\n      \"pos\":1307,\n      \"row\":0,\n      \"thread\":75,\n      \"query\":null\n   },\n   \"op\":\"u\",\n   \"ts_ms\":1723951151768,\n   \"transaction\":null\n}\n\n```\n\n删除这条数据，输出如下\n\n```\n{\n   \"before\":{\n      \"id\":\"AQ==\",\n      \"username\":\"test123\",\n      \"email\":\"test@test.com\"\n   },\n   \"after\":null,\n   \"source\":{\n      \"version\":\"1.9.8.Final\",\n      \"connector\":\"mysql\",\n      \"name\":\"mysql_binlog_source\",\n      \"ts_ms\":1723953255000,\n      \"snapshot\":\"false\",\n      \"db\":\"test\",\n      \"sequence\":null,\n      \"table\":\"user\",\n      \"server_id\":1,\n      \"gtid\":null,\n      \"file\":\"mysql-bin.000005\",\n      \"pos\":1627,\n      \"row\":0,\n      \"thread\":99,\n      \"query\":null\n   },\n   \"op\":\"d\",\n   \"ts_ms\":1723953255153,\n   \"transaction\":null\n}\n\n```\n\n### 使用flink SQL\n\n&nbsp;\n\n### 报错\n\n1.Caused by: org.apache.flink.table.api.ValidationException: The MySQL server has a timezone offset (0 seconds ahead of UTC) which does not match the configured timezone Asia/Shanghai. Specify the right server-time-zone to avoid inconsistencies for time-related fields.\n\n这个因为没有给mysql cdc任务指定时区，可以使用如下命令查看mysql的时区\n\n```\nmysql> show variables like '%time_zone%';\n+------------------+--------+\n| Variable_name    | Value  |\n+------------------+--------+\n| system_time_zone | UTC    |\n| time_zone        | SYSTEM |\n+------------------+--------+\n2 rows in set (0.02 sec)\n \nmysql> select now();\n+---------------------+\n| now()               |\n+---------------------+\n| 2024-08-17 15:29:48 |\n+---------------------+\n1 row in set (0.00 sec)\n\n```\n\n假设mysql的时区是UTC，则需要指定和mysql时区一样的时区配置，如下\n\n```\nMySqlSource<String> mySqlSource = MySqlSource.<String>builder()\n        .hostname(\"localhost\")\n        .port(55000)\n        .databaseList(\"default\") // set captured database, If you need to synchronize the whole database, Please set tableList to \".*\".\n        .tableList(\"default.test\") // set captured table\n        .username(\"root\")\n        .password(\"123456\")\n        .serverTimeZone(\"America/Danmarkshavn\")\n        .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String\n        .build();\n\n```\n\n配置参考：[https://github.com/apache/flink-cdc/blob/master/docs/content/docs/connectors/flink-sources/mysql-cdc.md](https://github.com/apache/flink-cdc/blob/master/docs/content/docs/connectors/flink-sources/mysql-cdc.md)\n\n2.Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured\n\n需要假设mysql的binlog是否是开启ON状态，如果是OFF状态的话则会报错\n\n```\nmysql>  show variables like 'log_bin';\n\n```\n\n&nbsp;\n\n### MySQL CDC业界使用案例\n\n1.[基于 Flink CDC + Hudi 湖仓一体方案实践](https://www.cnblogs.com/zourui4271/p/15561517.html)&nbsp;（37互娱）\n\n2.[Flink CDC + Hudi 海量数据入湖在顺丰的实践](https://developer.aliyun.com/article/949402) （顺丰）\n","tags":["Flink"]},{"title":"SpringBoot学习笔记——mock","url":"/SpringBoot学习笔记——mock.html","content":"可以使用mock对springboot web接口进行测试\n\n1.依赖\n\n```\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-test</artifactId>\n    <scope>test</scope>\n</dependency>\n\n```\n\n2.编写测试用例\n\n```\nimport org.apache.tomcat.util.codec.binary.Base64;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.springframework.boot.test.autoconfigure.web.servlet.AutoConfigureMockMvc;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport org.springframework.http.HttpHeaders;\nimport org.springframework.test.context.junit4.SpringRunner;\nimport org.springframework.test.web.servlet.MockMvc;\nimport org.springframework.test.web.servlet.request.MockMvcRequestBuilders;\nimport org.springframework.test.web.servlet.result.MockMvcResultHandlers;\nimport org.springframework.test.web.servlet.result.MockMvcResultMatchers;\n\nimport javax.annotation.Resource;\n\nimport java.nio.charset.StandardCharsets;\n\n@RunWith(SpringRunner.class)\n@SpringBootTest\n@AutoConfigureMockMvc\npublic class HelloControllerTest {\n\n    @Resource\n    private MockMvc mockMvc;\n\n    @Test\n    public void helloTest() throws Exception {\n        String expect = \"{\\\"code\\\":200,\\\"msg\\\":\\\"hello: 1\\\",\\\"data\\\": null}\";\n        String auth = \"admin:admin\";\n        byte[] originAuth = auth.getBytes(StandardCharsets.US_ASCII);\n        byte[] encodedAuth = Base64.encodeBase64(originAuth);\n        String authHeader = \"Basic \" + new String(encodedAuth);\n        HttpHeaders headers = new HttpHeaders();\n        headers.set(\"Authorization\", authHeader);\n        mockMvc.perform(MockMvcRequestBuilders.get(\"/hello/1\")\n                .headers(headers))\n                .andExpect(MockMvcResultMatchers.content().json(expect))\n                .andDo(MockMvcResultHandlers.print());\n    }\n\n}\n\n```\n\n测试GET请求\n\n```\nMockHttpServletRequest:\n      HTTP Method = GET\n      Request URI = /hello/1\n       Parameters = {}\n          Headers = [Authorization:\"Basic YWRtaW46YWRtaW4=\"]\n             Body = null\n    Session Attrs = {SPRING_SECURITY_CONTEXT=SecurityContextImpl [Authentication=UsernamePasswordAuthenticationToken [Principal=org.springframework.security.core.userdetails.User [Username=admin, Password=[PROTECTED], Enabled=true, AccountNonExpired=true, credentialsNonExpired=true, AccountNonLocked=true, Granted Authorities=[]], Credentials=[PROTECTED], Authenticated=true, Details=WebAuthenticationDetails [RemoteIpAddress=127.0.0.1, SessionId=1], Granted Authorities=[ROLE_USER]]]}\n\nHandler:\n             Type = com.example.demo.controller.HelloController\n           Method = com.example.demo.controller.HelloController#hello(Integer)\n\nAsync:\n    Async started = false\n     Async result = null\n\nResolved Exception:\n             Type = null\n\nModelAndView:\n        View name = null\n             View = null\n            Model = null\n\nFlashMap:\n       Attributes = null\n\nMockHttpServletResponse:\n           Status = 200\n    Error message = null\n          Headers = [Content-Type:\"application/json\", X-Content-Type-Options:\"nosniff\", X-XSS-Protection:\"1; mode=block\", Cache-Control:\"no-cache, no-store, max-age=0, must-revalidate\", Pragma:\"no-cache\", Expires:\"0\", X-Frame-Options:\"DENY\"]\n     Content type = application/json\n             Body = {\"code\":200,\"msg\":\"hello: 1\",\"data\":null}\n    Forwarded URL = null\n   Redirected URL = null\n          Cookies = []\n\nProcess finished with exit code 0\n\n```\n\n测试POST请求\n\n```\n    @Test\n    @Transactional\n    public void createUserTest() throws Exception {\n        String expect = \"{\\\"code\\\":200,\\\"msg\\\":\\\"success\\\",\\\"data\\\": null}\";\n        String auth = \"admin:admin\";\n        byte[] originAuth = auth.getBytes(StandardCharsets.US_ASCII);\n        byte[] encodedAuth = Base64.encodeBase64(originAuth);\n        String authHeader = \"Basic \" + new String(encodedAuth);\n        HttpHeaders headers = new HttpHeaders();\n        headers.set(\"Authorization\", authHeader);\n        mockMvc.perform(\n                MockMvcRequestBuilders.post(\"/user\")\n                        .headers(headers)\n                        .contentType(MediaType.APPLICATION_JSON)\n                        .content(\"{\\n\" +\n                                \"  \\\"username\\\": \\\"admin\\\",\\n\" +\n                                \"  \\\"password\\\": \\\"admin\\\"\\n\" +\n                                \"}\"))\n                .andExpect(MockMvcResultMatchers.content().json(expect))\n                .andDo(MockMvcResultHandlers.print());\n    }　　\n```\n\n在对POST请求的测试中，添加了<!--more-->\n&nbsp;@Transactional 注解，用于在对测试中插入的数据进行回滚，避免插入脏数据\n\n使用 @SpringBootTest注解 如何配置测试环境请参考：[Spring Boot 2 实战：mock测试你的web应用](https://juejin.cn/post/6844903961984499720)\n\n&nbsp;\n","tags":["SpringBoot"]},{"title":"SpringBoot学习笔记——统一的接口response格式","url":"/SpringBoot学习笔记——统一的接口response格式.html","content":"对于接口的返回结果，需要有统一的结构，因为对于不用考虑流量费用的内部系统，对接口数据长度往往不太介意\n\n## 开源项目的接口规范案例：\n\n### 1.阿里云：\n\n阿里云健康码引擎的response结构<!--more-->\n&nbsp;ResponseResult\n\n<img src=\"/images/517519-20210603153043911-48126906.png\" alt=\"\" loading=\"lazy\" />\n\nResponseResult代码 参考：\n\n```\nhttps://github.com/aliyun/alibabacloud-whiteboard-callbackservice-demo/blob/master/src/main/java/com/aliyun/rtc/whiteboard/models/ResponseResult.java\n\n```\n\n返回体结构\n\n```\n/**\n* 自动生成的请求ID，建议回传互动白板服务，以便日志跟踪\n*/\nprivate String requestId;\n\n/**\n* 响应状态\n*/\nprivate boolean responseSuccess;\n\n/**\n* 响应成功结果体\n*/\nprivate T result;\n\n/**\n* 响应失败错误信息\n*/\nprivate String errorCode;\nprivate String errorMsg;\n\n/**\n* 成功响应\n*/\npublic static <T> ResponseResult<T> getSuccessResult(String requestId, T v) {\n\tResponseResult<T> result = new ResponseResult<>();\n\tresult.setRequestId(requestId);\n\tresult.setResponseSuccess(true);\n\tresult.setResult(v);\n\treturn result;\n}\n\n/**\n* 错误响应\n*/\npublic static <T> ResponseResult<T> getErrorResult(String requestId, String errorCode, String errorMsg) {\n\tResponseResult<T> result = new ResponseResult<>();\n\tresult.setRequestId(requestId);\n\tresult.setResponseSuccess(false);\n\tresult.setErrorCode(errorCode);\n\tresult.setErrorMsg(errorMsg);\n\treturn result;\n}\n\n```\n\n线上阿里云的dataworks的某接口API的response字段：\n\ndata是返回的数据，errCode是错误码，errMsg是错误信息，requestId是请求的uuid\n\n<img src=\"/images/517519-20210603144927325-846728504.png\" alt=\"\" loading=\"lazy\" />\n\n### 2.apache dolphinscheduler\n\n参考：\n\n```\nhttps://github.com/apache/dolphinscheduler/blob/master/dolphinscheduler-api/src/main/java/org/apache/dolphinscheduler/api/utils/Result.java\n\n```\n\n### 3.antd design pro\n\nantd design pro作为一个前端框架，也对后端的接口规范给出了建议，参考：[https://v5-pro.ant.design/zh-CN/docs/request](https://v5-pro.ant.design/zh-CN/docs/request)\n\n<img src=\"/images/517519-20221126211403707-52601357.png\" width=\"600\" height=\"574\" loading=\"lazy\" />\n\n## 我设计的后端接口规范：\n\n对于接口的response，目前设计了3个字段：code，msg和data\n\ncode是状态码，msg是返回消息，data的返回数据\n\nhttp status是200的时候，code默认是200\n\nhttp status是4xx的时候，code会是4xxxx，比如http status为403的时候，code会是40301、40302、40303...标识 Forbidden&nbsp;的具体原因\n\nhttp status是5xx的时候，code会是5xxxx，比如http status为500的时候，code会是50001、50002、50003...标识 Internal Server Error 的具体原因\n\n```\nimport lombok.*;\n\nimport java.io.Serializable;\nimport org.springframework.http.HttpStatus;\n\n@Data\n@ToString\n@AllArgsConstructor\n@NoArgsConstructor\n@Builder\npublic class ControllerResponseT<T> implements Serializable {\n\n    /**\n     * 状态码\n     */\n    private int code;\n\n    /**\n     * 消息\n     */\n    private String msg;\n\n    /**\n     * 数据内容，比如列表，实体\n     */\n    private T data;\n\n    public static <T> ControllerResponseT<T> ofSuccess() {\n        return ControllerResponseT.<T>builder()\n                .code(HttpStatus.OK.value())\n                .msg(HttpStatus.OK.getReasonPhrase())\n                .build();\n    }\n\n    public static <T> ControllerResponseT<T> ofSuccess(String message) {\n        return ControllerResponseT.<T>builder()\n                .code(HttpStatus.OK.value())\n                .msg(message)\n                .build();\n    }\n\n    public static <T> ControllerResponseT<T> ofSuccess(String message, T data) {\n        return ControllerResponseT.<T>builder()\n                .code(HttpStatus.OK.value())\n                .msg(message)\n                .data(data)\n                .build();\n    }\n\n    public static <T> ControllerResponseT<T> ofFail(int code, String message, T data) {\n        return ControllerResponseT.<T>builder()\n                .code(code)\n                .msg(message)\n                .data(data)\n                .build();\n    }\n\n}\n\n```\n\ncontroller\n\n```\nimport com.example.demo.common.ControllerResponseT;\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.http.HttpStatus;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RequestMethod;\nimport org.springframework.web.bind.annotation.ResponseStatus;\nimport org.springframework.web.bind.annotation.RestController;\n\n@Slf4j\n@RestController\npublic class HelloController {\n\n    @RequestMapping(path = \"/hello\", method = RequestMethod.GET)\n    public ControllerResponseT hello() {\n        return ControllerResponseT.ofSuccess(\"hello\");\n    }\n\n    @ResponseStatus(HttpStatus.INTERNAL_SERVER_ERROR)\n    @RequestMapping(path = \"/fail\", method = RequestMethod.GET)\n    public ControllerResponseT fail() {\n        return ControllerResponseT.ofFail(50001, \"error\", null);\n    }\n\n}\n```\n\n返回\n\n<img src=\"/images/517519-20210604150645567-373620208.png\" alt=\"\" loading=\"lazy\" />\n\n参考：[Spring Boot 无侵入式 实现API接口统一JSON格式返回](https://blog.csdn.net/qq_34347620/article/details/102239179)\n","tags":["SpringBoot"]},{"title":"Java——下拉列表框，复选框，列表框，文件选择框","url":"/Java——下拉列表框，复选框，列表框，文件选择框.html","content":"<br class=\"Apple-interchange-newline\" /><img src=\"/images/517519-20160325093521479-1942813553.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n```\nimport java.awt.Container;\nimport java.awt.GridLayout;\nimport java.awt.event.WindowAdapter;\nimport java.awt.event.WindowEvent;\nimport java.util.Vector;\n\nimport javax.swing.BorderFactory;\nimport javax.swing.JComboBox;\nimport javax.swing.JFrame;\nimport javax.swing.JLabel;\nimport javax.swing.JList;\n\nclass MyComboBox{\n\tprivate JFrame frame = new JFrame(\"窗体\");\t\t\t\t\t//定义窗体\n\tprivate Container cont = frame.getContentPane();\t\t\t//得到窗体容器\n\tprivate JComboBox jcb1  = null;\t\t\t\t\t\t\t\t\t\t\t//定义下拉列表框\n\tprivate JComboBox jcb2  = null;\t\t\t\t\t\t\t\t\t\t\t//定义下拉列表框\n\tpublic MyComboBox(){\n\t\tthis.frame.setLayout(new GridLayout(2,2));\n\t\tString nations[] = {\"中国\",\"美国\",\"韩国\",\"法国\",\"英国\"};\n\t\tVector<String> v = new Vector<String>();\t\t\t\t\t\t//定义一个Vector集合\n\t\tv.add(\"元素1\");\n\t\tv.add(\"元素2\");\n\t\tv.add(\"元素3\");\n\t\tthis.jcb1 = new JComboBox(nations);\n\t\tthis.jcb2 = new JComboBox(v);\n\t\t//定义一个列表框的边框显示条\n\t\tjcb1.setBorder(BorderFactory.createTitledBorder(\"哪个国家？\"));\n\t\tjcb2.setBorder(BorderFactory.createTitledBorder(\"Vector?\"));\n\t\tjcb1.setMaximumRowCount(3);\t\t\t\t//最多显示3个选项\n\t\tjcb2.setMaximumRowCount(3);\n\t\tcont.add(this.jcb1);\n\t\tcont.add(this.jcb2);\n\t\tcont.add(new JLabel(\"下拉列表框\"));\n\t\tthis.frame.setSize(330,200);\n\t\tthis.frame.setVisible(true);\n\t\tthis.frame.addWindowListener(new WindowAdapter(){\t//加入事件监听\n\t\t\tpublic void windowClosing(WindowEvent arg0) {\t\t\t//窗口关闭时触发，按下关闭按钮\n\t\t\t// TODO 自动生成的方法存根\n\t\t\tSystem.out.println(\"windowClosing-->窗口关闭\");\n\t\t\tSystem.exit(1);\n\t\t\t}\n\t\t});\n\t}\n}\n\n\n//主类\n//Function        : \tMyComboBox_demo\npublic class MyComboBox_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tnew MyComboBox();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160325100339308-2000377911.png\" alt=\"\" />\n\n```\nimport java.awt.Container;\nimport java.awt.GridLayout;\nimport java.awt.event.WindowAdapter;\nimport java.awt.event.WindowEvent;\nimport java.util.Vector;\n\nimport javax.swing.AbstractListModel;\nimport javax.swing.BorderFactory;\nimport javax.swing.ComboBoxModel;\nimport javax.swing.JComboBox;\nimport javax.swing.JFrame;\nimport javax.swing.JLabel;\nimport javax.swing.JList;\n\nclass MyComboBoxModel extends AbstractListModel implements ComboBoxModel{\n\t\n\tString nations[] = {\"中国\",\"美国\",\"韩国\",\"法国\",\"英国\"};\n\tString item = null;\n\t\n\t@Override\n\tpublic int getSize() {\n\t\t// TODO 自动生成的方法存根\n\t\treturn this.nations.length;\n\t}\n\n\t@Override\n\tpublic Object getElementAt(int index) {\n\t\t// TODO 自动生成的方法存根\n\t\treturn this.nations[index];\n\t}\n\n\t@Override\n\tpublic void setSelectedItem(Object anItem) {\n\t\t// TODO 自动生成的方法存根\n\t\tthis.item = (String) anItem;\n\t}\n\n\t@Override\n\tpublic Object getSelectedItem() {\n\t\t// TODO 自动生成的方法存根\n\t\treturn this.item;\n\t}\n\t\n}\n\nclass MyComboBox{\n\tprivate JFrame frame = new JFrame(\"窗体\");\t\t\t\t\t//定义窗体\n\tprivate Container cont = frame.getContentPane();\t\t\t//得到窗体容器\n\tprivate JComboBox jcb1  = null;\t\t\t\t\t\t\t\t\t\t\t//定义下拉列表框\n\tprivate JComboBox jcb2  = null;\t\t\t\t\t\t\t\t\t\t\t//定义下拉列表框\n\tpublic MyComboBox(){\n\t\tthis.frame.setLayout(new GridLayout(2,2));\n//\t\tString nations[] = {\"中国\",\"美国\",\"韩国\",\"法国\",\"英国\"};\n//\t\tVector<String> v = new Vector<String>();\t\t\t\t\t\t//定义一个Vector集合\n//\t\tv.add(\"元素1\");\n//\t\tv.add(\"元素2\");\n//\t\tv.add(\"元素3\");\n//\t\tthis.jcb1 = new JComboBox(nations);\n//\t\tthis.jcb2 = new JComboBox(v);\n\t\tthis.jcb1 = new JComboBox(new MyComboBoxModel());\t\t//实例化JComboBox\n\t\t//定义一个列表框的边框显示条\n\t\tjcb1.setBorder(BorderFactory.createTitledBorder(\"哪个国家？\"));\n//\t\tjcb2.setBorder(BorderFactory.createTitledBorder(\"Vector?\"));\n\t\tjcb1.setMaximumRowCount(3);\t\t\t\t//最多显示3个选项\n//\t\tjcb2.setMaximumRowCount(3);\n\t\tcont.add(this.jcb1);\n//\t\tcont.add(this.jcb2);\n\t\tcont.add(new JLabel(\"下拉列表框\"));\n\t\tthis.frame.setSize(330,200);\n\t\tthis.frame.setVisible(true);\n\t\tthis.frame.addWindowListener(new WindowAdapter(){\t//加入事件监听\n\t\t\tpublic void windowClosing(WindowEvent arg0) {\t\t\t//窗口关闭时触发，按下关闭按钮\n\t\t\t// TODO 自动生成的方法存根\n\t\t\tSystem.out.println(\"windowClosing-->窗口关闭\");\n\t\t\tSystem.exit(1);\n\t\t\t}\n\t\t});\n\t}\n}\n\npublic class MyComboBox_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tnew MyComboBox();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160325102222104-1663436961.png\" alt=\"\" />\n\n```\nimport java.awt.Container;\nimport java.awt.Font;\nimport java.awt.GridLayout;\nimport java.awt.event.ActionEvent;\nimport java.awt.event.ActionListener;\nimport java.awt.event.ItemEvent;\nimport java.awt.event.ItemListener;\nimport java.awt.event.WindowAdapter;\nimport java.awt.event.WindowEvent;\nimport java.util.Vector;\n\nimport javax.swing.AbstractListModel;\nimport javax.swing.Action;\nimport javax.swing.BorderFactory;\nimport javax.swing.ComboBoxModel;\nimport javax.swing.JComboBox;\nimport javax.swing.JFrame;\nimport javax.swing.JLabel;\nimport javax.swing.JList;\n\nclass MyComboBox implements ItemListener,ActionListener{\n\tprivate JFrame frame = new JFrame(\"窗体\");\t\t\t\t\t//定义窗体\n\tprivate Container cont = frame.getContentPane();\t\t\t//得到窗体容器\n\tprivate JComboBox jcb1  = null;\t\t\t\t\t\t\t\t\t\t\t//定义下拉列表框\n\tprivate JLabel label  = new JLabel(\"标签\");\t\t\t\t\t\t\t//定义标签\n\tprivate String fontSize[] = {\"10\",\"11\",\"12\"};\n\t\n\tpublic MyComboBox(){\n\t\tthis.frame.setLayout(new GridLayout(2,2));\n\n\t\tthis.jcb1 = new JComboBox(this.fontSize);\t\t//实例化JComboBox\n\t\t//定义一个列表框的边框显示条\n\t\tjcb1.setBorder(BorderFactory.createTitledBorder(\"请选择显示文字大小\"));\n\n\t\tjcb1.addItemListener(this); \t\t\t\t\t//加入选项监听\n\t\tjcb1.addActionListener(this); \t\t\t\t//加入动作监听\n\t\t\n\t\tjcb1.setMaximumRowCount(3);\t\t\t\t//最多显示3个选项\n\t\tjcb1.setEditable(true);\t\t\t\t\t\t\t\t//设置可编辑文本\n\t\tjcb1.configureEditor(jcb1.getEditor(), \"12\"); \t\t//定义默认值\n\t\tthis.changeFontSize(12);     \t\t\t\t\t//设置默认字体\n\n\t\tcont.add(this.jcb1);\n\t\tcont.add(label);\n\n\t\tcont.add(new JLabel(\"下拉列表框\"));\n\t\tthis.frame.setSize(330,200);\n\t\tthis.frame.setVisible(true);\n\t\tthis.frame.addWindowListener(new WindowAdapter(){\t//加入事件监听\n\t\t\tpublic void windowClosing(WindowEvent arg0) {\t\t\t//窗口关闭时触发，按下关闭按钮\n\t\t\t// TODO 自动生成的方法存根\n\t\t\tSystem.out.println(\"windowClosing-->窗口关闭\");\n\t\t\tSystem.exit(1);\n\t\t\t}\n\t\t});\n\t}\n\t@Override\n\tpublic void actionPerformed(ActionEvent e) {\t\t//输入信息时触发\n\t\t// TODO 自动生成的方法存根\n\t\tString itemSize = (String) this.jcb1.getSelectedItem();\t//得到选项\n\t\tint Size = 12;\n\t\ttry{\n\t\t\tSize = Integer.parseInt(itemSize);\t\t//字符串转整数\n\t\t}catch(Exception ex){\n\t\t\tthis.jcb1.getEditor().setItem(\"输入的不是数字\");\n\t\t}\n\t\tthis.changeFontSize(Size);\n\t\tif(!this.isExists(itemSize)){\n\t\t\tthis.jcb1.addItem(jcb1.getSelectedItem()); \t\t//不存在，加入下拉选项\n\t\t}\n\t}\n\t@Override\n\tpublic void itemStateChanged(ItemEvent e) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t}\n\t\n\tpublic void changeFontSize(int size){\t\t\t//改变文字大小\n\t\tFont font = new Font(\"Serief\",Font.BOLD,size);\t\t//定义Font对象\n\t\tthis.label.setFont(font);\t\t\t\t\t\t\t\t\t\t\t\t\t//设置文字大小\n\t}\n\t\n\tpublic boolean isExists(String item){\n\t\tboolean flag = false;\n\t\tfor(int i =0;i<this.jcb1.getItemCount();i++){\n\t\t\tif(item.equals(this.jcb1.getItemAt(i))){\n\t\t\t\tflag = true;\n\t\t\t}\n\t\t}\n\t\treturn flag;\n\t}\n\t\n}\n\npublic class MyComboBox_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tnew MyComboBox();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160324173628714-552423380.png\" alt=\"\" />\n\n&nbsp;\n\n```\nimport java.awt.Container;\nimport java.awt.GridLayout;\nimport java.awt.event.ItemEvent;\nimport java.awt.event.ItemListener;\nimport java.awt.event.WindowAdapter;\nimport java.awt.event.WindowEvent;\n\nimport javax.swing.BorderFactory;\nimport javax.swing.ButtonGroup;\nimport javax.swing.JCheckBox;\nimport javax.swing.JFrame;\nimport javax.swing.JPanel;\nimport javax.swing.JRadioButton;\n\nclass MyCheckBox implements ItemListener{\n\tprivate JFrame frame = new JFrame(\"窗体\");\n\tprivate Container cont = frame.getContentPane();\n\tprivate JCheckBox jrb1  = new JCheckBox(\"单选1\");\n\tprivate JCheckBox jrb2  = new JCheckBox(\"单选2\"); \n\tprivate JCheckBox jrb3  = new JCheckBox(\"单选3\"); \n\tprivate JPanel pan = new JPanel();\n\tpublic MyCheckBox(){\n\t\t//定义一个面板的边框显示条\n\t\tpan.setBorder(BorderFactory.createTitledBorder(\"请选择按钮\"));\n\t\tpan.setLayout(new GridLayout(1,3));\n\n\t\tpan.add(this.jrb1);\n\t\tpan.add(this.jrb2);\n\t\tpan.add(this.jrb3);\n\t\t\n\t\tjrb1.addItemListener(this); \t\t\t//加入事件监听\n\t\tjrb2.addItemListener(this); \t\t\t//加入事件监听\n\t\tjrb3.addItemListener(this); \t\t\t//加入事件监听\n\t\t\n\t\tcont.add(pan);\n\t\tthis.frame.setSize(330,80);\n\t\tthis.frame.setVisible(true);\n\t\tthis.frame.addWindowListener(new WindowAdapter(){\n\t\t\tpublic void windowClosing(WindowEvent e) {\t\t\t//窗口关闭时触发，按下关闭按钮\n\t\t\t// TODO 自动生成的方法存根\n\t\t\tSystem.out.println(\"windowClosing-->窗口关闭\");\n\t\t\tSystem.exit(1);\n\t\t\t}\n\t\t});\n\t}\n\t@Override\n\tpublic void itemStateChanged(ItemEvent e) {\n\t\t// TODO 自动生成的方法存根\n\t\tif(jrb1.isSelected()){\n\t\t\tSystem.out.println(\"jrb1\");\n\t\t}\n\t\tif(jrb2.isSelected()){\n\t\t\tSystem.out.println(\"jrb2\");\n\t\t}\n\t\tif(jrb3.isSelected()){\n\t\t\tSystem.out.println(\"jrb3\");\n\t\t}\n\t}\n}\n\npublic class JCheckBox_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tnew MyCheckBox();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160324175844276-1165278993.png\" alt=\"\" />\n\n```\nimport java.awt.Container;\nimport java.awt.GridLayout;\nimport java.awt.event.WindowAdapter;\nimport java.awt.event.WindowEvent;\nimport java.util.Vector;\n\nimport javax.swing.BorderFactory;\nimport javax.swing.JFrame;\nimport javax.swing.JList;\nimport javax.swing.JRadioButton;\nimport javax.swing.ListSelectionModel;\n\nclass MyList{\n\tprivate JFrame frame = new JFrame(\"窗体\");\t\t\t\t\t//定义窗体\n\tprivate Container cont = frame.getContentPane();\t\t\t//得到窗体容器\n\tprivate JList list1  = null;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//定义列表框\n\tprivate JList list2  = null;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//定义列表框\n\tpublic MyList(){\n\t\tthis.frame.setLayout(new GridLayout(1,3));\t\t\t\t\t//定义排版，1行2列\n\t\tString nations[] = {\"中国\",\"美国\",\"韩国\",\"法国\",\"英国\"};\n\t\tVector<String> v = new Vector<String>();\t\t\t\t\t\t//定义一个Vector集合\n\t\tv.add(\"Vector1\");\n\t\tv.add(\"Vector2\");\n\t\tv.add(\"Vector3\");\n\t\tthis.list1 = new JList(nations);\t\t\t\t\t//实例化JList\n\t\tthis.list2 = new JList(v);\t\t\t\t\t\t\t\t//实例化JList\n\t\t//定义一个列表框的边框显示条\n\t\tlist1.setBorder(BorderFactory.createTitledBorder(\"哪个国家？\"));\n\t\tlist2.setBorder(BorderFactory.createTitledBorder(\"Vector?\"));\n\t\t//第一个列表框一次可以选择多个选项\n\t\tlist1.setSelectionMode(ListSelectionModel.MULTIPLE_INTERVAL_SELECTION);\n\t\t//第二个列表框每次可以选择一个选项\n\t\tlist2.setSelectionMode(ListSelectionModel.SINGLE_SELECTION);\n\t\tcont.add(this.list1);\t\t\t\t\t//加入面板\n\t\tcont.add(this.list2);\t\t\t\t\t//加入面板\n\t\tthis.frame.setSize(330,200);\n\t\tthis.frame.setVisible(true);\n\t\tthis.frame.addWindowListener(new WindowAdapter(){\n\t\t\tpublic void windowClosing(WindowEvent e) {\t\t\t//窗口关闭时触发，按下关闭按钮\n\t\t\t// TODO 自动生成的方法存根\n\t\t\tSystem.out.println(\"windowClosing-->窗口关闭\");\n\t\t\tSystem.exit(1);\n\t\t\t}\n\t\t});\n\t}\n}\n\npublic class JList_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tnew MyList();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160324224519354-2108801364.png\" alt=\"\" />\n\n&nbsp;\n\n```\nimport java.awt.Container;\nimport java.awt.GridLayout;\nimport java.awt.event.WindowAdapter;\nimport java.awt.event.WindowEvent;\nimport java.util.Vector;\n\nimport javax.swing.AbstractListModel;\nimport javax.swing.BorderFactory;\nimport javax.swing.JFrame;\nimport javax.swing.JList;\nimport javax.swing.ListSelectionModel;\n\nclass MyListModel extends AbstractListModel{\n\n\tprivate String nations[] = {\"中国\",\"美国\",\"韩国\",\"法国\",\"英国\"};\n\t\n\t@Override\n\tpublic int getSize() {\t\t\t\t\t\t\t\n\t\t// TODO 自动生成的方法存根\n\t\treturn this.nations.length;\n\t}\n\n\t@Override\n\tpublic Object getElementAt(int index) {\t\t//返回指定标号的元素\n\t\t// TODO 自动生成的方法存根\n\t\tif(index<this.nations.length){\n\t\t\treturn this.nations[index];\n\t\t}else{\n\t\t\treturn null;\n\t\t}\n\t}\n}\n\nclass myList{\n\tprivate JFrame frame = new JFrame(\"窗体\");\t\t\t\t\t//定义窗体\n\tprivate Container cont = frame.getContentPane();\t\t\t//得到窗体容器\n\tprivate JList list1  = null;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//定义列表框\n\tprivate JList list2  = null;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//定义列表框\n\tpublic myList(){\n\t\tthis.list1 = new JList(new MyListModel());\n\t\tthis.list2 = new JList(new MyListModel());\n\t\t//定义一个列表框的边框显示条\n\t\tlist1.setBorder(BorderFactory.createTitledBorder(\"哪个国家？\"));\n\t\tlist2.setBorder(BorderFactory.createTitledBorder(\"Vector?\"));\n\t\t//第一个列表框一次可以选择多个选项\n\t\tlist1.setSelectionMode(ListSelectionModel.MULTIPLE_INTERVAL_SELECTION);\n\t\t//第二个列表框每次可以选择一个选项\n\t\tlist2.setSelectionMode(ListSelectionModel.SINGLE_SELECTION);\n\t\tcont.add(this.list1);\t\t\t\t\t//加入面板\n\t\tcont.add(this.list2);\t\t\t\t\t//加入面板\n\t\tthis.frame.setSize(330,200);\n\t\tthis.frame.setVisible(true);\n\t\tthis.frame.addWindowListener(new WindowAdapter(){\n\t\t\tpublic void windowClosing(WindowEvent e) {\t\t\t//窗口关闭时触发，按下关闭按钮\n\t\t\t// TODO 自动生成的方法存根\n\t\t\tSystem.out.println(\"windowClosing-->窗口关闭\");\n\t\t\tSystem.exit(1);\n\t\t\t}\n\t\t});\n\t}\n}\n\npublic class JModel_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tnew MyList();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160324230622229-767459993.png\" alt=\"\" />\n\n**监听只能加一个**\n\n```\nimport java.awt.Container;\nimport java.awt.GridLayout;\nimport java.awt.event.WindowAdapter;\nimport java.awt.event.WindowEvent;\nimport java.util.Vector;\n\nimport javax.swing.AbstractListModel;\nimport javax.swing.BorderFactory;\nimport javax.swing.JFrame;\nimport javax.swing.JList;\nimport javax.swing.JScrollPane;\nimport javax.swing.ListSelectionModel;\nimport javax.swing.event.ListSelectionEvent;\nimport javax.swing.event.ListSelectionListener;\n\nclass MyListModel extends AbstractListModel{\n\n\tprivate String nations[] = {\"中国\",\"美国\",\"韩国\",\"法国\",\"英国\"};\n\t\n\t@Override\n\tpublic int getSize() {\t\t\t\t\t\t\t\n\t\t// TODO 自动生成的方法存根\n\t\treturn this.nations.length;\n\t}\n\n\t@Override\n\tpublic Object getElementAt(int index) {\t\t//返回指定标号的元素\n\t\t// TODO 自动生成的方法存根\n\t\tif(index<this.nations.length){\n\t\t\treturn this.nations[index];\n\t\t}else{\n\t\t\treturn null;\n\t\t}\n\t}\n}\n\nclass myList implements ListSelectionListener{\n\tprivate JFrame frame = new JFrame(\"窗体\");\t\t\t\t\t//定义窗体\n\tprivate Container cont = frame.getContentPane();\t\t\t//得到窗体容器\n\tprivate JList list1  = null;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//定义列表框\n//\tprivate JList list2  = null;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//定义列表框\n\tpublic myList(){\n\t\tthis.list1 = new JList(new MyListModel());\n//\t\tthis.list2 = new JList(new MyListModel());\n\t\t//定义一个列表框的边框显示条\n\t\tlist1.setBorder(BorderFactory.createTitledBorder(\"哪个国家？\"));\n//\t\tlist2.setBorder(BorderFactory.createTitledBorder(\"Vector?\"));\n\t\t//第一个列表框一次可以选择多个选项\n\t\tlist1.setSelectionMode(ListSelectionModel.MULTIPLE_INTERVAL_SELECTION);\n\t\t//第二个列表框每次可以选择一个选项\n//\t\tlist2.setSelectionMode(ListSelectionModel.SINGLE_SELECTION);\n\t\t\n\t\tlist1.addListSelectionListener(this); \t\t\t//加入事件监听\n//\t\tlist2.addListSelectionListener(this); \t\t\t//加入事件监听\n\t\tcont.add(new JScrollPane(this.list1));\t\t\t//加入滚动条\n\t\t\n\t\tcont.add(this.list1);\t\t\t\t\t//加入面板\n//\t\tcont.add(this.list2);\t\t\t\t\t//加入面板\n\t\tthis.frame.setSize(330,200);\n\t\tthis.frame.setVisible(true);\n\t\tthis.frame.addWindowListener(new WindowAdapter(){\t//加入事件监听\n\t\t\tpublic void windowClosing(WindowEvent arg0) {\t\t\t//窗口关闭时触发，按下关闭按钮\n\t\t\t// TODO 自动生成的方法存根\n\t\t\tSystem.out.println(\"windowClosing-->窗口关闭\");\n\t\t\tSystem.exit(1);\n\t\t\t}\n\t\t});\n\t}\n\t\n\tpublic void valueChanged(ListSelectionEvent e) {\n\t\t// TODO 自动生成的方法存根\n\t\tint temp[] = list1.getSelectedIndices();\t\t\t\t//取得全部选定\n\t\tSystem.out.println(\"选定的内容：\");\n\t\tfor(int i=0;i<temp.length;i++){\n\t\t\tSystem.out.println(list1.getModel().getElementAt(i)+\"、\");\n\t\t}\n\t}\n}\n\n//主类\n//Function        : \tJModel_demo\npublic class JModel_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tnew myList();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160325170848370-457936242.png\" alt=\"\" />\n\n```\nimport java.awt.BorderLayout;\nimport java.awt.Container;\nimport java.awt.event.ActionEvent;\nimport java.awt.event.ActionListener;\nimport java.awt.event.WindowAdapter;\nimport java.awt.event.WindowEvent;\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.PrintStream;\nimport java.util.Scanner;\n\nimport javax.imageio.stream.FileImageInputStream;\nimport javax.swing.JButton;\nimport javax.swing.JFileChooser;\nimport javax.swing.JFrame;\nimport javax.swing.JLabel;\nimport javax.swing.JPanel;\nimport javax.swing.JScrollPane;\nimport javax.swing.JTextArea;\n\nclass Note implements ActionListener{\n\n\tprivate JFrame frame = new JFrame(\"窗体\");\t\t\t\t\t//定义窗体\n\tprivate JTextArea area = new JTextArea(\"JTextArea\");\t//定义文本区\n\tprivate JButton open = new JButton(\"打开文件\");\t\t\t//打开文件\n\tprivate JButton save = new JButton(\"保存文件\");\t\t\t//保存文件\n\tprivate JLabel label = new JLabel(\"现在没有打开的文件\");\n\t\n\tprivate JPanel butpan = new JPanel();\n\t\n\tpublic Note(){\n\t\tthis.butpan.add(open);\n\t\tthis.butpan.add(save);\n\t\t//设置窗体中的布局管理器为BorderLayout，所有的组件水平和垂直间距为3\n\t\tframe.setLayout(new BorderLayout(3,3));\n\t\tframe.add(this.label,BorderLayout.NORTH);\n\t\tframe.add(this.butpan,BorderLayout.SOUTH);\n\t\tframe.add(this.area, BorderLayout.CENTER);\n\t\tthis.frame.setSize(330,180);\n\t\tthis.frame.setVisible(true);\n\t\tthis.frame.addWindowListener(new WindowAdapter(){\t//加入事件监听\n\t\t\tpublic void windowClosing(WindowEvent arg0) {\t\t\t//窗口关闭时触发，按下关闭按钮\n\t\t\t// TODO 自动生成的方法存根\n\t\t\tSystem.out.println(\"windowClosing-->窗口关闭\");\n\t\t\tSystem.exit(1);\n\t\t\t}\n\t\t});\n\t\tthis.open.addActionListener(this);\n\t\tthis.save.addActionListener(this);\n//\t\tthis.frame.setSize(330,180);\n//\t\tthis.frame.setVisible(true);\n\t}\n\t\n\t@Override\n\tpublic void actionPerformed(ActionEvent e) {\t//按键事件监听\n\t\t// TODO 自动生成的方法存根\n\t\tFile file = null;\n\t\tint result = 0;\n\t\tJFileChooser fileChooser = new JFileChooser();\n\t\tif(e.getSource() == this.open){\n\t\t\tthis.area.setText(\"\");\n\t\t\tfileChooser.setApproveButtonText(\"确定\");\t\t//定义&ldquo;确定&ldquo;按钮&rdquo;\n\t\t\tfileChooser.setDialogTitle(\"打开文件\");\t\t\t\t//定义文件选择框标题\n\t\t\tresult = fileChooser.showOpenDialog(this.frame);\t\t//显示打开对话框\n\t\t\tif(result == JFileChooser.APPROVE_OPTION){\n\t\t\t\tfile = fileChooser.getSelectedFile();\n\t\t\t\tthis.label.setText(\"打开的文件名称为：\"+file.getName());\n\t\t\t}else if(result == JFileChooser.CANCEL_OPTION){\n\t\t\t\tthis.label.setText(\"没有选择任何文件\");\n\t\t\t}else{\n\t\t\t\tthis.label.setText(\"操作出现错误\");\n\t\t\t}\n\t\t\tif(file != null){\n\t\t\t\ttry{\n\t\t\t\t\tScanner scan = new Scanner(new FileInputStream(file));//设置输入流\n\t\t\t\t\tscan.useDelimiter(\"\\n\");\t\t\t\t\t\t//设置换行为分隔符\n\t\t\t\t\twhile(scan.hasNext()){\t\t\t\t\t\t\t//循环读取\n\t\t\t\t\t\tthis.area.append(scan.next());\t\t//读取内容到文本区\n\t\t\t\t\t\tthis.area.append(\"\\n\");\t\t\t\t\t//设置换行\n\t\t\t\t\t}\n\t\t\t\t\tscan.close();\t\t\t\t\t\t\t\t\t\t\t\t//关闭\n\t\t\t\t}catch(Exception ex){\n\t\t\t\t\tthis.label.setText(\"文件读取出错\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t\tif(e.getSource() == this.save){\n\n\t\t\tresult = fileChooser.showSaveDialog(this.frame);\t\t//显示保存文件框\n\t\t\tif(result == JFileChooser.APPROVE_OPTION){\n\t\t\t\tfile = fileChooser.getSelectedFile();\n\t\t\t\tthis.label.setText(\"存储的文件名称为：\"+file.getName());\n\t\t\t}else if(result == JFileChooser.CANCEL_OPTION){\n\t\t\t\tthis.label.setText(\"没有选择任何文件\");\n\t\t\t}else{\n\t\t\t\tthis.label.setText(\"操作出现错误\");\n\t\t\t}\n\t\t\tif(file != null){\n\t\t\t\ttry{\n\t\t\t\t\tPrintStream out = new PrintStream(new FileOutputStream(file));\n\t\t\t\t\tout.print(this.area.getText());\n\t\t\t\t\tout.close();\n\t\t\t\t}catch(Exception ex){\n\t\t\t\t\tthis.label.setText(\"文件保存出错\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t}\n\t\n}\n\n\n\n//主类\n//Function        : \tJFileChooser_demo\npublic class JFileChooser_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tnew Note();\n\t\t\n\t\t\n\t\t\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["Java"]},{"title":"Java——Swing事件处理","url":"/Java——Swing事件处理.html","content":"<img src=\"/images/517519-20160323234523386-2051310609.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n```\nimport java.awt.event.WindowEvent;\nimport java.awt.event.WindowListener;\n\nimport javax.swing.JFrame;\n\nclass MyWindowEventHandle implements WindowListener{\t//实现窗口监听\n\n\t@Override\n\tpublic void windowOpened(WindowEvent e) {\t\t\t//窗口打开时触发\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"windowOpened-->窗口被打开\");\n\t}\n\n\t@Override\n\tpublic void windowClosing(WindowEvent e) {\t\t\t//窗口关闭时触发，按下关闭按钮\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"windowClosing-->窗口关闭\");\n\t}\n\n\t@Override\n\tpublic void windowClosed(WindowEvent e) {\t\t\t//窗口被关闭时触发\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"windowClosed-->窗口被关闭\");\n\t}\n\n\t@Override\n\tpublic void windowIconified(WindowEvent e) {\t\t//窗口最小化时触发\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"windowIconified-->窗口最小化\");\n\t}\n\n\t@Override\n\tpublic void windowDeiconified(WindowEvent e) {\t\t//窗口从最小化恢复\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"windowDeiconified-->窗口从最小化恢复\");\n\t}\n\n\t@Override\n\tpublic void windowActivated(WindowEvent e) {\t\t\t//设置为非活动窗口是触发\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"windowActivated-->窗口被选中\");\n\t}\n\n\t@Override\n\tpublic void windowDeactivated(WindowEvent e) {\t\t//设置为活动窗口是触发\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"windowDeactivated-->取消窗口选中\");\n\t}\n\t\n}\n\npublic class MyWindowListener_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tJFrame f = new JFrame(\"Swing窗口\");\t\t\t\t\t\t\t//实例化窗体对象\n\t\tf.addWindowListener(new MyWindowEventHandle());\n\t\tf.setSize(440, 320);  \t\t\t//设置窗体\n\t\tf.setLocation(300,200);\t\t//设置显示位置\n\t\tf.setVisible(true);\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160324093233776-673895835.png\" alt=\"\" />\n\n&nbsp;\n\n```\nimport java.awt.event.WindowEvent;\nimport java.awt.event.WindowListener;\n\nimport javax.swing.JFrame;\n\nclass myWindowEventHandle extends WindowAdapter{\n\t//此时可以根据自己的需要覆写方法\n\tpublic void windowClosing(WindowEvent e) {\t\t\t//窗口关闭时触发，按下关闭按钮\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"windowClosing-->窗口关闭\");\n\t\tSystem.exit(1);\n\t}\n}\n\npublic class WindowAdapter {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tJFrame f = new JFrame(\"Swing窗口\");\t\t\t\t\t\t\t//实例化窗体对象\n\t\tf.addWindowListener(new myWindowEventHandle());\n//\t\tf.addWindowListener(new WindowAdapter(){\n//\t\t\t\n//\t\t\tpublic void windowClosing(WindowEvent e) {\t\t\t//窗口关闭时触发，按下关闭按钮\n//\t\t\t\t// TODO 自动生成的方法存根\n//\t\t\t\tSystem.out.println(\"windowClosing-->窗口关闭\");\n//\t\t\t\tSystem.exit(1);\n//\t\t\t}\n//\t\t});\n\t\t\n\t\tf.setSize(440, 320);  \t\t\t//设置窗体\n\t\tf.setLocation(300,200);\t\t//设置显示位置\n\t\tf.setVisible(true);\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160324114615151-1914501915.png\" alt=\"\" />\n\n```\nimport java.awt.Font;\nimport java.awt.GridLayout;\nimport java.awt.event.ActionEvent;\nimport java.awt.event.ActionListener;\nimport java.awt.event.WindowAdapter;\nimport java.awt.event.WindowEvent;\n\nimport javax.swing.JButton;\nimport javax.swing.JFrame;\nimport javax.swing.JLabel;\nimport javax.swing.JPanel;\nimport javax.swing.JTextField;\n\nclass ActionHandle{\n\tprivate JFrame frame = new JFrame(\"窗口\");\t\t\t//定义一个窗口对象\n\tprivate JButton but = new JButton(\"显示按钮\");\t\t//定义一个按钮\n\tprivate JLabel lab = new JLabel();\t\t\t\t\t\t\t\t\t//定义一个标签\n\tprivate JTextField jtf = new JTextField(10);\t\t\t\t\t//定义一个文本域\n\tprivate JPanel pan  = new JPanel();\n\tpublic ActionHandle(){\n\t\tFont font = new Font(\"Serief\",Font.ITALIC+Font.BOLD,28);\n\t\tlab.setFont(font);\n\t\tlab.setText(\"设置显示的文字\");\n\t\tbut.addActionListener(new ActionListener(){\t\t\t//采用匿名内部类\n\t\t\tpublic void actionPerformed(ActionEvent arg0){\n\t\t\t\tif(arg0.getSource() == but){\t\t\t\t\t\t\t\t\t\t//判断触发源是否是标签\n\t\t\t\t\tlab.setText(jtf.getText());\t\t\t\t\t\t\t\t\t\t//将文本文字设置到标签\n\t\t\t\t}\n\t\t\t}\n\t\t});\n\t\t\n\t\tframe.addWindowListener(new WindowAdapter(){\t\t//加入动作监听\n\t\t\tpublic void windowClosing(WindowEvent e) {\t\t\t//窗口关闭时触发，按下关闭按钮\n\t\t\t// TODO 自动生成的方法存根\n\t\t\tSystem.out.println(\"windowClosing-->窗口关闭\");\n\t\t\tSystem.exit(1);\n\t\t\t}\n\t\t});\n\t\tframe.setLayout(new GridLayout(2,1));\t\t//定义窗体布局，2行1列\n\t\tpan.setLayout(new GridLayout(1,2));\t\t\t//定义窗体布局，1行2列\n\t\tpan.add(jtf);\n\t\tpan.add(but);\n\t\tframe.add(pan);\n\t\tframe.add(lab);\n\t\tframe.pack();\n\t\tframe.setVisible(true);\n\t}\n}\n\npublic class ActionListener_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tnew ActionHandle();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160324121309511-672361646.png\" alt=\"\" />\n\n```\nimport java.awt.event.KeyEvent;\nimport java.awt.event.KeyListener;\n\nimport javax.swing.JFrame;\nimport javax.swing.JScrollPane;\nimport javax.swing.JTextArea;\n\nclass MyKeyHandle extends JFrame implements KeyListener{\n\t\n\tprivate JTextArea text = new JTextArea();\t\t//\n\t\n\tpublic MyKeyHandle(){\n\t\tsuper.setTitle(\"键盘\");\n\t\tJScrollPane scr = new JScrollPane(text);\t\t//加入滚动条\n\t\tscr.setBounds(5, 5, 300, 200);\n\t\tsuper.add(scr);\n\t\ttext.addKeyListener(this);\n\t\tsuper.setSize(310, 210);\n\t\tsuper.setVisible(true);\n\t}\n\t\n\t@Override\n\tpublic void keyTyped(KeyEvent e) {\t\t\t\t//输入内容\n\t\t// TODO 自动生成的方法存根\n\t\ttext.append(\"输入的内容是：\"+e.getKeyChar()+\"\\n\");\n\t}\n\n\t@Override\n\tpublic void keyPressed(KeyEvent e) {\t\t\t//键盘按下\n\t\t// TODO 自动生成的方法存根\n\t\ttext.append(\"键盘：\"+KeyEvent.getKeyText(e.getKeyCode())+\"键按下\\n\");\n\t}\n\n\t@Override\n\tpublic void keyReleased(KeyEvent e) {\t\t\t//键盘释放\n\t\t// TODO 自动生成的方法存根\n\t\ttext.append(\"键盘：\"+KeyEvent.getKeyText(e.getKeyCode())+\"键松开\\n\");\n\t}\n\t\n}\n\npublic class KeyListener_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tnew MyKeyHandle();\n\t}\n\n}\n\n```\n\n**&nbsp;使用适配器**\n\n```\nimport java.awt.event.KeyAdapter;\nimport java.awt.event.KeyEvent;\nimport java.awt.event.KeyListener;\nimport java.awt.event.WindowAdapter;\nimport java.awt.event.WindowEvent;\n\nimport javax.swing.JFrame;\nimport javax.swing.JScrollPane;\nimport javax.swing.JTextArea;\n\n//class MyKeyHandle extends JFrame implements KeyListener{\n//\t\n//\tprivate JTextArea text = new JTextArea();\t\t//\n//\t\n//\tpublic MyKeyHandle(){\n//\t\tsuper.setTitle(\"键盘\");\n//\t\tJScrollPane scr = new JScrollPane(text);\t\t//加入滚动条\n//\t\tscr.setBounds(5, 5, 300, 200);\n//\t\tsuper.add(scr);\n//\t\ttext.addKeyListener(this);\n//\t\tsuper.setSize(310, 210);\n//\t\tsuper.setVisible(true);\n//\t}\n//\t\n//\t@Override\n//\tpublic void keyTyped(KeyEvent e) {\t\t\t\t//输入内容\n//\t\t// TODO 自动生成的方法存根\n//\t\ttext.append(\"输入的内容是：\"+e.getKeyChar()+\"\\n\");\n//\t}\n//\n//\t@Override\n//\tpublic void keyPressed(KeyEvent e) {\t\t\t//键盘按下\n//\t\t// TODO 自动生成的方法存根\n//\t\ttext.append(\"键盘：\"+KeyEvent.getKeyText(e.getKeyCode())+\"键按下\\n\");\n//\t}\n//\n//\t@Override\n//\tpublic void keyReleased(KeyEvent e) {\t\t\t//键盘释放\n//\t\t// TODO 自动生成的方法存根\n//\t\ttext.append(\"键盘：\"+KeyEvent.getKeyText(e.getKeyCode())+\"键松开\\n\");\n//\t}\n//\t\n//}\n\nclass MyKeyHandle extends JFrame{\n\t\n\tprivate JTextArea text = new JTextArea();\t\t//\n\t\n\tpublic MyKeyHandle(){\n\t\tsuper.setTitle(\"键盘\");\n\t\tJScrollPane scr = new JScrollPane(text);\t\t//加入滚动条\n\t\tscr.setBounds(5, 5, 300, 200);\n\t\tsuper.add(scr);\n\t\ttext.addKeyListener(new KeyAdapter() {\n\t\t\t\n\t\t\t@Override\n\t\t\tpublic void keyTyped(KeyEvent e) {\n\t\t\t\t// TODO 自动生成的方法存根\n\t\t\t\ttext.append(\"输入的内容是：\"+e.getKeyChar()+\"\\n\");\n\t\t\t}\n\t\t});\n\t\tsuper.setSize(310, 210);\n\t\tsuper.setVisible(true);\n\t\tsuper.addWindowListener(new WindowAdapter() {\n\t\t\tpublic void windowClosing(WindowEvent arg0){\n\t\t\t\tSystem.exit(1);\n\t\t\t}\n\t\t});\n\t}\n\t\n\t\n\t\n}\n\npublic class KeyListener_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tnew MyKeyHandle();\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160324161935042-585447330.png\" alt=\"\" />\n\n```\nimport java.awt.event.MouseEvent;\nimport java.awt.event.MouseMotionListener;\nimport java.awt.event.WindowAdapter;\nimport java.awt.event.WindowEvent;\n\nimport javax.swing.JFrame;\n\nclass MyMouseMotion extends JFrame{\n\tpublic MyMouseMotion(){\n\t\tsuper.setTitle(\"键盘\");\n\t\n\t\tsuper.addMouseMotionListener(new MouseMotionListener() {\n\t\t\t\n\t\t\t@Override\n\t\t\tpublic void mouseMoved(MouseEvent e) {\n\t\t\t\t// TODO 自动生成的方法存根\n\t\t\t\tSystem.out.println(\"鼠标移动到窗体\");\n\t\t\t}\n\t\t\t\n\t\t\t@Override\n\t\t\tpublic void mouseDragged(MouseEvent e) {\n\t\t\t\t// TODO 自动生成的方法存根\n\t\t\t\tSystem.out.println(\"鼠标拖拽到窗体：X\"+e.getX()+\",Y\"+e.getY());\n\t\t\t}\n\t\t}\n\n\t\t\t\t\n\t\t);\n\t\tsuper.setSize(310, 210);\n\t\tsuper.setVisible(true);\n\t\tsuper.addWindowListener(new WindowAdapter(){\n\t\t\tpublic void windowClosing(WindowEvent e) {\t\t\t//窗口关闭时触发，按下关闭按钮\n\t\t\t// TODO 自动生成的方法存根\n\t\t\tSystem.out.println(\"windowClosing-->窗口关闭\");\n\t\t\tSystem.exit(1);\n\t\t\t}\n\t\t});\n\t}\n\t\n\t\n}\n\npublic class MouseListener_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tnew MyMouseMotion();\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["Java"]},{"title":"SpringBoot学习笔记——aop","url":"/SpringBoot学习笔记——aop.html","content":"它是一种在运行时，动态地将代码切入到类的指定方法、指定位置上的编程思想。用于切入到指定类指定方法的代码片段叫做切面，而切入到哪些类中的哪些方法叫做切入点\n\nAOP编程允许把遍布应用各处的功能分离出来形成可重用的组件\n\n<!--more-->\n&nbsp;\n\n实现一个AOP可以分成下面几个步骤：\n\n1.引入依赖&nbsp;\n\n```\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-aop</artifactId>\n</dependency>\n\n```\n\n2.自定义注解 @Permission，其中的 @Target 和 @Retention 元注解参考：[元注解(Annotation)](https://www.cnblogs.com/tonglin0325/p/5297979.html)\n\n```\nimport java.lang.annotation.ElementType;\nimport java.lang.annotation.Retention;\nimport java.lang.annotation.RetentionPolicy;\nimport java.lang.annotation.Target;\n\n@Target({ElementType.METHOD})\n@Retention(RetentionPolicy.RUNTIME)\npublic @interface Permission {\n}\n```\n\n参考\n\n<img src=\"/images/517519-20210607112800757-7032215.png\" alt=\"\" loading=\"lazy\" />\n\n3.编写切点\n\n常用的Aspect指示器：@execution，@within，@annotation\n\n@execution：用于在方法执行时触发，如：在com.example.demo.controller.XXXController.register方法在任意参数(..)下执行，返回任意类型(*)的情况下执行\n\n```\n@Pointcut(\"execution(* com.example.demo.controller.XXXController.register(..))\")\n\n```\n\n@within：用于在方法执行时触发，如：在com.example.demo.controller包下的任意方法执行\n\n```\n@Pointcut(\"within(com.example.demo.controller.*)\")\n\n```\n\n@annotation：用于注解对象，如：指定 com.example.xxx 包下及其所有子目录下的所有带有 @hello 注解的方法体为切点\n\n```\n@Pointcut(\"within(com.example.xxx.*) &amp;&amp; @annotation(hello)\")\n\n```\n\n　　\n\n4.编写切点checkTokenCut，以及定义切面Aspect，简单检查request请求的cookie中是否包含token，且切点设置在添加了 @Permission 注解的 方法上\n\n此外还可以在aop类上添加 @Order注解，来定义Spring IOC容器中Bean的执行顺序的优先级，值越小拥有越高的优先级，可为负数\n\n```\nimport lombok.extern.slf4j.Slf4j;\nimport org.aspectj.lang.ProceedingJoinPoint;\nimport org.aspectj.lang.annotation.Around;\nimport org.aspectj.lang.annotation.Aspect;\nimport org.aspectj.lang.annotation.Pointcut;\nimport org.springframework.stereotype.Component;\nimport org.springframework.web.context.request.RequestContextHolder;\nimport org.springframework.web.context.request.ServletRequestAttributes;\n\nimport javax.servlet.http.Cookie;\nimport javax.servlet.http.HttpServletRequest;\n\n@Slf4j\n@Aspect\n@Component\npublic class PermissionAspect {\n\n    @Pointcut(\"@annotation(com.example.demo.annotation.Permission) &amp;&amp; within(com.example.demo.controller.*)\")\n    public void checkTokenCut() {\n    }\n\n    @Around(\"checkTokenCut()\")\n    public Object doCheckToken(ProceedingJoinPoint point) throws Throwable {\n        ServletRequestAttributes attr = (ServletRequestAttributes) RequestContextHolder.currentRequestAttributes();\n        HttpServletRequest request = attr.getRequest();\n        Cookie[] cookies = request.getCookies();\n        int len = cookies.length;\n        for (int i = 0; i < len; i++) {\n            Cookie cookie = cookies[i];\n            if (\"token\".equals(cookie.getName())) {\n                log.info(\"has token\");\n                break;\n            }\n            if (i == len - 1) {\n                log.info(\"no token\");\n            }\n        }\n　　 //执行代理目标方法\n        return point.proceed();\n    }\n\n}\n\n```\n\n5.注入AspectJ切面 @Permission\n\n```\n@ResponseStatus(HttpStatus.INTERNAL_SERVER_ERROR)\n@RequestMapping(path = \"/fail\", method = RequestMethod.GET)\n@Permission\npublic ControllerResponseT fail() {\nreturn ControllerResponseT.ofFail(50001, \"error\", null);\n}\n```\n\n请求接口的时候将会执行check token逻辑\n\n&nbsp;<img src=\"/images/517519-20210608144340898-1612149581.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n参考：[SpringBoot使用AOP+注解实现简单的权限验证](https://segmentfault.com/a/1190000012845239)\n\n[前后端分离：使用spring的Aop实现Token验证](https://my.oschina.net/keeperv/blog/735685)\n\n&nbsp;\n\n其他aop的例子：\n\n使用aop实现数据脱敏：[改造了以前写的数据脱敏插件，更好用了](https://developer.51cto.com/art/202106/665733.htm)\n\n使用aop实现接口响应时间记录：[Spring Boot 2实践系列(四十七)：Spring AOP 实现API接口处理请求耗时监控](http://www.gxitsky.com/article/1605496279127153)\n\n使用aop实现全局异常处理以及统一打印接口请求入参和返回结果日志，打印接口访问性能日志，处理sql注入攻击以及处理入参特殊字符等问题：[Springboot项目全局异常统一处理](https://blog.csdn.net/hao_kkkkk/article/details/80538955)\n","tags":["SpringBoot"]},{"title":"SpringBoot学习笔记——统一的service接口","url":"/SpringBoot学习笔记——统一的service接口.html","content":"统一的service接口基于统一的mapper，参考：[Mybatis学习笔记&mdash;&mdash;通用mapper](https://www.cnblogs.com/tonglin0325/p/5267526.html)\n\n接口AbstractService<T>\n\n```\npackage com.example.demo.core.service;\n\nimport java.util.List;\n\npublic interface AbstractService<T> {\n\n    /**\n     * 获取所有\n     *\n     * @return List<T>\n     */\n    List<T> listObjects();\n\n    /**\n     * 通过key查找\n     *\n     * @param key Object\n     * @return T\n     */\n    T selectByKey(Object key);\n\n    /**\n     * 根据实体条件查找\n     *\n     * @param example Object\n     * @return List<T>\n     */\n    List<T> selectByExample(Object example);\n\n    /**\n     * 持久化\n     *\n     * @param entity T\n     * @return key\n     */\n    int save(T entity);\n\n    /**\n     * 通过主鍵刪除\n     *\n     * @param key Object\n     */\n    int deleteByKey(Object key);\n\n    /**\n     * 通过example条件刪除\n     *\n     * @param example T\n     */\n    int deleteByExample(T example);\n\n    /**\n     * 更新\n     *\n     * @param entity T\n     */\n    int update(T entity);\n\n}\n\n```\n\n抽象类AbstractServiceImpl<T>\n\n```\npackage com.example.demo.core.service.impl;\n\nimport com.example.demo.core.mapper.MyMapper;\nimport com.example.demo.core.service.AbstractService;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.transaction.annotation.Propagation;\nimport org.springframework.transaction.annotation.Transactional;\n\nimport java.util.List;\n\n@Transactional(propagation = Propagation.SUPPORTS, readOnly = true, rollbackFor = Exception.class)\npublic abstract class AbstractServiceImpl<T> implements AbstractService<T> {\n\n    @Autowired\n    protected MyMapper<T> mapper;\n\n    @Override\n    public List<T> listObjects() {\n        return mapper.selectAll();\n    }\n\n    @Override\n    public T selectByKey(Object key) {\n        return mapper.selectByPrimaryKey(key);\n    }\n\n    @Override\n    public List<T> selectByExample(Object example) {\n        return mapper.selectByExample(example);\n    }\n\n    @Override\n    @Transactional\n    public int save(T entity) {\n        return mapper.insert(entity);\n    }\n\n    @Override\n    @Transactional\n    public int deleteByKey(Object key) {\n        return mapper.deleteByPrimaryKey(key);\n    }\n\n    @Override\n    @Transactional\n    public int deleteByExample(Object key) {\n        return mapper.deleteByExample(key);\n    }\n\n    @Override\n    @Transactional\n    public int update(T entity) {\n        return mapper.updateByPrimaryKeySelective(entity);\n    }\n\n}\n\n```\n\n参考：\n\n```\nhttps://github.com/febsteam/FEBS-Security/blob/master/febs-common/src/main/java/cc/mrbird/common/service/impl/BaseService.java\n\n```\n\n以及\n\n```\nhttps://github.com/Zoctan/WYUOnlineJudge/blob/master/api/src/main/java/com/zoctan/api/core/service/AbstractService.java\n\n```\n\n之后在使用的时候，可以通过继承AbstractServiceImpl<T>的方式，来省略一些通用service方法的编写，比如\n\nUserService接口\n\n```\npackage com.example.demo.service;\n\nimport com.example.demo.core.service.AbstractService;\nimport com.example.demo.model.User;\n\npublic interface UserService extends AbstractService<User> {\n}\n\n```\n\nUserServiceImpl实现类\n\n```\npackage com.example.demo.service.impl;\n\nimport com.example.demo.core.service.impl.AbstractServiceImpl;\nimport com.example.demo.model.User;\nimport com.example.demo.service.UserService;\nimport org.springframework.stereotype.Service;\nimport org.springframework.transaction.annotation.Transactional;\n\n@Service\n@Transactional(rollbackFor = Exception.class)\npublic class UserServiceImpl extends AbstractServiceImpl<User> implements UserService {\n}\n```\n\n进行测试\n\n```\npackage com.example.demo.service.impl;\n\nimport com.example.demo.model.User;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport org.springframework.test.context.junit4.SpringJUnit4ClassRunner;\n\nimport java.util.List;\n\n@RunWith(SpringJUnit4ClassRunner.class)\n@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.DEFINED_PORT)\npublic class UserServiceImplTest {\n\n    @Autowired\n    private UserServiceImpl userService;\n\n    @Test\n    public void UserMapper() {\n        List<User> list = userService.listObjects();\n        for (User user: list) {\n            System.out.println(user.getId());\n        }\n    }\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["SpringBoot"]},{"title":"ElasticSearch学习笔记——集群优化","url":"/ElasticSearch学习笔记——集群优化.html","content":"1.索引预创建，避免在零点的时候对集群的master节点造成过大的压力\n\n2.集群冷热分离\n\n3.索引生命周期：所以预创建->热节点->warm节点->索引关闭->索引删除\n\n<!--more-->\n&nbsp;\n\n参考：[Elasticsearch集群优化实战](https://www.cnblogs.com/luxiaoxun/p/10742524.html)\n\n&nbsp;\n\n1.虚拟内存优化\n\n修改&nbsp;/etc/sysctl.conf配置文件，添加\n\n```\nvm.max_map_count=262144\n\n```\n\n参考：\n\n```\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html\n\n```\n\n&nbsp;\n\n2.配置 swappiness\n\n修改&nbsp;/etc/sysctl.conf 配置文件，添加\n\n```\nvm.swappiness=1\n\n```\n\n参考：[elasticsearch es调优实践经验总结](https://blog.csdn.net/yonggeit/article/details/109092869)\n\n修改 /etc/security/limits.conf 配置文件，添加\n\n最大打开文件描述符也可以设置成&nbsp;100728\n\n```\n* soft nproc 204800\n* hard nproc 204800\n\n* soft nofile 655360\n* hard nofile 655360\n\n* soft memlock unlimited\n* hard memlock unlimited\n\n```\n\n参考：[ELK Stack系列之基础篇(五) - 配置elasticsearch集群需要注意哪些方面？](https://cloud.tencent.com/developer/article/1584165)\n\n&nbsp;\n","tags":["ELK"]},{"title":"Java——布局：FlowLayout、BorderLayout、GridLayout、CardLayout","url":"/Java——布局：FlowLayout、BorderLayout、GridLayout、CardLayout.html","content":"在Swing中使用的所有布局管理器都可以实现LayoutManager接口，在Swing中主要使用的5种布局管理器：FlowLayout、BorderLayout、GridLayout、CardLayout、绝对定位。\n\n<!--more-->\n&nbsp;\n\n<img src=\"/images/517519-20160322222446011-868214286.png\" alt=\"\" />\n\n```\nimport java.awt.FlowLayout;\n\nimport javax.swing.JButton;\nimport javax.swing.JFrame;\n\n//主类\n//Function        : \tFlowLayout_demo\npublic class FlowLayout_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tJFrame f = new JFrame(\"Swing窗口\");\t\t\t\t\t\t\t//实例化窗体对象\n\t\t//设置窗体中的布局管理器为FlowLayout，所有的组件居中对齐，水平和垂直间距为3\n\t\tf.setLayout(new FlowLayout(FlowLayout.CENTER,3,3));\t\n\t\tJButton but = null;\n\t\tfor(int i=0;i<9;i++){\n\t\t\tbut = new JButton(\"按钮--\"+i);\n\t\t\tf.add(but);\n\t\t}\n\t\tf.setSize(500, 200);\n\t\tf.setVisible(true);\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160322223301104-1207824682.png\" alt=\"\" />\n\n```\nimport java.awt.BorderLayout;\nimport java.awt.FlowLayout;\n\nimport javax.swing.JButton;\nimport javax.swing.JFrame;\n\n//主类\n//Function        : \tBorderLayout_demo\npublic class BorderLayout_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tJFrame f = new JFrame(\"Swing窗口\");\t\t\t\t\t\t\t//实例化窗体对象\n\t\t//设置窗体中的布局管理器为BorderLayout，水平和垂直间距为3\n\t\tf.setLayout(new BorderLayout(3,3));\t\n\t\tf.add(new JButton(\"东\"),BorderLayout.EAST);\n\t\tf.add(new JButton(\"西\"),BorderLayout.WEST);\n\t\tf.add(new JButton(\"南\"),BorderLayout.SOUTH);\n\t\tf.add(new JButton(\"北\"),BorderLayout.NORTH);\n\t\tf.add(new JButton(\"中\"),BorderLayout.CENTER);\n\t\tf.pack(); \t\t\t\t\t\t//根据组件自动调整窗体的大小\n\t\tf.setVisible(true);\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160322223842245-68119194.png\" alt=\"\" />\n\n&nbsp;\n\n```\nimport java.awt.BorderLayout;\nimport java.awt.FlowLayout;\nimport java.awt.GridLayout;\n\nimport javax.swing.JButton;\nimport javax.swing.JFrame;\n\n//主类\n//Function        : \tGridLayout_demo\npublic class GridLayout_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tJFrame f = new JFrame(\"Swing窗口\");\t\t\t\t\t\t\t//实例化窗体对象\n\t\t//设置窗体中的布局管理器为GridLayout，3乘以5，水平和垂直间距为3\n\t\tf.setLayout(new GridLayout(3,5,3,3));\t\n\t\tJButton but = null;\n\t\tfor(int i=0;i<13;i++){\n\t\t\tbut = new JButton(\"按钮--\"+i);\n\t\t\tf.add(but);\n\t\t}\n\t\tf.pack(); \t\t//根据组件自动调整窗体大小\n\t\tf.setVisible(true);\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160322224404323-820655363.png\" alt=\"\" />\n\n```\nimport java.awt.BorderLayout;\nimport java.awt.CardLayout;\nimport java.awt.Container;\nimport java.awt.FlowLayout;\nimport java.awt.GridLayout;\n\nimport javax.swing.JButton;\nimport javax.swing.JFrame;\nimport javax.swing.JLabel;\n\n//主类\n//Function        : \tCardLayout_demo\npublic class CardLayout_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tJFrame f = new JFrame(\"Swing窗口\");\t\t\t\t\t\t\t//实例化窗体对象\n\t\tContainer cont = f.getContentPane();\t\t\t\t\t\t\t//取得窗体容器\n\t\tCardLayout card = new CardLayout();\t\t\t\t\t\t\t//定义布局管理器\n\t\tf.setLayout(card);\n\t\tcont.add(new JLabel(\"东123456789\",JLabel.CENTER),\"first\");\n\t\tcont.add(new JLabel(\"西123456789\",JLabel.CENTER),\"second\");\n\t\tcont.add(new JLabel(\"南123456789\",JLabel.CENTER),\"third\");\n\t\tcont.add(new JLabel(\"北123456789\",JLabel.CENTER),\"fourth\");\n\t\tcont.add(new JLabel(\"中123456789\",JLabel.CENTER),\"fifth\");\n\t\tf.pack(); \t\t//根据组件自动调整窗体大小\n\t\tf.setVisible(true);\n\t\tcard.show(cont, \"third\");\n\t\tfor(int i=0;i<5;i++){\t\t\t//循环显示每张卡片\n\t\t\ttry{\n\t\t\t\tThread.sleep(3000);\n\t\t\t}catch(InterruptedException e){\n\t\t\t}\n\t\t\tcard.next(cont);\n\t\t}\n\t\t\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160322230200979-330416392.png\" alt=\"\" />\n\n&nbsp;\n\n```\nimport java.awt.BorderLayout;\nimport java.awt.CardLayout;\nimport java.awt.Container;\nimport java.awt.FlowLayout;\nimport java.awt.GridLayout;\n\nimport javax.swing.JButton;\nimport javax.swing.JFrame;\nimport javax.swing.JLabel;\n\n//主类\n//Function        : \tAbsoluteLayout_demo\npublic class AbsoluteLayout_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tJFrame f = new JFrame(\"Swing窗口\");\t\t\t\t\t\t\t//实例化窗体对象\n\t\tf.setLayout(null);\t\t\t\t//绝对定位\n\t\tJLabel title = new JLabel(\"标签对象\");\t\t\t\t\t\t\t//建立标签对象\n\t\tJButton enter = new JButton(\"进入\");\t\t\t\t\t\t\t//建立按钮对象\n\t\tJButton help = new JButton(\"帮助\");\n\t\tf.setSize(200,90); \t\t\t\t\t\t//设置窗体大小\n\t\ttitle.setBounds(45,5,150,20); \t\t\t//设置组件的位置及其大小\n\t\tenter.setBounds(10,30,80,20); \t\t\t//设置组件的位置及其大小\n\t\thelp.setBounds(100,30,80,20); \t\t\t//设置组件的位置及其大小\n\t\tf.add(title);\n\t\tf.add(enter);\n\t\tf.add(help);\n\t\tf.setVisible(true);\n\t\t\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["Java"]},{"title":"Java——窗体：JFrame、JPanel等","url":"/Java——窗体：JFrame、JPanel等.html","content":"## 1.JFrame\n\n<img src=\"/images/517519-20160322162822745-1848028276.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n**创建一个新的窗体**\n\n```\nimport java.awt.Color;\nimport javax.swing.JFrame;\n\n//主类\n//Function        : \tJFrame_demo\npublic class JFrame_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tJFrame f = new JFrame(\"Swing窗口\");\n\t\tf.setSize(230,80);\n\t\tf.setBackground(Color.WHITE);\n\t\tf.setLocation(300, 200);\n\t\tf.setVisible(true);\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160322165435354-289235089.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160322165453495-1250342397.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160322165759261-1585463115.png\" alt=\"\" />\n\n除了JFrame表示之外，还有其他几种常见的窗体：JPanel、JSplitPane、JTabbedPane、JScrollPane、JDesktopPane、JInternalFrame等。\n\n## 2.JPanel\n\n<img src=\"/images/517519-20160322232153261-531867446.png\" alt=\"\" />\n\n```\nimport javax.swing.JButton;\nimport javax.swing.JFrame;\nimport javax.swing.JLabel;\nimport javax.swing.JPanel;\n\n//主类\n//Function        : \tJPanel_demo\npublic class JPanel_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tJFrame f = new JFrame(\"Swing窗口\");\t\t\t\t\t\t\t//实例化窗体对象\n\t\tJPanel pan = new JPanel();\t\t\t\t\t\t\t\t\t\t\t\t//实例化JPanel对象\n\t\tpan.add(new JLabel(\"标签--A\"));\n\t\tpan.add(new JLabel(\"标签--B\"));\n\t\tpan.add(new JLabel(\"标签--C\"));\n\t\tpan.add(new JButton(\"按钮--A\"));\n\t\tpan.add(new JButton(\"按钮--B\"));\n\t\tpan.add(new JButton(\"按钮--C\"));\n\t\tf.add(pan);\t\t\t//将JPanel加入到窗体中\n\t\tf.pack();\n\t\tf.setLocation(300,200);\t\t//设置显示位置\n\t\tf.setVisible(true);\n\t}\n\n}\n\n```\n\n## 3.JSplitPane\n\n<img src=\"/images/517519-20160322233028667-1647369272.png\" alt=\"\" />\n\n```\nimport java.awt.Container;\n\nimport javax.swing.JButton;\nimport javax.swing.JFrame;\nimport javax.swing.JLabel;\nimport javax.swing.JPanel;\nimport javax.swing.JSplitPane;\n\n//主类\n//Function        : \tJSplitPane_demo\npublic class JSplitPane_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tJFrame f = new JFrame(\"Swing窗口\");\t\t\t\t\t\t\t//实例化窗体对象\n\t\tContainer cont = f.getContentPane();\t\t\t\t\t\t\t//得到窗体容器\n\t\tJSplitPane lfsplit = null;\t\t\t\t\t\t\t\t\t\t\t\t\t//左右分割的JSplitPane\n\t\tJSplitPane tbsplit = null;\t\t\t\t\t\t\t\t\t\t\t\t\t//上下分割的JSplitPane\n\t\t\n\t\tlfsplit = new JSplitPane(JSplitPane.HORIZONTAL_SPLIT,new JLabel(\"左标签\"),new JLabel(\"右标签\"));\n\t\tlfsplit.setDividerSize(20); \t\t\t\t\t//设置左右的分割线大小\n\t\t\n\t\ttbsplit = new JSplitPane(JSplitPane.VERTICAL_SPLIT,lfsplit,new JLabel(\"右标签\"));\n\t\ttbsplit.setDividerSize(10);\t\t//按百分比设置上下的分割线大小\n\t\ttbsplit.setOneTouchExpandable(true); \t\t//快速展开/折叠分割条\n\t\tcont.add(tbsplit); \t\t\t\t\t\t\t\t//将tbsplit加入到窗体\n\t\tf.setSize(230, 80);  \t\t\t\t//设置窗体\n\t\tf.setLocation(300,200);\t\t//设置显示位置\n\t\tf.setVisible(true);\n\t}\n\n}\n\n```\n\n## 4.JTabbedPane\n\n<img src=\"/images/517519-20160323154916120-1427306364.png\" alt=\"\" />\n\n&nbsp;\n\n```\nimport java.awt.Container;\n\nimport javax.swing.ImageIcon;\nimport javax.swing.JButton;\nimport javax.swing.JFrame;\nimport javax.swing.JLabel;\nimport javax.swing.JPanel;\nimport javax.swing.JTabbedPane;\n\n//主类\n//Function        : \tJTabbedPane_demo\npublic class JTabbedPane_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tJFrame f = new JFrame(\"Swing窗口\");\t\t\t\t\t\t\t//实例化窗体对象\n\t\tContainer cont = f.getContentPane();\t\t\t\t\t\t\t//得到窗体容器\n\t\tJTabbedPane tab = null;\n\t\ttab = new JTabbedPane(JTabbedPane.TOP);\t\t\t\t//设置标签在顶部显示\n\t\tJPanel pan1 = new JPanel();\t\t\t\t\t//设置面板\n\t\tJPanel pan2 = new JPanel();\t\t\t\t\t//设置面板\n\t\tJButton but = new JButton(\"按钮\");\t  \t//定义按钮\n\t\tJLabel lab = new JLabel(\"标签\");\t\t\t\t//定义标签\n\t\tpan1.add(but);\n\t\tpan2.add(lab);\n\t\tString picPath = \"/home/common/software/database/123.jpg\";\n\t\ttab.addTab(\"图片选项\", new ImageIcon(picPath), pan1,\"图像\");\n\t\ttab.addTab(\"文字选项\", pan2);\n\t\tcont.add(tab);\t\t\t\t\t//加入到容器中\n\t\tf.setSize(440, 320);  \t\t\t\t//设置窗体\n\t\tf.setLocation(300,200);\t\t//设置显示位置\n\t\tf.setVisible(true);\n\t}\n\n}\n\n```\n\n## 5.JScrollPane\n\n<img src=\"/images/517519-20160323165611104-831237740.png\" alt=\"\" />\n\n&nbsp;\n\n```\nimport java.awt.Container;\n\nimport javax.swing.Icon;\nimport javax.swing.ImageIcon;\nimport javax.swing.JFrame;\nimport javax.swing.JLabel;\nimport javax.swing.JPanel;\nimport javax.swing.JScrollPane;\n\n//主类\n//Function        : \tJScrollPane_demo\npublic class JScrollPane_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tJFrame f = new JFrame(\"Swing窗口\");\t\t\t\t\t\t\t//实例化窗体对象\n\t\tContainer cont = f.getContentPane();\t\t\t\t\t\t\t//得到窗体容器\n\t\tString picPath = \"/home/common/software/database/photo.jpg\";\n\t\tIcon icon = new ImageIcon(picPath);\t\t\t\t//实例化Icon对象\n\t\tJPanel pan = new JPanel();\t\t\t\t\t\t\t\t\t//定义一个面板\n\t\tJLabel lab = new JLabel(icon);\t\t\t\t\t\t\t\t//定义一个标签，显示图片\n\t\tpan.add(lab); \t\t\t\t\t//将标签加到面板中\n\t\tJScrollPane scrl = null;\t\t\t//声明滚动面板\n\t\tscrl = new JScrollPane(pan,JScrollPane.VERTICAL_SCROLLBAR_ALWAYS,JScrollPane.HORIZONTAL_SCROLLBAR_AS_NEEDED);\n\t\tcont.add(scrl);\t\t\t\t\t\t//加入到容器中\n\t\tf.setSize(440, 320);  \t\t\t\t//设置窗体\n\t\tf.setLocation(300,200);\t\t//设置显示位置\n\t\tf.setVisible(true);\n\t}\n\n}\n\n```\n\n## 6.JDesktopPane和JInternalFrame\n\n<img src=\"/images/517519-20160323170812136-221138358.png\" alt=\"\" />\n\n```\nimport java.awt.BorderLayout;\nimport java.awt.Container;\n\nimport javax.swing.Icon;\nimport javax.swing.ImageIcon;\nimport javax.swing.JDesktopPane;\nimport javax.swing.JFrame;\nimport javax.swing.JInternalFrame;\nimport javax.swing.JLabel;\nimport javax.swing.JPanel;\n\n//主类\n//Function        : \tJDesktopPane_demo\npublic class JDesktopPane_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tJFrame f = new JFrame(\"Swing窗口\");\t\t\t\t\t\t\t//实例化窗体对象\n\t\tf.setLayout(new BorderLayout());\t\t\t\t\t\t\t\t\t//设置布局管理器\n\t\tContainer cont = f.getContentPane();\t\t\t\t\t\t\t//得到窗体容器\n\t\tJDesktopPane desk = new JDesktopPane();\t\t\t\t//实例化JDesktopPane容器\n\t\tcont.add(desk, BorderLayout.CENTER); \t\t\t\t\t//将Desktop加入到窗体\n\t\tJInternalFrame jif = null;\t\t\t\t\t\t//声明一个内部窗体对象\n\t\tString picPath = \"/home/common/software/database/123.jpg\";\n\t\tIcon icon = new ImageIcon(picPath);\t\t\t\t//实例化Icon对象\n\t\tJPanel pan = null;\n\t\tfor(int i=0;i<3;i++){\n\t\t\tjif = new JInternalFrame(\"指定标签\"+i,true,true,true,true);\n\t\t\tpan = new JPanel();\t\t\t\t//定义一个面板\n\t\t\tpan.add(new JLabel(icon));\t\t//加入一个标签\n\t\t\tjif.setLocation(35-i*10, 35-i*10); \t\t//设置显示位置\n\t\t\tjif.add(pan);\n\t\t\t\n\t\t\tjif.pack();\n\t\t\tjif.setVisible(true);\n\t\t\tdesk.add(jif);\t\t\t//将内部窗体加入到Desktop中\n\t\t}\n\t\tf.setSize(440, 320);  \t\t\t//设置窗体\n\t\tf.setLocation(300,200);\t\t//设置显示位置\n\t\tf.setVisible(true);\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["Java"]},{"title":"Java数据库——CallableStatement接口","url":"/Java数据库——CallableStatement接口.html","content":"<img src=\"/images/517519-20160322093027948-1802627962.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n**建立一个过程，建立的时候要加DELIMITER //**\n\n**<img src=\"/images/517519-20160322110605370-304261417.png\" alt=\"\" />**\n\n&nbsp;\n\n<img src=\"/images/517519-20160322111405854-480453254.png\" alt=\"\" />\n\n&nbsp;\n\n**IN&mdash;&mdash;只能输入，不能输出，修改之后保留修改的数据**\n\n**INOUT&mdash;&mdash;可以的输入，也可以输出，修改时输出的是修改后的数据，但是再次输出的时候是最先的数据**\n\n**OUT&mdash;&mdash;只能输出，不能输入**\n\n&nbsp;\n\n**调用myproc存储过程**\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.InputStream;\nimport java.sql.*;\nimport java.text.SimpleDateFormat;\nimport java.util.Scanner;\n\npublic class CallableStatement_demo {\n\n\t//定义MySQL的数据库驱动程序\n\tpublic static final String DBDRIVER = \"org.gjt.mm.mysql.Driver\";\n\t//定义MySQL数据库的连接地址\n\tpublic static final String DBURL = \"jdbc:mysql://localhost:3306/mysql_demo\";\n\t//MySQL数据库的连接用户名\n\tpublic static final String DBUSER = \"root\";\n\t//MySQL数据库的连接密码\n\tpublic static final String DBPASS = \"123456\";\n\t\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tConnection conn = null;\t\t\t\t\t\t//数据库连接\n\t\tCallableStatement cstmt = null;\t\t//数据库操作\n\t\tString sql = \"{CALL myproc(?,?,?)}\";\t//调用过程\n\t\t\n\t\tClass.forName(DBDRIVER);\t\t\t//加载驱动程序\n\t\t//连接MySQL数据库时，要写上连接的用户名和密码\n\t\tconn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n\t\tcstmt = conn.prepareCall(sql);\t\t//实例化PreparedStatement\n\t\tcstmt.setInt(1, 70);\t\t\t\t//设置第一个参数是70\n\t\tcstmt.setInt(2, 80);\t\t\t\t//设置第二个参数是70\n\t\tcstmt.registerOutParameter(2,Types.INTEGER);\t//设置返回值类型\n\t\tcstmt.registerOutParameter(3,Types.INTEGER);\n\t\tcstmt.execute();\n\t\tSystem.out.println(\"INOUT的返回值：\"+cstmt.getInt(2));\n\t\tSystem.out.println(\"OUT的返回值：\"+cstmt.getInt(3));\n\t\tcstmt.close();\n\t\tconn.close();\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["JDBC"]},{"title":"Java数据库——JDBC和JDBC 2.0 API","url":"/Java数据库——JDBC和JDBC 2.0 API.html","content":"**JDBC(Java Database Connectivity)**，Java数据库连接，提供了一种与平台无关的用于执行SQL语句的标准Java API，可以方便地实现多种关系型数据库的统一操作，它由一组用Java语言编写的类和接口组成。\n\n<img src=\"/images/517519-20160320171155881-506030953.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n**JDBC的主要操作类及接口**\n\n**<img src=\"/images/517519-20160320171801631-1294746242.png\" alt=\"\" />**\n\n&nbsp;\n\n**JDBC 2.0 API**\n\n<img src=\"/images/517519-20160322120221854-67523927.png\" alt=\"\" />\n\n&nbsp;\n\n**可滚动的结果集**\n\n**<img src=\"/images/517519-20160322132744979-101436524.png\" alt=\"\" />**\n\n**让结果集滚动起来**\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.InputStream;\nimport java.sql.*;\nimport java.text.SimpleDateFormat;\nimport java.util.Scanner;\n\npublic class JDBC20_demo {\n\t\n\t//定义MySQL的数据库驱动程序\n\tpublic static final String DBDRIVER = \"org.gjt.mm.mysql.Driver\";\n\t//定义MySQL数据库的连接地址\n\tpublic static final String DBURL = \"jdbc:mysql://localhost:3306/mysql_demo\";\n\t//MySQL数据库的连接用户名\n\tpublic static final String DBUSER = \"root\";\n\t//MySQL数据库的连接密码\n\tpublic static final String DBPASS = \"123456\";\n\t\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\t Connection conn = null;                     //数据库连接\n        PreparedStatement pstmt = null; //数据库操作\n        ResultSet rs = null;                                //保存结果集\n         \n        String sql = \"SELECT id,name,password,age,sex,birthday \"+  \" FROM user\";\n        Class.forName(DBDRIVER);            //加载驱动程序\n        //连接MySQL数据库时，要写上连接的用户名和密码\n        conn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n        //实例化PreparedStatement\n        pstmt = conn.prepareStatement(sql,ResultSet.TYPE_SCROLL_SENSITIVE,ResultSet.CONCUR_READ_ONLY);\n        rs = pstmt.executeQuery();                          //查询\n        \n        System.out.println(\"第二条数据：\");\n        rs.absolute(1);\t\t//输出第2条，从0开始\n        print(rs,1);\n        \n        System.out.println(\"第一条数据：\");\n        rs.beforeFirst();\t\t//输出第1条\n        print(rs,1);\n        \n        System.out.println(\"第二条数据：\");\n        rs.afterLast();\t\t//输出最后1条\n        print(rs,-1);\n\t}\n\n\tpublic static void print(ResultSet rs,int re) throws Exception{\n\t\tif(re>0){\n\t\t\trs.next();\t\t\t\t//由前向后输出\n\t\t}else{\n\t\t\trs.previous();\t\t//有后向前输出\n\t\t}\n\t\tint id = rs.getInt(1);\n\t\tString name = rs.getString(2);\n\t\tString pass = rs.getString(3);\n\t\tint age = rs.getInt(4);\n\t\tString sex = rs.getString(5);\n\t\tjava.util.Date d = rs.getDate(6);\n\t\tSystem.out.println(\"编号：\"+ id);\n\t\tSystem.out.println(\"姓名：\"+name);\n\t\tSystem.out.println(\"密码：\"+pass);\n\t\tSystem.out.println(\"年龄：\"+age);\n\t\tSystem.out.println(\"性别：\"+sex);\n\t\tSystem.out.println(\"生日：\"+d);\n\t}\n\t\n}\n\n```\n\n&nbsp;\n\n**使用结果集插入数据**\n\n**<img src=\"/images/517519-20160322140647854-1249662781.png\" alt=\"\" />**\n\n&nbsp;\n\n**　　直接在user表中增加数据**\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.InputStream;\nimport java.sql.*;\nimport java.text.SimpleDateFormat;\nimport java.util.Scanner;\n\npublic class JDBC20_demo {\n\t\n\t//定义MySQL的数据库驱动程序\n\tpublic static final String DBDRIVER = \"org.gjt.mm.mysql.Driver\";\n\t//定义MySQL数据库的连接地址\n\tpublic static final String DBURL = \"jdbc:mysql://localhost:3306/mysql_demo\";\n\t//MySQL数据库的连接用户名\n\tpublic static final String DBUSER = \"root\";\n\t//MySQL数据库的连接密码\n\tpublic static final String DBPASS = \"123456\";\n\t\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tConnection conn = null;                     //数据库连接\n        PreparedStatement pstmt = null; //数据库操作\n        ResultSet rs = null;                                //保存结果集\n         \n        String sql = \"SELECT id,name,password,age,sex,birthday \"+  \" FROM user\";\n        Class.forName(DBDRIVER);            //加载驱动程序\n        //连接MySQL数据库时，要写上连接的用户名和密码\n        conn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n        //实例化PreparedStatement\n        pstmt = conn.prepareStatement(sql,ResultSet.TYPE_SCROLL_SENSITIVE,ResultSet.CONCUR_UPDATABLE);\n        rs = pstmt.executeQuery();                       //实例化ResulitSet对象\n        rs.moveToInsertRow(); \t\t\t\t\t\t\t//移动到可以插入的数据行\n        rs.updateString(\"name\", \"张三\");\n        rs.updateString(\"password\", \"zhangsan\");\n        rs.updateInt(\"age\", 14);\n        rs.updateString(\"sex\", \"女\");\n        rs.updateDate(\"birthday\", new java.sql.Date(new java.util.Date().getTime()));\n        rs.insertRow(); \t\t\t//插入数据\n        rs.close();\n        pstmt.close();\n        conn.close();\n        \n\t}\n\n\tpublic static void print(ResultSet rs,int re) throws Exception{\n\t\tif(re>0){\n\t\t\trs.next();\t\t\t\t//由前向后输出\n\t\t}else{\n\t\t\trs.previous();\t\t//有后向前输出\n\t\t}\n\t\tint id = rs.getInt(1);\n\t\tString name = rs.getString(2);\n\t\tString pass = rs.getString(3);\n\t\tint age = rs.getInt(4);\n\t\tString sex = rs.getString(5);\n\t\tjava.util.Date d = rs.getDate(6);\n\t\tSystem.out.println(\"编号：\"+ id);\n\t\tSystem.out.println(\"姓名：\"+name);\n\t\tSystem.out.println(\"密码：\"+pass);\n\t\tSystem.out.println(\"年龄：\"+age);\n\t\tSystem.out.println(\"性别：\"+sex);\n\t\tSystem.out.println(\"生日：\"+d);\n\t}\n\t\n}\n\n```\n\n&nbsp;\n\n**使用结果集更新数据**\n\n**　　使用限定查询，查询id编号为3的用户信息，并更新其信息**\n\n**查询到了才能更新，如果没有就不能更新**\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.InputStream;\nimport java.sql.*;\nimport java.text.SimpleDateFormat;\nimport java.util.Scanner;\n\npublic class JDBC20_demo {\n\t\n\t//定义MySQL的数据库驱动程序\n\tpublic static final String DBDRIVER = \"org.gjt.mm.mysql.Driver\";\n\t//定义MySQL数据库的连接地址\n\tpublic static final String DBURL = \"jdbc:mysql://localhost:3306/mysql_demo\";\n\t//MySQL数据库的连接用户名\n\tpublic static final String DBUSER = \"root\";\n\t//MySQL数据库的连接密码\n\tpublic static final String DBPASS = \"123456\";\n\t\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n        \n\t\tConnection conn = null;                     //数据库连接\n        PreparedStatement pstmt = null; //数据库操作\n        ResultSet rs = null;                                //保存结果集\n         \n        String sql = \"SELECT id,name,password,age,sex,birthday \"+  \" FROM user WHERE id=?\";\n        Class.forName(DBDRIVER);            //加载驱动程序\n        //连接MySQL数据库时，要写上连接的用户名和密码\n        conn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n        //实例化PreparedStatement\n        pstmt = conn.prepareStatement(sql,ResultSet.TYPE_SCROLL_SENSITIVE,ResultSet.CONCUR_UPDATABLE);\n        pstmt.setInt(1, 2);  \t\t\t\t\t\t\t\t\t\t//更新3号用户\n        rs = pstmt.executeQuery();                       //实例化ResulitSet对象\n        rs.last();\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//移动到最后一行\n        rs.updateString(\"name\", \"王小明\");\n        rs.updateString(\"password\", \"wangxiaoming\");\n        rs.updateInt(\"age\", 24);\n        rs.updateString(\"sex\", \"男\");\n        rs.updateDate(\"birthday\", new java.sql.Date(new java.util.Date().getTime()));\n        rs.updateRow(); \t\t\t\t\t\t//更新数据\n        rs.close();\t\t\t\t\t\t\t\t\t//关闭结果集\n        pstmt.close();\n        conn.close();\n\t}\n\n\tpublic static void print(ResultSet rs,int re) throws Exception{\n\t\tif(re>0){\n\t\t\trs.next();\t\t\t\t//由前向后输出\n\t\t}else{\n\t\t\trs.previous();\t\t//有后向前输出\n\t\t}\n\t\tint id = rs.getInt(1);\n\t\tString name = rs.getString(2);\n\t\tString pass = rs.getString(3);\n\t\tint age = rs.getInt(4);\n\t\tString sex = rs.getString(5);\n\t\tjava.util.Date d = rs.getDate(6);\n\t\tSystem.out.println(\"编号：\"+ id);\n\t\tSystem.out.println(\"姓名：\"+name);\n\t\tSystem.out.println(\"密码：\"+pass);\n\t\tSystem.out.println(\"年龄：\"+age);\n\t\tSystem.out.println(\"性别：\"+sex);\n\t\tSystem.out.println(\"生日：\"+d);\n\t}\n\t\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160322142203448-143222157.png\" alt=\"\" />\n\n&nbsp;\n\n**使用结果集删除数据**\n\n**　　使用deleteRow()方法**\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.InputStream;\nimport java.sql.*;\nimport java.text.SimpleDateFormat;\nimport java.util.Scanner;\n\npublic class JDBC20_demo {\n\t\n\t//定义MySQL的数据库驱动程序\n\tpublic static final String DBDRIVER = \"org.gjt.mm.mysql.Driver\";\n\t//定义MySQL数据库的连接地址\n\tpublic static final String DBURL = \"jdbc:mysql://localhost:3306/mysql_demo\";\n\t//MySQL数据库的连接用户名\n\tpublic static final String DBUSER = \"root\";\n\t//MySQL数据库的连接密码\n\tpublic static final String DBPASS = \"123456\";\n\t\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tConnection conn = null;                     //数据库连接\n        PreparedStatement pstmt = null; //数据库操作\n        ResultSet rs = null;                                //保存结果集\n         \n        String sql = \"SELECT id,name,password,age,sex,birthday \"+  \" FROM user WHERE id=?\";\n        Class.forName(DBDRIVER);            //加载驱动程序\n        //连接MySQL数据库时，要写上连接的用户名和密码\n        conn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n        //实例化PreparedStatement\n        pstmt = conn.prepareStatement(sql,ResultSet.TYPE_SCROLL_SENSITIVE,ResultSet.CONCUR_UPDATABLE);\n        pstmt.setInt(1, 2);  \t\t\t\t\t\t\t\t\t\t//更新3号用户\n        rs = pstmt.executeQuery();                       //实例化ResulitSet对象\n        rs.last();\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//移动到最后一行\n        rs.deleteRow(); \t\t\t\t\t\t//删除数据\n        rs.close();\t\t\t\t\t\t\t\t\t//关闭结果集\n        pstmt.close();\n        conn.close();\n\t}\n\n\tpublic static void print(ResultSet rs,int re) throws Exception{\n\t\tif(re>0){\n\t\t\trs.next();\t\t\t\t//由前向后输出\n\t\t}else{\n\t\t\trs.previous();\t\t//有后向前输出\n\t\t}\n\t\tint id = rs.getInt(1);\n\t\tString name = rs.getString(2);\n\t\tString pass = rs.getString(3);\n\t\tint age = rs.getInt(4);\n\t\tString sex = rs.getString(5);\n\t\tjava.util.Date d = rs.getDate(6);\n\t\tSystem.out.println(\"编号：\"+ id);\n\t\tSystem.out.println(\"姓名：\"+name);\n\t\tSystem.out.println(\"密码：\"+pass);\n\t\tSystem.out.println(\"年龄：\"+age);\n\t\tSystem.out.println(\"性别：\"+sex);\n\t\tSystem.out.println(\"生日：\"+d);\n\t}\n\t\n}\n\n```\n\n&nbsp;\n\n**批处理**\n\n**<img src=\"/images/517519-20160322143830417-1205642859.png\" alt=\"\" />**\n\n&nbsp;\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.InputStream;\nimport java.sql.*;\nimport java.text.SimpleDateFormat;\nimport java.util.Scanner;\n\npublic class JDBC20_demo {\n\t\n\t//定义MySQL的数据库驱动程序\n\tpublic static final String DBDRIVER = \"org.gjt.mm.mysql.Driver\";\n\t//定义MySQL数据库的连接地址\n\tpublic static final String DBURL = \"jdbc:mysql://localhost:3306/mysql_demo\";\n\t//MySQL数据库的连接用户名\n\tpublic static final String DBUSER = \"root\";\n\t//MySQL数据库的连接密码\n\tpublic static final String DBPASS = \"123456\";\n\t\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t  Connection conn = null;                     //数据库连接\n      PreparedStatement pstmt = null; //数据库操作\n       \n      String sql = \"INSERT INTO user (name,password,age,sex,birthday) \"+  \" VALUES (?,?,?,?,?)\";\n      Class.forName(DBDRIVER);            //加载驱动程序\n      //连接MySQL数据库时，要写上连接的用户名和密码\n      conn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n      //实例化PreparedStatement\n      pstmt = conn.prepareStatement(sql);\n      for(int i=0;i<10;i++){\n    \t  pstmt.setString(1, \"张三&mdash;\"+i);\n    \t  pstmt.setString(2, \"mima&mdash;\"+i);\n    \t  pstmt.setInt(3, 20+i);\n    \t  pstmt.setString(4, \"男\");\n    \t  pstmt.setDate(5, new Date(new java.util.Date().getTime()));\n    \t  pstmt.addBatch(); \t\t\t//加入批处理等待执行\n      }\n      int temp[] = pstmt.executeBatch(); \t\t\t\t//批量执行\n      pstmt.close();\n      conn.close();\n\t}\n\n\tpublic static void print(ResultSet rs,int re) throws Exception{\n\t\tif(re>0){\n\t\t\trs.next();\t\t\t\t//由前向后输出\n\t\t}else{\n\t\t\trs.previous();\t\t//有后向前输出\n\t\t}\n\t\tint id = rs.getInt(1);\n\t\tString name = rs.getString(2);\n\t\tString pass = rs.getString(3);\n\t\tint age = rs.getInt(4);\n\t\tString sex = rs.getString(5);\n\t\tjava.util.Date d = rs.getDate(6);\n\t\tSystem.out.println(\"编号：\"+ id);\n\t\tSystem.out.println(\"姓名：\"+name);\n\t\tSystem.out.println(\"密码：\"+pass);\n\t\tSystem.out.println(\"年龄：\"+age);\n\t\tSystem.out.println(\"性别：\"+sex);\n\t\tSystem.out.println(\"生日：\"+d);\n\t}\n\t\n}\n\n```\n\n&nbsp;\n","tags":["JDBC"]},{"title":"Java数据库——事务处理","url":"/Java数据库——事务处理.html","content":"<img src=\"/images/517519-20160322150235073-1470483584.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160322152146042-1022146203.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n**在数据库中执行5条SQL语句，这些SQL语句本身需要保持一致，即要么同时成功，要么同时失败**\n\n**事务基本操作**\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.InputStream;\nimport java.sql.*;\nimport java.text.SimpleDateFormat;\nimport java.util.Scanner;\n\npublic class JDBC20_demo {\n\t\n\t//定义MySQL的数据库驱动程序\n\tpublic static final String DBDRIVER = \"org.gjt.mm.mysql.Driver\";\n\t//定义MySQL数据库的连接地址\n\tpublic static final String DBURL = \"jdbc:mysql://localhost:3306/mysql_demo\";\n\t//MySQL数据库的连接用户名\n\tpublic static final String DBUSER = \"root\";\n\t//MySQL数据库的连接密码\n\tpublic static final String DBPASS = \"123456\";\n\t\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\t  Connection conn = null;                     //数据库连接\n\t      Statement stmt = null; \t\t\t\t\t\t//数据库操作\n\t       \n\t      Class.forName(DBDRIVER);            //加载驱动程序\n\t      //连接MySQL数据库时，要写上连接的用户名和密码\n\t      conn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n\t      conn.setAutoCommit(false);     \t\t//取消自动提交\n\t      //实例化PreparedStatement\n\t      stmt = conn.createStatement();  \t//实例化对象\n\t      //加入5条SQL语句\n\t      stmt.addBatch(\"INSERT INTO user(name,password,age,sex,birthday)\"+\"VALUES('lisi','lisimima',14,'男','1987-09-23')\");\n\t      stmt.addBatch(\"INSERT INTO user(name,password,age,sex,birthday)\"+\"VALUES('lisi','lisimima',14,'男','1987-09-23')\");\n\t      stmt.addBatch(\"INSERT INTO user(name1,password,age,sex,birthday)\"+\"VALUES('lisi','lisimima',14,'男','1987-09-23')\");\n\t      stmt.addBatch(\"INSERT INTO user(name,password,age,sex,birthday)\"+\"VALUES('lisi','lisimima',14,'男','1987-09-23')\");\n\t      stmt.addBatch(\"INSERT INTO user(name,password,age,sex,birthday)\"+\"VALUES('lisi','lisimima',14,'男','1987-09-23')\");\n\t      try{\n\t    \t  int temp[] = stmt.executeBatch(); \t\t//批量执行\n\t    \t  System.out.println(\"更新了\"+temp.length+\"条数据。\");\n\t    \t  conn.commit();\t\t\t\t//提交事务\n\t      }catch(Exception e){\n\t    \t  try{\n\t    \t\t  conn.rollback();\n\t    \t  }catch(Exception ex){\n\t    \t\t  ex.printStackTrace();\n\t    \t  }\n\t      }\n\t      stmt.close();\n\t      conn.close();\n\t}\n\n\tpublic static void print(ResultSet rs,int re) throws Exception{\n\t\tif(re>0){\n\t\t\trs.next();\t\t\t\t//由前向后输出\n\t\t}else{\n\t\t\trs.previous();\t\t//有后向前输出\n\t\t}\n\t\tint id = rs.getInt(1);\n\t\tString name = rs.getString(2);\n\t\tString pass = rs.getString(3);\n\t\tint age = rs.getInt(4);\n\t\tString sex = rs.getString(5);\n\t\tjava.util.Date d = rs.getDate(6);\n\t\tSystem.out.println(\"编号：\"+ id);\n\t\tSystem.out.println(\"姓名：\"+name);\n\t\tSystem.out.println(\"密码：\"+pass);\n\t\tSystem.out.println(\"年龄：\"+age);\n\t\tSystem.out.println(\"性别：\"+sex);\n\t\tSystem.out.println(\"生日：\"+d);\n\t}\n}\n\n```\n\n&nbsp;\n","tags":["JDBC"]},{"title":"Java数据库——使用元数据分析数据库","url":"/Java数据库——使用元数据分析数据库.html","content":"在JDBC中提供了DatabaseMetaData和ResultSetMetaData接口来分析数据库的元数据。\n\n<!--more-->\n&nbsp;\n\n**DatabaseMetaData**\n\n**<img src=\"/images/517519-20160322154851995-1206784721.png\" alt=\"\" />**\n\n&nbsp;\n\n**使用DatabaseMetaData取得数据库的元信息**\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.InputStream;\nimport java.sql.*;\nimport java.text.SimpleDateFormat;\nimport java.util.Scanner;\n\npublic class DatabaseMetaData_demo {\n\n\t//定义MySQL的数据库驱动程序\n\tpublic static final String DBDRIVER = \"org.gjt.mm.mysql.Driver\";\n\t//定义MySQL数据库的连接地址\n\tpublic static final String DBURL = \"jdbc:mysql://localhost:3306/mysql_demo\";\n\t//MySQL数据库的连接用户名\n\tpublic static final String DBUSER = \"root\";\n\t//MySQL数据库的连接密码\n\tpublic static final String DBPASS = \"123456\";\n\t\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t  Connection conn = null;                     //数据库连接\n      DatabaseMetaData dmd = null; //数据库元数据\n      ResultSet rs = null;                                //保存结果集\n       \n      Class.forName(DBDRIVER);            //加载驱动程序\n      //连接MySQL数据库时，要写上连接的用户名和密码\n      conn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n      dmd = conn.getMetaData();\t\t\t\t//实例化元数据\n      System.out.println(\"数据库名称：\"+dmd.getDatabaseProductName());\n      System.out.println(\"数据库版本：\"+dmd.getDriverMajorVersion()+\".\"+dmd.getDriverMinorVersion());\n      rs = dmd.getPrimaryKeys(null, null, \"user\");\t//得到表的主键\n      while(rs.next()){\n    \t  System.out.println(\"表类型：\"+rs.getString(1));\n    \t  System.out.println(\"表模式：\"+rs.getString(2));\n    \t  System.out.println(\"表名称：\"+rs.getString(3));\n    \t  System.out.println(\"列名称：\"+rs.getString(4));\n    \t  System.out.println(\"主键序列号：\"+rs.getString(5));\n    \t  System.out.println(\"主键名称：\"+rs.getString(6));\n      }\n      \n      conn.close();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**ResultSetMetaData**\n\n**<img src=\"/images/517519-20160322160309401-1104264056.png\" alt=\"\" />**\n\n&nbsp;\n\n<img src=\"/images/517519-20160322160352323-157937471.png\" alt=\"\" />\n\n&nbsp;\n","tags":["JDBC"]},{"title":"Java数据库——处理大数据对象","url":"/Java数据库——处理大数据对象.html","content":"**处理大数据对象**\n\n**CLOB中可以存储海量文字**\n\n**<strong>BLOB中可以存储海量二进制数据**</strong>\n\n**<strong>如果程序中要想处理这样的大对象操作，则必须使用<strong>PreparedStatement**完成，所有的内容要通过IO流的方式从大文本字段中保存和读取。</strong></strong>\n\n**<strong><img src=\"/images/517519-20160321211940667-71866615.png\" alt=\"\" />**</strong>\n\n<!--more-->\n&nbsp;\n\n&nbsp;\n\n**写入大文本数据**\n\n&nbsp;<img src=\"/images/517519-20160321225039526-173573384.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160321194706433-1906890709.png\" alt=\"\" />\n\n**汉字的编码要改成gbk**\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.InputStream;\nimport java.sql.*;\nimport java.text.SimpleDateFormat;\n\npublic class Clob_demo {\n\n\t//定义MySQL的数据库驱动程序\n\tpublic static final String DBDRIVER = \"org.gjt.mm.mysql.Driver\";\n\t//定义MySQL数据库的连接地址\n\tpublic static final String DBURL = \"jdbc:mysql://localhost:3306/mysql_demo\";\n\t//MySQL数据库的连接用户名\n\tpublic static final String DBUSER = \"root\";\n\t//MySQL数据库的连接密码\n\tpublic static final String DBPASS = \"123456\";\n\t\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tConnection conn = null;\t\t\t\t\t\t//数据库连接\n\t\tPreparedStatement pstmt = null;\t//数据库操作\n\n\t\tString name = \"张三\";\n\t\tString sql = \"INSERT INTO userclob(name,note) VALUES (?,?) \";\n\t\tClass.forName(DBDRIVER);\t\t\t//加载驱动程序\n\t\t//连接MySQL数据库时，要写上连接的用户名和密码\n\t\tconn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n\t\tpstmt = conn.prepareStatement(sql);\t\t//实例化PreparedStatement\n\t\t//声明一个File对象，用于找到要操作的大文本文件\n\t\tFile f = new File(\"/home/common/software/database/zhangsan.txt\");\n\t\tInputStream input = null;\t\t\t\t//通过输入流读取内容\n\t\tinput = new FileInputStream(f);\t\t//通过输入流读取文件\n\t\tpstmt.setString(1, name); \t\t\t\t//设置第一个&ldquo;？&rdquo;的内容\n\t\tpstmt.setAsciiStream(2,input, (int)f.length()); \t\t\t\t//设置输入流\n\t\tpstmt.executeUpdate();\t\t\t\t\t//执行数据库更新操作\n\t\tpstmt.close();\t\t\t\t\t\t\t\t\t\t//操作关闭\n\t\tconn.close();\t\t\t\t\t\t\t\t\t\t//数据库关闭\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**读取大文本字段**\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.InputStream;\nimport java.sql.*;\nimport java.text.SimpleDateFormat;\nimport java.util.Scanner;\n\npublic class Clob_demo {\n\n\t//定义MySQL的数据库驱动程序\n\tpublic static final String DBDRIVER = \"org.gjt.mm.mysql.Driver\";\n\t//定义MySQL数据库的连接地址\n\tpublic static final String DBURL = \"jdbc:mysql://localhost:3306/mysql_demo\";\n\t//MySQL数据库的连接用户名\n\tpublic static final String DBUSER = \"root\";\n\t//MySQL数据库的连接密码\n\tpublic static final String DBPASS = \"123456\";\n\t\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tConnection conn = null;\t\t\t\t\t\t//数据库连接\n\t\tPreparedStatement pstmt = null;\t//数据库操作\n\t\tResultSet rs = null;\t\t\t\t\t\t\t\t//保存结果集\n\t\t\n\t\tint id = 2;\n\t\tString sql = \"SELECT name,note FROM userclob WHERE id=?\";\n\t\tClass.forName(DBDRIVER);\t\t\t//加载驱动程序\n\t\t//连接MySQL数据库时，要写上连接的用户名和密码\n\t\tconn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n\t\tpstmt = conn.prepareStatement(sql);\t\t//实例化PreparedStatement\n\t\tpstmt.setInt(1, id);     \t\t\t\t\t\t\t\t\t//设置查询的id\n\t\trs = pstmt.executeQuery();\t\t\t\t\t\t\t//查询\n\t\t\n\t\twhile(rs.next()){\n\t\t\tString name = rs.getString(1);\n\t\t\tStringBuffer note = new StringBuffer();\n\t\t\tSystem.out.println(\"姓名：\"+name);\n\t\t\tInputStream input = rs.getAsciiStream(2);\t\t//接收全部的文本数据\n\t\t\tScanner scan = new Scanner(input);\t\t\t\t\t//接收数据\n\t\t\tscan.useDelimiter(\"\\r\\n\");\t\t\t\t\t\t\t\t\t\t//将文件换行作为分隔符\n\t\t\twhile(scan.hasNext()){\n\t\t\t\tnote.append(scan.next()).append(\"\\n\");\t\t\t//不断读取内容\n\t\t\t}\n\t\t\tSystem.out.println(\"内容：\"+note);\n\t\t\tinput.close();\n\t\t}\n\t\t\n\t\tpstmt.close();\t\t\t\t\t\t\t\t\t\t//操作关闭\n\t\tconn.close();\t\t\t\t\t\t\t\t\t\t//数据库关闭\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160321233619323-906633626.png\" alt=\"\" />\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.InputStream;\nimport java.sql.*;\nimport java.text.SimpleDateFormat;\nimport java.util.Scanner;\n\npublic class Clob_demo {\n\n\t//定义MySQL的数据库驱动程序\n\tpublic static final String DBDRIVER = \"org.gjt.mm.mysql.Driver\";\n\t//定义MySQL数据库的连接地址\n\tpublic static final String DBURL = \"jdbc:mysql://localhost:3306/mysql_demo\";\n\t//MySQL数据库的连接用户名\n\tpublic static final String DBUSER = \"root\";\n\t//MySQL数据库的连接密码\n\tpublic static final String DBPASS = \"123456\";\n\t\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n//\t\tConnection conn = null;\t\t\t\t\t\t//数据库连接\n//\t\tPreparedStatement pstmt = null;\t//数据库操作\n//\n//\t\tString name = \"张三\";\n//\t\tString sql = \"INSERT INTO userclob(name,note) VALUES (?,?) \";\n//\t\tClass.forName(DBDRIVER);\t\t\t//加载驱动程序\n//\t\t//连接MySQL数据库时，要写上连接的用户名和密码\n//\t\tconn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n//\t\tpstmt = conn.prepareStatement(sql);\t\t//实例化PreparedStatement\n//\t\t//声明一个File对象，用于找到要操作的大文本文件\n//\t\tFile f = new File(\"/home/common/software/database/无标题文档\");\n//\t\tInputStream input = null;\t\t\t\t//通过输入流读取内容\n//\t\tinput = new FileInputStream(f);\t\t//通过输入流读取文件\n//\t\tpstmt.setString(1, name); \t\t\t\t//设置第一个&ldquo;？&rdquo;的内容\n//\t\tpstmt.setAsciiStream(2,input, (int)f.length()); \t\t\t\t//设置输入流\n//\t\tpstmt.executeUpdate();\t\t\t\t\t//执行数据库更新操作\n//\t\tpstmt.close();\t\t\t\t\t\t\t\t\t\t//操作关闭\n//\t\tconn.close();\t\t\t\t\t\t\t\t\t\t//数据库关闭\n\t\t\n//\t\tConnection conn = null;\t\t\t\t\t\t//数据库连接\n//\t\tPreparedStatement pstmt = null;\t//数据库操作\n//\t\tResultSet rs = null;\t\t\t\t\t\t\t\t//保存结果集\n//\t\t\n//\t\tint id = 2;\n//\t\tString sql = \"SELECT name,note FROM userclob WHERE id=?\";\n//\t\tClass.forName(DBDRIVER);\t\t\t//加载驱动程序\n//\t\t//连接MySQL数据库时，要写上连接的用户名和密码\n//\t\tconn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n//\t\tpstmt = conn.prepareStatement(sql);\t\t//实例化PreparedStatement\n//\t\tpstmt.setInt(1, id);     \t\t\t\t\t\t\t\t\t//设置查询的id\n//\t\trs = pstmt.executeQuery();\t\t\t\t\t\t\t//查询\n//\t\t\n//\t\twhile(rs.next()){\n//\t\t\tString name = rs.getString(1);\n//\t\t\tStringBuffer note = new StringBuffer();\n//\t\t\tSystem.out.println(\"姓名：\"+name);\n//\t\t\tInputStream input = rs.getAsciiStream(2);\t\t//接收全部的文本数据\n//\t\t\tScanner scan = new Scanner(input);\t\t\t\t\t//接收数据\n//\t\t\tscan.useDelimiter(\"\\r\\n\");\t\t\t\t\t\t\t\t\t\t//将文件换行作为分隔符\n//\t\t\twhile(scan.hasNext()){\n//\t\t\t\tnote.append(scan.next()).append(\"\\n\");\t\t\t//不断读取内容\n//\t\t\t}\n//\t\t\tSystem.out.println(\"内容：\"+note);\n//\t\t\tinput.close();\n//\t\t}\n//\t\t\n//\t\tpstmt.close();\t\t\t\t\t\t\t\t\t\t//操作关闭\n//\t\tconn.close();\t\t\t\t\t\t\t\t\t\t//数据库关闭\n\t\t\n\t\tConnection conn = null;\t\t\t\t\t\t//数据库连接\n\t\tPreparedStatement pstmt = null;\t//数据库操作\n\t\tResultSet rs = null;\t\t\t\t\t\t\t\t//保存结果集\n\t\t\n\t\tint id = 2;\n\t\tString sql = \"SELECT name,note FROM userclob WHERE id=?\";\n\t\tClass.forName(DBDRIVER);\t\t\t//加载驱动程序\n\t\t//连接MySQL数据库时，要写上连接的用户名和密码\n\t\tconn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n\t\tpstmt = conn.prepareStatement(sql);\t\t//实例化PreparedStatement\n\t\tpstmt.setInt(1, id);     \t\t\t\t\t\t\t\t\t//设置查询的id\n\t\trs = pstmt.executeQuery();\t\t\t\t\t\t\t//查询\n\t\t\n\t\twhile(rs.next()){\t\n\t\t\tString name = rs.getString(1);\t\t\t//取出name列的内容\n\t\t\tClob c = rs.getClob(2);\t\t\t\t\t\t\t//取出大文本数据\n\t\t\tString note = c.getSubString(1, (int)c.length());\t//CLOB开始的位置为1\n\t\t\tSystem.out.println(\"姓名：\"+name);\n\t\t\tSystem.out.println(\"内容：\"+note);\n\t\t\tc.truncate(100);\n\t\t\tSystem.out.println(\"部分的读取内容：\"+c.getSubString(1,  (int)c.length()));\n\t\t}\n\t\t\n\t\tpstmt.close();\t\t\t\t\t\t\t\t\t\t//操作关闭\n\t\tconn.close();\t\t\t\t\t\t\t\t\t\t//数据库关闭\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**处理BLOB数据**\n\n**<img src=\"/images/517519-20160322082205683-1114963237.png\" alt=\"\" />**\n\n&nbsp;\n\n```\ncreate table userblob(id INT AUTO_INCREMENT PRIMARY KEY,name VARCHAR(30) NOT NULL,photo LONGBLOB);\n\n```\n\n&nbsp;\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.InputStream;\nimport java.sql.*;\nimport java.text.SimpleDateFormat;\nimport java.util.Scanner;\n\npublic class Blob_demo {\n\n\t//定义MySQL的数据库驱动程序\n\tpublic static final String DBDRIVER = \"org.gjt.mm.mysql.Driver\";\n\t//定义MySQL数据库的连接地址\n\tpublic static final String DBURL = \"jdbc:mysql://localhost:3306/mysql_demo\";\n\t//MySQL数据库的连接用户名\n\tpublic static final String DBUSER = \"root\";\n\t//MySQL数据库的连接密码\n\tpublic static final String DBPASS = \"123456\";\n\t\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\t\n//\t\tConnection conn = null;\t\t\t\t\t\t//数据库连接\n//\t\tPreparedStatement pstmt = null;\t//数据库操作\n//\t\t\n//\t\tString name = \"赵六\";\n//\t\tString sql = \"INSERT INTO userblob(name,photo) VALUES (?,?) \";\n//\t\tClass.forName(DBDRIVER);\t\t\t//加载驱动程序\n//\t\t//连接MySQL数据库时，要写上连接的用户名和密码\n//\t\tconn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n//\t\tpstmt = conn.prepareStatement(sql);\t\t//实例化PreparedStatement\n//\t\t//声明一个File对象，用于找到要操作的大文本文件\n//\t\tFile f = new File(\"/home/common/software/database/photo.jpg\");\n//\t\tInputStream input = null;\t\t\t\t//通过输入流读取内容\n//\t\tinput = new FileInputStream(f);\t\t//通过输入流读取文件\n//\t\tpstmt.setString(1, name); \t\t\t\t//设置第一个&ldquo;？&rdquo;的内容\n//\t\tpstmt.setBinaryStream(2,input, (int)f.length()); \t\t\t\t//设置输入流\n//\t\tpstmt.executeUpdate();\t\t\t\t\t//执行数据库更新操作\n//\t\tpstmt.close();\t\t\t\t\t\t\t\t\t\t//操作关闭\n//\t\tconn.close();\t\t\t\t\t\t\t\t\t\t//数据库关闭\n\t\t\n\t\tConnection conn = null;\t\t\t\t\t\t//数据库连接\n\t\tPreparedStatement pstmt = null;\t//数据库操作\n\t\tResultSet rs = null;\t\t\t\t\t\t\t\t//保存结果集\n\t\t\n\t\tint id = 1;\n\t\tString sql = \"SELECT name,photo FROM userblob WHERE id=?\";\n\t\tClass.forName(DBDRIVER);\t\t\t//加载驱动程序\n\t\t//连接MySQL数据库时，要写上连接的用户名和密码\n\t\tconn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n\t\tpstmt = conn.prepareStatement(sql);\t\t//实例化PreparedStatement\n\t\tpstmt.setInt(1, id);\t\t\t\t//设置查询的id\n\t\trs = pstmt.executeQuery();\t\t\t\t\t\t\t//查询\n\t\t\n\t\twhile(rs.next()){\n\t\t\tString name = rs.getString(1);\n\t\t\tStringBuffer note = new StringBuffer();\n\t\t\tSystem.out.println(\"姓名：\"+name);\n\t\t\tInputStream input = rs.getBinaryStream(2);\t\t//接收全部的大文本数据\n\t\t\tFileOutputStream out = null;\n\t\t\tout = new FileOutputStream(new File(\"/home/common/software/database/photo_copy.jpg\"));\n\t\t\tint temp = 0;\n\t\t\twhile((temp = input.read()) != -1){\t\t\t\t\t\t\t//边读边写\n\t\t\t\tout.write(temp);\n\t\t\t}\n\t\t\tinput.close();\n\t\t\tout.close();\n\t\t}\n\t\tpstmt.close();\t\t\t\t\t\t\t\t\t\t//操作关闭\n\t\tconn.close();\t\t\t\t\t\t\t\t\t\t//数据库关闭\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**使用Ｂlob读取内容**\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.InputStream;\nimport java.sql.*;\nimport java.text.SimpleDateFormat;\nimport java.util.Scanner;\n\npublic class Blob_demo {\n\n\t//定义MySQL的数据库驱动程序\n\tpublic static final String DBDRIVER = \"org.gjt.mm.mysql.Driver\";\n\t//定义MySQL数据库的连接地址\n\tpublic static final String DBURL = \"jdbc:mysql://localhost:3306/mysql_demo\";\n\t//MySQL数据库的连接用户名\n\tpublic static final String DBUSER = \"root\";\n\t//MySQL数据库的连接密码\n\tpublic static final String DBPASS = \"123456\";\n\t\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\t\t\t\n\t\tConnection conn = null;\t\t\t\t\t\t//数据库连接\n\t\tPreparedStatement pstmt = null;\t//数据库操作\n\t\tResultSet rs = null;\t\t\t\t\t\t\t\t//保存结果集\n\t\t\n\t\tint id = 1;\n\t\tString sql = \"SELECT name,photo FROM userblob WHERE id=?\";\n\t\tClass.forName(DBDRIVER);\t\t\t//加载驱动程序\n\t\t//连接MySQL数据库时，要写上连接的用户名和密码\n\t\tconn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n\t\tpstmt = conn.prepareStatement(sql);\t\t//实例化PreparedStatement\n\t\tpstmt.setInt(1, id);\t\t\t\t//设置查询的id\n\t\trs = pstmt.executeQuery();\t\t\t\t\t\t\t//查询\n\t\t\n\t\tif(rs.next()){\n\t\t\tString name = rs.getString(1);\n\t\t\tSystem.out.println(\"姓名：\"+name);\n\t\t\tBlob b = rs.getBlob(2);\t\t\t\t\t//读取Ｂlob数据\n\t\t\tFileOutputStream out = null;\n\t\t\tout = new FileOutputStream(new File(\"/home/common/software/database/photo_copy2.jpg\"));\n\t\t\tout.write(b.getBytes(1, (int)b.length()));\n\t\t\tout.close();\n\t\t}\n\t\tpstmt.close();\t\t\t\t\t\t\t\t\t\t//操作关闭\n\t\tconn.close();\t\t\t\t\t\t\t\t\t\t//数据库关闭\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["JDBC"]},{"title":"MySQL学习笔记——存储过程","url":"/MySQL学习笔记——存储过程.html","content":"**MySQL存储过程**是一个事先编译好并存储在数据库中的一组 SQL 语句集合，类似于程序中的子程序或函数。\n\n存储过程可以接受参数、执行一系列 SQL 语句，并返回结果。存储过程可以大大简化复杂的查询和数据处理操作，并提高数据库性能和安全性。\n\n假设有3张表，歌曲表song，歌手表singer，歌曲和歌手的关联表song_singer\n\n```\n# 歌曲\ncreate table `default`.song(\n    id int PRIMARY KEY auto_increment,\n    name varchar(256) UNIQUE\n) engine=InnoDB default charset=utf8;\n\n# 歌手\ncreate table `default`.singer(\n    id int PRIMARY KEY auto_increment,\n    name varchar(256) UNIQUE\n) engine=InnoDB default charset=utf8;\n\n# 关联表\ncreate table `default`.song_singer(\n    id int PRIMARY KEY auto_increment,\n    song_id int,\n    singer_id int,\n    UNIQUE KEY unique_index_name (song_id, singer_id)\n) engine=InnoDB default charset=utf8;\n\n```\n\n现在要写一个存储过程，同时添加歌曲，歌手，歌曲和歌手的关联表数据\n\n```\nCREATE PROCEDURE add_song_and_singer(\n    IN song_name VARCHAR(256) CHARACTER SET utf8 COLLATE utf8_general_ci,\n    IN singer_name VARCHAR(256) CHARACTER SET utf8 COLLATE utf8_general_ci\n)\nBEGIN\n\n    DECLARE result_code INTEGER DEFAULT 0; -- 定义返回结果并赋初值0\n    DECLARE CONTINUE HANDLER FOR SQLEXCEPTION SET result_code=1; -- 在执行过程中出任何异常设置result_code为1\n    DECLARE CONTINUE HANDLER FOR NOT FOUND SET result_code = 2; -- 如果表中没有下一条数据则置为2\n\n    START TRANSACTION;\n\n    # 写入歌曲\n    INSERT INTO song (name) VALUES (song_name);\n    SET @new_song_id = LAST_INSERT_ID();\n\n    # 写入歌手\n    INSERT INTO singer (name) VALUES (singer_name);\n    SET @new_singer_id = LAST_INSERT_ID();\n\n    # 写入关联表\n    INSERT INTO song_singer (song_id, singer_id) VALUES (@new_song_id, @new_singer_id);\n\n    -- 可以根据不同的业务逻辑错误返回不同的result_code，这里只定义了1和0\n    IF result_code = 1 THEN\n        # 回滚\n        ROLLBACK;\n        select 'transaction roll back';\n    ELSE\n        # 提交事务\n        COMMIT;\n    END IF;\n\nEND;\n\n```\n\n调用存储过程\n\n```\ncall add_song('发如雪', '周杰伦');\n\n```\n\n如果过程中任何SQL失败，则输出\n\n```\ntransaction roll back\n\n```\n\n参考：[MySQL 存储过程](https://www.runoob.com/w3cnote/mysql-stored-procedure.html)\n\n[MySQL存储过程中实现回滚](https://www.cnblogs.com/lujiulong/p/6000773.html)\n\n<!--more-->\n&nbsp;\n","tags":["MySQL"]},{"title":"Play学习笔记——Hello World","url":"/Play学习笔记——Hello World.html","content":"参考\n\n```\nhttps://github.com/lupingqiu/metadata-scala-play\n\n```\n\n在project文件夹下添加\n\nbuild.properties\n\n```\nsbt.version=1.2.1\n\n```\n\nplugins.sbt\n\n```\n// The Typesafe repository\nresolvers += \"Typesafe repository\" at \"https://repo.typesafe.com/typesafe/releases/\"\n\n// Typesafe snapshots\nresolvers += \"Typesafe Snapshots\" at \"https://repo.typesafe.com/typesafe/snapshots/\"\n\n// Use the Play sbt plugin for Play projects\naddSbtPlugin(\"com.typesafe.play\" % \"sbt-plugin\" % \"2.6.12\")\n\n```\n\n运行\n\n```\nsbt\nrun -Dhttp.port=8080\n\n```\n\n　　\n","tags":["Play"]},{"title":"SpringBoot学习笔记——swagger-ui","url":"/SpringBoot学习笔记——swagger-ui.html","content":"swagger-ui用于给API添加文档，还支持API的请求调用，可以降低前后端联调的沟通成本\n\n<img src=\"/images/517519-20210604153416620-1580870078.png\" width=\"1000\" height=\"339\" loading=\"lazy\" />\n\n1.依赖\n\n```\n<!-- swagger2 -->\n<dependency>\n    <groupId>io.springfox</groupId>\n    <artifactId>springfox-swagger2</artifactId>\n    <version>2.8.0</version>\n</dependency>\n<dependency>\n    <groupId>io.springfox</groupId>\n    <artifactId>springfox-swagger-ui</artifactId>\n    <version>2.8.0</version>\n</dependency>\n\n```\n\n2.配置swagger，注意修改basePackage成实际的包名\n\n```\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.web.context.request.async.DeferredResult;\nimport springfox.documentation.builders.ApiInfoBuilder;\nimport springfox.documentation.builders.PathSelectors;\nimport springfox.documentation.builders.RequestHandlerSelectors;\nimport springfox.documentation.service.ApiInfo;\nimport springfox.documentation.spi.DocumentationType;\nimport springfox.documentation.spring.web.plugins.Docket;\nimport springfox.documentation.swagger2.annotations.EnableSwagger2;\n\n@Configuration\n@EnableSwagger2\npublic class SwaggerConfig {\n\n    @Bean\n    public Docket createRestApi() {\n        return new Docket(DocumentationType.SWAGGER_2)\n                .genericModelSubstitutes(DeferredResult.class)\n                .useDefaultResponseMessages(false)\n                .forCodeGeneration(true)\n                .apiInfo(apiInfo())\n                .pathMapping(\"/\") // base，最终调用接口后会和paths拼接在一起\n                .select()\n                .apis(RequestHandlerSelectors.basePackage(\"com.example.demo\")) // 过滤的接口\n                .paths(PathSelectors.any())\n                .build();\n    }\n\n    private ApiInfo apiInfo() {\n        return new ApiInfoBuilder()\n                .title(\"api文档\") // 标题\n                .description(\"xxxx\") // 描述\n                .termsOfServiceUrl(\"https://xxx.xxx.xxx\")\n                .version(\"1.0\")\n                .build();\n    }\n\n}\n\n```\n\n3.给controller添加<!--more-->\n&nbsp;ApiOperation 注解\n\n```\n@ApiOperation(value = \"hello接口\", notes = \"取得id，打印hello\")\n@RequestMapping(path = \"/hello/{id}\", method = RequestMethod.GET)\npublic ControllerResponseT hello(@ApiParam(name = \"id\", value = \"id\", required = true) @PathVariable(\"id\") Integer id) {\nreturn ControllerResponseT.ofSuccess(\"hello: \" + id);\n}\n\n```\n\n4.测试\n\n<img src=\"/images/517519-20210604174408696-1043600631.png\" width=\"800\" height=\"429\" loading=\"lazy\" />\n\n&nbsp;\n\n如果在添加了统一的接口返回值配置之后出现swagger-ui.html 404，需要额外添加如下配置\n\n```\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.web.servlet.config.annotation.ResourceHandlerRegistry;\nimport org.springframework.web.servlet.config.annotation.WebMvcConfigurer;\n\n@Configuration\npublic class WebAppConfig implements WebMvcConfigurer {\n\n    @Override\n    public void addResourceHandlers(ResourceHandlerRegistry registry) {\n        registry.addResourceHandler(\"/**\")\n                .addResourceLocations(\"classpath:/public/\");\n        registry.addResourceHandler(\"swagger-ui.html\")\n                .addResourceLocations(\"classpath:/META-INF/resources/\");\n        registry.addResourceHandler(\"/webjars/**\")\n                .addResourceLocations(\"classpath:/META-INF/resources/webjars/\");\n    }\n}\n\n```\n\n　　\n","tags":["SpringBoot"]},{"title":"Java——组件：标签组件，文本组件，菜单组件，表格组件，按钮组件","url":"/Java——组件：标签组件，文本组件，菜单组件，表格组件，按钮组件.html","content":"<!--more-->\n&nbsp;\n\n<img src=\"/images/517519-20160322170004667-119370150.png\" alt=\"\" />\n\n&nbsp;\n\n**使用一个标签**\n\n```\nimport java.awt.Color;\nimport java.awt.Dimension;\nimport java.awt.Point;\n\nimport javax.swing.JFrame;\nimport javax.swing.JLabel;\n\npublic class JFrame_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tJFrame f = new JFrame(\"Swing窗口\");\t\t\t\t\t\t\t//实例化窗体对象\n\t\tJLabel lab = new JLabel(\"Label\",JLabel.CENTER);\t\t//实例化对象，使用剧中对齐\n\t\tf.add(lab);\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//向容器中加入组建\n\t\tDimension dim = new Dimension();\t\t\t\t\t\t\t\t//实例化Dimension对象\n\t\t\n\t\tdim.setSize(230,80);\t\t\t//设置大小\n\t\tf.setSize(dim);\t\t\t\t\t\t//设置组件大小\n\t\tf.setBackground(Color.WHITE);\n\t\tPoint point = new Point(300,200);\t\t//设置现实的坐标点\n\t\tf.setLocation(point);\n\t\tf.setVisible(true);\n\t}\n\n}\n\n```\n\n&nbsp;\n\n&nbsp;\n\n**更改JLabel的文本样式**\n\n**<img src=\"/images/517519-20160322215153964-1773363108.png\" alt=\"\" />**\n\n&nbsp;\n\n**设置标签的显示字体、大小、背景颜色**\n\n```\nimport java.awt.Color;\nimport java.awt.Dimension;\nimport java.awt.Font;\nimport java.awt.Point;\n\nimport javax.swing.JFrame;\nimport javax.swing.JLabel;\n\npublic class JFrame_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tJFrame f = new JFrame(\"Swing窗口\");\t\t\t\t\t\t\t//实例化窗体对象\n\t\tJLabel lab = new JLabel(\"Label\",JLabel.CENTER);\t\t//实例化对象，使用剧中对齐\n\t\tFont fnt = new Font(\"Serief\",Font.ITALIC+Font.BOLD,28);\n\t\tlab.setFont(fnt); \t\t\t\t\t\t\t\t\t//设置标签的显示字体\n\t\tlab.setForeground(Color.RED); \t//设置标签的文字颜色\n\t\tf.add(lab);\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//向容器中加入组建\n\t\tDimension dim = new Dimension();\t\t\t\t\t\t\t\t//实例化Dimension对象\n\t\t\n\t\tdim.setSize(230,80);\t\t\t//设置大小\n\t\tf.setSize(dim);\t\t\t\t\t\t//设置组件大小\n\t\tf.setBackground(Color.WHITE);\n\t\tPoint point = new Point(300,200);\t\t//设置现实的坐标点\n\t\tf.setLocation(point);\n\t\tf.setVisible(true);\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160322215855370-317888727.png\" alt=\"\" />\n\n&nbsp;\n\n**在JLabel中设置图片**\n\n**<img src=\"/images/517519-20160322220144339-1860416016.png\" alt=\"\" />**\n\n&nbsp;\n\n```\nimport java.awt.Color;\nimport java.awt.Dimension;\nimport java.awt.Font;\nimport java.awt.Point;\n\nimport javax.swing.Icon;\nimport javax.swing.ImageIcon;\nimport javax.swing.JFrame;\nimport javax.swing.JLabel;\n\npublic class JFrame_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tJFrame f = new JFrame(\"Swing窗口\");\t\t\t\t\t\t\t//实例化窗体对象\n\t\tString picPath = \"/home/common/software/database/photo.jpg\";\n\t\tIcon icon = new ImageIcon(picPath);\n\t\t\n\t\tJLabel lab = new JLabel(icon,JLabel.CENTER);\t\t//实例化对象，使用剧中对齐\n\t\tFont fnt = new Font(\"Serief\",Font.ITALIC+Font.BOLD,28);\n\t\tlab.setFont(fnt); \t\t\t\t\t\t\t\t\t//设置标签的显示字体\n\t\tlab.setForeground(Color.RED); \t//设置标签的文字颜色\n\t\tf.add(lab);\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//向容器中加入组建\n\t\tDimension dim = new Dimension();\t\t\t\t\t\t\t\t//实例化Dimension对象\n\t\t\n\t\tdim.setSize(230,80);\t\t\t//设置大小\n\t\tf.setSize(dim);\t\t\t\t\t\t//设置组件大小\n\t\tf.setBackground(Color.WHITE);\n\t\tPoint point = new Point(300,200);\t\t//设置现实的坐标点\n\t\tf.setLocation(point);\n\t\tf.setVisible(true);\n\t\t\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160323190326917-178714958.png\" alt=\"\" />\n\n&nbsp;\n\n<img src=\"/images/517519-20160323190434120-208037545.png\" alt=\"\" />\n\n&nbsp;\n\n```\nimport java.awt.GridLayout;\n\nimport javax.swing.JFrame;\nimport javax.swing.JLabel;\nimport javax.swing.JTextField;\n\npublic class JText_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tJFrame f = new JFrame(\"Swing窗口\");\t\t\t\t\t\t\t//实例化窗体对象\n\t\tJTextField name = new JTextField(30);\t\t\t\t\t\t\t//定义文本框，并指定长度\n\t\tJTextField name_1 = new JTextField(\"指定内容\",10);//定义文本框，并指定内容和长度\n\t\tJLabel nameLab = new JLabel(\"输入用户姓名：\");\t//\t定义标签\n\t\tJLabel name_1Lab = new JLabel(\"不可编辑文本：\");\t//\t定义标签\n\t\tname_1.setEnabled(false);\t\t\t\t//此文本框不可编辑\n\t\tnameLab.setBounds(10,10,100,20); \t\t//设置组件位置和大小\n\t\tname_1Lab.setBounds(10,40,100,20); \t\t//设置组件位置和大小\n\t\tname.setBounds(110,10,100,20); \t\t//设置组件位置和大小\n\t\tname_1.setBounds(110,40,100,20); \t\t//设置组件位置和大小\n\t\tname.setColumns(10); \t\t\t\t\t\t//设置长度，但是此时不起作用\n\t\tname_1.setColumns(10); \t\t\t\t\t//设置长度，但是此时不起作用\n//\t\tf.setLayout(new GridLayout(2,2));\t\t//设置容器的布局管理器\n\t\tf.setLayout(null);\t\t//设置容器的布局管理器\n\t\tf.add(nameLab);\t\t\t\t\t//向容器中增加组件\n\t\tf.add(name);\t\t\t\t\t\t//向容器中增加组件\n\t\tf.add(name_1Lab);\t\t\t\t\t//向容器中增加组件\n\t\tf.add(name_1);\t\t\t\t\t\t//向容器中增加组件\n\t\tf.setSize(440, 320);  \t\t\t//设置窗体\n\t\tf.setLocation(300,200);\t\t//设置显示位置\n\t\tf.setVisible(true);\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160323230242526-1631709652.png\" alt=\"\" />\n\n```\nimport javax.swing.JFrame;\nimport javax.swing.JLabel;\nimport javax.swing.JPasswordField;\nimport javax.swing.JTextField;\n\npublic class JPassword_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tJFrame f = new JFrame(\"Swing窗口\");\t\t\t\t\t\t\t//实例化窗体对象\n\t\tJPasswordField jpf1 = new JPasswordField();\t\t\t//定义秘文框\n\t\tJPasswordField jpf2 = new JPasswordField();\t\t\t//定义秘文框\n\t\tjpf2.setEchoChar('#');  \t\t\t\t\t\t//设置回显字符\n\t\tJLabel nameLab = new JLabel(\"默认的显示：\");\t//\t定义标签\n\t\tJLabel name_1Lab = new JLabel(\"修改后的显示：\");\t//\t定义标签\n\t\t\n\t\tnameLab.setBounds(10,10,100,20); \t\t//设置组件位置和大小\n\t\tname_1Lab.setBounds(10,40,100,20); \t\t//设置组件位置和大小\n\t\tjpf1.setBounds(110,10,100,20); \t\t//设置组件位置和大小\n\t\tjpf2.setBounds(110,40,100,20); \t\t//设置组件位置和大小\n\n\t\tf.setLayout(null);\t\t//设置容器的布局管理器\n\t\tf.add(nameLab);\t\t\t\t\t//向容器中增加组件\n\t\tf.add(jpf1);\t\t\t\t\t\t//向容器中增加组件\n\t\tf.add(name_1Lab);\t\t\t\t\t//向容器中增加组件\n\t\tf.add(jpf2);\t\t\t\t\t\t//向容器中增加组件\n\t\tf.setSize(440, 320);  \t\t\t//设置窗体\n\t\tf.setLocation(300,200);\t\t//设置显示位置\n\t\tf.setVisible(true);\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160323230925761-1171054922.png\" alt=\"\" />\n\n```\nimport javax.swing.JFrame;\nimport javax.swing.JLabel;\nimport javax.swing.JTextArea;\nimport javax.swing.JTextField;\n\npublic class JTextArea_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tJFrame f = new JFrame(\"Swing窗口\");\t\t\t\t\t\t\t//实例化窗体对象\n\t\tJTextArea name = new JTextArea(\"1231231231231312312\",3,10);\t\t//定义文本框\n\t\tname.setLineWrap(true); \t\t\t\t\t//如果内容过长，自动换行\n\t\tJLabel nameLab = new JLabel(\"多行文本域\");\t//\t定义标签\n\n\t\tnameLab.setBounds(10,10,120,20); \t\t//设置组件位置和大小\n\t\tname.setBounds(130,10,150,100); \t\t//设置组件位置和大小\n\t\tf.setLayout(null);\t\t//设置容器的布局管理器\n\t\tf.add(nameLab);\t\t\t\t\t//向容器中增加组件\n\t\tf.add(name);\t\t\t\t\t\t//向容器中增加组件\n\t\tf.setSize(440, 320);  \t\t\t//设置窗体\n\t\tf.setLocation(300,200);\t\t//设置显示位置\n\t\tf.setVisible(true);\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160325110340464-809406451.png\" alt=\"\" />\n\n&nbsp;\n\n```\nimport java.awt.Container;\nimport java.awt.event.WindowAdapter;\nimport java.awt.event.WindowEvent;\n\nimport javax.swing.ImageIcon;\nimport javax.swing.JComboBox;\nimport javax.swing.JFrame;\nimport javax.swing.JLabel;\nimport javax.swing.JMenu;\nimport javax.swing.JMenuBar;\nimport javax.swing.JMenuItem;\nimport javax.swing.JScrollPane;\nimport javax.swing.JTextArea;\nimport javax.swing.KeyStroke;\n\npublic class Jmenu_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tJFrame frame = new JFrame(\"窗体\");\t\t\t\t\t//定义窗体\n\t\tJTextArea text = new JTextArea();\t\t\t\t\t\t\t//定义文本域\n\t\ttext.setEditable(true); \t\t\t\t\t\t\t\t\t\t\t\t//定义文本组件可编辑\n\t\tframe.getContentPane().add(new JScrollPane(text)); \t//在面板中加入文本框及滚动条\n\t\t\n\t\tJMenu menuFile = new JMenu(\"文件\");\t\t\t\t//定义JMenu组件\n\t\t//定义显示图标\n\t\tmenuFile.setIcon(new ImageIcon(\"/home/common/software/database/123.jpg\"));\n\t\tJMenuBar menuBar = new JMenuBar();\t\t\t\t//定义JMenu组件\n\t\t//定义显示图标\n\t\tJMenuItem newMenu = new JMenuItem(\"新建\",new ImageIcon(\"/home/common/software/database/123.jpg\"));\n\t\t//定义显示图标\n\t\tJMenuItem openMenu = new JMenuItem(\"打开\",new ImageIcon(\"/home/common/software/database/123.jpg\"));\n\t\t\n\t\tnewMenu.setMnemonic('N');\n\t\topenMenu.setMnemonic('O');\n\t\t\n\t\tnewMenu.setAccelerator(KeyStroke.getKeyStroke('N',java.awt.Event.CTRL_MASK));\n\t\topenMenu.setAccelerator(KeyStroke.getKeyStroke('O',java.awt.Event.CTRL_MASK));\n\t\t\n\t\tmenuFile.add(newMenu);\t\t\t//加入菜单项\n\t\tmenuFile.addSeparator();\t\t\t//加入分割线\n\t\tmenuFile.add(openMenu);\t\t\t//加入菜单项\n\t\t\n\t\tmenuBar.add(menuFile);\t\t\t\t//加入JMenu\n\t\tframe.addWindowListener(new WindowAdapter(){\t\t//加入动作监听\n\t\t\tpublic void windowClosing(WindowEvent e) {\t\t\t//窗口关闭时触发，按下关闭按钮\n\t\t\t// TODO 自动生成的方法存根\n\t\t\tSystem.out.println(\"windowClosing-->窗口关闭\");\n\t\t\tSystem.exit(1);\n\t\t\t}\n\t\t});\n\t\tframe.setJMenuBar(menuBar); \t\t\t//在窗体中加入JMenuBar组件\n\t\tframe.setVisible(true);\n\t\tframe.setLocation(300,200);\t\t//设置显示位置\n\t\tframe.setSize(300, 180);\n\t\t\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160325170741604-562191370.png\" alt=\"\" />\n\n```\nimport java.awt.Container;\nimport java.awt.event.ActionEvent;\nimport java.awt.event.ActionListener;\nimport java.awt.event.WindowAdapter;\nimport java.awt.event.WindowEvent;\n\nimport javax.swing.ImageIcon;\nimport javax.swing.JComboBox;\nimport javax.swing.JFrame;\nimport javax.swing.JLabel;\nimport javax.swing.JMenu;\nimport javax.swing.JMenuBar;\nimport javax.swing.JMenuItem;\nimport javax.swing.JScrollPane;\nimport javax.swing.JTextArea;\nimport javax.swing.KeyStroke;\n\npublic class Jmenu_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tJFrame frame = new JFrame(\"窗体\");\t\t\t\t\t//定义窗体\n\t\tJTextArea text = new JTextArea();\t\t\t\t\t\t\t//定义文本域\n\t\ttext.setEditable(true); \t\t\t\t\t\t\t\t\t\t\t\t//定义文本组件可编辑\n\t\tframe.getContentPane().add(new JScrollPane(text)); \t//在面板中加入文本框及滚动条\n\t\t\n\t\tJMenu menuFile = new JMenu(\"文件\");\t\t\t\t//定义JMenu组件\n\t\t//定义显示图标\n\t\tmenuFile.setIcon(new ImageIcon(\"/home/common/software/database/123.jpg\"));\n\t\tJMenuBar menuBar = new JMenuBar();\t\t\t\t//定义JMenu组件\n\t\t//定义显示图标\n\t\tJMenuItem newMenu = new JMenuItem(\"新建\",new ImageIcon(\"/home/common/software/database/123.jpg\"));\n\t\t//定义显示图标\n\t\tJMenuItem openMenu = new JMenuItem(\"打开\",new ImageIcon(\"/home/common/software/database/123.jpg\"));\n\t\t\n\t\tnewMenu.setMnemonic('N');\n\t\topenMenu.setMnemonic('O');\n\t\t\n\t\tnewMenu.setAccelerator(KeyStroke.getKeyStroke('N',java.awt.Event.CTRL_MASK));\n\t\topenMenu.setAccelerator(KeyStroke.getKeyStroke('O',java.awt.Event.CTRL_MASK));\n\t\t\n\t\tmenuFile.add(newMenu);\t\t\t//加入菜单项\n\t\tmenuFile.addSeparator();\t\t\t//加入分割线\n\t\tmenuFile.add(openMenu);\t\t\t//加入菜单项\n\t\t\n\t\tmenuBar.add(menuFile);\t\t\t\t//加入JMenu\n\t\t\n\t\tnewMenu.addActionListener(new ActionListener() {\n\t\t\t\n\t\t\t@Override\n\t\t\tpublic void actionPerformed(ActionEvent e) {\n\t\t\t\t// TODO 自动生成的方法存根\n\t\t\t\ttext.append(\"选择了newMenu\");\n\t\t\t}\n\t\t});\n\t\t\n\t\topenMenu.addActionListener(new ActionListener() {\n\t\t\t\n\t\t\t@Override\n\t\t\tpublic void actionPerformed(ActionEvent e) {\n\t\t\t\t// TODO 自动生成的方法存根\n\t\t\t\ttext.append(\"选择了openMenu\");\n\t\t\t}\n\t\t});\n\t\t\n\t\tframe.addWindowListener(new WindowAdapter(){\t\t//加入动作监听\n\t\t\tpublic void windowClosing(WindowEvent e) {\t\t\t//窗口关闭时触发，按下关闭按钮\n\t\t\t// TODO 自动生成的方法存根\n\t\t\tSystem.out.println(\"windowClosing-->窗口关闭\");\n\t\t\tSystem.exit(1);\n\t\t\t}\n\t\t});\n\t\t\n\t\tframe.setJMenuBar(menuBar); \t\t\t//在窗体中加入JMenuBar组件\n\t\t\n\t\tframe.setVisible(true);\n\t\tframe.setLocation(300,200);\t\t//设置显示位置\n\t\tframe.setSize(300, 180);\n\t\t\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160325191828214-76173339.png\" alt=\"\" />\n\n&nbsp;\n\n```\nimport java.awt.event.WindowAdapter;\nimport java.awt.event.WindowEvent;\n\nimport javax.swing.JFrame;\nimport javax.swing.JScrollPane;\nimport javax.swing.JTable;\n\npublic class JTable_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tJFrame frame = new JFrame(\"窗体\");\t\t\t\t\t//定义窗体\n\t\tString[] titles = {\"姓名\",\"年龄\",\"性别\",\"成绩\",\"是否及格\"};\n\t\tObject[][] userInfo = {{\"张三\",22,\"男\",99,true},{\"张三\",22,\"男\",99,true}\n\t\t,{\"张三\",22,\"男\",99,true},{\"张三\",22,\"男\",99,true},{\"张三\",22,\"男\",99,true}};\n\t\t\n\t\tJTable table = new JTable(userInfo,titles);\n\t\t\n\t\tJScrollPane scr = new JScrollPane(table); \n\t\t\n\t\tframe.add(scr);\n\t\tframe.setSize(330,100);\n\t\tframe.setVisible(true);\n\t\tframe.addWindowListener(new WindowAdapter(){\t//加入事件监听\n\t\t\tpublic void windowClosing(WindowEvent arg0) {\t\t\t//窗口关闭时触发，按下关闭按钮\n\t\t\t// TODO 自动生成的方法存根\n\t\t\tSystem.out.println(\"windowClosing-->窗口关闭\");\n\t\t\tSystem.exit(1);\n\t\t\t}\n\t\t});\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160322221129870-1542079507.png\" alt=\"\" />\n\n&nbsp;\n\n```\nimport java.awt.Color;\nimport java.awt.Dimension;\nimport java.awt.Font;\nimport java.awt.Point;\n\nimport javax.swing.Icon;\nimport javax.swing.ImageIcon;\nimport javax.swing.JButton;\nimport javax.swing.JFrame;\nimport javax.swing.JLabel;\n\npublic class JFrame_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tJFrame f = new JFrame(\"Swing窗口\");\t\t\t\t\t\t\t//实例化窗体对象\n\t\tString picPath = \"/home/common/software/database/photo.jpg\";\n\t\tIcon icon = new ImageIcon(picPath);\n\t\t\n\t\tJButton but = new JButton(icon);\n\t\tf.add(but);\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//向容器中加入组建\n\t\tDimension dim = new Dimension();\t\t\t\t\t\t\t\t//实例化Dimension对象\n\t\t\n\t\tdim.setSize(230,80);\t\t\t//设置大小\n\t\tf.setSize(dim);\t\t\t\t\t\t//设置组件大小\n\t\tf.setBackground(Color.WHITE);\n\t\tPoint point = new Point(300,200);\t\t//设置现实的坐标点\n\t\tf.setLocation(point);\n\t\tf.setVisible(true);\n\t\t\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160323185240808-10315153.png\" alt=\"\" />\n\n```\nimport java.awt.GridLayout;\n\nimport javax.swing.JFrame;\nimport javax.swing.JToggleButton;\n\npublic class JToggleButton_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tJFrame f = new JFrame(\"Swing窗口\");\t\t\t\t\t\t\t//实例化窗体对象\n\t\tJToggleButton but1 = new JToggleButton(\"已选中\",true);\n\t\tJToggleButton but2 = new JToggleButton(\"未选中\");\n\t\tJToggleButton but3 = new JToggleButton(\"按我\");\n\t\tf.setLayout(new GridLayout(3,1));\t\t\t\t//设置排版样式\n\t\tf.add(but1);\n\t\tf.add(but2);\n\t\tf.add(but3);\n\t\tf.setSize(440, 320);  \t\t\t//设置窗体\n\t\tf.setLocation(300,200);\t\t//设置显示位置\n\t\tf.setVisible(true);\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160324162156901-886785836.png\" alt=\"\" />\n\n&nbsp;\n\n```\nimport java.awt.Container;\nimport java.awt.GridLayout;\nimport java.awt.event.WindowAdapter;\nimport java.awt.event.WindowEvent;\n\nimport javax.swing.BorderFactory;\nimport javax.swing.ButtonGroup;\nimport javax.swing.JFrame;\nimport javax.swing.JPanel;\nimport javax.swing.JRadioButton;\n\nclass MyRadio{\n\tprivate JFrame frame = new JFrame(\"窗体\");\n\tprivate Container cont = frame.getContentPane();\n\tprivate JRadioButton jrb1  = new JRadioButton(\"单选1\");\n\tprivate JRadioButton jrb2  = new JRadioButton(\"单选2\"); \n\tprivate JRadioButton jrb3  = new JRadioButton(\"单选3\"); \n\tprivate JPanel pan = new JPanel();\n\tpublic MyRadio(){\n\t\t//定义一个面板的边框显示条\n\t\tpan.setBorder(BorderFactory.createTitledBorder(\"请选择按钮\"));\n\t\tpan.setLayout(new GridLayout(1,3));\n\t\tButtonGroup group = new ButtonGroup(); \n\t\tgroup.add(this.jrb1);\n\t\tgroup.add(this.jrb2);\n\t\tgroup.add(this.jrb3);\n\t\tpan.add(this.jrb1);\n\t\tpan.add(this.jrb2);\n\t\tpan.add(this.jrb3);\n\t\tcont.add(pan);\n\t\tthis.frame.setSize(330,80);\n\t\tthis.frame.setVisible(true);\n\t\tthis.frame.addWindowListener(new WindowAdapter(){\n\t\t\tpublic void windowClosing(WindowEvent e) {\t\t\t//窗口关闭时触发，按下关闭按钮\n\t\t\t// TODO 自动生成的方法存根\n\t\t\tSystem.out.println(\"windowClosing-->窗口关闭\");\n\t\t\tSystem.exit(1);\n\t\t\t}\n\t\t});\n\t}\n}\n\n\n//主类\n//Function        : \tJRadioButton_demo\npublic class JRadioButton_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tnew MyRadio();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n&nbsp;<img src=\"/images/517519-20160324162156901-886785836.png\" alt=\"\" />\n\n```\nimport java.awt.Container;\nimport java.awt.GridLayout;\nimport java.awt.event.ItemEvent;\nimport java.awt.event.ItemListener;\nimport java.awt.event.WindowAdapter;\nimport java.awt.event.WindowEvent;\n\nimport javax.swing.BorderFactory;\nimport javax.swing.ButtonGroup;\nimport javax.swing.JFrame;\nimport javax.swing.JPanel;\nimport javax.swing.JRadioButton;\n\nclass MyRadio implements ItemListener{\n\tprivate JFrame frame = new JFrame(\"窗体\");\n\tprivate Container cont = frame.getContentPane();\n\tprivate JRadioButton jrb1  = new JRadioButton(\"单选1\");\n\tprivate JRadioButton jrb2  = new JRadioButton(\"单选2\"); \n\tprivate JRadioButton jrb3  = new JRadioButton(\"单选3\"); \n\tprivate JPanel pan = new JPanel();\n\tpublic MyRadio(){\n\t\t//定义一个面板的边框显示条\n\t\tpan.setBorder(BorderFactory.createTitledBorder(\"请选择按钮\"));\n\t\tpan.setLayout(new GridLayout(1,3));\n\t\tButtonGroup group = new ButtonGroup(); \n\t\tgroup.add(this.jrb1);\n\t\tgroup.add(this.jrb2);\n\t\tgroup.add(this.jrb3);\n\t\tpan.add(this.jrb1);\n\t\tpan.add(this.jrb2);\n\t\tpan.add(this.jrb3);\n\t\t\n\t\tjrb1.addItemListener(this); \t\t\t//加入事件监听\n\t\tjrb2.addItemListener(this); \t\t\t//加入事件监听\n\t\tjrb3.addItemListener(this); \t\t\t//加入事件监听\n\t\t\n\t\tcont.add(pan);\n\t\tthis.frame.setSize(330,80);\n\t\tthis.frame.setVisible(true);\n\t\tthis.frame.addWindowListener(new WindowAdapter(){\n\t\t\tpublic void windowClosing(WindowEvent e) {\t\t\t//窗口关闭时触发，按下关闭按钮\n\t\t\t// TODO 自动生成的方法存根\n\t\t\tSystem.out.println(\"windowClosing-->窗口关闭\");\n\t\t\tSystem.exit(1);\n\t\t\t}\n\t\t});\n\t}\n\t@Override\n\tpublic void itemStateChanged(ItemEvent e) {\n\t\t// TODO 自动生成的方法存根\n\t\tif(e.getSource() == jrb1){\n\t\t\tSystem.out.println(\"jrb1\");\n\t\t}else if(e.getSource() == jrb2){\n\t\t\tSystem.out.println(\"jrb2\");\n\t\t}else{\n\t\t\tSystem.out.println(\"jrb3\");\n\t\t}\n\t}\n}\n\n\n//主类\n//Function        : \tJRadioButton_demo\npublic class JRadioButton_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tnew MyRadio();\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["Java"]},{"title":"Java数据库——PreparedStatement接口","url":"/Java数据库——PreparedStatement接口.html","content":"**PreparedStatement接口**是Statement的子接口，属于**预处理操作**，与直接使用Statement不同的是，PreparedStatement在操作时，是先在数据表中准备好了一条SQL语句，但是此SQL语句的具体内容暂时不设置，而是之后再进行设置。\n\n<img src=\"/images/517519-20160321194706433-1906890709.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160321194742792-1087355534.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n**使用PreparedStatement完成数据的增加和查询操作**\n\n```\nimport java.sql.*;\nimport java.text.SimpleDateFormat;\n\npublic class PreparedStatement_demo {\n\n\t//定义MySQL的数据库驱动程序\n\tpublic static final String DBDRIVER = \"org.gjt.mm.mysql.Driver\";\n\t//定义MySQL数据库的连接地址\n\tpublic static final String DBURL = \"jdbc:mysql://localhost:3306/mysql_demo\";\n\t//MySQL数据库的连接用户名\n\tpublic static final String DBUSER = \"root\";\n\t//MySQL数据库的连接密码\n\tpublic static final String DBPASS = \"123456\";\n\t\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tConnection conn = null;\t\t\t\t\t\t//数据库连接\n\t\tPreparedStatement pstmt = null;\t//数据库操作\n\t\t\n\t\tString name = \"王五\";\n\t\tString password = \"pwd2\";\n\t\tint age = 25;\n\t\tString sex = \"女\";\n\t\tString birthday = \"2002-11-21\";\n\t\tjava.util.Date temp = null;\t\t\t\t//声明一个Date对象\n\t\t//通过SimpleDateFormat类将一个字符串变成java.util.Date类型\n\t\ttemp = new SimpleDateFormat(\"yyyy-MM-dd\").parse(birthday);\n\t\t//通过java.util.Date取出具体的日期数，并将其变成java.sql.Date类型\n\t\tjava.sql.Date bir = new java.sql.Date(temp.getTime());\n\t\tString sql = \"INSERT INTO user(name,password,age,sex,birthday)\"+\"VALUES(?,?,?,?,?)\";//编写预处理SQL\n\t\tClass.forName(DBDRIVER);\t\t\t//加载驱动程序\n\t\t//连接MySQL数据库时，要写上连接的用户名和密码\n\t\tconn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n\t\tpstmt = conn.prepareStatement(sql);\t\t//实例化PreparedStatement\n\t\tpstmt.setString(1, name);\t\t\t\t\t//设置第一个&ldquo;？&rdquo;的内容\n\t\tpstmt.setString(2, password);\t\t\t//设置第二个&ldquo;？&rdquo;的内容\n\t\tpstmt.setInt(3, age);\t\t\t\t\t\t\t\t//设置第三个&ldquo;？&rdquo;的内容\n\t\tpstmt.setString(4, sex);\t\t\t\t\t\t\t//设置第四个&ldquo;？&rdquo;的内容\n\t\tpstmt.setDate(5, bir);\t\t\t\t\t\t\t//设置第五个&ldquo;？&rdquo;的内容\n\t\tpstmt.executeUpdate();\t\t\t\t\t\t//执行数据库更新操作，不需要SQL\n\t\t\n\t\tpstmt.close(); \t\t\t\t\t\t\t\t\t\t//操作关闭\n\t\tconn.close(); \t\t\t\t\t\t\t\t\t\t//数据库关闭\n\t\t\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**模糊查询**\n\n```\nimport java.sql.*;\nimport java.text.SimpleDateFormat;\n\npublic class PreparedStatement_demo {\n\n\t//定义MySQL的数据库驱动程序\n\tpublic static final String DBDRIVER = \"org.gjt.mm.mysql.Driver\";\n\t//定义MySQL数据库的连接地址\n\tpublic static final String DBURL = \"jdbc:mysql://localhost:3306/mysql_demo\";\n\t//MySQL数据库的连接用户名\n\tpublic static final String DBUSER = \"root\";\n\t//MySQL数据库的连接密码\n\tpublic static final String DBPASS = \"123456\";\n\t\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tConnection conn = null;\t\t\t\t\t\t//数据库连接\n\t\tPreparedStatement pstmt = null;\t//数据库操作\n\t\tString keyWord = \"王\";\t\t\t\t\t\t\t//设置查询关键字\n\t\tString keyWord1 = \"男\";\t\t\t\t\t\t\t//设置查询关键字\n\t\tResultSet rs = null;\t\t\t\t\t\t\t\t//保存查询结果\n\t\tString sql = \"SELECT id,name,password,age,sex,birthday\"+\n\t\t\t\t\" FROM user WHERE name LIKE ? OR password LIKE ? OR sex LIKE ?\";\n\t\tClass.forName(DBDRIVER);\t\t\t//加载驱动程序\n\t\t//连接MySQL数据库时，要写上连接的用户名和密码\n\t\tconn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n\t\tpstmt = conn.prepareStatement(sql);\t\t//实例化PreparedStatement\n\t\tpstmt.setString(1, \"%\"+keyWord+\"%\");\t\t\t\t\t//设置第一个&ldquo;？&rdquo;的内容\n\t\tpstmt.setString(2, \"%\"+keyWord+\"%\");\t\t\t\t\t//设置第一个&ldquo;？&rdquo;的内容\n\t\tpstmt.setString(3, \"%\"+keyWord1+\"%\");\t\t\t\t\t//设置第一个&ldquo;？&rdquo;的内容\n\t\trs = pstmt.executeQuery();\t\t\t\t\t\t//实例化ResultSet对象\n\t\twhile(rs.next()){\n\t\t\tint id = rs.getInt(1);\n\t\t\tString name = rs.getString(2);\n\t\t\tString pass = rs.getString(3);\n\t\t\tint age = rs.getInt(4);\n\t\t\tString sex = rs.getString(5);\n\t\t\tjava.util.Date d = rs.getDate(6);\n\t\t\tSystem.out.println(\"编号：\"+id);\n\t\t\tSystem.out.println(\"名字：\"+name);\n\t\t\tSystem.out.println(\"密码：\"+pass);\n\t\t\tSystem.out.println(\"年龄：\"+age);\n\t\t\tSystem.out.println(\"性别：\"+sex);\n\t\t\tSystem.out.println(\"生日：\"+d);\n\t\t}\n\t\trs.close();\t\t\t\t\t\t\t\t\t\t\t\t//关闭结果集\n\t\tpstmt.close(); \t\t\t\t\t\t\t\t\t\t//操作关闭\n\t\tconn.close(); \t\t\t\t\t\t\t\t\t\t//数据库关闭\n\t\t\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160321210828745-485575237.png\" alt=\"\" />\n\n&nbsp;\n","tags":["JDBC"]},{"title":"Java数据库——ResultSet接口","url":"/Java数据库——ResultSet接口.html","content":"使用SQL中的SELECT语句可以查询出数据库的全部结果，在JDBC的操作中数据库的所有查询记录将使用ResultSet进行接收，并使用ResultSet显示内容。\n\n<img src=\"/images/517519-20160321174614136-221449233.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n**从user表中查询数据**\n\n```\nimport java.sql.*;\n\npublic class MySQL_demo {\n\t//定义MySQL的数据库驱动程序\n\tpublic static final String DBDRIVER = \"org.gjt.mm.mysql.Driver\";\n\t//定义MySQL数据库的连接地址\n\tpublic static final String DBURL = \"jdbc:mysql://localhost:3306/mysql_demo\";\n\t//MySQL数据库的连接用户名\n\tpublic static final String DBUSER = \"root\";\n\t//MySQL数据库的连接密码\n\tpublic static final String DBPASS = \"123456\";\n\t\n\tpublic static void main(String[] args) throws Exception {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tConnection conn = null;\t\t\t\t\t//数据库连接\n\t\tStatement stmt = null;\t\t\t\t\t//数据库操作\n\t\t\n\t\tResultSet rs = null;\t\t\t//保存查询结果\n\t\tString sql = \"SELECT id,name,password,age,sex,birthday FROM user\";\n\t\tClass.forName(DBDRIVER);\t\t\t//加载驱动程序\n\t\t//连接MySQL数据库时，要写上连接的用户名和密码\n\t\tconn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n\t\tstmt = conn.createStatement();\t\t//实例化Statement对象\n\t\trs = stmt.executeQuery(sql);\t\t\t//实例化ResultSet对象\n\t\twhile(rs.next()){\n//\t\t\tint id = rs.getInt(\"id\");\n//\t\t\tString name = rs.getString(\"name\");\n//\t\t\tString pass = rs.getString(\"password\");\n//\t\t\tint age = rs.getInt(\"age\");\n//\t\t\tString sex = rs.getString(\"sex\");\n//\t\t\tDate d = rs.getDate(\"birthday\");\n\t\t\tint id = rs.getInt(1);\n\t\t\tString name = rs.getString(2);\n\t\t\tString pass = rs.getString(3);\n\t\t\tint age = rs.getInt(4);\n\t\t\tString sex = rs.getString(5);\n\t\t\tDate d = rs.getDate(6);\n\t\t\tSystem.out.println(\"编号：\"+id);\n\t\t\tSystem.out.println(\"名字：\"+name);\n\t\t\tSystem.out.println(\"密码：\"+pass);\n\t\t\tSystem.out.println(\"年龄：\"+age);\n\t\t\tSystem.out.println(\"性别：\"+sex);\n\t\t\tSystem.out.println(\"生日：\"+d);\n\t\t}\n\t\t\n\t\trs.close(); \t\t\t\t\t\t\t\t\t\t\t\t//关闭结果集\n\t\tstmt.close(); \t\t\t\t\t\t\t\t\t\t//操作关闭\n\t\tconn.close(); \t\t\t\t\t\t\t\t\t\t//数据库关闭\n\t}\n}\n\n```\n\n&nbsp;\n","tags":["JDBC"]},{"title":"Java数据库——连接关闭、增删改查","url":"/Java数据库——连接关闭、增删改查.html","content":"**连接数据库**\n\n**<img src=\"/images/517519-20160321152355401-1010128728.png\" alt=\"\" />**\n\n<!--more-->\n&nbsp;\n\n<img src=\"/images/517519-20160321152417933-1448767738.png\" alt=\"\" />\n\n```\nimport java.sql.*;\n\npublic class MySQL_demo {\n\t//定义MySQL的数据库驱动程序\n\tpublic static final String DBDRIVER = \"org.gjt.mm.mysql.Driver\";\n\t//定义MySQL数据库的连接地址\n\tpublic static final String DBURL = \"jdbc:mysql://localhost:3306/mysql_demo\";\n\t//MySQL数据库的连接用户名\n\tpublic static final String DBUSER = \"root\";\n\t//MySQL数据库的连接密码\n\tpublic static final String DBPASS = \"123456\";\n\t\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tConnection conn = null;\t\t\t\t\t//数据库连接\n\t\ttry{\n\t\t\tClass.forName(DBDRIVER);\t\t//加载MYSQL JDBC驱动程序\n\t\t}catch(ClassNotFoundException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\ttry{\n\t\t\t//连接MySQL数据库时，要写上连接的用户名和密码\n\t\t\tconn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n\t\t}catch(SQLException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\tSystem.out.println(conn);\n\t\ttry{\n\t\t\tconn.close(); \t\t\t\t//数据库关闭\n\t\t}catch(SQLException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160321152523839-2007601331.png\" alt=\"\" />\n\n建立一个user表\n\n```\nCREATE TABLE user(id INT PRIMARY KEY AUTO_INCREMENT,name VARCHAR(30) NOT NULL,password VARCHAR(32) NOT NULL,age INT NOT NULL,sex VARCHAR(2),birthday DATE);\n\n```\n\n&nbsp;\n\n**<1>数据库的更新操作**\n\n<img src=\"/images/517519-20160321162340604-128594204.png\" alt=\"\" />\n\n&nbsp;\n\n**执行数据库插入操作，执行一次插入一条**\n\n```\nimport java.sql.*;\n\npublic class MySQL_demo {\n\t//定义MySQL的数据库驱动程序\n\tpublic static final String DBDRIVER = \"org.gjt.mm.mysql.Driver\";\n\t//定义MySQL数据库的连接地址\n\tpublic static final String DBURL = \"jdbc:mysql://localhost:3306/mysql_demo\";\n\t//MySQL数据库的连接用户名\n\tpublic static final String DBUSER = \"root\";\n\t//MySQL数据库的连接密码\n\tpublic static final String DBPASS = \"123456\";\n\t\n\tpublic static void main(String[] args) throws Exception {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tConnection conn = null;\t\t\t\t\t//数据库连接\n\t\tStatement stmt = null;\t\t\t\t\t//数据库操作\n\t\tString sql = \"INSERT INTO user(name,password,age,sex,birthday)\"\n\t\t\t\t+\"VALUES('张三','mima',30,'男','2014-01-11')\";\n\t\tClass.forName(DBDRIVER);\t\t\t//加载驱动程序\n\t\t//连接MySQL数据库时，要写上连接的用户名和密码\n\t\tconn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n\t\tstmt = conn.createStatement();\t\t//实例化Statement对象\n\t\tstmt.executeUpdate(sql);\t\t\t\t//执行数据库更新操作\n\t\tstmt.close(); \t\t\t\t\t\t\t\t\t\t//操作关闭\n\t\tconn.close(); \t\t\t\t\t\t\t\t\t\t//数据库关闭\n\t}\n}\n\n```\n\n&nbsp;\n\n&nbsp;\n\n```\nimport java.sql.*;\n\npublic class MySQL_demo {\n\t//定义MySQL的数据库驱动程序\n\tpublic static final String DBDRIVER = \"org.gjt.mm.mysql.Driver\";\n\t//定义MySQL数据库的连接地址\n\tpublic static final String DBURL = \"jdbc:mysql://localhost:3306/mysql_demo\";\n\t//MySQL数据库的连接用户名\n\tpublic static final String DBUSER = \"root\";\n\t//MySQL数据库的连接密码\n\tpublic static final String DBPASS = \"123456\";\n\t\n\tpublic static void main(String[] args) throws Exception {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tConnection conn = null;\t\t\t\t\t//数据库连接\n\t\tStatement stmt = null;\t\t\t\t\t//数据库操作\t\t\n\t\tString name = \"李四\";\n\t\tString password = \"pwd\";\n\t\tint age = 22;\n\t\tString sex = \"女\";\n\t\tString birthday = \"2012-01-01\";\n\t\tString sql = \"INSERT INTO user(name,password,age,sex,birthday)\"\n\t\t\t\t+\"VALUES('\"+name+\"','\"+password+\"',\"+age+\",'\"+sex+\"','\"+birthday+\"')\";\n\t\tClass.forName(DBDRIVER);\t\t\t//加载驱动程序\n\t\t//连接MySQL数据库时，要写上连接的用户名和密码\n\t\tconn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n\t\tstmt = conn.createStatement();\t\t//实例化Statement对象\n\t\tstmt.executeUpdate(sql);\t\t\t\t//执行数据库更新操作\n\t\tstmt.close(); \t\t\t\t\t\t\t\t\t\t//操作关闭\n\t\tconn.close(); \t\t\t\t\t\t\t\t\t\t//数据库关闭\n\t}\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160321170736292-287010520.png\" alt=\"\" />\n\n&nbsp;\n\n**<2>执行数据库的修改**\n\n```\nimport java.sql.*;\n\npublic class MySQL_demo {\n\t//定义MySQL的数据库驱动程序\n\tpublic static final String DBDRIVER = \"org.gjt.mm.mysql.Driver\";\n\t//定义MySQL数据库的连接地址\n\tpublic static final String DBURL = \"jdbc:mysql://localhost:3306/mysql_demo\";\n\t//MySQL数据库的连接用户名\n\tpublic static final String DBUSER = \"root\";\n\t//MySQL数据库的连接密码\n\tpublic static final String DBPASS = \"123456\";\n\t\n\tpublic static void main(String[] args) throws Exception {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tConnection conn = null;\t\t\t\t\t//数据库连接\n\t\tStatement stmt = null;\t\t\t\t\t//数据库操作\n\t\t\n\t\tint id = 2;\n\t\tString name = \"王五\";\n\t\tString password = \"pwd2\";\n\t\tint age = 25;\n\t\tString sex = \"女\";\n\t\tString birthday = \"2002-11-21\";\n\t\tString sql = \"UPDATE user SET name= '\"+name+\"' , password='\"+password+\"' , age=\"+age+\" , sex='\"+sex+\"' , birthday='\"+birthday+\"' WHERE id=\"+id;\n\t\tClass.forName(DBDRIVER);\t\t\t//加载驱动程序\n\t\t//连接MySQL数据库时，要写上连接的用户名和密码\n\t\tconn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n\t\tstmt = conn.createStatement();\t\t//实例化Statement对象\n\t\tstmt.executeUpdate(sql);\t\t\t\t//执行数据库更新操作\n\t\tstmt.close(); \t\t\t\t\t\t\t\t\t\t//操作关闭\n\t\tconn.close(); \t\t\t\t\t\t\t\t\t\t//数据库关闭\n\t}\n}\n\n```\n\n&nbsp;\n\n**<3>执行数据库删除操作**\n\n```\nimport java.sql.*;\n\npublic class MySQL_demo {\n\t//定义MySQL的数据库驱动程序\n\tpublic static final String DBDRIVER = \"org.gjt.mm.mysql.Driver\";\n\t//定义MySQL数据库的连接地址\n\tpublic static final String DBURL = \"jdbc:mysql://localhost:3306/mysql_demo\";\n\t//MySQL数据库的连接用户名\n\tpublic static final String DBUSER = \"root\";\n\t//MySQL数据库的连接密码\n\tpublic static final String DBPASS = \"123456\";\n\t\n\tpublic static void main(String[] args) throws Exception {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tConnection conn = null;\t\t\t\t\t//数据库连接\n\t\tStatement stmt = null;\t\t\t\t\t//数据库操作\n\t\t\n\t\tint id = 3;\n\t\tString sql = \"DELETE FROM user WHERE id=\" + id;\n\t\tClass.forName(DBDRIVER);\t\t\t//加载驱动程序\n\t\t//连接MySQL数据库时，要写上连接的用户名和密码\n\t\tconn = DriverManager.getConnection(DBURL,DBUSER,DBPASS);\n\t\tstmt = conn.createStatement();\t\t//实例化Statement对象\n\t\tstmt.executeUpdate(sql);\t\t\t\t//执行数据库更新操作\n\t\tstmt.close(); \t\t\t\t\t\t\t\t\t\t//操作关闭\n\t\tconn.close(); \t\t\t\t\t\t\t\t\t\t//数据库关闭\n\t\t\n\t}\n}\n\n```\n\n&nbsp;\n\n在JDK1.7中，java.sql中的Connection，Statement，ResultSet接口都继承了AutoCloseable，所以使用了try-with-resource\n\n可以参考：[java try-with-resource语句使用](https://www.jianshu.com/p/258c5ce1a2bd)\n\n<img src=\"/images/517519-20191226162339866-2003675.png\" alt=\"\" />\n\n&nbsp;<img src=\"/images/517519-20191226162359211-1538278295.png\" alt=\"\" />\n\n<img src=\"/images/517519-20191226162410223-271522680.png\" alt=\"\" />\n\n参考：[浅谈 Java 中的 AutoCloseable 接口](https://cloud.tencent.com/developer/article/1737681)\n","tags":["JDBC"]},{"title":"Avro学习笔记——avro-tools工具","url":"/Avro学习笔记——avro-tools工具.html","content":"## 1.下载avro-tools.jar\n\n```\nhttps://archive.apache.org/dist/avro/avro-1.10.1/java/\n```\n\navro-tools.jar常用命令：[Working with Apache Avro files in Amazon S3](https://garystafford.medium.com/previewing-apache-avro-files-in-amazon-s3-98f41e98f656)　　\n\n也可以查看help\n\n```\njava -jar ./avro-tools-1.10.1.jar help\nVersion 1.10.1 of Apache Avro\nCopyright 2010-2015 The Apache Software Foundation\n\nThis product includes software developed at\nThe Apache Software Foundation (https://www.apache.org/).\n----------------\nAvailable tools:\n    canonical  Converts an Avro Schema to its canonical form\n          cat  Extracts samples from files\n      compile  Generates Java code for the given schema.\n       concat  Concatenates avro files without re-compressing.\n        count  Counts the records in avro files or folders\n  fingerprint  Returns the fingerprint for the schemas.\n   fragtojson  Renders a binary-encoded Avro datum as JSON.\n     fromjson  Reads JSON records and writes an Avro data file.\n     fromtext  Imports a text file into an avro data file.\n      getmeta  Prints out the metadata of an Avro data file.\n    getschema  Prints out schema of an Avro data file.\n          idl  Generates a JSON schema from an Avro IDL file\n idl2schemata  Extract JSON schemata of the types from an Avro IDL file\n       induce  Induce schema/protocol from Java class/interface via reflection.\n   jsontofrag  Renders a JSON-encoded Avro datum as binary.\n       random  Creates a file with randomly generated instances of a schema.\n      recodec  Alters the codec of a data file.\n       repair  Recovers data from a corrupt Avro Data file\n  rpcprotocol  Output the protocol of a RPC service\n   rpcreceive  Opens an RPC Server and listens for one message.\n      rpcsend  Sends a single RPC message.\n       tether  Run a tethered mapreduce job.\n       tojson  Dumps an Avro data file as JSON, record per line or pretty.\n       totext  Converts an Avro data file to a text file.\n     totrevni  Converts an Avro data file to a Trevni file.\n  trevni_meta  Dumps a Trevni file's metadata as JSON.\ntrevni_random  Create a Trevni file filled with random instances of a schema.\ntrevni_tojson  Dumps a Trevni file as JSON.\n\n```\n\n## **2.查看avro文件的schema**\n\n```\njava -jar ./avro-tools-1.10.1.jar getschema ./xxxx.avro\n\n```\n\n## **3.查看avro文件内容的json格式**\n\n```\njava -jar ./avro-tools-1.10.1.jar tojson ./nova_ads_access_log-0-0008589084.avro | less\n\n```\n\n## **4.使用avro-tools编译java代码**\n\n编译avro IDL文件，参考\n\n```\nhttps://avro.apache.org/docs/current/gettingstartedjava.html\nhttps://yanbin.blog/convert-apache-avro-to-parquet-format-in-java/\n```\n\n定义schema文件kst.avsc\n\n```\n{\n  \"namespace\": \"com.linkedin.haivvreo\",\n  \"name\": \"test_serializer\",\n  \"type\": \"record\",\n  \"fields\": [\n    { \"name\":\"string1\", \"type\":\"string\" },\n    { \"name\":\"int1\", \"type\":\"int\" },\n    { \"name\":\"tinyint1\", \"type\":\"int\" },\n    { \"name\":\"smallint1\", \"type\":\"int\" },\n    { \"name\":\"bigint1\", \"type\":\"long\" },\n    { \"name\":\"boolean1\", \"type\":\"boolean\" },\n    { \"name\":\"float1\", \"type\":\"float\" },\n    { \"name\":\"double1\", \"type\":\"double\" },\n    { \"name\":\"list1\", \"type\":{\"type\":\"array\", \"items\":\"string\"} },\n    { \"name\":\"map1\", \"type\":{\"type\":\"map\", \"values\":\"int\"} },\n    { \"name\":\"struct1\", \"type\":{\"type\":\"record\", \"name\":\"struct1_name\", \"fields\": [\n          { \"name\":\"sInt\", \"type\":\"int\" }, { \"name\":\"sBoolean\", \"type\":\"boolean\" }, { \"name\":\"sString\", \"type\":\"string\" } ] } },\n    { \"name\":\"union1\", \"type\":[\"float\", \"boolean\", \"string\"] },\n    { \"name\":\"enum1\", \"type\":{\"type\":\"enum\", \"name\":\"enum1_values\", \"symbols\":[\"BLUE\",\"RED\", \"GREEN\"]} },\n    { \"name\":\"nullableint\", \"type\":[\"int\", \"null\"] },\n    { \"name\":\"bytes1\", \"type\":\"bytes\" },\n    { \"name\":\"fixed1\", \"type\":{\"type\":\"fixed\", \"name\":\"threebytes\", \"size\":3} }\n  ] }\n\n```\n\n编译avro IDL文件\n\n```\njava -jar ./src/main/resources/avro-tools-1.10.1.jar compile schema ./src/main/avro/kst.avsc ./src/main/java\n\n```\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20211206215138031-892945827.png\" width=\"200\" height=\"206\" loading=\"lazy\" />\n\n这时编译出来的java代码中，IDL的string类型实际上是CharSequence\n\n如果想编译成string，则可以添加-string参数\n\n```\njava -jar ./src/main/resources/avro-tools-1.10.1.jar compile -string schema ./src/main/avro/kst.avsc ./src/main/java\n\n```\n\n&nbsp;\n","tags":["avro"]},{"title":"Impala学习笔记——impala shell","url":"/Impala学习笔记——impala shell.html","content":"进入impala shell，port为Impala Daemon Beeswax 端口\n\n```\nimpala-shell -i ip:port -k\n\n```\n\n使用impala shell直接运行SQL\n\n```\nimpala-shell -i ip:port -k --quiet -B -d default -q \"select count(*) from table1 where ds='2021-01-20'\"\n\n```\n\n退出\n\n```\nexit;\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["impala"]},{"title":"Ubuntu下的MySQL安装","url":"/Ubuntu下的MySQL安装.html","content":"## **1.安装mysql-server**\n\n```\nsudo apt-get update\nsudo apt-get install mysql-server mysql-client\n\n```\n\n## **2.重新启动mysql服务**\n\n```\nsudo service mysql restart\n\n```\n\n## **<strong>3.让apache支持mysql**</strong>\n\n```\nsudo apt-get install libapache2-mod-auth-mysql\n\n```\n\n**<strong><strong>16.04使用下面命令**</strong></strong>\n\n```\nsudo apt-get install libmysqlclient-dev\n\n```\n\n## **<strong><strong>4.登录mysql**</strong></strong>\n\n```\nmysql -u root -p\n\n```\n\n**<strong><strong>如果修改了<strong>配置文件<strong>my.cnf配置文件，**需要重启数据库（修改方法在下面），重启数据库之前需要先重新载入apparmor配置文件，使用下面命令重新载入：</strong></strong></strong></strong>\n\n```\nsudo /etc/init.d/apparmor restart\n\n```\n\n**<strong><strong>重新启动数据库**</strong></strong>\n\n```\nsudo /etc/init.d/mysql start\n\n```\n\n## **<strong><strong>5.查看数据库的编码**</strong></strong>\n\n1.查看MySQL数据库服务器和数据库MySQL字符集。\n\n```\nSHOW VARIABLES LIKE 'character_set_%';\n\n```\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20170102145239722-1148625481.png\" alt=\"\" width=\"465\" height=\"233\" />\n\n如果需要修改的话\n\n```\nset character_set_client=utf8;\n\n```\n\n<br />2.查看MySQL数据表（table）的MySQL字符集，**spring_user是数据库的名字，t_user是表名**\n\n```\nmysql> show table status from spring_user like '%t_user%';\n\n```\n\n&nbsp;或者\n\n```\nmysql> show create table t_user;\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20170102145342394-1935480796.png\" alt=\"\" width=\"534\" height=\"167\" />\n\n**修改方法：**\n\n```\nalter table t_user convert to character set utf8;\n\n```\n\n&nbsp;\n\n3.查看MySQL数据列（column）的MySQL字符集，**t_user是表名**\n\n```\nmysql> show full columns from t_user;\n\n```\n\n&nbsp;<img src=\"/images/517519-20170102145425050-1690528432.png\" alt=\"\" />\n\n&nbsp;\n\n## **<strong><strong>6.修改mysql配置文件**</strong></strong>\n\n```\nsudo vim /etc/my.cnf\n\n```\n\n**<strong><strong>因为ubuntu下mySQL默认的数据库的路径是在/var/lib/mysql，所以要修改这个路径的话，参考**</strong></strong>\n\n**<strong><strong>http://www.2cto.com/database/201501/373939.html**</strong></strong>\n\n**<strong><strong>注意修改的时候/mysql要加上**</strong></strong>\n\n**<strong><strong><img src=\"/images/517519-20160320201042803-1211804642.png\" alt=\"\" />**</strong></strong>\n\n&nbsp;\n\nubuntu下mysql的默认配置地址\n\n```\nsudo vim /etc/mysql/mysql.conf.d/mysqld.cnf\n\n```\n\n如果要开放myslq的外网访问权限，在该配置文件中添加\n\n```\nbind-address            = 0.0.0.0\n\n```\n\n如果bind到127.0.0.1的话，使用netstat命令可以看到local address是127.0.0.1，此时只能本地访问\n\n```\nsudo netstat -nltp | grep mysql<br />tcp&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 127.0.0.1:3306&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.0.0.0:*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; LISTEN&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 31772/mysqld\n\n```\n\n修改后重新mysql服务\n\n```\nsudo systemctl restart mysql\n\n```\n\n此时就可以看到local address变成0.0.0.0\n\n```\ntcp        0      0 0.0.0.0:3306            0.0.0.0:*               LISTEN      31989/mysqld\n\n```\n\n&nbsp;\n\n修改的**my.cnf**文件，**修改完****切记一定要重启mysql服务**\n\n```\n#\n# The MySQL database server configuration file.\n#\n# You can copy this to one of:\n# - \"/etc/mysql/my.cnf\" to set global options,\n# - \"~/.my.cnf\" to set user-specific options.\n# \n# One can use all long options that the program supports.\n# Run program with --help to get a list of available options and with\n# --print-defaults to see which it would actually understand and use.\n#\n# For explanations see\n# http://dev.mysql.com/doc/mysql/en/server-system-variables.html\n\n# This will be passed to all mysql clients\n# It has been reported that passwords should be enclosed with ticks/quotes\n# escpecially if they contain \"#\" chars...\n# Remember to edit /etc/mysql/debian.cnf when changing the socket location.\n[client]\nport\t\t= 3306\nsocket\t\t= /var/run/mysqld/mysqld.sock\ndefault-character-set=utf8\n\n# Here is entries for some specific programs\n# The following values assume you have at least 32M ram\n\n# This was formally known as [safe_mysqld]. Both versions are currently parsed.\n[mysqld_safe]\ndefault-character-set=utf8\ncharacter_set_server = utf8\nsocket\t\t= /var/run/mysqld/mysqld.sock\nnice\t\t= 0\n\n[mysqld]\n#\n# * Basic Settings\n#\nuser\t\t= mysql\npid-file\t= /var/run/mysqld/mysqld.pid\nsocket\t\t= /var/run/mysqld/mysqld.sock\ncharacter_set_server=utf8\ninit_connect='SET NAMES utf8'\nport\t\t= 3306\nbasedir\t\t= /usr\ndatadir\t\t= /home/common/software/database/mysql\ntmpdir\t\t= /tmp\nlc-messages-dir\t= /usr/share/mysql\nskip-external-locking\n#\n# Instead of skip-networking the default is now to listen only on\n# localhost which is more compatible and is not less secure.\nbind-address\t\t= 127.0.0.1\n#\n# * Fine Tuning\n#\nkey_buffer\t\t= 16M\nmax_allowed_packet\t= 16M\nthread_stack\t\t= 192K\nthread_cache_size       = 8\n# This replaces the startup script and checks MyISAM tables if needed\n# the first time they are touched\nmyisam-recover         = BACKUP\n#max_connections        = 100\n#table_cache            = 64\n#thread_concurrency     = 10\n#\n# * Query Cache Configuration\n#\nquery_cache_limit\t= 1M\nquery_cache_size        = 16M\n#\n# * Logging and Replication\n#\n# Both location gets rotated by the cronjob.\n# Be aware that this log type is a performance killer.\n# As of 5.1 you can enable the log at runtime!\n#general_log_file        = /var/log/mysql/mysql.log\n#general_log             = 1\n#\n# Error log - should be very few entries.\n#\nlog_error = /var/log/mysql/error.log\n#\n# Here you can see queries with especially long duration\n#log_slow_queries\t= /var/log/mysql/mysql-slow.log\n#long_query_time = 2\n#log-queries-not-using-indexes\n#\n# The following can be used as easy to replay backup logs or for replication.\n# note: if you are setting up a replication slave, see README.Debian about\n#       other settings you may need to change.\n#server-id\t\t= 1\n#log_bin\t\t\t= /var/log/mysql/mysql-bin.log\nexpire_logs_days\t= 10\nmax_binlog_size         = 100M\n#binlog_do_db\t\t= include_database_name\n#binlog_ignore_db\t= include_database_name\n#\n# * InnoDB\n#\n# InnoDB is enabled by default with a 10MB datafile in /var/lib/mysql/.\n# Read the manual for more InnoDB related options. There are many!\n#\n# * Security Features\n#\n# Read the manual, too, if you want chroot!\n# chroot = /var/lib/mysql/\n#\n# For generating SSL certificates I recommend the OpenSSL GUI \"tinyca\".\n#\n# ssl-ca=/etc/mysql/cacert.pem\n# ssl-cert=/etc/mysql/server-cert.pem\n# ssl-key=/etc/mysql/server-key.pem\n\n\n\n[mysqldump]\nquick\nquote-names\nmax_allowed_packet\t= 16M\n\n[mysql]\ndefault-character-set=utf8\n#no-auto-rehash\t# faster start of mysql but no tab completition\n\n[isamchk]\nkey_buffer\t\t= 16M\n\n#\n# * IMPORTANT: Additional settings that can override those from this file!\n#   The files must end with '.cnf', otherwise they'll be ignored.\n#\n!includedir /etc/mysql/conf.d/\n\n```\n\n&nbsp;\n\n# Ubuntu下Eclipse部署MySQL JDBC驱动\n\n参考http://www.linuxidc.com/Linux/2011-10/44355.htm中的上半部分\n\n&nbsp;\n\n**Linux下自动启动MySQL**\n\n**<img src=\"/images/517519-20160404184745250-1269677896.png\" width=\"500\" height=\"259\" />**\n\n　　\n\n一、查看MySQL数据库服务器和数据库MySQL字符集。\n\n命令：\n<td class=\"code\">`mysql>&nbsp;show&nbsp;variables&nbsp;``like`&nbsp;`'%char%'``;`</td>\n\n[<img src=\"/images/21a4462309f79052210e0c4f0af3d7ca7bcbd5aa.jpg\" alt=\"\" class=\"ikqb_img\" />](https://gss0.baidu.com/9vo3dSag_xI4khGko9WTAnF6hhy/zhidao/pic/item/21a4462309f79052210e0c4f0af3d7ca7bcbd5aa.jpg)\n\n二、查看MySQL数据表（table）的MySQL字符集。\n\n命令：\n<td class=\"code\">`mysql>&nbsp;show&nbsp;``table`&nbsp;`status&nbsp;``from`&nbsp;`sqlstudy_db&nbsp;``like`&nbsp;`'%countries%'``;`</td>\n\n[<img src=\"/images/d6ca7bcb0a46f21f7e381522f0246b600c33aeb7.jpg\" alt=\"\" class=\"ikqb_img\" />](https://gss0.baidu.com/94o3dSag_xI4khGko9WTAnF6hhy/zhidao/pic/item/d6ca7bcb0a46f21f7e381522f0246b600c33aeb7.jpg)\n\n三、查看MySQL数据列（column）的MySQL字符集。\n\n命令：\n<td class=\"code\">`mysql>&nbsp;show&nbsp;``full`&nbsp;`columns&nbsp;``from`&nbsp;`countries;`</td>\n\n[<img src=\"/images/3801213fb80e7becaf434890292eb9389b506b22.jpg\" alt=\"\" class=\"ikqb_img img_show\" style=\"max-width: 547px; max-height: 214px;\" />](/images/3801213fb80e7becaf434890292eb9389b506b22.jpg)\n\n&nbsp;\n\n四、修改MySQL的密码\n\n首先用root登录MySQL，然后执行\n\n```\nUPDATE user SET password=PASSWORD('123456') WHERE user='root';\nFLUSH PRIVILEGES;\n\n```\n\n或者\n\n```\nSET PASSWORD FOR 'root'@'localhost' = PASSWORD('newpass');\n\n```\n\n&nbsp;\n\n如果要安装5.6版本的mysql\n\n```\nsudo add-apt-repository 'deb http://archive.ubuntu.com/ubuntu trusty universe'\nsudo apt-get update\nsudo apt install mysql-server-5.6 mysql-client-5.6\n\n```\n\n如果卸载5.7再安装的时候遇到\n\n/var/cache/apt/archives/mysql-server-5.6_5.6.16-1~exp1_amd64.deb E: Sub-process /usr/bin/dpkg returned an error code (1)\n\n```\nsudo apt-get remove --purge mysql-server mysql-client mysql-common\nsudo apt-get autoremove\nsudo apt-get autoclean\n\n```\n\n先确保 mysql service是启动的,没启动会报找不到mysqld.sock\n\n```\nservice mysql status\nservice mysql start\n\n```\n\n&nbsp;\n","tags":["MySQL"]},{"title":"元注解(Annotation)","url":"/元注解(Annotation).html","content":"J2SE 5.0提供了很多新的特征。其中一个很重要的特征就是对元数据(Metadata)的支持。在J2SE 5.0中，这种元数据称为注解(Annotation)。\n\n通过使用注解，程序开发人员可以在不改变原有逻辑的情况下，在源文件嵌入一些补充的信息。\n\n<img src=\"/images/517519-20160320155033209-655249060.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n**系统内建的Annotation**\n\n在JDK1.5之后，系统已经建立了如下3个内建的Annotation类型，用户直接使用即可。\n\n<img src=\"/images/517519-20160320160958240-1970967121.png\" alt=\"\" />\n\n&nbsp;\n\n**@Override**\n\n**<img src=\"/images/517519-20160320161622443-1889132700.png\" alt=\"\" />**\n\n<img src=\"/images/517519-20160320161647021-1021893822.png\" alt=\"\" />\n\n&nbsp;\n\n&nbsp;\n\n**@Deprecated**\n\n**<img src=\"/images/517519-20160320161839068-695793988.png\" alt=\"\" />**\n\n&nbsp;<img src=\"/images/517519-20160320161916990-1006172221.png\" alt=\"\" />\n\n&nbsp;\n\n**@SuppressWarmings**\n\n**<img src=\"/images/517519-20160320162854881-1189057904.png\" alt=\"\" />**\n\n<img src=\"/images/517519-20160320162922209-1197306765.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160320162940256-1905530168.png\" alt=\"\" />\n\n&nbsp;\n\n**自定义Annotation**\n\n**@Target注释**\n\n**<img src=\"/images/517519-20160320165935240-188474414.png\" alt=\"\" />**\n\n&nbsp;\n\n<img src=\"/images/517519-20160320165959974-581735706.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160320170019896-1615827559.png\" alt=\"\" />\n\n&nbsp;\n\n**@Documented注释**\n\n**<img src=\"/images/517519-20160320170141084-541942734.png\" alt=\"\" />**\n\n&nbsp;\n\n<img src=\"/images/517519-20160320170202224-1278362548.png\" alt=\"\" />\n\n&nbsp;\n\n**@Inherited注释**\n\n**<img src=\"/images/517519-20160320170258599-1434978593.png\" alt=\"\" />**\n\n&nbsp;\n\n<img src=\"/images/517519-20160320170321349-810533225.png\" alt=\"\" />\n\n#### @Retention\n\nReteniton注解的作用是：描述注解保留的时间范围(即：被描述的注解在它所修饰的类中可以被保留到何时).\n\nReteniton注解用来限定那些被它所注解的注解类在注解到其他类上以后，可被保留到何时，一共有三种策略，定义在RetentionPolicy枚举中\n\n```\npublic enum RetentionPolicy {\n \n    SOURCE,    // 源文件保留\n    CLASS,       // 编译期保留，默认值\n    RUNTIME   // 运行期保留，可通过反射去获取注解信息\n}\n\n```\n\n生命周期长度 SOURCE < CLASS < RUNTIME ，前者能作用的地方后者一定也能作用。\n\n如果需要在运行时去动态获取注解信息，那只能用 RUNTIME 注解；\n\n如果要在编译时进行一些预处理操作，比如生成一些辅助代码（如 ButterKnife），就用 CLASS注解；\n\n如果只是做一些检查性的操作，比如 @Override 和 @SuppressWarnings，则可选用 SOURCE 注解。\n","tags":["Java"]},{"title":"SpringBoot学习笔记——事务","url":"/SpringBoot学习笔记——事务.html","content":"事务管理在系统开发中是不可缺少的一部分，Spring提供了很好事务管理机制，主要分为编程式事务和声明式事务两种。\n\n参考：[一口气说出 6种，@Transactional注解的失效场景](https://zhuanlan.zhihu.com/p/114461128)\n","tags":["SpringBoot"]},{"title":"go学习笔记——基本语法","url":"/go学习笔记——基本语法.html","content":"## 1.*和&amp;的区别\n\n&amp; 是取地址符号 , 即取得某个变量的地址 , 如 &amp;a\n\n* 是指针运算符 , 可以表示一个变量是指针类型 , 也可以表示一个指针变量所指向的存储单元 , 也就是这个地址所存储的值\n\n参考：[Go中*和&amp;区别](https://www.cnblogs.com/staff/p/13215017.html)\n\nprintln打印对象只能打印出其指针，需要使用fmt.Printf，如下\n\n```\nfmt.Printf(\"%+v\\n\", user)\n\n```\n\n参考：[Golang 通过fmt包输出完整struct信息](https://blog.cyeam.com/golang/2017/03/06/go-fmt-v)\n\n## 2.defer\n\ndefer是Go语言提供的一种用于注册延迟调用的机制：让函数或语句可以在当前函数执行完毕后（包括通过return正常结束或者panic导致的异常结束）执行。\n\ndefer语句通常用于一些成对操作的场景：打开连接/关闭连接；加锁/释放锁；打开文件/关闭文件等。\n\ndefer在一些需要回收资源的场景非常有用，可以很方便地在函数结束前做一些清理操作。在打开资源语句的下一行，直接一句defer就可以在函数返回前关闭资源，可谓相当优雅。\n\n```\nf, _ := os.Open(\"defer.txt\")\ndefer f.Close()\n\n```\n\n参考：[Golang之轻松化解defer的温柔陷阱 ](https://www.cnblogs.com/qcrao-2018/p/10367346.html)\n\n## 3.日志库\n\n### 1.[sirupsen/logrus](https://github.com/sirupsen/logrus)\n\n```\ngo get -u github.com/sirupsen/logrus\n\n```\n\n文档\n\n```\nhttps://pkg.go.dev/github.com/sirupsen/logrus#section-readme\n\n```\n\n使用\n\n```\npackage main\n\nimport (\n  \"os\"\n  \"github.com/sirupsen/logrus\"\n)\n\n// Create a new instance of the logger. You can have any number of instances.\nvar log = logrus.New()\n\nfunc main() {\n  // The API for setting attributes is a little different than the package level\n  // exported logger. See Godoc.\n  log.Out = os.Stdout\n\n  // You could set this to any `io.Writer` such as a file\n  // file, err := os.OpenFile(\"logrus.log\", os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0666)\n  // if err == nil {\n  //  log.Out = file\n  // } else {\n  //  log.Info(\"Failed to log to file, using default stderr\")\n  // }\n\n  log.WithFields(logrus.Fields{\n    \"animal\": \"walrus\",\n    \"size\":   10,\n  }).Info(\"A group of walrus emerges from the ocean\")\n}\n\n```\n\n### 2.[rs/zerolog](https://github.com/rs/zerolog)\n\n```\ngo get -u github.com/rs/zerolog/log\n\n```\n\n使用\n\n```\nlog.Logger = log.With().Caller().Logger()\nlog.Info().Msg(\"hello world\")\n\n// Output: {\"level\": \"info\", \"message\": \"hello world\", \"caller\": \"/go/src/your_project/some_file:21\"}\n\n```\n\n### 3.uber/zap\n\n```\ngo get -u go.uber.org/zap\n\n```\n\n文档\n\n```\nhttps://pkg.go.dev/go.uber.org/zap\n\n```\n\nzap提供了2种logger，分别是Logger和SugaredLogger\n\n在性能要求高但是不是很重要的场景下，适合使用SugaredLogger\n\n```\nlogger, _ := zap.NewProduction()\ndefer logger.Sync() // flushes buffer, if any\nsugar := logger.Sugar()\nsugar.Infow(\"failed to fetch URL\",\n  // Structured context as loosely typed key-value pairs.\n  \"url\", url,\n  \"attempt\", 3,\n  \"backoff\", time.Second,\n)\nsugar.Infof(\"Failed to fetch URL: %s\", url)\n\n```\n\n输出\n\n```\n{\"level\":\"info\",\"ts\":1703922949.209576,\"caller\":\"server/main.go:109\",\"msg\":\"failed to fetch URL\",\"url\":\"http://example.com\",\"attempt\":3,\"backoff\":1}\n{\"level\":\"info\",\"ts\":1703922949.209731,\"caller\":\"server/main.go:115\",\"msg\":\"Failed to fetch URL: http://example.com\"}\n\n```\n\n在性能要求高且需要类型安全的场景下，适合使用Logger\n\n```\nlogger, _ := zap.NewProduction()\ndefer logger.Sync()\nurl := \"http://example.com\"\nlogger.Info(\"failed to fetch URL\",\n\t// Structured context as strongly typed Field values.\n\tzap.String(\"url\", url),\n\tzap.Int(\"attempt\", 3),\n\tzap.Duration(\"backoff\", time.Second),\n)\n\n```\n\n输出\n\n```\n{\"level\":\"info\",\"ts\":1703923022.603034,\"caller\":\"server/main.go:108\",\"msg\":\"failed to fetch URL\",\"url\":\"http://example.com\",\"attempt\":3,\"backoff\":1}\n\n```\n\nNewExample/NewDevelopment/NewProduction区别\n\nNewExample适用于测试代码，它将DebugLevel及以上级别的日志以JSON格式标准输出，但省略了时间戳和调用函数，以保持示例输出简短和确定。\n\nNewDevelopment适用于开发环境，它以人类友好的格式将DebugLevel及以上级别的日志写入标准错误。\n\nNewProduction适用于生产环境，它将info level及以上级别的日志以JSON格式写入标准错误。\n\n其他参考文档：[Go 每日一库之 zap](https://darjun.github.io/2020/04/23/godailylib/zap/)\n\n[golang常用库包：log日志记录-uber的Go日志库zap使用详解 ](https://www.cnblogs.com/jiujuan/p/17304844.html)\n\n### 4.[kratos logrus](https://pkg.go.dev/github.com/go-kratos/kratos/v2/log#section-readme)\n\n参考：[go各框架的log日志](https://www.cnblogs.com/gz-wod/p/17248450.html)\n\n### 5.日志文件回滚\n\n日志文件按时间回滚：[natefinch/lumberjack](https://github.com/natefinch/lumberjack)\n\n```\ngo get gopkg.in/natefinch/lumberjack.v2\n\n```\n\n## 4.strconv包\n\nint转string\n\n```\ns := strconv.Itoa(i)\n\n```\n\nint64转string\n\n```\ns := strconv.FormatInt(i, 10)\n\n```\n\nstring转int\n\n```\ni, err := strconv.Atoi(s)\n\n```\n\nstring转int64\n\n```\ni, err := strconv.ParseInt(s, 10, 64)\n\n```\n\nfloat转string\n\n```\nv := 3.1415926535\ns1 := strconv.FormatFloat(v, 'E', -1, 32)//float32\ns2 := strconv.FormatFloat(v, 'E', -1, 64)//float64\n\n```\n\nstring转float\n\n```\ns := \"3.1415926535\"\nv1, err := strconv.ParseFloat(v, 32)\nv2, err := strconv.ParseFloat(v, 64)\n\n```\n\n参考：[Go语言从入门到精通 - 【精华篇】strconv包详解](https://www.cnblogs.com/Survivalist/articles/10287383.html)\n\n## 5.属性复制\n\n可以使用 [jinzhu/copier](https://github.com/jinzhu/copier)\n\n使用jinzhu/copier\n\n```\ngo get github.com/jinzhu/copier\n\n```\n\ncopy\n\n```\ncopier.Copy(&amp;employee, &amp;user)\n\n```\n\ndeepcopy，区别是deepcopy的时候，对dst的属性进行修改，是肯定不会影响src的\n\n```\nvar dst User\ncopier.CopyWithOption(&amp;dst, src, copier.Option{DeepCopy: true})\n\n```\n\n其他类似的库还有ulule/deepcoper，参考：[golang struct拷贝工具（类似于java中 BeanUtils.copyProperties()）](https://blog.csdn.net/fengxiandada/article/details/128810055)\n\n以及\n\n```\nhttps://github.com/jinzhu/copier\nhttps://github.com/mohae/deepcopy\nhttps://github.com/ulule/deepcopier\nhttps://github.com/mitchellh/copystructure\nhttps://github.com/globusdigital/deep-copy\nhttps://github.com/getlantern/deepcopy\nhttps://github.com/antlabs/deepcopy \nhttps://github.com/go-toolsmith/astcopy\nhttps://github.com/qdm12/reprint\nhttps://github.com/huandu/go-clone\nhttps://github.com/wzshiming/deepclone \nhttps://github.com/davidwalter0/go-clone\n\n```\n\n## 6.其他数据结构\n\ngolang没有set，priorityqueue这些数据结构，可以使用[emirpasic/gods](https://pkg.go.dev/github.com/emirpasic/gods)\n\n参考：[https://github.com/emirpasic/gods](https://github.com/emirpasic/gods)\n\n## 7.切片处理\n\n可以使用[**samber/lo**](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fsamber%2Flo)来对切片、数组或集合进行处理\n\n```\ngo get github.com/samber/lo@v1\n\n```\n\n比如filter操作\n\n```\neven := lo.Filter([]int{1, 2, 3, 4}, func(x int, index int) bool {\n    return x%2 == 0\n})\n// []int{2, 4}\n\n```\n\nmap操作\n\n```\nimport \"github.com/samber/lo\"\n\nlo.Map([]int64{1, 2, 3, 4}, func(x int64, index int) string {\n    return strconv.FormatInt(x, 10)\n})\n// []string{\"1\", \"2\", \"3\", \"4\"}\n\n```\n\n去重操作\n\n```\nuniqValues := lo.Uniq([]int{1, 2, 2, 1})\n// []int{1, 2}\n\n```\n\n获得map的key，并转换成数组\n\n不使用lo\n\n```\nm := map[string]int{\n\t\"foo\": 1,\n\t\"bar\": 2,\n}\nkeys := make([]string, 0, len(m))\nfor key := range m {\n\tkeys = append(keys, key)\n}\n\n```\n\n使用lo\n\n```\nm := map[string]int{\n\t\"foo\": 1,\n\t\"bar\": 2,\n}\nkeys := lo.Keys[string, int](m)\nfmt.Println(keys)\n\n// [foo bar]\n\n```\n\nrange操作\n\n```\nresult := lo.Range(4)\n// [0, 1, 2, 3]\n\nresult := lo.Range(-4)\n// [0, -1, -2, -3]\n\nresult := lo.RangeFrom(1, 5)\n// [1, 2, 3, 4, 5]\n\nresult := lo.RangeFrom[float64](1.0, 5)\n// [1.0, 2.0, 3.0, 4.0, 5.0]\n\nresult := lo.RangeWithSteps(0, 20, 5)\n// [0, 5, 10, 15]\n\nresult := lo.RangeWithSteps[float32](-1.0, -4.0, -1.0)\n// [-1.0, -2.0, -3.0]\n\nresult := lo.RangeWithSteps(1, 4, -1)\n// []\n\nresult := lo.Range(0)\n// []\n\n```\n\n## 8.时间处理\n\n可以使用time或者[carbon](https://github.com/golang-module/carbon)\n\n### 1.time\n\n字符串转time.Time\n\n```\nimport (\n\t\"fmt\"\n\t\"time\"\n)\n\nfunc main() {\n\ttimeStr := \"2023-07-21 14:30:00\"\n\t// DateTime   = \"2006-01-02 15:04:05\"\n\t// DateOnly   = \"2006-01-02\"\n\t// TimeOnly   = \"15:04:05\"\n\t// RFC3339     = \"2006-01-02T15:04:05Z07:00\"\n\n\tparsedTime, err := time.Parse(time.DateTime, timeStr)\n\tif err != nil {\n\t\tfmt.Println(\"解析时间字符串时出错:\", err)\n\t\treturn\n\t}\n\n\tfmt.Println(\"解析后的时间:\", parsedTime)\n}\n\n```\n\ntime.Time转字符串\n\n```\nimport (\n\t\"fmt\"\n\t\"time\"\n)\n\nfunc main() {\n\t// 获取当前时间\n\tcurrentTime := time.Now()\n\t// DateTime   = \"2006-01-02 15:04:05\"\n\t// DateOnly   = \"2006-01-02\"\n\t// TimeOnly   = \"15:04:05\"\n\t// RFC3339     = \"2006-01-02T15:04:05Z07:00\"\n\ttimeStr := currentTime.Format(time.DateOnly)\n\t// 输出格式化后的时间字符串\n\tfmt.Println(\"格式化后的时间字符串:\", timeStr)\n}\n\n```\n\n### 2.carbon\n\n```\ngo get -u github.com/golang-module/carbon/v2\n\n```\n\n文档\n\n```\nhttps://pkg.go.dev/github.com/golang-module/carbon/v2\n\n```\n\n## 9.断言\n\n可以使用[stretchr/testify](https://github.com/stretchr/testify)\n\n```\ngo get -u github.com/stretchr/testify\n\n```\n\n使用assert\n\n```\nimport (\n  \"testing\"\n  \"github.com/stretchr/testify/assert\"\n)\n\nfunc TestSomething(t *testing.T) {\n\n  // assert equality\n  assert.Equal(t, 123, 123, \"they should be equal\")\n\n  // assert inequality\n  assert.NotEqual(t, 123, 456, \"they should not be equal\")\n\n  // assert for nil (good for errors)\n  assert.Nil(t, object)\n\n  // assert for not nil (good when you expect something)\n  if assert.NotNil(t, object) {\n\n    // now we know that object isn't nil, we are safe to make\n    // further assertions without causing any errors\n    assert.Equal(t, \"Something\", object.Value)\n\n  }\n\n}\n\n```\n\n## 10.resty\n\n在go中请求接口可以使用[go-resty/resty](https://github.com/go-resty/resty)框架\n\n```\ngo get github.com/go-resty/resty/v2\n\n```\n\nget请求\n\n```\n// Create a Resty Client\nclient := resty.New()\n\nresp, err := client.R().\n      SetQueryParams(map[string]string{\n          \"page_no\": \"1\",\n          \"limit\": \"20\",\n          \"sort\":\"name\",\n          \"order\": \"asc\",\n          \"random\":strconv.FormatInt(time.Now().Unix(), 10),\n      }).\n      SetHeader(\"Accept\", \"application/json\").\n      SetResult(&amp;Result{}).\n      SetAuthToken(\"BC594900518B4F7EAC75BD37F019E08FBC594900518B4F7EAC75BD37F019E08F\").\n      Get(\"/search_result\")\n\n```\n\npost请求\n\n```\n// Create a Resty Client\nclient := resty.New()\n\n// POST JSON string\n// No need to set content type, if you have client level setting\nresp, err := client.R().\n      SetHeader(\"Content-Type\", \"application/json\").\n      SetBody(`{\"username\":\"testuser\", \"password\":\"testpass\"}`).\n      SetResult(&amp;AuthSuccess{}).    // or SetResult(AuthSuccess{}).\n      Post(\"https://myapp.com/login\")\n\n```\n\n## 11.单元测试\n\n使用testing框架进行单元测试\n\n```\npackage dao\n\nimport (\n\t\"fmt\"\n\t\"gin-template/internal/database\"\n\t\"gin-template/internal/model\"\n\t\"testing\"\n)\n\nfunc Test_userDo_Create(t *testing.T) {\n\tuser := model.User{\n\t\tUsername: \"test\",\n\t\tEmail:    \"test@test\",\n\t}\n\tSetDefault(database.DB)\n\n\terr := User.Create(&amp;user)\n\tif err != nil {\n\t\tfmt.Println(\"creat user\")\n\t}\n}\n\n```\n\n如果遇到go testing flag provided but not defined: -test.v的报错，解决方法是添加一个init.go文件\n\n<img src=\"/images/517519-20240203162551151-631373911.png\" width=\"200\" height=\"370\" loading=\"lazy\" />\n\n```\npackage test\n\nimport \"testing\"\n\nfunc init() {\n\ttesting.Init()\n}\n\n```\n\n然后在使用了flag的地方添加的import中\n\n```\n_ \"gin-template/internal/test\"\n\n```\n\n<img src=\"/images/517519-20240203162722882-1100534949.png\" width=\"250\" height=\"274\" loading=\"lazy\" />\n\n参考：[问题记录：flag provided but not defined: -test.v 异常处理过程](https://www.jianshu.com/p/a4c5071c5c05)\n\n## 12.打桩工具&mdash;&mdash;go monkey\n\ngo monkey可以用于在单元测试中进行打桩（指补齐未实现的代码）\n\n参考：[Go单测从零到溜系列4&mdash;使用monkey打桩](https://www.liwenzhou.com/posts/Go/unit-test-4/)\n\n<!--more-->\n&nbsp;\n","tags":["golang"]},{"title":"Java反射机制","url":"/Java反射机制.html","content":"如果要通过**一个对象找到一个类的名称**，此时就需要用到**反射机制（反射技术是用来做框架的，一般情况下Java私有对象不能被访问，但是暴力反射可以访问私有对象）**。\n\n任何一个类如果没有明确地声明继承自哪个父类的时候，则**默认继承Object类**，所以getClass()方法是Object类中的。\n\n文件在包**java_reflect目录**下\n\n<!--more-->\n&nbsp;\n\n```\n// 类名：Y\n// 属性：\n// 方法：\nclass Y{\n\t\n}\n\n```\n\n&nbsp;\n\n```\nclass Y{\n\t\n}\n\n//主类\n//Function        : \tGetClass_demo;\npublic class GetClass_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tY y = new Y();\n\t\tSystem.out.println(y.getClass().getName());\n\t}\n\n}\n\n```\n\n**&nbsp;输出　　<img src=\"/images/517519-20170106103430878-567807263.png\" alt=\"\" />**\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n\n<img src=\"/images/517519-20160318162450506-2125954345.png\" alt=\"\" />\n\n&nbsp;\n\n**实例化Class类对象的3种方法**\n\n```\nclass Y{\n\t\n}\n\npublic class GetClass_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tClass<?> c1 = null;\n\t\tClass<?> c2 = null;\n\t\tClass<?> c3 = null;\n\t\ttry{\n\t\t\tc1 = Class.forName(\"java_basic.Y\");\t\t\t\t\t\t\t//通过forName()方法实例化对象\n\t\t}catch(ClassNotFoundException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\t\n\t\tc2 = new Y().getClass();\t\t\t\t\t\t\t\t\t//通过Object类中的方法实例\n\t\t\t\n\t\tc3 = Y.class;\t\t\t\t\t\t\t\t\t\t\t\t\t//通过类.class实例化\n\t\tSystem.out.println(\"类名称\"+c1.getName());\n\t\tSystem.out.println(\"类名称\"+c2.getName());\n\t\tSystem.out.println(\"类名称\"+c3.getName());\n\t}\n\n}\n\n```\n\n**&nbsp;输出　　<img src=\"/images/517519-20170106104807362-1564709107.png\" alt=\"\" />　**\n\n&nbsp;\n\n**Class类的使用**\n\nClass类在开发中最常见的用法就是**实例化对象**的操作，即可以**通过一个给定的字符串**（此字符串包含了完整的&ldquo;包.类&rdquo;的路径）**来实例化一个类的对象**。\n\n&nbsp;\n\n**　　1.通过无参构造实例化对象**\n\n如果要想**通过Class类对象**实例化**其他类的对象**，则可以使用**newInstance()方法**，但是必须要**保证被实例化的类中存在一个无参构造方法**\n\n```\npublic class InstanceClass_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tClass<?> c = null;\t\t//指定泛型\n\t\ttry{\n\t\t\tc = Class.forName(\"person\");\t\t\t//传入要实例化类的完整包.类名称\n\t\t}catch(ClassNotFoundException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\tperson per = null;\t\t\t//声明person对象\n\t\ttry{\n\t\t\tper = (person)c.newInstance();\t//实例化person对象\n\t\t}catch(Exception e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\tper.setAge(30);\n\t\tper.setName(\"张三\");\n\t\tSystem.out.println(per);\n\t}\n\n}\n\n```\n\n**输出　　<img src=\"/images/517519-20170106113528003-1514054492.png\" alt=\"\" />**\n\n&nbsp;\n\n**　　2.调用有参构造实例化对象**\n\n<img src=\"/images/517519-20160318231346834-159134658.png\" alt=\"\" />\n\n```\nimport java.lang.reflect.Constructor;\n\npublic class InstanceClass_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tClass<?> c = null;\t\t//指定泛型\n\t\ttry{\n\t\t\tc = Class.forName(\"person\");\t\t\t//传入要实例化类的完整包.类名称\n\t\t}catch(ClassNotFoundException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\tperson per = null;\t\t\t\t\t\t//声明person对象\n\t\tConstructor<?> cons[] = null;\t\t//声明一个表示构造方法的数组\n\t\tcons = c.getConstructors();\t\t\t//通过反射取得全部构造\n\t\ttry{\t\n\t\t\tper = (person) cons[0].newInstance(\"张三\",30);\t//实例化person对象\n\t\t}catch(Exception e){\n\t\t\te.printStackTrace();\n\t\t}\n//\t\tper.setAge(30);\n//\t\tper.setName(\"张三\");\n\t\tSystem.out.println(per);\n\t}\n\n}\n\n```\n\n&nbsp;**输出　　<img src=\"/images/517519-20170106113528003-1514054492.png\" alt=\"\" />**\n\n&nbsp;\n\n可以通过反射**得到一个类的完整结构**，需要使用到java.lang.reflect包中的以下几个类。\n\n**Constructor**:表示类中的构造方法\n\n**Field**:表示类中的属性\n\n**Method**:表示类中的方法\n\n这3个类都是**AccessibleObject**类中的子类\n\n```\ninterface China{\n\tpublic static final String NATIONAL = \"China\";\n\tpublic static final String AUTHOR = \"张三\";\n\tpublic void sayChina();\n\tpublic String sayHello(String name,int age);\n}\n\nclass Person_5 implements China{\n\n\tprivate String name;\n\tprivate int age;\n\t\n\tpublic Person_5(){\t\t//无参构造方法\n\t\t\n\t}\n\t\n\tpublic Person_5(String name) {\t//声明两个参数的构造方法\n\t\tsuper();\n\t\tthis.name = name;\n\t}\n\n\n\tpublic String getName() {\n\t\treturn name;\n\t}\n\n\tpublic void setName(String name) {\n\t\tthis.name = name;\n\t}\n\n\tpublic int getAge() {\n\t\treturn age;\n\t}\n\n\tpublic void setAge(int age) {\n\t\tthis.age = age;\n\t}\n\n\tpublic Person_5(String name, int age) {\n\t\tsuper();\n\t\tthis.name = name;\n\t\tthis.age = age;\n\t}\n\n\t@Override\n\tpublic void sayChina() {\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"作者：\"+AUTHOR+\"国籍：\"+NATIONAL);\n\t}\n\n\t@Override\n\tpublic String sayHello(String name, int age) {\n\t\t// TODO 自动生成的方法存根\n\t\treturn name+\"age:\"+age;\n\t}\n\n\n\t\n}\n\npublic class ConstructorClass_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tClass<?> c1 = null;\t\t//指定泛型\n\t\ttry{\n\t\t\tc1 = Class.forName(\"Person_5\");\t\t\t//传入要实例化类的完整包.类名称\n\t\t}catch(ClassNotFoundException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\tClass<?> c[] = c1.getInterfaces();\t\t\t//取得实现的全部接口\n\t\tfor(int i=0;i<c.length;i++){\n\t\t\tSystem.out.println(\"实现的接口名称：\"+c[i].getName());\n\t\t}\n\t\t\n\t}\n\n}\n\n```\n\n<img src=\"/images/517519-20160318235136428-140625015.png\" alt=\"\" />\n\n&nbsp;\n\n**取得父类**\n\n一个类可以实现多个接口，但是只能继承一个父类，所以如果要取得一个类的父类，可以直接使用class类中的**getSuperclass()方法**。\n\n```\ninterface China{\n\tpublic static final String NATIONAL = \"China\";\n\tpublic static final String AUTHOR = \"张三\";\n\tpublic void sayChina();\n\tpublic String sayHello(String name,int age);\n}\n\nclass Person_5 implements China{\n\n\tprivate String name;\n\tprivate int age;\n\t\n\tpublic Person_5(){\t\t//无参构造方法\n\t\t\n\t}\n\t\n\tpublic Person_5(String name) {\t//声明两个参数的构造方法\n\t\tsuper();\n\t\tthis.name = name;\n\t}\n\n\n\tpublic String getName() {\n\t\treturn name;\n\t}\n\n\tpublic void setName(String name) {\n\t\tthis.name = name;\n\t}\n\n\tpublic int getAge() {\n\t\treturn age;\n\t}\n\n\tpublic void setAge(int age) {\n\t\tthis.age = age;\n\t}\n\n\tpublic Person_5(String name, int age) {\n\t\tsuper();\n\t\tthis.name = name;\n\t\tthis.age = age;\n\t}\n\n\t@Override\n\tpublic void sayChina() {\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"作者：\"+AUTHOR+\"国籍：\"+NATIONAL);\n\t}\n\n\t@Override\n\tpublic String sayHello(String name, int age) {\n\t\t// TODO 自动生成的方法存根\n\t\treturn name+\"age:\"+age;\n\t}\n\n\n\t\n}\n\npublic class ConstructorClass_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tClass<?> c1 = null;\t\t//指定泛型\n\t\ttry{\n\t\t\tc1 = Class.forName(\"Person_5\");\t\t\t//传入要实例化类的完整包.类名称\n\t\t}catch(ClassNotFoundException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\t\n\t\tClass<?> c2 = c1.getSuperclass();\t\t\t//取得父类\n\t\tSystem.out.println(\"实现的接口名称：\"+c2.getName());\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**取得全部构造方法**\n\n要取得一个类中的全部构造方法，则必须使用Class类中的**getConstructors()方法**。\n\n```\nimport java.lang.reflect.Constructor;\n\ninterface China{\n\tpublic static final String NATIONAL = \"China\";\n\tpublic static final String AUTHOR = \"张三\";\n\tpublic void sayChina();\n\tpublic String sayHello(String name,int age);\n}\n\nclass Person_5 implements China{\n\n\tprivate String name;\n\tprivate int age;\n\t\n\tpublic Person_5(){\t\t//无参构造方法\n\t\t\n\t}\n\t\n\tpublic Person_5(String name) {\t//声明两个参数的构造方法\n\t\tsuper();\n\t\tthis.name = name;\n\t}\n\n\n\tpublic String getName() {\n\t\treturn name;\n\t}\n\n\tpublic void setName(String name) {\n\t\tthis.name = name;\n\t}\n\n\tpublic int getAge() {\n\t\treturn age;\n\t}\n\n\tpublic void setAge(int age) {\n\t\tthis.age = age;\n\t}\n\n\tpublic Person_5(String name, int age) {\n\t\tsuper();\n\t\tthis.name = name;\n\t\tthis.age = age;\n\t}\n\n\t@Override\n\tpublic void sayChina() {\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"作者：\"+AUTHOR+\"国籍：\"+NATIONAL);\n\t}\n\n\t@Override\n\tpublic String sayHello(String name, int age) {\n\t\t// TODO 自动生成的方法存根\n\t\treturn name+\"age:\"+age;\n\t}\n\n\n\t\n}\n\npublic class ConstructorClass_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tClass<?> c1 = null;\t\t//指定泛型\n\t\ttry{\n\t\t\tc1 = Class.forName(\"Person_5\");\t\t\t//传入要实例化类的完整包.类名称\n\t\t}catch(ClassNotFoundException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\t\n\t\tConstructor<?> c[] = c1.getConstructors();\t\t\t//取得全部的构造方法\n\t\tfor(int i=0;i<c.length;i++){\n\t\t\tSystem.out.println(\"全部的构造方法：\"+c[i]);\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**取得全部方法**\n\n要取得一个类中的全部方法，可以使用Class类中的getMethods()方法，此方法返回一个Method类的对象数组。\n\n如果要进一步取得方法的具体信息，例如，方法的参数、抛出的异常声明等，则就必须依靠Method类。\n\n<img src=\"/images/517519-20160319161535912-17690054.png\" alt=\"\" />\n\n&nbsp;\n\n```\nimport java.lang.reflect.Method;\nimport java.lang.reflect.Modifier;\n\npublic class GetMethod_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tClass<?> c1 = null;\t\t//指定泛型\n\t\ttry{\n\t\t\tc1 = Class.forName(\"Person_5\");\t\t\t//传入要实例化类的完整包.类名称\n\t\t}catch(ClassNotFoundException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\t\n\t\tMethod m[] = c1.getMethods();\t\t//取得全部的方法\n\t\tfor(int i=0;i<m.length;i++){\t\t\t\t//循环输出\n\t\t\tClass<?> r = m[i].getReturnType();\t//得到方法的返回值类型\n\t\t\tClass<?> p[] = m[i].getParameterTypes();\t//得到全部的参数类型\n\t\t\tint xx = m[i].getModifiers();\t\t\t\t//得到方法的修饰符，即public\n\t\t\t\n\t\t\tSystem.out.print(Modifier.toString(xx)+\" \");\t//还原修饰符\n\t\t\tSystem.out.print(r.getName()+\" \");\t\t\t\t\t//得到方法返回值\n\t\t\tSystem.out.print(m[i].getName());\t\t\t\t\t//取得方法的名称\n\t\t\tSystem.out.print(\"(\");\n\t\t\tfor(int x=0;x<p.length;x++){\n\t\t\t\tSystem.out.print(p[x].getName()+\" \"+\"arg\"+x);\t//输出参数\n\t\t\t\tif(x<p.length-1){\n\t\t\t\t\tSystem.out.print(\",\");\n\t\t\t\t}\n\t\t\t}\n\t\t\tClass<?> ex[] = m[i].getExceptionTypes();\t//得到全部的异常抛出\n\t\t\tif(ex.length>0){\t\t\t\t\t\t\t\t\t\t\t\t\t//判断是否有异常\n\t\t\t\tSystem.out.print(\") throws \");\n\t\t\t}else{\n\t\t\t\tSystem.out.print(\") \");\n\t\t\t}\n\t\t\tfor(int j=0;j<ex.length;j++){\t\t\t\t\t\t\t\t\n\t\t\t\tSystem.out.println(ex[j].getName());\t\t//输出异常信息\n\t\t\t\tif(j<ex.length-1){\n\t\t\t\t\tSystem.out.println(\",\");\n\t\t\t\t}\n\t\t\t}\n\t\t\tSystem.out.println();\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n&nbsp;\n\n**取得全部属性**\n\n**<img src=\"/images/517519-20160319170602381-1899441975.png\" alt=\"\" />**\n\n```\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Method;\nimport java.lang.reflect.Modifier;\n\npublic class GetMethod_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tClass<?> c1 = null;\t\t//指定泛型\n\t\ttry{\n\t\t\tc1 = Class.forName(\"Person_5\");\t\t\t//传入要实例化类的完整包.类名称\n\t\t}catch(ClassNotFoundException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\t\n//\t\tMethod m[] = c1.getMethods();\t\t//取得全部的方法\n//\t\tfor(int i=0;i<m.length;i++){\t\t\t\t//循环输出\n//\t\t\tClass<?> r = m[i].getReturnType();\t//得到方法的返回值类型\n//\t\t\tClass<?> p[] = m[i].getParameterTypes();\t//得到全部的参数类型\n//\t\t\tint xx = m[i].getModifiers();\t\t\t\t//得到方法的修饰符，即public\n//\t\t\t\n//\t\t\tSystem.out.print(Modifier.toString(xx)+\" \");\t//还原修饰符\n//\t\t\tSystem.out.print(r.getName()+\" \");\t\t\t\t\t//得到方法返回值\n//\t\t\tSystem.out.print(m[i].getName());\t\t\t\t\t//取得方法的名称\n//\t\t\tSystem.out.print(\"(\");\n//\t\t\tfor(int x=0;x<p.length;x++){\n//\t\t\t\tSystem.out.print(p[x].getName()+\" \"+\"arg\"+x);\t//输出参数\n//\t\t\t\tif(x<p.length-1){\n//\t\t\t\t\tSystem.out.print(\",\");\n//\t\t\t\t}\n//\t\t\t}\n//\t\t\tClass<?> ex[] = m[i].getExceptionTypes();\t//得到全部的异常抛出\n//\t\t\tif(ex.length>0){\t\t\t\t\t\t\t\t\t\t\t\t\t//判断是否有异常\n//\t\t\t\tSystem.out.print(\") throws \");\n//\t\t\t}else{\n//\t\t\t\tSystem.out.print(\") \");\n//\t\t\t}\n//\t\t\tfor(int j=0;j<ex.length;j++){\t\t\t\t\t\t\t\t\n//\t\t\t\tSystem.out.println(ex[j].getName());\t\t//输出异常信息\n//\t\t\t\tif(j<ex.length-1){\n//\t\t\t\t\tSystem.out.println(\",\");\n//\t\t\t\t}\n//\t\t\t}\n//\t\t\tSystem.out.println();\n//\t\t}\n\t\t\n\t\t{\n\t\t\tField f[] = c1.getDeclaredFields();\t\t\t//取得本类属性\t\t\t\n\t\t\tfor(int i=0;i<f.length;i++){\n\t\t\t\tClass<?> r = f[i].getType();\t\t\t//取出属性的类型\n\t\t\t\tint mo = f[i].getModifiers();\t\t//得到修饰符数字\n\t\t\t\tString priv = Modifier.toString(mo);\t//取得属性的修饰符\n\t\t\t\tSystem.out.print(\"本类属性：\");\n\t\t\t\tSystem.out.print(priv + \" \");\t\t\t\t\t\t//输出修饰符\n\t\t\t\tSystem.out.print(r.getName()+\" \"); \t\t//输出属性类型\n\t\t\t\tSystem.out.print(f[i].getName()); \t\t\t//输出属性名称\n\t\t\t\tSystem.out.println(\" ;\");\n\t\t\t}\n\t\t}\n\t\t\t\n\t\t{\n\t\t\tField f[] = c1.getFields();\t\t\t//取得父类公共属性\t\t\t\n\t\t\tfor(int i=0;i<f.length;i++){\n\t\t\t\tClass<?> r = f[i].getType();\t\t\t//取出属性的类型\n\t\t\t\tint mo = f[i].getModifiers();\t\t//得到修饰符数字\n\t\t\t\tString priv = Modifier.toString(mo);\t//取得属性的修饰符\n\t\t\t\tSystem.out.print(\"本类属性：\");\n\t\t\t\tSystem.out.print(priv + \" \");\t\t\t\t\t\t//输出修饰符\n\t\t\t\tSystem.out.print(r.getName()+\" \"); \t\t//输出属性类型\n\t\t\t\tSystem.out.print(f[i].getName()); \t\t\t//输出属性名称\n\t\t\t\tSystem.out.println(\" ;\");\n\t\t\t}\n\t\t}\n\t\t\n\t\t\n\t}\n\n}\n\n```\n\n&nbsp;\n\n反射机制还可以**调用类中的指定方法或指定属性**，并且可以通过反射完成对数组的操作。\n\n**通过反射调用类中的方法**\n\n<img src=\"/images/517519-20160319174045396-448568407.png\" alt=\"\" />\n\n```\nimport java.lang.reflect.Method;\n\npublic class InvokeChina_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tClass<?> c1 = null;\t\t//指定泛型\n\t\ttry{\n\t\t\tc1 = Class.forName(\"Person_5\");\t\t\t//传入要实例化类的完整包.类名称\n\t\t}catch(ClassNotFoundException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\ttry{\n\t\t\tMethod met = c1.getMethod(\"sayChina\");\t\t//取得一个Method对象\n\t\t\tmet.invoke(c1.newInstance());\t\t\t\t\t\t\t//使用invoke进行调用，必须传递对象实例\n\t\t}catch(Exception e){\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**可以向方法中传递两个参数**\n\n```\nimport java.lang.reflect.Method;\n\npublic class InvokeChina_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tClass<?> c1 = null;\t\t//指定泛型\n\t\ttry{\n\t\t\tc1 = Class.forName(\"Person_5\");\t\t\t//传入要实例化类的完整包.类名称\n\t\t}catch(ClassNotFoundException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\ttry{\n\t\t\t\n\t\t\tMethod met = c1.getMethod(\"sayHello\",String.class,int.class);\t//此方法需要两个参数\n\t\t\tString rv = null;\t\t\t//接收方法的返回值\n\t\t\trv = (String)met.invoke(c1.newInstance(),\"李四\",32);\t\t\t\t\t\t\t//使用invoke进行调用，必须传递对象实例\n\t\t\tSystem.out.println(rv); \t\t\t\n\t\t}catch(Exception e){\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**调用setter和getter方法**\n\n```\nimport java.lang.reflect.Method;\n\npublic class InvokeSetGet_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tClass<?> c1 = null;\t\t//指定泛型\n\t\tObject obj = null;\t\t//声明一个对象\n\t\ttry{\n\t\t\tc1 = Class.forName(\"Person_5\");\t\t\t//传入要实例化类的完整包.类名称\n\t\t}catch(ClassNotFoundException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\ttry{\n\t\t\tobj = c1.newInstance();\t\t\t//实例化操作对象\n\t\t}catch(InstantiationException | IllegalAccessException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\t\n\t\tsetter(obj, \"name\", \"张三\", String.class);\n\t\tsetter(obj, \"age\", 25, int.class);\n\t\tSystem.out.println(\"姓名：\");\n\t\tgetter(obj, \"name\");\n\t\tSystem.out.println(\"年龄：\");\n\t\tgetter(obj, \"age\");\n\t}\n\t\n\tpublic static void setter(Object obj,String att,Object value,Class<?> type){\n\t\ttry{\n\t\t\tMethod met = obj.getClass().getMethod(\"set\"+initStr(att),type);\t//设置方法参数类型\n\t\t\tmet.invoke(obj, value);\t\t\t\t//调用方法\n\t\t}catch(Exception e){\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n\t\n\tpublic static void getter(Object obj,String att){\n\t\ttry{\n\t\t\tMethod met = obj.getClass().getMethod(\"get\"+initStr(att));\t\n\t\t\tSystem.out.println(met.invoke(obj)); \t\t//接收方法的返回值\n\t\t}catch(Exception e){\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n\t\n\tpublic static String initStr(String old){\n\t\tString str = old.substring(0, 1).toUpperCase()+old.substring(1);\t//首字母大写\n\t\treturn str;\n\t\t\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**直接操作类中的属性**\n\n```\nimport java.lang.reflect.Field;\n\npublic class InvokeField_demo {\n\n\tpublic static void main(String[] args) throws Exception {\n\t\t// TODO 自动生成的方法存根\n\t\tClass<?> c1 = null;\t\t//声明Class对象\n\t\tObject obj = null;\t\t//声明一个对象\n\t\t\n\t\tc1 =Class.forName(\"Person_5\");\t//实例化Class对象\n\t\tobj = c1.newInstance();\t\t\t\t\t//实例化对象\n\t\t\n\t\tField nameField = null;\t\t//表示name属性\n\t\tField ageField = null;\t\t\t//表示age属性\n\t\tnameField = c1.getDeclaredField(\"name\");\t//取得name属性\n\t\tageField = c1.getDeclaredField(\"age\");\t\t\t//取得age属性\n\t\t\n\t\tnameField.setAccessible(true); \t\t\t\t\t\t//将name属性设置成可被外部访问\n\t\tnameField.set(obj, \"张三\"); \t\t\t\t\t\t\t\t//设置name属性内容\n\t\tageField.setAccessible(true); \t\t\t\t\t\t\t//将age属性设置成可被外部访问\n\t\tageField.set(obj, 33); \t\t\t\t\t\t\t\t\t\t\t//设置age属性内容\n\t\t\n\t\tSystem.out.println(\"姓名：\"+nameField.get(obj));\t//通过get取得属性内容\n\t\tSystem.out.println(\"年龄：\"+ageField.get(obj));\n\t}\n\n}\n\n```\n\n**通过反射操作数组**\n\n**<img src=\"/images/517519-20160320145713631-575369806.png\" alt=\"\" />**\n\n```\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Array;\n\npublic class InvokeField_demo {\n\n\tpublic static void main(String[] args) throws Exception {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tint temp[] = {1,2,3};\n\t\tClass<?> c = temp.getClass().getComponentType();\t//取得数组的Class对象\n\t\t\n\t\tSystem.out.println(\"类型：\"+c.getName()); \t\t\t\t//得到数组类型名称\n\t\tSystem.out.println(\"长度：\"+Array.getLength(temp)); \t\t\t\t//得到数组长度\n\t\tSystem.out.println(\"第一个内容：\"+Array.get(temp,0)); \t\t\t//得到第一个内容\n\t\tArray.set(temp,0,6);\t\t//修改第一个内容\n\t\tSystem.out.println(\"第一个内容：\"+Array.get(temp,0)); \t\t\t//得到第一个内容\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["Java"]},{"title":"Airflow分布式部署","url":"/Airflow分布式部署.html","content":"airflow分布式部署所依赖的组件有mysql，redis，web-server，scheduler，worker等，可以将这些组件部署在不同的机器上，来减低单台机器的压力\n\n单机安装请参考：[Ubuntu16.04安装apache-airflow](https://www.cnblogs.com/tonglin0325/p/7911022.html)\n\n然后在不同的机器上启动不同的组件\n\n```\nairflow webserver -D\nairflow scheduler -D\nairflow worker -D\n\n```\n\n安装airflow其他支持组件，比如airflow-kerberos，参考\n\n```\nhttps://awesome.dbyun.net/study/details?mid=160&amp;id=7427\n\n```\n\nairflow启用kerberos，参考\n\n```\nhttps://github.com/astronomer/airflow-guides/blob/main/guides/kerberos.md\n\n```\n\n　　\n","tags":["Airbnb"]},{"title":"MySQL学习笔记——事务和隔离级别","url":"/MySQL学习笔记——事务和隔离级别.html","content":"## ACID\n\n**ACID** 是**数据库事务管理**的四个关键属性，用于确保数据在并发环境下的可靠性和一致性。\n\n### 1.Atomicity（原子性）\n\n原子性指的是一个事务中的所有操作要么全部执行成功，要么全部不执行。换句话说，事务是不可分割的最小单位。ACID模型的原子性主要涉及InnoDB事务。\n\n例如，在银行转账操作中，假设从账户 A 转账到账户 B 是一个事务。如果在从 A 扣款后，B 未能成功入账，整个操作就会回滚到初始状态，A 账户的余额也不会减少。\n\n相关的MySQL功能包括:\n\n- AUTOCOMMIT设置。\n- COMMIT语句。<!--more-->\n&nbsp;\n- ROLLBACK语句。\n\n### 2.Consistency（一致性）\n\n数据库通常有各种完整性约束，如主键约束（Primary Key Constraint）、外键约束（Foreign Key Constraint）、唯一性约束（Unique Constraint）、检查约束（Check Constraint）等。一致性要求在事务执行之前和之后，这些约束必须始终满足。\n\n例如，假设数据库要求每个用户的余额不能为负数，如果一个事务试图将用户的余额更新为负数，该事务将会失败并回滚，因为它违反了数据库的一致性约束。\n\n### 3.Isolation（隔离性）\n\n隔离性指的是在**并发环境**下，**各个事务相互隔离执行**，事务的中间状态对其他事务是不可见的。这意味着每个事务的执行应该不会受到其他事务的影响。\n\n数据库通过使用锁和其他并发控制机制来实现隔离性。例如，在两个事务同时试图更新同一条记录的情况下，数据库会确保这些事务不会相互干扰，一个事务必须在另一个事务完成之前等待。\n\n### 4.Durability（持久性）\n\n持久性指的是一旦事务提交，数据库系统必须确保事务的结果被永久保存下来，即使在系统崩溃或断电的情况下也是如此。\n\n这通常通过将事务的更改写入持久存储（如硬盘）来实现。例如，如果一个银行转账事务已经提交，即使服务器随后崩溃了，系统在恢复后仍然能保证该事务的结果是持久的，转账操作已被正确记录。\n\n参考：[https://dev.mysql.com/doc/refman/5.7/en/mysql-acid.html](https://dev.mysql.com/doc/refman/5.7/en/mysql-acid.html)\n\n## InnoDB引擎如何保证ACID\n\n- 原子性是通过 undo log（回滚日志） 来保证的；\n- 持久性是通过 redo log （重做日志）来保证的；\n- 隔离性是通过 MVCC（多版本并发控制） 或锁机制来保证的；\n- 一致性则是通过持久性+原子性+隔离性来保证；\n\n## MySQL数据库隔离级别\n\n可以使用如下命令查看MySQL的隔离级别，**可重复读(Repeated Read)是****InnoDB的默认隔离级别**\n\n```\nmysql> SELECT @@transaction_isolation;\n+-------------------------+\n| @@transaction_isolation |\n+-------------------------+\n| REPEATABLE-READ         |\n+-------------------------+\n1 row in set (0.00 sec)\n\nmysql> SHOW VARIABLES LIKE 'transaction_isolation';\n+-----------------------+-----------------+\n| Variable_name         | Value           |\n+-----------------------+-----------------+\n| transaction_isolation | REPEATABLE-READ |\n+-----------------------+-----------------+\n1 row in set (0.00 sec)\n\nmysql> SHOW VARIABLES LIKE 'tx_isolation'; # 从 MySQL 5.7.20 和 8.0.3 开始，tx_isolation 已被弃用，推荐使用 transaction_isolation\n+---------------+-----------------+\n| Variable_name | Value           |\n+---------------+-----------------+\n| tx_isolation  | REPEATABLE-READ |\n+---------------+-----------------+\n1 row in set (0.00 sec)\n\n```\n\n在数据库操作中，为了有效保证并发读取数据的正确性，提出的事务隔离级别。我们的数据库锁，也是为了构建这些隔离级别存在的。总共有4种：\n\n**1.未提交读(Read Uncommitted)**：允许脏读，也就是可能读取到其他会话中未提交事务修改的数据\n\n**2.提交读(Read Committed)**：只能读取到已经提交的数据。Oracle等多数数据库默认都是该级别 (不重复读)\n\n**3.可重复读(Repeated Read)**：可重复读。在同一个事务内的查询都是事务开始时刻一致的，**是****InnoDB的默认隔离级别**。在SQL标准中，该隔离级别消除了不可重复读，但是还存在幻象读\n\n**4.串行读(Serializable)**：完全串行化的读，每次读都需要获得**表级共享锁**，读写相互都会阻塞\n\n<img src=\"/images/517519-20210424201124177-1141693302.png\" width=\"1000\" height=\"177\" loading=\"lazy\" />\n\n参考美团点评的文章：[Innodb中的事务隔离级别和锁的关系](https://tech.meituan.com/2014/08/20/innodb-lock.html)\n\n## 脏读、幻读与不可重复读\n\n**脏读（<strong>dirty read**）</strong>：所谓脏读是指一个事务中**访问到了另外一个事务未提交的数据**\n\n**幻读（<strong>phantom read**）</strong>：在同一个事务中两次执行相同的查询，**得到的记录数**却因为其他事务的插入操作而不同。\n\n具体来说，当一个事务 `T1` 多次查询满足某个条件的数据集时，另一个事务 `T2` 在两次查询之间插入了新的数据行，导致第二次查询返回了更多的行，这些新行就是&ldquo;幻影&rdquo;数据。\n\n**不可重复读（<strong>non-repeatable read**）</strong>：一个事务内，读取同一条记录2次，**得到的数据**不一致，说明发生了不可重复读。\n\n在可重复读隔离级别下，不会产生不可重复读：因为对每一行数据的读取会加上行级锁，这确保了在该事务进行期间，这些行不会被其他事务修改。\n\n参考：[脏读、幻读与不可重复读](https://juejin.cn/post/6844903665367547918)\n","tags":["MySQL"]},{"title":"hudi-cli使用","url":"/hudi-cli使用.html","content":"在 Amazon EMR 版本 5.28.0 及更高版本中， Amazon EMR 默认情况下会在安装 Spark、Hive 或 Presto 时安装 Hudi 组件。\n\n参考：[创建安装了 Hudi 的集群](https://docs.aws.amazon.com/zh_cn/emr/latest/ReleaseGuide/emr-hudi-installation-and-configuration.html)\n\nAmazon EMR的版本和hudi的版本对应可以参考文档：[Hudi 发行版历史记录](https://docs.aws.amazon.com/zh_cn/emr/latest/ReleaseGuide/Hudi-release-history.html)\n\n可以使用下面命令进入hudi-cli\n\n```\n/usr/lib/hudi/cli/bin/hudi-cli.sh\n\n```\n\nhudi-cli如下\n\n<img src=\"/images/517519-20230110153539593-1470562504.png\" alt=\"\" loading=\"lazy\" />\n\n退出cli，输入exit\n\n我们可以在hive中注册hudi表，然后使用hive，sparkSQL或者presto对其进行查询\n\n参考：[Hudi 的工作原理](https://docs.aws.amazon.com/zh_cn/emr/latest/ReleaseGuide/emr-hudi-how-it-works.html)\n\n常用命令：[Apache Hudi数据湖hudi-cli客户端使用](https://blog.csdn.net/yy8623977/article/details/123861951)\n\n官方文档：\n\n```\nhttps://hudi.apache.org/docs/cli#local-set-up\n```\n\n## 1.连接hudi表\n\n```\nconnect --path s3://bucket_name/path_xxx\n\n```\n\n## 2.查看hudi表的属性\n\n```\ndesc\n\n```\n\n<img src=\"/images/517519-20230414170849276-830600102.png\" alt=\"\" loading=\"lazy\" /><!--more-->\n&nbsp;\n\n## 3.查看commit的完成情况\n\n```\ncommits show\n\n```\n\n<img src=\"/images/517519-20230414171335323-1714933573.png\" alt=\"\" loading=\"lazy\" />&nbsp;\n\n## 4.查看compaction的完成情况\n\n```\ncompactions show all\n\n```\n\n<img src=\"/images/517519-20230414170519265-776996355.png\" alt=\"\" loading=\"lazy\" />\n\n当第一个compaction的状态是COMPLETED的时候，说明第一个合并任务已经完成，此时路径下就会出现parquet文件，如果是MOR表，就可以在ro表中查询到数据\n\n## 5.查看未完成的instant\n\n```\ntimeline show incomplete\n\n```\n\n<img src=\"/images/517519-20230414232114027-1495081129.png\" width=\"600\" height=\"116\" loading=\"lazy\" />\n\n注意查看deltacommit的状态变为completed状态的时候，需要**重新connect**这张表或者使用**commits refresh**命令来刷新元数据，然后再命令timeline show incomplete，才能获取到最新的deltacommit，否者永远都是之前的deltacommit\n\n<img src=\"/images/517519-20230612143721394-427748823.png\" alt=\"\" loading=\"lazy\" />\n\n重新连接hudi表，看到新的 06-12 06:35 的deltacommit\n\n<img src=\"/images/517519-20230612144005971-1637716121.png\" alt=\"\" loading=\"lazy\" />\n\n或者使用commits refresh命令刷新一下\n\n<img src=\"/images/517519-20230612144317495-1007228306.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["Hudi"]},{"title":"mongo基本操作","url":"/mongo基本操作.html","content":"## 1.安装mongo客户端\n\n参考：[ubuntu安装mongodb-4.4(通过apt命令)](https://blog.csdn.net/m0_38138879/article/details/117249425)\n\n## 2.连接mongodb\n\n```\nmongo ip:27017/db_name -u user_name -p\n```\n\n## 3.创建collection\n\n参考：[MongoDB 教程](https://www.runoob.com/mongodb/mongodb-create-collection.html)\n\n```\nuse xx_db\ndb.createCollection(\"xx_collection\")\n```\n\n## 4.插入数据\n\ninsert one\n\n```\nuse xx_db\n\ndb.xx_collection.insertOne(\n    {\n        \"id\":\"100000\",\n        \"time\":new Date(),\n        \"abc\":\"123456\"\n    }\n)\n\n```\n\ninsert many\n\n```\ndb.xx_collection.insertMany(\n[\n    {\n        \"id\":\"200000\",\n        \"time\":new Date(),\n        \"token\":\"1111111\"\n    }, {\n        \"id\":\"300000\",\n        \"time\":new Date(),\n        \"token\":\"2222222\"\n    }\n]\n)\n```\n\n## 5.删除数据\n\ndelete one\n\n```\nuse xx_db\ndb.xx_collection.deleteOne({'id':'200000'})\n\n```\n\ndelete many\n\n```\ndb.xx_collection.deleteMany({'token':'1111111'})\n\n```\n\n## 6.查看Mongo的collection大小\n\n```\ndb.getCollection(\"your_collection\").stats()\n\n```\n\n<img src=\"/images/517519-20230613151257342-1840298846.png\" alt=\"\" loading=\"lazy\" />\n\n参考：[获取mongo 数据大小及collection大小](https://www.cnblogs.com/baby123/p/13527801.html)\n\n## 7.查看collection是否是sharded\n\n```\ndb.getCollection(\"your_collection\").stats()\n\n```\n\n没有sharded字段，或者sharded值为false，都不是sharded collection\n\n<img src=\"/images/517519-20230613150816471-937556156.png\" alt=\"\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n","tags":["mongo"]},{"title":"Helm基本操作","url":"/Helm基本操作.html","content":"Helm是k8s的包管理工具，使用helm可以简化k8s应用部署\n\n而使用了helm之后，helm会提供一个模板，将这些yaml文件作为一个整体进行管理，高效复用，同时方便版本管理\n\n<!--more-->\n&nbsp;\n\n## **1.安装Helm和配置仓库**\n\n```\nhttps://helm.sh/zh/docs/intro/install/\n\n```\n\nmac安装helm\n\n```\nbrew install helm\n\n```\n\nubuntu安装helm\n\n```\ncurl https://baltocdn.com/helm/signing.asc | sudo apt-key add -\nsudo apt-get install apt-transport-https --yes\necho \"deb https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n\n```\n\n添加helm&nbsp;bitnami源\n\n```\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\n\n```\n\n添加helm阿里云源\n\n```\nhelm repo remove stable\nhelm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts\nhelm repo update\n\n```\n\n删除helm源\n\n```\nhelm repo remove stable\n```\n\n## 2.查找chart\n\n```\nhelm search repo nginx\n\nNAME                            \tCHART VERSION\tAPP VERSION\tDESCRIPTION\nbitnami/nginx                   \t9.8.0        \t1.21.6     \tNGINX Open Source is a web server that can be a...\nbitnami/nginx-ingress-controller\t9.1.7        \t1.1.1      \tNGINX Ingress Controller is an Ingress controll...\nbitnami/nginx-intel             \t0.1.4        \t0.4.7      \tNGINX Open Source for Intel is a lightweight se...\nstable/nginx-ingress            \t0.9.5        \t0.10.2     \tAn nginx Ingress controller that uses ConfigMap...\nstable/nginx-lego               \t0.3.1        \t           \tChart for nginx-ingress-controller and kube-lego\nbitnami/kong                    \t5.0.2        \t2.7.0      \tKong is a scalable, open source API layer (aka ...\nstable/gcloud-endpoints         \t0.1.0        \t           \tDevelop, deploy, protect and monitor your APIs ...\n\nhelm search repo kubernetes-dashboard\nNAME                            CHART VERSION   APP VERSION     DESCRIPTION\nstable/kubernetes-dashboard     0.6.0           1.8.3           General-purpose web UI for Kubernetes clusters\n\n```\n\n　　\n\n## 3.部署release\n\n其中mynginx是部署后的release名称，bitnami/nginx是chart的名称\n\n```\nhelm install mynginx bitnami/nginx\n\nNAME: mynginx\nLAST DEPLOYED: Wed Jun  1 17:29:42 2022\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nCHART NAME: nginx\nCHART VERSION: 9.8.0\nAPP VERSION: 1.21.6\n\n** Please be patient while the chart is being deployed **\n\nNGINX can be accessed through the following DNS name from within your cluster:\n\n    mynginx.default.svc.cluster.local (port 80)\n\nTo access NGINX from outside the cluster, follow the steps below:\n\n1. Get the NGINX URL by running these commands:\n\n  NOTE: It may take a few minutes for the LoadBalancer IP to be available.\n        Watch the status with: 'kubectl get svc --namespace default -w mynginx'\n\n    export SERVICE_PORT=$(kubectl get --namespace default -o jsonpath=\"{.spec.ports[0].port}\" services mynginx)\n    export SERVICE_IP=$(kubectl get svc --namespace default mynginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n    echo \"http://${SERVICE_IP}:${SERVICE_PORT}\"\n\n```\n\n如果想卸载release\n\n```\nhelm uninstall mynginx -n default\n\nrelease \"mynginx\" uninstalled\n\n```\n\n　　\n\n## 4.查看部署的release\n\n```\nhelm list\n\nNAME   \tNAMESPACE\tREVISION\tUPDATED                             \tSTATUS  \tCHART      \tAPP VERSION\nmynginx\tdefault  \t1       \t2022-06-01 17:29:42.587416 +0800 CST\tdeployed\tnginx-9.8.0\t1.21.6\n\n```\n\n或者\n\n```\nhelm ls -A\nNAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION\nkubernetes-dashboard    kube-system     1               2022-03-01 20:31:13.287929871 +0800 CST deployed        kubernetes-dashboard-0.6.0      1.8.3\n\n```\n\n查看部署release的状态\n\n```\nhelm status mynginx\n\nNAME: mynginx\nLAST DEPLOYED: Wed Jun  1 23:28:47 2022\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nCHART NAME: nginx\nCHART VERSION: 9.8.0\nAPP VERSION: 1.21.6\n\n** Please be patient while the chart is being deployed **\n\nNGINX can be accessed through the following DNS name from within your cluster:\n\n    mynginx.default.svc.cluster.local (port 80)\n\nTo access NGINX from outside the cluster, follow the steps below:\n\n1. Get the NGINX URL by running these commands:\n\n  NOTE: It may take a few minutes for the LoadBalancer IP to be available.\n        Watch the status with: 'kubectl get svc --namespace default -w mynginx'\n\n    export SERVICE_PORT=$(kubectl get --namespace default -o jsonpath=\"{.spec.ports[0].port}\" services mynginx)\n    export SERVICE_IP=$(kubectl get svc --namespace default mynginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n    echo \"http://${SERVICE_IP}:${SERVICE_PORT}\"\n\n```\n\n可以看到pod已经启动\n\n```\nkubectl get pod -A\nNAMESPACE     NAME                                     READY   STATUS    RESTARTS        AGE\ndefault       mynginx-5998cdd48b-x59wz                 1/1     Running   0               3m29s\n\n```\n\n查看service\n\n```\nkubectl get svc --namespace default mynginx\nNAME      TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE\nmynginx   LoadBalancer   10.99.159.33   localhost     80:31833/TCP   3m57s\n\n```\n\n可以通过访问localhost:80访问nginx的页面\n\n<img src=\"/images/517519-20220601233406810-104346788.png\" width=\"700\" height=\"192\" loading=\"lazy\" />\n\n&nbsp;\n\n## 5.更新values.yaml\n\n首先下载chart并解压\n\n```\nhelm fetch bitnami/nginx\ntar -zxvf nginx-9.8.0.tgz\n\n```\n\n编辑values.yaml，修改nginx HTTP port 80成18000\n\n<img src=\"/images/517519-20220601235208777-53807205.png\" width=\"400\" height=\"173\" loading=\"lazy\" />\n\n如果修改了values文件，可以这样更新\n\n```\nhelm upgrade mynginx -f ./values.yaml . --namespace default\n\n```\n\n可以看到service端口已经被更新到18000\n\n```\nkubectl get svc -A\nNAMESPACE     NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE\ndefault       kubernetes   ClusterIP      10.96.0.1       <none>        443/TCP                  17d\ndefault       mynginx      LoadBalancer   10.111.232.86   localhost     18000:32395/TCP          3m58s\n\n```\n\n或者也可以直接编辑service来修改端口\n\n```\nkubectl edit svc mynginx\n\n```\n\n<img src=\"/images/517519-20220602111143018-654368923.png\" width=\"200\" height=\"81\" loading=\"lazy\" />&nbsp;\n\n&nbsp;&nbsp;\n\n## 6.如何创建chart\n\n创建chart模板\n\n```\nhelm create mychart\n\n```\n\n或者\n\n<img src=\"/images/517519-20220602121810663-259187919.png\" width=\"400\" height=\"170\" loading=\"lazy\" />\n\n创建deployment.yaml\n\n```\nkubectl create deployment mynginx --image=nginx --dry-run=client -o yaml > deployment.yaml\n\n➜  /Users/lintong/coding/helm/mychart/templates $ ls\ndeployment.yaml\n\n```\n\n创建pod\n\n```\nkubectl create deployment mynginx --image=nginx\ndeployment.apps/mynginx created\n\nkubectl get pods -A\nNAMESPACE     NAME                                     READY   STATUS    RESTARTS        AGE\ndefault       mynginx-654f8684f8-74dlg                 1/1     Running   0               26s\n\n```\n\n创建service.yaml\n\n```\nkubectl expose deployment mynginx --port=18000 --target-port=80 --type=NodePort --dry-run=client -o yaml > service.yaml\n\n➜  /Users/lintong/coding/helm/mychart/templates $ ls\ndeployment.yaml service.yaml\n\n```\n\n之后就可以安装了mychart了\n\n```\n➜  /Users/lintong/coding/helm $ helm install mynginx mychart/\n\n```\n\n可以在values.yaml文件中定义变量，然后供deployment.yaml和service.yaml使用，比如\n\n```\nreplicas: 1\nimage: nginx\ntag: 1.16\nlabel: nginx\nport: 18000\n\n```\n\n然后修改deployment，service和ingress的模板，以 .Values.变量名称 的方式来使用values.yaml中的变量\n\ndeployment.yaml\n\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: {{ .Values.label }}\n  name: {{ .Release.Name }}-deploy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: {{ .Values.label }}\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: {{ .Values.label }}\n    spec:\n      containers:\n      - image: {{ .Values.image }}\n        name: {{ .Values.label }}\n        resources: {}\nstatus: {}\n\n```\n\nservice.yaml\n\n```\napiVersion: v1\nkind: Service\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: {{ .Values.label }}\n  name: {{ .Release.Name }}-svc\nspec:\n  ports:\n  - port: {{ .Values.port }}\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: {{ .Values.label }}\n  type: NodePort\nstatus:\n  loadBalancer: {}\n\n```\n\ningress.yaml\n\n```\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: {{ .Release.Name }}-ingress\n  annotations:\n    kubernetes.io/ingress.class: nginx-internal\n  labels:\n    app: {{ .Values.label }}\nspec:\n  rules:\n  - host: {{ .Values.label }}.k8s.xxx.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: {{ .Release.Name }}-svc\n            port:\n              number: {{ .Values.port }}\n        path: /\n        pathType: ImplementationSpecific\n\n```\n\n验证模板\n\n```\nhelm install myweb mychart/ --dry-run\n\nNAME: myweb\nLAST DEPLOYED: Thu Jun  2 17:02:29 2022\nNAMESPACE: default\nSTATUS: pending-install\nREVISION: 1\nTEST SUITE: None\nHOOKS:\nMANIFEST:\n---\n# Source: mychart/templates/service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: nginx\n  name: myweb-svc\nspec:\n  ports:\n  - port: 18000\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: nginx\n  type: NodePort\nstatus:\n  loadBalancer: {}\n---\n# Source: mychart/templates/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: nginx\n  name: myweb-deploy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        resources: {}\nstatus: {}\n\n```\n\ninstall release\n\n```\nhelm install myweb mychart/\n\n```\n\n接下来就可以使用下面命令查看域名，来访问服务\n\n```\nkubectl get ingress -n default\n\n```\n\n如果想要卸载release的话\n\n```\nhelm install myweb -n default\n\n```\n\n其他helm变量用法可以参考：[Helm教程](https://www.cnblogs.com/lyc94620/p/10945430.html)\n\n&nbsp;\n","tags":["k8s"]},{"title":"Hudi学习笔记——基本操作","url":"/Hudi学习笔记——基本操作.html","content":"# 1.Hudi概念\n\n参考：\n\n英文官方文档：[https://hudi.apache.org/docs/concepts/](https://hudi.apache.org/docs/concepts/)\n\n中文官方文档：[https://hudi.apache.org/cn/docs/0.9.0/concepts/](https://hudi.apache.org/cn/docs/0.9.0/concepts/)\n\n[Apache Hudi架构设计和基本概念](http://shiyanjun.cn/archives/2043.html)\n\n## 1.Hudi表的存储类型\n\nhudi表的类型有2种，分成cow和mor\n\ncow是Copy On Write的缩写，含义是**写入时复制**（只有parquet文件）\n\nmor是Merge On Read的缩写，含义是**读取时合并**（log文件+parquet文件）\n\n2种hudi表类型的对比\n\n<img src=\"/images/517519-20230130115036946-292986041.png\" alt=\"\" width=\"496\" height=\"224\" loading=\"lazy\" />\n\n适用场景，参考：[ByteLake：字节跳动基于Apache Hudi的实时数据湖平台](https://cloud.tencent.com/developer/article/1897241)\n\n**COW表**适用于**批量离线更新场景**\n\n**MOR表**适用于**实时高频更新场景**\n\n### 1.写时复制\n\n**写时复制存储**中的文件片仅包含基本/列文件，并且每次提交都会生成新版本的基本文件。 换句话说，我们压缩每个提交，从而所有的数据都是以列数据的形式储存。在这种情况下，写入数据非常昂贵（我们需要重写整个列数据文件，即使只有一个字节的新数据被提交），而读取数据的成本则没有增加。 这种视图有利于读取繁重的分析工作。\n\ncow类型的hudi表文件目录如下，参考：[https://jxeditor.github.io/2021/05/07/Hudi初探/#开始文件分析COW](https://jxeditor.github.io/2021/05/07/Hudi%E5%88%9D%E6%8E%A2/#%E5%BC%80%E5%A7%8B%E6%96%87%E4%BB%B6%E5%88%86%E6%9E%90COW)\n\n```\n.\n├── .hoodie\n│   ├── .aux\n│   │   └── .bootstrap\n│   │       ├── .fileids\n│   │       └── .partitions\n│   ├── .temp\n│   ├── 20210511100234.commit\n│   ├── 20210511100234.commit.requested\n│   ├── 20210511100234.inflight\n│   ├── 20210511100304.commit\n│   ├── 20210511100304.commit.requested\n│   ├── 20210511100304.inflight\n│   ├── 20210511100402.commit\n│   ├── 20210511100402.commit.requested\n│   ├── 20210511100402.inflight\n│   ├── 20210511100503.commit.requested\n│   ├── 20210511100503.inflight\n│   ├── archived\n│   └── hoodie.properties\n└── par1\n    ├── .hoodie_partition_metadata\n    ├── 48ca22d7-d503-4073-93ec-be7a33e6e8f4_0-1-0_20210511100234.parquet\n    ├── 99aaf2d1-8e82-42d9-a982-65ed8d258f28_0-1-0_20210511100304.parquet\n    └── 99aaf2d1-8e82-42d9-a982-65ed8d258f28_0-1-0_20210511100402.parquet\n\n```\n\n使用写时复制的时候，对于update操作，当commit了之后，即使只是update了一行数据的一个字段，hudi会将这行数据所在的文件再copy一遍，然后加个这个更新之后，重新生成一个文件，所以在使用COW表的时候可能会产生**写放大**的问题，参考：[Hudi COW表的数据膨胀（清除历史版本）问题](https://blog.csdn.net/bluishglc/article/details/115531015)\n\nhudi对于历史数据的保留版本，由hoodie.cleaner.commits.retained参数来控制，默认是24\n\n在查询COW表的时候，可以通过read.start-commit加上read.end-commit参数来按commit时间来控制查询范围，格式为yyyyMMddHHmmss，所有只使用一个参数的话，只有查询最新的一个commit，参考官方配置文档：[https://hudi.apache.org/docs/configurations/](https://hudi.apache.org/docs/configurations/)\n\n### **2.读时合并**\n\n**读时合并存储**是写时复制的升级版，从某种意义上说，它仍然可以通过读优化表提供数据集的读取优化视图（写时复制的功能）。 此外，它将每个文件组的更新插入存储到基于行的增量日志中，通过文件id，将增量日志和最新版本的基本文件进行合并，从而提供近实时的数据查询。因此，此存储类型智能地平衡了读和写的成本，以提供近乎实时的查询。 这里最重要的一点是压缩器，它现在可以仔细挑选需要压缩到其列式基础文件中的增量日志（根据增量日志的文件大小），以保持查询性能（较大的增量日志将会提升近实时的查询时间，并同时需要更长的合并时间）。\n\nmor类型的hudi表文件目录如下，参考：[https://jxeditor.github.io/2021/05/07/Hudi初探/#开始文件分析MOR](https://jxeditor.github.io/2021/05/07/Hudi%E5%88%9D%E6%8E%A2/#%E5%BC%80%E5%A7%8B%E6%96%87%E4%BB%B6%E5%88%86%E6%9E%90MOR)\n\n可以看到，采用读时合并的目录下除了.parquet文件，还有.log文件，其中.log文件采用的就是avro序列化格式，log文件格式结构可以参考：[Hudi Log 文件格式与读写流程](https://juejin.cn/post/7045527350519431204)\n\n```\n.\n├── .hoodie\n│   ├── .aux\n│   │   ├── .bootstrap\n│   │   │   ├── .fileids\n│   │   │   └── .partitions\n│   │   ├── 20210510175131.compaction.requested\n│   │   ├── 20210510181521.compaction.requested\n│   │   ├── 20210510190422.compaction.requested\n│   │   └── 20210511091552.compaction.requested\n│   ├── .temp\n│   │   ├── 20210510175131\n│   │   │   └── par1\n│   │   │       ├── 0a0110a8-b26b-4595-b5fb-b2388263ac54_0-1-0_20210510175131.parquet.marker.CREATE\n│   │   │       ├── 39798d42-eb9f-46c7-aece-bdbe9fed3c45_0-1-0_20210510175131.parquet.marker.CREATE\n│   │   │       └── 75dff8c0-614a-43e9-83c7-452dfa1dc797_0-1-0_20210510175131.parquet.marker.CREATE\n│   │   ├── 20210510181521\n│   │   │   └── par1\n│   │   │       └── 39798d42-eb9f-46c7-aece-bdbe9fed3c45_0-1-0_20210510181521.parquet.marker.MERGE\n│   │   └── 20210510190422\n│   │       └── par1\n│   │           └── 39798d42-eb9f-46c7-aece-bdbe9fed3c45_0-1-0_20210510190422.parquet.marker.MERGE\n│   ├── 20210510174713.deltacommit\n│   ├── 20210510174713.deltacommit.inflight\n│   ├── 20210510174713.deltacommit.requested\n......\n│   ├── 20210510181212.rollback\n│   ├── 20210510181212.rollback.inflight\n......\n│   ├── 20210510181521.commit\n│   ├── 20210510181521.compaction.inflight\n│   ├── 20210510181521.compaction.requested\n......\n│   ├── 20210511090334.clean\n│   ├── 20210511090334.clean.inflight\n│   ├── 20210511090334.clean.requested\n......\n│   ├── archived -- 归档目录,操作未达到默认值时,没有产生对应文件\n│   └── hoodie.properties\n└── par1\n    ├── .39798d42-eb9f-46c7-aece-bdbe9fed3c45_20210510181521.log.1_0-1-0\n    ├── .39798d42-eb9f-46c7-aece-bdbe9fed3c45_20210510190422.log.1_0-1-0\n    ├── .39798d42-eb9f-46c7-aece-bdbe9fed3c45_20210511091552.log.1_0-1-2\n    ├── .hoodie_partition_metadata\n    ├── 0a0110a8-b26b-4595-b5fb-b2388263ac54_0-1-0_20210510175131.parquet\n    ├── 39798d42-eb9f-46c7-aece-bdbe9fed3c45_0-1-0_20210510181521.parquet\n    ├── 39798d42-eb9f-46c7-aece-bdbe9fed3c45_0-1-0_20210510190422.parquet\n    └── 75dff8c0-614a-43e9-83c7-452dfa1dc797_0-1-0_20210510175131.parquet\n\n```\n\nparquet文件什么时候合并由参数 compaction.delta_commits 和<!--more-->\n&nbsp;compaction.delta_seconds[​](https://hudi.apache.org/docs/0.12.1/configurations#compactiondelta_seconds)控制，delta_commits其值为3的话表示，在3次commit之后，会compaction产生parquet文件\n\n由于这里设置的commit时间是每5分钟一次，所以可以看到对于标红的log文件，第一次commit是08:46，第二次是08:51，第三次是08:56，之后产生parquet文件是09:02\n\n<img src=\"/images/517519-20230328171135828-108554827.png\" alt=\"\" loading=\"lazy\" />\n\n对于.hoodie下面的文件，20210510181521.compaction.requested 表示20210510181521的instant请求compaction，20210510181521.compaction.inflight 表示20210510181521的instant正在compaction，20210510181521.commit 表示20210510181521的instant compaction已经完成\n\n## 2.Hudi的数据类型\n\n```\nhttps://hudi.apache.org/docs/next/table_management/#supported-types\n```\n\n## 3.Hudi Query的类型（视图）\n\nHudi query的类型有3种：Snapshot Queries，Incremental Queries和Read Optimized Queries\n\n### **1.读优化视图（Snapshot Queries）**\n\n查询看到表的最新快照作为给定的提交或压缩操作。\n\n在读取merge on read表的情况下，它通过即时合并最新文件切片的基础文件和增量文件来公开近实时数据（几分钟）。\n\n对于copy on write表，它提供了对现有表的直接替换，同时提供更新插入/删除和其他写入端功能。\n\n### **2.增量视图（Incremental Queries）**\n\n查询只能看到自给定提交/压缩以来写入表的新数据。这有效地提供了更改流以启用增量数据管道。\n\n### **3.实时视图（Read Optimized Queries）**\n\n查询查看截至给定提交/压缩操作的表的最新快照。仅公开最新文件片中的基/列式文件，并保证与非 hudi 列式表相同的列式查询性能。\n\n对于COW和MOR，所支持的query类型也不同，参考：\n\n**写时复制（copy on write）**支持Snapshot Queries和Incremental Queries\n\n**读时合并（merge on read）**支持Snapshot Queries，Incremental Queries和Read Optimized Queries\n\n<img src=\"/images/517519-20230214163608836-379844128.png\" alt=\"\" width=\"511\" height=\"129\" loading=\"lazy\" />\n\n不同视图的对比：\n\n<img src=\"/images/517519-20230130115311758-860839012.png\" alt=\"\" width=\"375\" height=\"115\" loading=\"lazy\" />\n\n## 4.Hudi的payload\n\n目前提供的payload\n\n```\nhttps://github.com/apache/hudi/tree/master/hudi-common/src/main/java/org/apache/hudi/common/model\n\n```\n\n参考：[超硬核解析！Apache Hudi灵活的Payload机制](https://www.cnblogs.com/leesf456/p/16068688.html)&nbsp;\n\n## 5.Hudi的写入方式\n\n### 1.insert\n\n### 2.bulk_insert\n\n### 3.upsert\n\n### 4.delete\n\n## 6.Hudi删除数据的方式\n\n&nbsp;\n\n## 7.hudi和hive集成\n\n&nbsp;\n\n## 8.Hudi问题集合\n\n## 9.Hudi业界使用案例\n","tags":["Hudi"]},{"title":"java程序内存泄露排查","url":"/java程序内存泄露排查.html","content":"1.生成内存快照文件(Heap Profile)\n\n```\njmap -dump:format=b,file=heap.hprof ${pid}\n\n```\n\n2.使用Eclipse Memory Analyzer工具对hprof文件进行分析\n\n1.12.0版本需要jdk11，所以下载1.10.0版本\n\n```\nhttp://www.eclipse.org/downloads/download.php?file=/mat/1.10.0/rcp/MemoryAnalyzer-1.10.0.20200225-linux.gtk.x86_64.zip\n\n```\n\n由于堆栈文件可能会很大，所以需要修改 MemoryAnalyzer.ini 文件中的-Xmx1024m，比如改成-Xmx10240m，否则可能会遇到下面报错\n\n```\nan internal error occurred during parsing heap dump from\n\n```\n\n3.分析堆栈文件\n\n<img src=\"/images/517519-20210728112351955-126381468.png\" alt=\"\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n查看内存泄露报告\n\n&nbsp;<img src=\"/images/517519-20210803201054664-987536574.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n发现有2处可疑的泄露\n\n<img src=\"/images/517519-20210803201131120-1487448117.png\" alt=\"\" loading=\"lazy\" />\n\n点击details，排查到可能是&nbsp;class io.lettuce.core.AbstractRedisClient @ 0x41bc37380 引起的\n\n&nbsp;<img src=\"/images/517519-20210803203206550-1663657782.png\" alt=\"\" loading=\"lazy\" />\n\n排查使用的redis依赖是否有内存泄露的可能，找到\n\n[springboot使用redis压力测试出现内存泄漏解决方案](https://blog.csdn.net/u014496893/article/details/113847297)\n\n&nbsp;\n","tags":["JVM"]},{"title":"kudu学习笔记——建表语句","url":"/kudu学习笔记——建表语句.html","content":"kudu支持的数据类型\n\n```\nboolean\n\n8-bit signed integer\n\n16-bit signed integer\n\n32-bit signed integer\n\n64-bit signed integer\n\ndate (32-bit days since the Unix epoch)\n\nunixtime_micros (64-bit microseconds since the Unix epoch)\n\nsingle-precision (32-bit) IEEE-754 floating-point number\n\ndouble-precision (64-bit) IEEE-754 floating-point number\n\ndecimal (see Decimal Type for details)\n\nvarchar (see Varchar Type for details)\n\nUTF-8 encoded string (up to 64KB uncompressed)\n\nbinary (up to 64KB uncompressed)\n\n```\n\n参考：[Apache Kudu Schema Design](https://kudu.apache.org/docs/schema_design.html)\n\nkudu的建表语句\n\n```\nCREATE TABLE sales_by_year(\t\t\t\t\t# 列存储\n\tyear INT, \t\t\t\t\t\t# 有限固定列，强类型\n\tsale_id INT COLPROPERTIES (encoding=&ldquo;bitshuffle&rdquo;),\t# 每一列均可以设置encoding及压缩方式\n\tamount INT,\n\tPRIMARY\tKEY (year,sale_id)\t\t\t\t# 主键索引\n)\nPARTITION BY HASH (sale_id) PARTITIONS 4,　　　　　　　　　　　　　　# 哈希分区\nRANGE (year)\n(\nPARTITION 2014 <= VALUES <= 2016,\t\t\t\t# 范围分区\nPARTITION VALUE\t= 2017\n)\nSTORED AS KUDU\nTBLPROPERTIES (replication=3);\t\t\t\t\t# 多副本\n\n```\n\n或者\n\n```\nCREATE TABLE xxxx.xxxx(\n  uuid STRING,\n  ds string,\n  `date` string,\n  `ts` string,\n  `col1` int,\n  `col2` bigint,\n  `col3` int,\nPRIMARY KEY (uuid, ds))\nPARTITION BY HASH (uuid, ds) PARTITIONS 9,\nRANGE(ds) (\nPARTITION VALUE=\"2020-09-05\",\nPARTITION VALUE=\"2020-09-06\",\nPARTITION VALUE=\"2020-09-07\",\nPARTITION VALUE=\"2020-09-08\",\nPARTITION VALUE=\"2020-09-09\"\n)\nSTORED AS KUDU;\n\n```\n\n参考：[DTCC2017-Kudu介绍-小米张震-final](http://docsplayer.com/148383520-Dtcc2017-kudu%E4%BB%8B%E7%BB%8D-%E5%B0%8F%E7%B1%B3%E5%BC%A0%E9%9C%87-final.html)\n\n<!--more-->\n&nbsp;\n\n支持若干种分区方式：\n\n参考：[kudu 表设计使用及限制 / Kudu table schema design and limited](https://atovk.github.io/post/bigdata/kudu_table_schema_design/)\n\n1.&nbsp;hash 分区\n\n```\ncreate table kudu_test.real_time_sales_temporary_kuduc (\n    sdt String,\n    shopId String,\n    updateTime String,\n    serialId String,\n    sheetId String,\n    goodsId String,\n\n    key_by String,\n    timeFrame String,\n    regionId String,\n    regionName String,\n    shopName String,\n    serviceRegionId String,\n    serviceRegionName String,\n    shopBelongId String,\n    shopBelongName String,\n    primary key(sdt,shopId,updatetime,serialId,sheetid)\n    )\n    PARTITION by \n    hash(sdt,shopId) partitions 32\n    COMMENT '销售流水表C'\n    STORED AS kudu;\n\n```\n\n如果不指定hash分区的字段，比如\n\n```\nPARTITION BY HASH  PARTITIONS 3\n\n```\n\n那么默认将会使用主键来进行hash，即\n\n```\nPARTITION BY HASH (sdt,shopId,updatetime,serialId,sheetid) PARTITIONS 3\n\n```\n\n2.&nbsp;范围分区\n\n```\nCREATE TABLE cust_behavior_table (\nid BIGINT,\nsku STRING,\nsalary STRING,\nedu_level INT,\nusergender STRING,\ngroup STRING,\ncity STRING,\npostcode STRING,\nlast_purchase_price FLOAT,\nlast_purchase_date BIGINT,\ncategory STRING,\nrating INT,\nfulfilled_date BIGINT,\nPRIMARY KEY (id, sku)\n)\nPARTITION BY RANGE (sku)\n(\nPARTITION VALUES < &lsquo;g&rsquo;,\nPARTITION &lsquo;g&rsquo; <= VALUES < &lsquo;o&rsquo;,\nPARTITION &lsquo;o&rsquo; <= VALUES < &lsquo;u&rsquo;,\nPARTITION &lsquo;u&rsquo; <= VALUES\n) STORED AS KUDU\nTBLPROPERTIES(\n&lsquo;kudu.table_name&rsquo; = &lsquo;cust_behavior_1 &lsquo;,&rsquo;kudu.master_addresses&rsquo; = &lsquo;hadoop5:7051&rsquo;);\n\n```\n\n3.&nbsp;混合分区（hash + range）\n\n```\nCREATE TABLE cust_behavior_1 (\nid BIGINT,\nsku STRING,\nsalary STRING,\nedu_level INT,\nusergender STRING,\ngroup STRING,\ncity STRING,\npostcode STRING,\nlast_purchase_price FLOAT,\nlast_purchase_date BIGINT,\ncategory STRING,\nrating INT,\nfulfilled_date BIGINT,\nPRIMARY KEY (id, sku)\n)\nPARTITION BY \nHASH (id) PARTITIONS 4,\nRANGE (sku)\n(\nPARTITION VALUES < &lsquo;g&rsquo;,\nPARTITION &lsquo;g&rsquo; <= VALUES < &lsquo;o&rsquo;,\nPARTITION &lsquo;o&rsquo; <= VALUES < &lsquo;u&rsquo;,\nPARTITION &lsquo;u&rsquo; <= VALUES\n) STORED AS KUDU\nTBLPROPERTIES(\n&lsquo;kudu.table_name&rsquo; = &lsquo;cust_behavior_1 &lsquo;,&rsquo;kudu.master_addresses&rsquo; = &lsquo;hadoop5:7051&rsquo;);\n\n```\n\n&nbsp;\n","tags":["kudu"]},{"title":"ubuntu16.04安装minikube","url":"/ubuntu16.04安装minikube.html","content":"使用原生包管理工具安装kubectl\n\n1.更新 apt 包索引，并安装使用 Kubernetes apt 仓库所需要的包\n\n```\nsudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl\n\n```\n\n2.下载 Google Cloud 公开签名秘钥，如果有网络问题的话，可以手动下载apt-key.gpg文件，然后将其改名并移动到/usr/share/keyrings/kubernetes-archive-keyring.gpg目录\n\n```\nsudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg\n\n```\n\n3.添加 Kubernetes apt 仓库：\n\n```\necho \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list\n\n```\n\n如果报错，可以使用中科大的源\n\n```\necho \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] http://mirrors.ustc.edu.cn/kubernetes/apt kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list\n\n```\n\n4.更新 apt 包索引，使之包含新的仓库并安装 kubectl：\n\n```\nsudo apt-get update\nsudo apt-get install -y kubectl\n\n```\n\n参考\n\n```\nhttps://kubernetes.io/zh/docs/tasks/tools/install-kubectl-linux/\n\n```\n\n　　\n\n使用minikube在单机上启动一个K8S机器用于测试\n\n```\nhttps://github.com/kubernetes/minikube/releases/tag/v1.21.0\n\n```\n\n1.安装minikube\n\n```\ncurl -LO https://github.com/kubernetes/minikube/releases/download/v1.21.0/minikube_1.21.0-0_amd64.deb\nsudo dpkg -i minikube_1.21.0-0_amd64.deb\n\n```\n\n2.创建集群\n\n```\nminikube start\n\n```\n\n如果遇到The image 'xxx' was not found; unable to add it to cache的报错，可以使用如下命令\n\n```\nminikube start --image-mirror-country='cn'\n\n```\n\n删除集群\n\n```\nminikube delete\n\n```\n\n创建特定版本的集群\n\n```\nminikube start --kubernetes-version=v1.21.1\n\n```\n\n3.查看集群\n\n```\nkubectl get po -A\nNAMESPACE              NAME                                        READY   STATUS    RESTARTS      AGE\nkube-system            coredns-64897985d-9hqwg                     1/1     Running   0             20h\nkube-system            etcd-minikube                               1/1     Running   0             20h\nkube-system            kube-apiserver-minikube                     1/1     Running   0             20h\nkube-system            kube-controller-manager-minikube            1/1     Running   0             20h\nkube-system            kube-proxy-gzjgt                            1/1     Running   0             20h\nkube-system            kube-scheduler-minikube                     1/1     Running   0             20h\nkube-system            storage-provisioner                         1/1     Running   1 (20h ago)   20h\nkubernetes-dashboard   dashboard-metrics-scraper-58549894f-sztfp   1/1     Running   0             132m\nkubernetes-dashboard   kubernetes-dashboard-ccd587f44-bvdc9        1/1     Running   0             132m\n\n```\n\nminikube一些命令：[K8s - Install Minikube in Linux](https://www.cnblogs.com/anliven/p/13844135.html)\n\n4.启动kubernetes-dashboard，需要在图形界面中\n\n```\nminikube dashboard\n\n```\n\n5.配置dashboard外网访问，address是你的运行minikube的机器ip，port是对外暴露的端口\n\n```\nkubectl proxy --port=10018 --address='192.168.xx.xx' --accept-hosts='^.*' &amp;\n\n```\n\n此时就可以访问如下的网址来访问minikube dashboard\n\n```\nhttp://master:10018/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/#/workloads?namespace=_all\n\n```\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20220210220722235-151062078.png\" alt=\"\" />\n\n参考\n\n```\nhttps://minikube.sigs.k8s.io/docs/start/\n\n```\n\n6.minikube使用ingress\n\n```\nhttps://kubernetes.io/zh-cn/docs/tasks/access-application-cluster/ingress-minikube/\n\n```\n\n　　\n\n&nbsp;\n","tags":["k8s"]},{"title":"使用Presto parser解析SQL","url":"/使用Presto parser解析SQL.html","content":"Presto的语法解析器是使用ANTLR生成的\n\nPrestoDB的parser g4语法文件\n\n```\nhttps://github.com/prestodb/presto/blob/master/presto-parser/src/main/antlr4/com/facebook/presto/sql/parser/SqlBase.g4\n\n```\n\nPrestoSQL（Trino）的parser g4语法文件\n\n```\nhttps://github.com/trinodb/trino/blob/master/core/trino-grammar/src/main/antlr4/io/trino/grammar/sql/SqlBase.g4\n\n```\n\n如果想在java代码中使用Presto的parser进行语法解析的话，可以引用下面的依赖\n\nPrestoDB（Facebook版本）\n\n```\n<dependency>\n    <groupId>com.facebook.presto</groupId>\n    <artifactId>presto-parser</artifactId>\n    <version>0.285.1</version>\n</dependency>\n\n```\n\nPrestoSQL（社区版本，350及其以下版本叫prestosql，以上改名为Trino）\n\n```\n<dependency>\n    <groupId>io.prestosql</groupId>\n    <artifactId>presto-parser</artifactId>\n    <version>330</version>\n</dependency>\n\n```\n\nTrino（社区版本，版本从351开始）\n\n```\n<dependency>\n    <groupId>io.trino</groupId>\n    <artifactId>trino-parser</artifactId>\n    <version>351</version>\n</dependency>\n\n```\n\npresto parser的使用\n\n可以参考：[《Hive、Spark、Presto SQL 输入输出表解析》](https://blog.csdn.net/qq_38783098/article/details/117340625)\n","tags":["presto"]},{"title":"使用jmap排查java程序内存泄露","url":"/使用jmap排查java程序内存泄露.html","content":"## 1.使用jmap命令生成内存快照文件(Heap Profile)\n\n```\njmap -dump:format=b,file=heap.hprof ${pid}\n\n```\n\n如果生成快照的时候遇到如下报错\n\n```\nDumping heap to /mnt/tmp/heap.hprof ...\nException in thread \"main\" java.io.IOException: Premature EOF\n\tat sun.tools.attach.HotSpotVirtualMachine.readInt(HotSpotVirtualMachine.java:292)\n\tat sun.tools.attach.LinuxVirtualMachine.execute(LinuxVirtualMachine.java:200)\n\tat sun.tools.attach.HotSpotVirtualMachine.executeCommand(HotSpotVirtualMachine.java:261)\n\tat sun.tools.attach.HotSpotVirtualMachine.dumpHeap(HotSpotVirtualMachine.java:224)\n\tat sun.tools.jmap.JMap.dump(JMap.java:247)\n\tat sun.tools.jmap.JMap.main(JMap.java:142)\n\n```\n\n添加如下参数就可以解决\n\n```\njmap -J-d64 -dump:format=b,file=/mnt/tmp/heap.hprof ${pid}\n```\n\n参考：[jmap -dump导出jvm堆内存信息时报错](https://little-star.love/posts/657c8eb4/)\n\n## 2.使用Eclipse Memory Analyzer工具对hprof文件进行分析\n\n如果机器的java版本是java8的话，需要注意MAT<!--more-->\n&nbsp;1.12.0版本需要jdk11，所以下载1.10.0版本\n\n```\nhttp://www.eclipse.org/downloads/download.php?file=/mat/1.10.0/rcp/MemoryAnalyzer-1.10.0.20200225-linux.gtk.x86_64.zip\n\n```\n\n否则会出现如下报错\n\n```\nUnrecognized option: --add-exports=java.base/jdk.internal.org.objectweb.asm=ALL-UNNAMED Error: Could not create the Java Virtual Machine. Error: A fatal exception has occurred. Program will exit. Unable to init server: Could not connect: Connection refused　　\n```\n\n由于堆栈快照文件可能会很大，所以需要修改 MemoryAnalyzer.ini 文件中的-Xmx1024m，比如改成-Xmx10240m，否则可能会遇到下面报错\n\n```\nan internal error occurred during parsing heap dump from\n```\n\n## 3.分析堆栈文件\n\n将快照文件从服务器上下载到本地，用MAT软件来分析hprof文件\n\n<img src=\"/images/517519-20210728112351955-126381468.png\" alt=\"\" loading=\"lazy\" />\n\n查看内存泄露报告\n\n&nbsp;<img src=\"/images/517519-20210803201054664-987536574.png\" alt=\"\" loading=\"lazy\" />\n\n发现有2处可疑的泄露\n\n<img src=\"/images/517519-20210803201131120-1487448117.png\" alt=\"\" loading=\"lazy\" />\n\n点击details，排查到可能是&nbsp;class io.lettuce.core.AbstractRedisClient @ 0x41bc37380 引起的\n\n&nbsp;<img src=\"/images/517519-20210803203206550-1663657782.png\" alt=\"\" loading=\"lazy\" />\n\n排查使用的redis依赖是否有内存泄露的可能，找到\n\n[springboot使用redis压力测试出现内存泄漏解决方案](https://blog.csdn.net/u014496893/article/details/113847297)\n\n&nbsp;\n\n如果堆栈快照文件很大，不方便从服务器上下载到本地的话，可以直接在服务器上进行分析，再将分析的结果下载下来进行查看，分析的结果会以zip包形式生成，生成路径和hprof文件相同，如下\n\n<img src=\"/images/517519-20230601110256937-1263049854.png\" alt=\"\" loading=\"lazy\" />\n\n将zip文件下载下来，解压查看网页，就可以看到堆栈的分析报告，如下\n\n<img src=\"/images/517519-20230601110659157-1160934237.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["JVM"]},{"title":"Flink学习笔记——Flink SQL","url":"/Flink学习笔记——Flink SQL.html","content":"flink官方从1.8.0开始提供Table&amp;SQL API，参考\n\n```\nhttps://github.com/apache/flink/tree/release-1.8.0/flink-table\n\n```\n\n如果想使用Flink SQL的话，可以参考官方的get start文档，如下\n\n```\nhttps://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/gettingstarted/\n\n```\n\n找到flink的安装目录\n\n```\ncd /usr/lib/flink\n\n```\n\n首先需要启动一个flink集群，这里使用yarn-session模式来启动一个flink session集群，比如\n\n```\n./bin/yarn-session.sh -n 3 -s 5 -jm 1024 -tm 4096 -d\n\n```\n\n一旦flink集群启动，你就可以访问一个本地的FlinkUI\n\n<img src=\"/images/517519-20230131155103757-1000146192.png\" alt=\"\" loading=\"lazy\" />\n\n如下\n\n<img src=\"/images/517519-20230131144128871-1534653344.png\" alt=\"\" loading=\"lazy\" />\n\n也可以在YARN上看到flink session cluster的任务\n\n<img src=\"/images/517519-20230131155205469-1160551191.png\" width=\"800\" height=\"137\" loading=\"lazy\" />\n\n这时再启动flink SQL\n\n```\n./bin/sql-client.sh\n\n```\n\n<img src=\"/images/517519-20230131144341230-1388879218.png\" width=\"500\" height=\"665\" loading=\"lazy\" />\n\n```\nSELECT 'Hello World';\n```\n\n<img src=\"/images/517519-20230131155422734-9316893.png\" alt=\"\" loading=\"lazy\" />\n\n运行结果\n\n<img src=\"/images/517519-20230131155809652-2036517278.png\" width=\"1000\" height=\"408\" loading=\"lazy\" />\n\n默认的显示模式是表格模式，可以通过一下语句进行调整\n\n```\nSET sql-client.execution.result-mode = table;\nSET sql-client.execution.result-mode = tableau;\nSET sql-client.execution.result-mode = changelog;\n\n```\n\ntableau模式，更接近于spark sql的显示方式\n\n<img src=\"/images/517519-20230131161007074-231894884.png\" alt=\"\" loading=\"lazy\" />\n\nchangelog模式，变更日志模式（changelog mode）不会物化结果。可视化展示由插入（+）和撤销（-）组成的持续查询结果流。\n\n<img src=\"/images/517519-20230131161256877-296222188.png\" width=\"1000\" height=\"415\" loading=\"lazy\" />\n\n参考：[flink sql-client 菜鸟记录](https://juejin.cn/post/6989587704153374728)\n\n<!--more-->\n&nbsp;\n","tags":["Flink"]},{"title":"Flink学习笔记——Standalone模式","url":"/Flink学习笔记——Standalone模式.html","content":"Flink的部署方式有如下几种，本文主要介绍standalone模式\n\n```\nYarn\nMesos\nDocker/Kubernetes\nStandalone\n\n```\n\nstandalone模式的官方文档可以参考\n\n```\nhttps://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/overview/#standalone\n\n```\n\n进入flink的安装目录\n\n```\n/usr/lib/flink\n\n```\n\n启动一个standalone的flink集群\n\n```\n./bin/start-cluster.sh\n\n```\n\n这时可以在默认的8081端口查看FlinkUI\n\n提交任务\n\n```\n./bin/flink run ./examples/streaming/TopSpeedWindowing.jar\n\n```\n\n关闭集群\n\n```\n./bin/stop-cluster.sh\n\n```\n\n　　\n","tags":["Flink"]},{"title":"Java程序开启JMX以及配置Promethus exporter","url":"/Java程序开启JMX以及配置Promethus exporter.html","content":"Java程序开启JMX的方法有2种：\n\n第1种是在启动参数中添加如下配置\n\n<!--more-->\n&nbsp;\n\n第2种是在环境变量中添加\n\n&nbsp;\n\n官方JMX exporter\n\n```\nhttps://github.com/prometheus/jmx_exporter\n\n```\n\n　　\n\njmxConnector.yaml配置如下\n\n```\nlowercaseOutputName: false\nlowercaseOutputLabelNames: false\nblacklistObjectNames: [ \"java.lang:*\", \"java.nio:*\", \"metrics:name=my_test_job.driver.BlockManager.disk.diskSpaceUsed_MB,*\"]\nwhitelistObjectNames: [ \"*:*\" ]\n\n```\n\n或者\n\n```\nlowercaseOutputLabelNames: true\nlowercaseOutputName: true\nrules:\n  - pattern: \".*\"\n\n```\n\n其中blacklistObjectNames表示不需要上报的指标，注意是括号中开头的是指标名称，尖括号中的kv是指标的属性\n\njava.lang:* 将不会上报下图中的指标\n\n<img src=\"/images/517519-20220628115032641-1784189549.png\" width=\"900\" height=\"491\" loading=\"lazy\" />\n\njava.nio:*&nbsp;将不会上报下图中的指标\n\n<img src=\"/images/517519-20220628115245944-1463686579.png\" width=\"900\" height=\"173\" loading=\"lazy\" />\n\nmetrics:name=my_test_job.driver.BlockManager.disk.diskSpaceUsed_MB,*&nbsp;将不会上报下图中的指标，其中 * 表示通配了type=gauges\n\n<img src=\"/images/517519-20220628115522456-383271174.png\" width=\"1000\" height=\"109\" loading=\"lazy\" />\n\n所有都屏蔽了之后，会剩下一些jmx自带的指标\n\n<img src=\"/images/517519-20220628120331848-373003933.png\" width=\"800\" height=\"314\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["Prometheus"]},{"title":"linux os参数调优","url":"/linux os参数调优.html","content":"**1、net.core.somaxconn**\n\nnet.core.somaxconn是Linux中的一个kernel参数，表示socket监听（listen）的backlog上限。什么是backlog呢？backlog就是socket的监听队列，当一个请求（request）尚未被处理或建立时，他会进入backlog。而socket server可以一次性处理backlog中的所有请求，处理后的请求不再位于监听队列中。当server处理请求较慢，以至于监听队列被填满后，新来的请求会被拒绝。在Hadoop 1.0中，参数ipc.server.listen.queue.size控制了服务端socket的监听队列长度，即backlog长度，默认值是128。而Linux的参数net.core.somaxconn默认值同样为128。当服务端繁忙时，如NameNode或JobTracker，128是远远不够的。这样就需要增大backlog，例如我们的3000台集群就将ipc.server.listen.queue.size设成了32768，为了使得整个参数达到预期效果，同样需要将kernel参数net.core.somaxconn设成一个大于等于32768的值。\n\n参考：[关于linux内核参数的调优，你需要知道](https://www.codenong.com/cs105985731/)\n\n<!--more-->\n&nbsp;\n\n**2、net.ipv4.tcp_max_syn_backlog**\n\n表示SYN队列长度，默认1024，改成8192，可以容纳更多等待连接的网络连接数。\n\n参考：[Linux内核参数调优](https://cloud.tencent.com/developer/article/1682961)\n\n&nbsp;\n\n**3、编辑/etc/sysctl.conf**\n\nnet.ipv4.tcp_syncookies&nbsp;=&nbsp;1<br />net.ipv4.tcp_tw_reuse&nbsp;=&nbsp;1<br />net.ipv4.tcp_tw_recycle&nbsp;=&nbsp;1<br />net.ipv4.tcp_fin_timeout&nbsp;=&nbsp;30\n\n然后执行/sbin/sysctl&nbsp;-p让参数生效。\n\nnet.ipv4.tcp_syncookies&nbsp;=&nbsp;1表示开启SYN&nbsp;Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭；\n\nnet.ipv4.tcp_tw_reuse&nbsp;=&nbsp;1表示开启重用。允许将TIME-WAIT&nbsp;sockets重新用于新的TCP连接，默认为0，表示关闭；\n\nnet.ipv4.tcp_tw_recycle&nbsp;=&nbsp;1表示开启TCP连接中TIME-WAIT&nbsp;sockets的快速回收，默认为0，表示关闭。\n\nnet.ipv4.tcp_fin_timeout修改系統默认的TIMEOUT时间\n\n修改之后，再用命令查看TIME_WAIT连接数\n\nnetstat&nbsp;-ae|grep&nbsp;&ldquo;TIME_WAIT&rdquo;&nbsp;|wc&nbsp;&ndash;l\n\n&nbsp;\n\n参考：[linux系统安装后的一些内核调优](https://blog.51cto.com/dadloveu/1357739)\n\n&nbsp;\n","tags":["Linux"]},{"title":"字符编码","url":"/字符编码.html","content":"<img src=\"/images/517519-20160315232539615-1473879989.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n<img src=\"/images/517519-20160315233612881-1466285335.png\" alt=\"\" />\n\n```\nimport java.io.File;\nimport java.io.FileNotFoundException;\nimport java.io.FileOutputStream;\n\nimport org.omg.CORBA.portable.OutputStream;\n\npublic class code_demo {\n\n\tpublic static void main(String[] args) throws Exception {\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"系统默认编码：\"+System.getProperty(\"file.encoding\"));\t\t//获取系统当前的编码\n\t\tFile file = new File(\"/home/common/software/coding/HelloWord/HelloWord/b.txt\");//路径\n\t\tFileOutputStream out = new FileOutputStream(file);\n\t\tbyte b[] = \"中国\".getBytes(\"ISO8859-1\");\n\t\tout.write(b);\n\t\tout.close();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160326233948776-1199368064.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160326234056542-37451885.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160326234124870-1912173043.png\" alt=\"\" />\n","tags":["Java"]},{"title":"Presto学习笔记——Python客户端连接Presto","url":"/Presto学习笔记——Python客户端连接Presto.html","content":"参考：[三种客户端连接Presto](https://juejin.cn/post/6884995375262859272)\n\n1.使用 presto-client\n\n```\npip install presto-client==0.302.0\n\n```\n\n查询\n\n```\nimport presto\n\n# demo about the usage of presto-python-client\nconn = presto.dbapi.connect(\n    host='localhost',\n    port=8889,\n    user='hadoop',\n    catalog='xx_catalog',\n    schema='default'\n    )\ncur = conn.cursor()\nsql = \"select * from xxx.xxx limit 10\"\ncur.execute(sql)\nrows = cur.fetchall()  \nprint(rows)\n\n```\n\n　　\n","tags":["presto"]},{"title":"Prometheus使用教程","url":"/Prometheus使用教程.html","content":"官方文档：[https://prometheus.io/docs/prometheus/latest/getting_started/](https://prometheus.io/docs/prometheus/latest/getting_started/)\n\n其他参考：[CentOS7安装部署Prometheus+Grafana<!--more-->\n&nbsp;](https://www.jianshu.com/p/967cb76cd5ca)\n\n## 1.安装Prometheus\n\n官方网站下载二进制安装包：[https://prometheus.io/download/](https://prometheus.io/download/)\n\n可以选择mac，linux和Windows版本\n\n```\nhttps://github.com/prometheus/prometheus/releases/download/v2.36.1/prometheus-2.36.1.linux-amd64.tar.gz\ntar -zxvf prometheus-2.36.1.linux-amd64.tar.gz\n\n```\n\n　　\n\n## 2.修改prometheus配置\n\n配置文件为prometheus.yml\n\n**prometheus采集数据的方法分成推和拉**\n\n### 1.prometheus定时通过exporter暴露的HTTP端口主动去采集监控数据的方式就是拉\n\n下面就是配置一个exporter采集任务的例子\n\n```\nscrape_configs:\n  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.\n  - job_name: \"prometheus\"\n\n    # metrics_path defaults to '/metrics'\n    # scheme defaults to 'http'.\n\n    static_configs:\n      - targets: [\"xx:xx\"]\n\n```\n\n### 2.程序主动上报metrics给prometheus的pushgateway则是推\n\n要主动上报metrics首先要安装pushgateway，参考：[prometheus-pushgateway安装](https://www.cnblogs.com/huandada/p/10932953.html)\n\n在官方的文档当中，pushgateway唯一推荐使用的场景是在批处理作业运行结果的采集上，其他的建议使用exporter：[when-to-use-the-pushgateway](https://prometheus.io/docs/practices/pushing/#when-to-use-the-pushgateway)\n\n官方地址\n\n```\nhttps://prometheus.io/download/#pushgateway/\n\n```\n\n下载和安装\n\n```\nwget https://github.com/prometheus/pushgateway/releases/download/v1.4.3/pushgateway-1.4.3.linux-amd64.tar.gz\ntar -zxvf pushgateway-1.4.3.linux-amd64.tar.gz\n\n```\n\n启动&nbsp;\n\n```\n./pushgateway\n\n```\n\n然后在prometheus的配置文件中添加pushgateway，注意：需要把pushgateway添加到prometheus的配置中并启动，pushgateway才会正常工作\n\n```\n-job_name: 'pushgateway'\n    scrape_interval: 30s\n    honor_labels: true  #加上此配置exporter节点上传数据中的一些标签将不会被pushgateway节点的相同标签覆盖\n    static_configs:\n      - targets: ['localhost:9091']\n        labels:\n          instance: pushgateway\n\n```\n\n原生的pushgateway会存储最后上报的指标，不会进行删除，当prometheus过来取数据的时候，会默认取最后上报的数据，也就是即使不上报指标了，在图上也会一直有一条直线存在\n\n如果要支持pushgateway数据自动过期的话，可以使用下面的pushgateway镜像\n\n```\ndocker run -d -p 9091:9091 dmathai/prom-pushgateway-ttl:latest --metric.timetolive=60s\n\n```\n\n效果如下，如果不上报，过了60s，数据就会中断\n\n<img src=\"/images/517519-20220624171631445-632808645.png\" width=\"700\" height=\"296\" loading=\"lazy\" />\n\n也可以使用curl命令上报指标\n\n```\necho \"logic_cpu  5\" |curl --data-binary @- http://xx:9091/metrics/job/my_test_job/role/driver/app_name/my+test+job/instance/xxxdeMacBook-Pro.local\n\n```\n\n&nbsp;\n\n　&nbsp;\n\n## 3.启动Prometheus\n\n```\n./prometheus --config.file=prometheus.yml\n\n```\n\n其默认的存储目录在data目录下\n\n```\n/Users/lintong/software/prometheus-2.36.1.darwin-amd64/data $ ls\nchunks_head    lock           queries.active wal\n\n```\n\n如果要修改的话，可以添加如下配置\n\n```\n./prometheus --config.file=prometheus.yml --storage.tsdb.path=/data/prometheus\n\n```\n\n之后访问9090端口就可以查看dashboar，启动instance和job是配置文件中采集的exporter的地址和名字\n\n<img src=\"/images/517519-20220623145612645-719439704.png\" width=\"1000\" height=\"490\" loading=\"lazy\" />\n\n也可以在target中看到添加的exporter\n\n<img src=\"/images/517519-20220623164725323-399603233.png\" width=\"1000\" height=\"370\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["Prometheus"]},{"title":"SpringBoot学习笔记——websocket","url":"/SpringBoot学习笔记——websocket.html","content":"可以在chrome上安装<!--more-->\n&nbsp;Simple WebSocket Client 插件来辅助调试websocket功能\n\nwebsocket client发送数据\n\n<img src=\"/images/517519-20210524174749919-1244729558.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\nwebsocket server接收数据\n\n<img src=\"/images/517519-20210524174842940-1132295953.png\" alt=\"\" loading=\"lazy\" />\n\n参考：[Spring-Boot快速集成WebSocket服务端 客户端(支持客户端消息同步回调)](https://blog.csdn.net/qq_42271561/article/details/105883251)\n\n&nbsp;\n\n在springboot中使用websocket的时候需要注意如果web服务有权限的话，websockete连接同样是会被拦截，需要开放拦截\n\n```\n.antMatchers(\n        HttpMethod.GET,\n        \"/ws/kafka/**\"\n).permitAll()\n\n```\n\n如果想在session中获得当前用户认证的信息，可以这样\n\n```\nUsernamePasswordAuthenticationToken auth = (UsernamePasswordAuthenticationToken) session.getUserPrincipal();\n\n```\n\n使用 SecurityContextHolder.getContext().getAuthentication() 是不可以的，将会是null\n\n&nbsp;\n\nwebsocket发送的是GET请求，WebSocket协议利用了HTTP协议来建立连接。\n\n<img src=\"/images/517519-20210527104220589-1020865578.png\" width=\"1100\" height=\"649\" loading=\"lazy\" />\n\nwebsocket请求和普通的HTTP请求有几点不同：\n\n```\n1.GET请求的地址不是类似/path/，而是以ws://开头的地址；\n2.请求头Upgrade: websocket和Connection: Upgrade表示这个连接将要被转换为WebSocket连接；\n3.Sec-WebSocket-Key是用于标识这个连接，并非用于加密数据；\n4.Sec-WebSocket-Version指定了WebSocket的协议版本。\n\n```\n\n参考：[socket 及 websocket的握手过程](https://www.huaweicloud.com/articles/82430eb95f48196ecef6a5e5b2ab885c.html)\n\n[SpringBoot2.x开发WebSocket](https://www.cnblogs.com/niunafei/p/12869342.html)\n\n&nbsp;\n","tags":["SpringBoot"]},{"title":"helm部署kubernetes-dashboard","url":"/helm部署kubernetes-dashboard.html","content":"kuberbetes-dashboard是K8S的管理工具，下面使用helm来进行部署，参考：\n\n```\nhttps://artifacthub.io/packages/helm/k8s-dashboard/kubernetes-dashboard\n\n```\n\n## 首先添加源\n\n```\nhelm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/\n\n```\n\n## 查找chart\n\n```\nhelm search repo kubernetes-dashboard\n\nNAME                                     \tCHART VERSION\tAPP VERSION\tDESCRIPTION\nkubernetes-dashboard/kubernetes-dashboard\t5.4.1        \t2.5.1      \tGeneral-purpose web UI for Kubernetes clusters\n\n```\n\n## 部署release\n\n```\nhelm install my-k8s-dashboard kubernetes-dashboard/kubernetes-dashboard\n\n```\n\n## 访问kubernetes dashboard\n\n### 1.修改kubernetes-dashboard的svc，将ClusterIp改成NodePort\n\n<img src=\"/images/517519-20220605192103426-1309192824.png\" width=\"400\" height=\"208\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n此时再查看svc\n\n```\nkubectl get svc -A\nNAMESPACE     NAME                                    TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE\ndefault       my-k8s-dashboard-kubernetes-dashboard   NodePort       10.110.255.104   <none>        443:32511/TCP            21h\n\n```\n\n访问如下网址即可，注意这里建议使用firefox浏览器来访问，提示HTTPS不安全的话可以选择忽略；如果使用chrome浏览器的话，会提示需要需要导入HTTPS证书，证书地址如下\n\n```\n/etc/kubernetes/pki/ca.crt　　\n```\n\n<img src=\"/images/517519-20220605192549919-2007285929.png\" width=\"600\" height=\"438\" loading=\"lazy\" />\n\n&nbsp;\n\n### 2.也使用kubectl proxy来访问\n\n参考\n\n```\nhttps://kubernetes.io/zh/docs/tasks/access-application-cluster/web-ui-dashboard/\n\n```\n\n设置proxy，address是你的运行minikube或者docker-desktop的机器ip，port是对外暴露的端口\n\n```\nkubectl proxy --address='localhost' --port=18000\n\n```\n\n查看kubernetes-dashboard的svc\n\n```\nkubectl get svc -A\nNAMESPACE     NAME                                    TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE\n\ndefault       my-k8s-dashboard-kubernetes-dashboard   ClusterIP      10.110.255.104   <none>        443/TCP                  87m\n\n```\n\n接下来在浏览器里面输入如下地址就可以访问kubernetes-dashboard了\n\nlocalhost是运行minikube或者docker-desktop的机器ip，18000就是kubectl proxy里面的address，default是kubernetes-dashboard部署的namespace，my-k8s-dashboard-kubernetes-dashboard:443是部署的service的name的port\n\n```\nhttp://localhost:18000/api/v1/namespaces/default/services/https:my-k8s-dashboard-kubernetes-dashboard:443/proxy/#/login\n\n```\n\n<img src=\"/images/517519-20220604225500730-1619604504.png\" width=\"800\" height=\"460\" loading=\"lazy\" />\n\n这时候部署的dashboard是HTTPS的\n\n### 3.在局域网访问kubernetes-dashboard\n","tags":["k8s"]},{"title":"如何申请HTTPS证书","url":"/如何申请HTTPS证书.html","content":"## HTTP和HTTPS区别\n\n**超文本传输协议（HTTP）**是用于客户端-服务器通信的协议或一组通信规则。当您访问网站时，您的浏览器会向 Web 服务器发送 HTTP 请求，该服务器将以 HTTP 响应进行响应。Web 服务器将以纯文本形式与您的浏览器交换数据。简而言之，HTTP 协议是为网络通信提供支持的底层技术。\n\n**安全超文本传输协议（HTTPS）**是 HTTP 的一种更安全的版本或扩展。在 HTTPS 中，浏览器与服务器会在传输数据之前建立安全的加密连接。\n\n## HTTPS 协议的工作原理\n\nHTTP 传输未加密的数据，这意味着从浏览器发送的信息可能会被第三方拦截和读取。这一过程并不理想，因此将其扩展成为 HTTPS，以便为通信再增加一层安全性。HTTPS 将 HTTP 请求和响应与 SSL 和 TLS 技术相结合。\n\nSSL 和 TLS 参考：[SSL 与 TLS 之间有何区别？](https://aws.amazon.com/cn/compare/the-difference-between-ssl-and-tls/)\n\nHTTPS 网站必须从独立证书颁发机构（CA）获取 SSL/TLS 证书。这些网站会在交换数据之前先与浏览器共享该证书，以建立信任。SSL 证书还包含加密信息，以便服务器和 Web 浏览器可以交换加密或刻意打乱的数据。\n\n该过程的工作原理是：\n\n1. 您通过在浏览器的地址栏中键入 https://URL 格式来访问 HTTPS 网站。\n1. 浏览器尝试通过请求服务器的 SSL 证书来验证该站点的真实性。\n1. 该服务器发送包含公钥的 SSL/TLS 证书作为回复（非对称加密）。\n1. 该网站的 SSL 证书将证明该服务器身份。浏览器确认一切妥当后，它将使用该公钥加密并发送包含secret session key的消息。\n1. Web 服务器使用其私钥解密消息并检索session key。然后，它将加密该session key，并向浏览器发送确认消息。\n1. 现在，浏览器和 Web 服务器都切换到使用相同的session key来安全地交换消息（对称加密）。\n\n参考：[HTTP 与 HTTPS 之间有什么区别？](https://aws.amazon.com/cn/compare/the-difference-between-https-and-http/)\n\n## 公钥私钥，加密解密，签名验签\n\n既然是加密，那肯定是不希望别人知道我的消息，所以只有我才能解密，所以可得出**公钥负责加密，私钥负责解密**；\n\n同理，既然是签名，那肯定是不希望有人冒充我发消息，只有我才能发布这个签名，所以可得出**私钥负责签名，公钥负责验证**。\n\n参考：[RSA的公钥和私钥到底哪个才是用来加密和哪个用来解密？](https://www.zhihu.com/question/25912483)\n\n[HTTPS 加密、证书、签名与握手](https://cloud.tencent.com/developer/article/1867985)\n\n## HTTPS既使用了对称加密，也使用了非对称加密\n\n在HTTPS协议中，对称加密和非对称加密都发挥了重要的作用。具体来说，HTTPS在内容传输上使用的是对称加密。这意味着在通信过程中，发送方和接收方会生成一个临时的密钥，并使用该密钥对数据进行加密和解密。由于对称加密的加解密速度快，因此可以有效地保障大量数据的传输安全。\n\n然而，仅仅使用对称加密是不够的。为了确保通信双方的身份验证以及防止中间人攻击，HTTPS还需要用到非对称加密。在证书验证阶段，HTTPS使用非对称加密来验证服务器的身份。服务器会向客户端发送自己的证书，其中包括了公钥等信息。客户端会使用证书中的公钥来验证服务器的身份，并确保数据传输过程中的安全性。\n\n1.**对称加密算法**：对称加密算法使用相同的密钥来加密和解密数据。在HTTPS通信中，客户端和服务器会协商一个对称密钥，通常是使用服务器的公钥加密生成的。这个对称密钥会用于加密和解密数据传输过程中的内容，因为对称加密效率高，适合用于加密大量数据的传输。\n\n2.**非对称加密算法**：非对称加密算法包括公钥和私钥。服务器会拥有自己的私钥，而与之对应的公钥会被安全地存储在数字证书中。在建立连接时，服务器会把自己的公钥发给客户端。客户端使用服务器的公钥对协商的对称密钥进行加密，然后传输给服务器，服务器再使用自己的私钥解密该对称密钥。\n\n参考：[HTTPS：对称加密与非对称加密的完美结合](https://developer.baidu.com/article/details/3032466)\n\n## 免费的SSL证书申请\n\n云厂商一般都提供这种服务，比如[AWS Certificate Manager](https://aws.amazon.com/certificate-manager/)（ACM），\n","tags":["计算机网络"]},{"title":"Kafka数据查询工具——Offset Explorer","url":"/Kafka数据查询工具——Offset Explorer.html","content":"官方地址下载，有mac，windows和linux版本\n\n```\nhttps://www.kafkatool.com/download.html\n\n```\n\n## 1.查看kafka topic中的数据（支持byte，string和avro）\n\n安装后添加kafka集群的地址，可以选择使用zk地址或者kafka的broker地址来连接集群，这里选择使用kafka broker地址来连接\n\n<img src=\"/images/517519-20220614163438052-780773238.png\" width=\"1000\" height=\"624\" loading=\"lazy\" />\n\n可以指定topic的partition和offset查看数据\n\n<img src=\"/images/517519-20230408093343422-1535182760.png\" width=\"1000\" height=\"623\" loading=\"lazy\" />\n\n默认查出来的数据是byte数组，即16进制的，可以借助在线转换工具将其转换成字符串后进行查看，比如\n\n```\nhttps://tool.lu/hexstr/\n\n```\n\n也可以在属性中设置value为String，这样就可以直接查看反序列化后字符串\n\n<img src=\"/images/517519-20230411143559215-1251291956.png\" width=\"900\" height=\"339\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\noffset explorer的新版本也支持了对**confluent schema registry**和**avro格式数据**的支持，使用方法如下\n\n调整offset explorer每次从topic取数的条数，原来的条数过多可能会比较卡，从每个partition取5000条调整成500条\n\n<img src=\"/images/517519-20230320115442149-1369412002.png\" alt=\"\" loading=\"lazy\" />\n\n给kafka集群配置schema registry的地址，注意需要加上http的前缀，然后update\n\n<img src=\"/images/517519-20230320115650608-1700329363.png\" alt=\"\" loading=\"lazy\" />\n\n给topic配置value的格式，设置成avro，然后update\n\n<img src=\"/images/517519-20230320115817216-559145634.png\" alt=\"\" loading=\"lazy\" />\n\n可以看到解析后的avro数据\n\n<img src=\"/images/517519-20230320115922085-773068801.png\" alt=\"\" loading=\"lazy\" />\n\n如果有报错的话，可以去查看日志，日志路径mac系统如下\n\n```\n/Applications/Offset Explorer 2.app/Contents/Resources/app/error.log\n```\n\n## 2.调整consumer的offset\n\noffset explorer也支持编辑consumer的offset，在consumer中选择你要修改offset的消费者，点击edit consumer offset\n\n<img src=\"/images/517519-20221101134501973-3634754.png\" width=\"1500\" height=\"271\" loading=\"lazy\" />\n\n编辑specific offset后update\n\n<img src=\"/images/517519-20221101134641563-1935449731.png\" width=\"800\" height=\"626\" loading=\"lazy\" />\n\n## 3.发送数据到kafka topic\n\n点到topic的具体某个partition，然后点击加好来add message\n\n<img src=\"/images/517519-20230411144212409-554588689.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n点击enter manually，所输入的消息需要和你设置的topic key和value格式对应，即value如果是string的话，需要输入文本；如果是byte的话，需要输入hex字节数组\n\n&nbsp;<img src=\"/images/517519-20230411144417817-652157403.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["kafka"]},{"title":"Flink学习笔记——Table和SQL API","url":"/Flink学习笔记——Table和SQL API.html","content":"Table API官方文档参考\n\n```\nhttps://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/tableapi/\n\n```\n\nRowKind\n\n<!--more-->\n&nbsp;\n\n自定义修改debezium-json format\n\n[Flink 1.11.1:flink CDC Debezium自定义修改debezium-json格式](https://blog.csdn.net/u012551524/article/details/108985945)\n\n[Flink cdc自定义format格式数据源](https://cloud.tencent.com/developer/article/1936360)\n\n&nbsp;\n\n如果要在flink sql中使用checkpoint，可以使用set命令，如下\n\n```\nSET 'state.checkpoints.dir' = 'hdfs:///bar/foo/';\nSET 'execution.checkpointing.mode' = 'EXACTLY_ONCE';\nSET 'execution.checkpointing.interval' = '30min';\nSET 'execution.checkpointing.min-pause' = '20min';\nSET 'execution.checkpointing.max-concurrent-checkpoints' = '1';\nSET 'execution.checkpointing.prefer-checkpoint-for-recovery' = 'true';\n\n```\n\n　　\n","tags":["Flink"]},{"title":"Java正则表达式","url":"/Java正则表达式.html","content":"程序中应用正则表达式则必须依靠Ｐattern类和Ｍatcher类，这两个类都是在**java.util.regex包**中定义。\n\n**Pattern类**的主要作用是进行正则规范的编写。\n\n**Ｍatcher类**主要是执行规范，验证一个字符串是否符合其规范。\n\n<img src=\"/images/517519-20160313101824397-615037445.png\" alt=\"\" />\n\n```\nimport java.util.regex.Pattern;\nimport java.util.regex.Matcher;;\n\npublic class Ｒegex_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n//\t\tString str = \"123456789\";\n//\t\tif(Pattern.compile(\"[0-9]+\").matcher(str).matches()){\t\t//使用正则表达式\n//\t\t\tSystem.out.println(\"是由数字组成！\");\n//\t\t} else{\n//\t\t\tSystem.out.println(\"不是由数字组成！\");\n//\t\t}\n\t\t\n//\t\tString str = \"2016-03-11\" ;\n//\t\tString pat = \"\\\\d{4}-\\\\d{2}-\\\\d{2}\";\t\t\t//定义规则，其中/不要转义，//d等于/d\n//\t\tPattern p = Pattern.compile(pat);\t\t\t//实例化Pattern类\n//\t\tMatcher m = p.matcher(str);\t\t\t\t\t//验证字符串内容是否合法\n//\t\tif(m.matches()){\n//\t\t\tSystem.out.println(\"日期格式合法！\");\n//\t\t}else{\n//\t\t\tSystem.out.println(\"日期格式不合法！\");\n//\t\t}\n\t\t\n\t\tString str = \"A123B123C123F123\" ;\n\t\tString pat = \"\\\\d+\";\n\t\tPattern p = Pattern.compile(pat);\t\t\t//实例化Pattern类\n\t\tString s[] = p.split(str);\t\t\t\t\t\t\t\t//进行字符串拆分\n\t\tfor(int i=0;i<s.length;i++){\n\t\t\tSystem.out.println(s[i]+\"\\t\");\t\t\t\t//输出其中符合字母的字符\n\t\t}\n\t\t\n\t\tMatcher m = p.matcher(str);\t\t\t\t\t\t//实例化Matcher类\n\t\tString newString = m.replaceAll(\"_\");\t\t//替换所有的数字\n\t\tSystem.out.println(newString);\n\t}\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n\n<img src=\"/images/517519-20160313103825850-1221901491.png\" alt=\"\" />\n\n&nbsp;\n\n&nbsp;\n\n<img src=\"/images/517519-20160313103901632-491342824.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160313103922522-900752191.png\" alt=\"\" />\n\n&nbsp;\n","tags":["正则表达式"]},{"title":"Kafka学习笔记——Producer API","url":"/Kafka学习笔记——Producer API.html","content":"参考kafka官方文档，版本1.0.x\n\n```\nhttp://kafka.apache.org/10/documentation.html#consumerapi\n\n```\n\n依赖，选择 Cloudera Rel 中的 1.0.1-kafka-3.1.0\n\n```\n<dependency>\n    <groupId>org.apache.kafka</groupId>\n    <artifactId>kafka-clients</artifactId>\n    <version>1.0.1-kafka-3.1.0</version>\n</dependency>\n\n```\n\nKafka的生产者配置\n\n```\nhttp://kafka.apache.org/10/documentation.html#producerconfigs\n\n```\n\n其中producer api中文含义参考：[kafka系列七、kafka核心配置](https://www.cnblogs.com/wangzhuxing/p/10111831.html)\n\nproducer是线程安全的，通常应该在所有线程之间共享，以获得最佳性能。producer管理着一个后台线程，该线程执行I/O操作，并与每个需要通信的broker建立TCP连接。如果使用后不能关闭producer，将会泄漏这些资源。\n","tags":["kafka"]},{"title":"Timer类和TimerTask类","url":"/Timer类和TimerTask类.html","content":"**Timer类**是一种**线程设施**，可以用来实现**在某一个时间或某一段时间后安排某一个任务执行一次或定期重复执行**。\n\n该功能要与TimerTask类配合使用。TimerTask类用来实现由Timer安排的一次或重复执行的某一任务。\n\n<img src=\"/images/517519-20160313105857304-1050123618.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n<img src=\"/images/517519-20160313105920882-417009375.png\" alt=\"\" />\n\n```\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\nimport java.util.TimerTask;\nimport java.util.Timer;\n\nclass MyTask extends TimerTask{\t\t\t\t//任务调度类要继承TimerTask\n\n\t@Override\n\tpublic void run() {\n\t\t// TODO 自动生成的方法存根\n\t\tSimpleDateFormat sdf = null;\n\t\tsdf = new SimpleDateFormat(\"yyyy-MM-dd HH-mm-ss:SSS\");\n\t\tSystem.out.println(\"当前系统的时间：\"+sdf.format(new Date()));\n\t}\n\t\n}\n\npublic class Timer_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tTimer t = new Timer();\t\t\t\t\t\t\t//建立Timer类对象\n\t\tMyTask myTask = new MyTask();\t\t//定义任务\n\t\tt.schedule(myTask, 1000, 2000);  \t\t//设置任务的执行，1秒后开始，每2秒重复\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["多线程"]},{"title":"spring-boot项目建立","url":"/spring-boot项目建立.html","content":"使用idea来开发spring-boot项目，对于community版本的idea，由于没有spring-boot插件，所有对于开发spring-boot的web项目来说不是很方便，所以安装Ultimate版本的idea，使用的版本是2017.1.6版本\n\n参考 :<!--more-->\n&nbsp;[https://blog.csdn.net/typa01_kk/article/details/76696618](https://blog.csdn.net/typa01_kk/article/details/76696618)\n\n## 1.创建项目\n\n<img src=\"/images/517519-20221011103255362-1766225317.png\" alt=\"\" loading=\"lazy\" />\n\n## 2.填写项目名称\n\n<img src=\"/images/517519-20221011103352110-413633770.png\" alt=\"\" loading=\"lazy\" />\n\n## 3.添加需要的框架\n\n这里勾选spring web\n\n<img src=\"/images/517519-20221011103615428-2092753995.png\" alt=\"\" loading=\"lazy\" />\n\n## 4.启动web服务\n\n<img src=\"/images/517519-20221011103949201-291499213.png\" alt=\"\" loading=\"lazy\" />\n\n访问 localhost:8080\n\n<img src=\"/images/517519-20180925172039910-151637999.png\" alt=\"\" />&nbsp;\n\n如果想在更新了代码之后实现**热更新**，请参考 :&nbsp;[https://blog.csdn.net/qq_42685050/article/details/81588584](https://blog.csdn.net/qq_42685050/article/details/81588584)\n\n如果社区版的idea，是无法安装springboot插件的，但是可以安装&nbsp;spring Assistant 插件，之后对于application.yml就有提示功能了，参考：[IDEA社区版创建spring boot项目的安装插件](https://blog.csdn.net/qq1808814025/article/details/109743809)\n\n<img src=\"/images/517519-20210531154334997-443271056.png\" width=\"800\" height=\"145\" loading=\"lazy\" />\n\n配置文件提示\n\n<img src=\"/images/517519-20210531154450886-755378301.png\" width=\"800\" height=\"192\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["SpringBoot"]},{"title":"Java——正则表达式","url":"/Java——正则表达式.html","content":"程序中应用正则表达式则必须依靠Ｐattern类和Ｍatcher类，这两个类都是在**java.util.regex包**中定义。\n\n**Pattern类**的主要作用是进行正则规范的编写。\n\n**Ｍatcher类**主要是执行规范，验证一个字符串是否符合其规范。\n\n<img src=\"/images/517519-20160313101824397-615037445.png\" alt=\"\" />\n\n```\nimport java.util.regex.Pattern;\nimport java.util.regex.Matcher;;\n\npublic class Ｒegex_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n//\t\tString str = \"123456789\";\n//\t\tif(Pattern.compile(\"[0-9]+\").matcher(str).matches()){\t\t//使用正则表达式\n//\t\t\tSystem.out.println(\"是由数字组成！\");\n//\t\t} else{\n//\t\t\tSystem.out.println(\"不是由数字组成！\");\n//\t\t}\n\t\t\n//\t\tString str = \"2016-03-11\" ;\n//\t\tString pat = \"\\\\d{4}-\\\\d{2}-\\\\d{2}\";\t\t\t//定义规则，其中/不要转义，//d等于/d\n//\t\tPattern p = Pattern.compile(pat);\t\t\t//实例化Pattern类\n//\t\tMatcher m = p.matcher(str);\t\t\t\t\t//验证字符串内容是否合法\n//\t\tif(m.matches()){\n//\t\t\tSystem.out.println(\"日期格式合法！\");\n//\t\t}else{\n//\t\t\tSystem.out.println(\"日期格式不合法！\");\n//\t\t}\n\t\t\n\t\tString str = \"A123B123C123F123\" ;\n\t\tString pat = \"\\\\d+\";\n\t\tPattern p = Pattern.compile(pat);\t\t\t//实例化Pattern类\n\t\tString s[] = p.split(str);\t\t\t\t\t\t\t\t//进行字符串拆分\n\t\tfor(int i=0;i<s.length;i++){\n\t\t\tSystem.out.println(s[i]+\"\\t\");\t\t\t\t//输出其中符合字母的字符\n\t\t}\n\t\t\n\t\tMatcher m = p.matcher(str);\t\t\t\t\t\t//实例化Matcher类\n\t\tString newString = m.replaceAll(\"_\");\t\t//替换所有的数字\n\t\tSystem.out.println(newString);\n\t}\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n\n<img src=\"/images/517519-20160313103825850-1221901491.png\" alt=\"\" />\n\n&nbsp;\n\n&nbsp;\n\n<img src=\"/images/517519-20160313103901632-491342824.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160313103922522-900752191.png\" alt=\"\" />\n\n&nbsp;\n","tags":["Java"]},{"title":"CDH5.16安装kudu","url":"/CDH5.16安装kudu.html","content":"1.kudu已经包含在cdh的parcels中了，所以直接添加服务\n\n<img src=\"/images/517519-20211113174542780-1193219579.png\" width=\"300\" height=\"136\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n2.选择kudu，然后安装\n\n<img src=\"/images/517519-20211113174658290-1947844274.png\" width=\"600\" height=\"589\" loading=\"lazy\" />\n\n3.在impala中配置kudu，然后重启\n\n<img src=\"/images/517519-20211113174859690-94626074.png\" width=\"600\" height=\"299\" loading=\"lazy\" />\n\n&nbsp;\n\n参考：[CDH5.16.1 安装配置Kudu](https://www.cnblogs.com/wuning/p/11638637.html)\n\n如果安装的时候报错如下\n\n```\nparceltagoversatisfiedexception: Multiple parcels providing required tag: kudu (CDH, KUDU)\n\n```\n\n这是因为有其他的parcels还没有安装完，比如GPLEXTRAS（LZO）的parcels，所以需要先把GPLEXTRAS（LZO）安装好\n","tags":["CDH","kudu"]},{"title":"Filebeat采集文本文件内容发送到kafka","url":"/Filebeat采集文本文件内容发送到kafka.html","content":"1. 安装filebeat，选择tar包安装方式，下载并解压\n\n```\nwget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.7.0-linux-x86_64.tar.gz\ntar -zxvf filebeat-7.7.0-linux-x86_64.tar.gz\n\n```\n\n2. 修改配置文件\n\n```\nmv filebeat.yml filebeat.yml.bak\ntouch filebeat.yml\n\n```\n\n配置文件内容<!--more-->\n&nbsp;\n\n```\nlogging.level: info\nlogging.to_files: true\nlogging.files:\n    path: /data/log_path/filebeat\n    name: filebeat\n    keepfiles: 7\n    permissions: 0644\n \n \nfilebeat.inputs:\n-   type: log\n    enabled: true\n    paths:\n        - /your_file_name\n    scan_frequency: 1s\n    backoff: 1s\n    max_backoff: 1s\n    ignore_older: 24h\n    close_inactive: 30m\n    close_timeout: 24h\n    clean_inactive: 720h\n    encoding: utf-8\n \noutput.kafka:\n    version: \"2.0.0\"\n    enabled: true\n    hosts: your_kafka_ip:tour_kafka_port\n    topic: your_kafka_topic  # topic name\n    username: xxxxxx   # kafka username\n    password: xxxxxx   # kafka password\n    required_acks: 1\n    compression: gzip\n    max_message_bytes: 1000000\n    codec.format:\n        string: '%{[message]}'\n\n```\n\n3. 启动脚本\n\n&nbsp;\n","tags":["filebeat"]},{"title":"Java观察者设计模式","url":"/Java观察者设计模式.html","content":"在java.util包中提供了**Observable类**和**Observer接口**，使用它们即可完成**观察者模式**。\n\n<!--more-->\n&nbsp;\n\n<img src=\"/images/517519-20160312234318397-1659122869.png\" alt=\"\" />\n\n&nbsp;\n\n**多个观察者都在关注着价格的变化，只要价格一有变化，则所有的观察者会立即有所行动。**\n\n```\nimport java.util.Observable;\nimport java.util.Observer;\n\nclass House extends Observable{\n\tprivate float price;\n\n\tpublic House(float price) {\n\t\tsuper();\n\t\tthis.price = price;\n\t}\n\n\tpublic float getPrice() {\n\t\treturn price;\n\t}\n\n\tpublic void setPrice(float price) {\n\t\tsuper.setChanged();   \t\t\t\t\t\t//设置变化点\n\t\tsuper.notifyObservers(price);\t\t//通知所有观察者价格改变\n\t\tthis.price = price;\t\t\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn \"房子价格为：\" + this.price;\n\t}\n\t\n}\n\nclass HouseObserver implements Observer{\n\tprivate String name;\t\t\t\t\t//观察者的名字\n\t\n\tpublic HouseObserver(String name) {\n\t\tsuper();\n\t\tthis.name = name;\n\t}\n\n\t@Override\n\tpublic void update(Observable o, Object arg) {\n\t\t// TODO 自动生成的方法存根\n\t\tif(arg instanceof Float){\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//判断参数类型\n\t\t\tSystem.out.println(this.name+\"观察到价格改变为\");\n\t\t\tSystem.out.println(((Float)arg).floatValue());\n\t\t}\n\t}\n\t\n}\n\npublic class Observer_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tHouse h = new House(1000000);\n\t\tHouseObserver ho1 = new HouseObserver(\"购房者1\");\n\t\tHouseObserver ho2 = new HouseObserver(\"购房者2\");\n\t\tHouseObserver ho3 = new HouseObserver(\"购房者3\");\n\t\th.addObserver(ho1);\t\t\t\t//加入观察者\n\t\th.addObserver(ho2);\n\t\th.addObserver(ho3);\n\t\tSystem.out.println(h);\t\t\t//输出房子的价格\n\t\th.setPrice(666);\t\t\t\t\t\t//修改房子的价格\n\t\tSystem.out.println(h);\t\t\t//输出房子的价格\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["设计模式"]},{"title":"SpringBoot学习笔记——动态代理","url":"/SpringBoot学习笔记——动态代理.html","content":"代理模式是一种设计模式，提供了对目标对象额外的访问方式，即通过代理对象访问目标对象，这样可以在不修改原目标对象的前提下，提供额外的功能操作，扩展目标对象的功能。\n\n<img src=\"/images/517519-20210627213011652-1197436829.png\" width=\"500\" height=\"471\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n**1.静态代理：**&nbsp;在编译时就已经实现，编译完成后代理类是一个实际的class文件\n\n静态代理的方式：创建一个接口，然后创建被代理的类实现该接口并且实现该接口中的抽象方法。之后再创建一个代理类，同时使其也实现这个接口。在代理类中持有一个被代理对象的引用，而后在代理类方法中调用该对象的方法。\n\n参考：[什么是动态代理？](https://mp.weixin.qq.com/s/GT1-yrxJ5KF0xeMydbJDCQ#)&nbsp;\n\n**2.动态代理：**&nbsp;在运行时动态生成的，即编译完成后没有实际的class文件，而是**在运行时动态生成类字节码，并加载到JVM中**\n\n动态代理的方式：JDK动态代理 和 CGLIB动态代理\n\n在springboot 2.x中默认使用的CGLIB动态代理，如果要强制使用JDK代理的话，需要添加配置\n\n```\nspring.aop.proxy-target-class=false\n\n```\n\n参考：[springboot2.x默认使用的代理是cglib代理](https://blog.csdn.net/u011242657/article/details/99747011)\n\n&nbsp;\n\nJDK动态代理 和 CGLIB动态代理的区别：\n\n1.JDK动态代理只能对实现了接口的类生成代理，而不能针对类\n\nJDK动态代理使用 java.lang.reflect.InvocationHandler，参考：[Java动态代理之InvocationHandler最简单的入门教程](https://www.jianshu.com/p/e575bba365f8)\n\n&nbsp;\n\n**使用JDK动态代理的五大步骤：** \n\n1. 通过实现InvocationHandler接口来自定义自己的InvocationHandler；\n1. 通过`Proxy.getProxyClass`获得动态代理类；\n1. 通过反射机制获得代理类的构造方法，方法签名为`getConstructor(InvocationHandler.class)`；\n1. 通过构造函数获得代理对象并将自定义的`InvocationHandler`实例对象传为参数传入；\n1. 通过代理对象调用目标方法；\n\n2.Cglib是针对类实现代理，主要是对指定的类生成一个子类，覆盖其中的方法，因为是继承，所以该类或方法最好不要声明成final\n\nCglib动态代理使用 org.springframework.cglib.proxy.*，\n\nCGLIB包的底层是通过使用一个小而快的字节码处理框架`ASM`，来转换字节码并生成新的类\n\n**CGLIB代理实现如下：**\n\n1. 首先实现一个MethodInterceptor，方法调用会被转发到该类的intercept()方法。\n1. 然后在需要使用的时候，通过CGLIB动态代理获取代理对象。\n\n参考：[【Spring教程】详解AOP的实现原理（动态代理）](https://blog.csdn.net/sinat_27933301/article/details/94198281)\n\n和 [什么是动态代理？](https://mp.weixin.qq.com/s/GT1-yrxJ5KF0xeMydbJDCQ#) \n\n&nbsp;\n\n在SpringBoot中如何使用动态代理，参考：[【Spring Boot实战与进阶】AOP的两种动态代理（JDK和Cglib）](https://blog.csdn.net/sinat_27933301/article/details/102324348)\n\n&nbsp;\n","tags":["设计模式","SpringBoot"]},{"title":"ubuntu16.04安装haproxy","url":"/ubuntu16.04安装haproxy.html","content":"清华镜像站\n\n```\nhttps://mirrors.tuna.tsinghua.edu.cn/ubuntu/pool/main/h/haproxy/\n\n```\n\n下载haproxy\n\n```\nhttps://mirrors.tuna.tsinghua.edu.cn/ubuntu/pool/main/h/haproxy/haproxy_1.6.3-1ubuntu0.3_amd64.deb\n\n```\n\n安装haproxy\n\n```\nsudo dpkg -i ./haproxy_1.6.3-1ubuntu0.3_amd64.deb\n\n```\n\n编辑配置文件\n\n```\nvim /etc/haproxy/haproxy.cfg\n\n```\n\n其默认配置如下\n\n```\nglobal\n        log /dev/log    local0\n        log /dev/log    local1 notice\n        chroot /var/lib/haproxy\n        stats socket /run/haproxy/admin.sock mode 660 level admin\n        stats timeout 30s\n        user haproxy\n        group haproxy\n        daemon\n\n        # Default SSL material locations\n        ca-base /etc/ssl/certs\n        crt-base /etc/ssl/private\n\n        # Default ciphers to use on SSL-enabled listening sockets.\n        # For more information, see ciphers(1SSL). This list is from:\n        #  https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/\n        ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS\n        ssl-default-bind-options no-sslv3\n\ndefaults\n        log     global\n        mode    http\n        option  httplog\n        option  dontlognull\n        timeout connect 5000\n        timeout client  50000\n        timeout server  50000\n        errorfile 400 /etc/haproxy/errors/400.http\n        errorfile 403 /etc/haproxy/errors/403.http\n        errorfile 408 /etc/haproxy/errors/408.http\n        errorfile 500 /etc/haproxy/errors/500.http\n        errorfile 502 /etc/haproxy/errors/502.http\n        errorfile 503 /etc/haproxy/errors/503.http\n        errorfile 504 /etc/haproxy/errors/504.http\n\n```\n\n<!--more-->\n&nbsp;\n\n**1. 开启stats page**\n\n添加如下配置开启haproxy的stats page页面，在页面中可以查看流程等信息\n\n```\nfrontend stats\n    bind *:8404\n    stats enable\n    stats uri /stats\n    stats refresh 10s\n#    stats admin if LOCALHOST\n    stats auth user:123\n#  stats hide-version # 可以隐藏haproxy的版本\n\n```\n\n参考文档\n\n```\nhttps://www.haproxy.com/blog/exploring-the-haproxy-stats-page/\n\n```\n\n访问localhost:8404可以查看haproxy状态\n\n<img src=\"/images/517519-20210519163218129-1556999307.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n**2. 给TCP端口配置load balancer，比如给hiveserver2配置负载均衡**\n\n参考文档\n\n```\nhttps://docs.cloudera.com/documentation/enterprise/5-15-x/topics/admin_ha_hiveserver2.html\n\n```\n\n添加如下配置开启haproxy作为load balancer\n\n```\nlisten hiveserver2\n    bind 0.0.0.0:10001\n    mode tcp\n    option tcplog\n    balance source\n    server hiveserver2_1 hs2-host-1.example.com:10000 check\n    server hiveserver2_2 hs2-host-2.example.com:10000 check\n    server hiveserver2_3 hs2-host-3.example.com:10000 check\n    server hiveserver2_4 hs2-host-4.example.com:10000 check\n\n```\n\n比如\n\n```\nlisten test\n    bind 0.0.0.0:10001\n    mode tcp\n    option tcplog\n    balance leastconn\n    server es localhost:9200 check\n    server cerebro localhost:9090 check\n\n```\n\n&nbsp;haproxy的负载均衡策略有，中文请参考：[关于haproxy负载均衡的算法整理](https://my.oschina.net/BambooLi/blog/506397)\n\n```\nroundrobin\nstatic-rr\nleastconn\nfirst\nsource\nURI\nURL parameter\nHDR\nrdp-cookie\n\n```\n\n参考：\n\n```\nhttps://d2c.io/post/haproxy-load-balancer-part-2-backend-section-algorithms\n\n```\n\n　　\n","tags":["HAProxy"]},{"title":"yarn学习笔记——yarn api","url":"/yarn学习笔记——yarn api.html","content":"参考：[Yarn 监控 - 监控任务运行状态 （包括Spark，MR 所有在Yarn中运行的任务）](https://blog.csdn.net/zhangshenghang/article/details/104447587)\n\n```\n    //获取任务的applicationId\n    public static String getAppId(String jobName) throws IOException {\n\n        Configuration conf = new Configuration();\n        System.setProperty(\"java.security.krb5.conf\", \"/etc/krb5.conf\");\n        conf.set(\"hadoop.security.authentication\", \"Kerberos\");\n        UserGroupInformation.setConfiguration(conf);\n        UserGroupInformation.loginUserFromKeytab(\"hdfs@XXXX\", \"/home/xxxx/hdfs.keytab\");\n\n        YarnClient client = YarnClient.createYarnClient();\n        client.init(conf);\n        client.start();\n        EnumSet<YarnApplicationState> appStates = EnumSet.noneOf(YarnApplicationState.class);\n\n        if (appStates.isEmpty()) {\n            appStates.add(YarnApplicationState.RUNNING);\n            appStates.add(YarnApplicationState.ACCEPTED);\n            appStates.add(YarnApplicationState.SUBMITTED);\n        }\n\n        List<ApplicationReport> appsReport = null;\n        try {\n            // 返回EnumSet<YarnApplicationState>中个人任务状态的所有任务\n            appsReport = client.getApplications(appStates);\n        } catch (YarnException | IOException e) {\n            e.printStackTrace();\n        }\n\n        assert appsReport != null;\n\n        for (ApplicationReport appReport : appsReport) {\n            System.out.println(appReport);\n            // 获取任务名\n            String jn = appReport.getName();\n            String applicationType = appReport.getApplicationType();\n            if (jn.equals(jobName)) { // &amp;&amp; \"Apache Flink\".equals(applicationType)) {\n                try {\n                    client.close();\n                } catch (IOException e) {\n                    e.printStackTrace();\n                }\n                return appReport.getApplicationId().toString();\n            }\n        }\n        try {\n            client.close();\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n        return null;\n    }\n\n    // 根据任务的applicationId去获取任务的状态\n    public static YarnApplicationState getState(String appId) throws IOException {\n\n        Configuration conf = new Configuration();\n        System.setProperty(\"java.security.krb5.conf\", \"/etc/krb5.conf\");\n        conf.set(\"hadoop.security.authentication\", \"Kerberos\");\n        UserGroupInformation.setConfiguration(conf);\n        UserGroupInformation.loginUserFromKeytab(\"hdfs@XXXXX\", \"/home/xxxx/hdfs.keytab\");\n\n        YarnClient client = YarnClient.createYarnClient();\n        client.init(conf);\n        client.start();\n        ApplicationId applicationId = ConverterUtils.toApplicationId(appId);\n        YarnApplicationState yarnApplicationState = null;\n        try {\n            ApplicationReport applicationReport = client.getApplicationReport(applicationId);\n            yarnApplicationState = applicationReport.getYarnApplicationState();\n        } catch (YarnException | IOException e) {\n            e.printStackTrace();\n        }\n        try {\n            client.close();\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n        return yarnApplicationState;\n    }\n\n    public static void main(String[] args) throws IOException, InterruptedException {\n        String state = getAppId(\"job_xxxxx\");\n        System.out.println(state);\n//        System.out.println(state == YarnApplicationState.RUNNING);\n\n    }\n\n```\n\n输出\n\n```\napplicationId { id: 279 cluster_timestamp: 1620359479641 } \nuser: \"dl\" \nqueue: \"root.xxxx\" \nname: \"xxx-flink\" \nhost: \"xxxxx\" \nrpc_port: 18000 \nclient_to_am_token { identifier: \"xxxx@XXXX\" password: \";xxxxx\" kind: \"YARN_CLIENT_TOKEN\" service: \"\" } \nyarn_application_state: RUNNING \ntrackingUrl: \"http://xxxxx:8088/proxy/application_xxxxx/\" \ndiagnostics: \"\" \nstartTime: 1620391776339 \nfinishTime: 0 \nfinal_application_status: APP_UNDEFINED \napp_resource_Usage { num_used_containers: 4 num_reserved_containers: 0 used_resources { memory: 8192 virtual_cores: 7 } reserved_resources { memory: 0 virtual_cores: 0 } needed_resources { memory: 8192 virtual_cores: 7 } memory_seconds: 12703546778 vcore_seconds: 10855065 } \noriginalTrackingUrl: \"http://xxxx:18000\" currentApplicationAttemptId { application_id { id: 279 cluster_timestamp: 1620359479641 } attemptId: 1 } progress: 1.0 \napplicationType: \"XXXX Flink\" \nlog_aggregation_status: LOG_NOT_START\n\n```\n\n　　\n\n<!--more-->\n&nbsp;\n","tags":["YARN"]},{"title":"Flink学习笔记——反压","url":"/Flink学习笔记——反压.html","content":"参考：\n\n```\nhttp://wuchong.me/blog/2016/04/26/flink-internals-how-to-handle-backpressure/\n\n```\n\n　　\n","tags":["Flink"]},{"title":"Mybatis学习笔记——通用mapper","url":"/Mybatis学习笔记——通用mapper.html","content":"在使用mybatis-generator自动生成mapper代码的时候，对于基本的增删改查方法可以通过继承通用mapper的方式进行简化，参考：[MyBatis 通用 Mapper<!--more-->\n&nbsp;&nbsp;5. 高级用法](https://mapperhelper.github.io/docs/5.professional/)\n\nmapper接口，参考：[Mapper 接口大全](https://mapperhelper.github.io/all/)\n\n1.依赖\n\n```\n<!-- mybatis -->\n<dependency>\n    <groupId>tk.mybatis</groupId>\n    <artifactId>mapper-spring-boot-starter</artifactId>\n    <version>2.1.5</version>\n</dependency>\n<dependency>\n    <groupId>tk.mybatis</groupId>\n    <artifactId>mapper</artifactId>\n    <version>3.3.9</version>\n</dependency>\n\n```\n\n2.由于需要使用增删改查的方法，所以通用的MyMapper类为\n\n```\nimport tk.mybatis.mapper.common.Mapper;\nimport tk.mybatis.mapper.common.MySqlMapper;\n\npublic interface MyMapper<T> extends Mapper<T>, MySqlMapper<T> {\n}\n\n```\n\n3.对于具体mapper，只主要继承MyMapper即可，如\n\n```\npublic interface UserMapper extends MyMapper<User> {\n}\n```\n\n对于UserMapper接口，需要添加 @Mapper 注解，或者使用&nbsp;@MapperScan(basePackages = \"com.example.demo.mapper\") 的方式，为其生成动态代理类\n\n**代理模式**是一种设计模式，提供了对目标对象额外的访问方式，即通过代理对象访问目标对象，这样可以在不修改原目标对象的前提下，提供额外的功能操作，扩展目标对象的功能。\n\n**动态代理**，参考：[什么是动态代理？](https://mp.weixin.qq.com/s/GT1-yrxJ5KF0xeMydbJDCQ#)&nbsp;\n\n[Java动态代理之InvocationHandler最简单的入门教程](https://www.jianshu.com/p/e575bba365f8)\n\n**静态代理：**&nbsp;在编译时就已经实现，编译完成后代理类是一个实际的class文件\n\n**动态代理：**&nbsp;在运行时动态生成的，即编译完成后没有实际的class文件，而是**在运行时动态生成类字节码，并加载到JVM中**\n\n&nbsp;\n\n@Mapper注解 和 @MapperScan注解，参考：[详解 @MapperScan 注解和 @Mapper 注解](https://www.cnblogs.com/muxi0407/p/11847794.html)\n","tags":["mybatis"]},{"title":"SpringBoot学习笔记——spring security","url":"/SpringBoot学习笔记——spring security.html","content":"Spring Security是提供了认证，鉴权以及其他的安全特性的java框架，下面是Spring Security的使用教程\n\n1.引入依赖\n\n```\n<dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-security</artifactId>\n</dependency>\n\n```\n\n引入依赖用会发现请求所有的接口都会跳转到 /login，要求你进行账号密码的认证\n\n<img src=\"/images/517519-20210610192537624-491689321.png\" alt=\"\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n其默认的用户是user，密码会在日志中打印出来，Using generated security password: xxxxxxxx\n\n账号密码正确后，接口就可以正常请求，且一般情况下同一个电脑同一个浏览器下的session是共享的，比如同个浏览器下多个窗口的session id是相同的\n\n<img src=\"/images/517519-20210610192732862-619310469.png\" alt=\"\" loading=\"lazy\" />\n\n如果想自定义认证的方式的话，可以通过继承 WebSecurityConfigurerAdapter 的方式，重写configure(HttpSecurity http) 方法\n\n不添加的话，默认的配置等于：使用fromLogin()表单方式对所有的request进行httpBasic()账号密码认证\n\n```\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.security.config.annotation.web.builders.HttpSecurity;\nimport org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;\n\n@Configuration\npublic class WebSecurityConfig extends WebSecurityConfigurerAdapter {\n\n    @Override\n    public void configure(HttpSecurity http) throws Exception {\n\n        http\n                .authorizeRequests()\n                .anyRequest().authenticated()\n                .and().formLogin()\n                .and().httpBasic();\n    }\n}\n```\n\n如果需要添加自定义账号密码，可以通过重写configure(final AuthenticationManagerBuilder auth) 方法\n\n```\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.http.HttpMethod;\nimport org.springframework.security.config.annotation.authentication.builders.AuthenticationManagerBuilder;\nimport org.springframework.security.config.annotation.web.builders.HttpSecurity;\nimport org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;\nimport org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;\nimport org.springframework.security.crypto.password.PasswordEncoder;\n\n@Configuration\npublic class WebSecurityConfig extends WebSecurityConfigurerAdapter {\n\n    @Bean\n    public PasswordEncoder passwordEncoder() {\n        return new BCryptPasswordEncoder();\n    }\n\n    @Override\n    public void configure(final AuthenticationManagerBuilder auth) throws Exception {\n\n        auth\n                .inMemoryAuthentication()\n                .withUser(\"admin\")\n                .password(this.passwordEncoder().encode(\"admin\"))\n                .roles(\"USER\");\n    }\n\n    @Override\n    public void configure(HttpSecurity http) throws Exception {\n\n        http\n                .authorizeRequests()\n                .antMatchers(HttpMethod.GET, \"/login\").permitAll()\n                .anyRequest().authenticated()\n                .and().formLogin()\n                .and().httpBasic();\n    }\n\n}\n\n```\n\n这时对非/login的请求，都需要进行认证\n\n需要注意的是，对于POST请求，添加了认证之后，仍然会报403，需要额外关闭csrf\n\n```\nhttp\n\t// 关闭csrf\n\t.csrf().disable();\n\n```\n\n参考：[spring boot post请求403，get请求成功](https://blog.csdn.net/topdeveloperr/article/details/105965655)\n\n[Security关闭CSRF](https://blog.csdn.net/doStruggle/article/details/80589126)\n\n可以在添加自定义 filter 来实现基于web token的认证\n\n```\nimport com.example.demo.jwt.JwtAuthenticationFilter;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.security.config.annotation.web.builders.HttpSecurity;\nimport org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;\nimport org.springframework.security.web.authentication.UsernamePasswordAuthenticationFilter;\n\nimport javax.annotation.Resource;\n\n@Configuration\npublic class WebSecurityConfig extends WebSecurityConfigurerAdapter {\n\n    @Resource\n    private JwtAuthenticationFilter jwtAuthenticationFilter;\n\n    @Override\n    public void configure(HttpSecurity http) throws Exception {\n\n        http\n                .authorizeRequests()\n                .anyRequest().authenticated();\n\n        http\n                .addFilterBefore(jwtAuthenticationFilter, UsernamePasswordAuthenticationFilter.class);\n\n    }\n}\n\n```\n\n自定义filter，JwtAuthenticationFilter，在其中对\n\n&nbsp;\n","tags":["SpringBoot"]},{"title":"avro序列化框架的schema evolution","url":"/avro序列化框架的schema evolution.html","tags":["avro"]},{"title":"docker学习笔记——网络模式","url":"/docker学习笔记——网络模式.html","content":"**查看容器的网络模式**\n\n```\ndocker ps -a\nCONTAINER ID        IMAGE                                    COMMAND                  CREATED             STATUS              PORTS               NAMES\n3f6822d8f262        confluentinc/cp-schema-registry:latest   \"/etc/confluent/dock&hellip;\"   13 minutes ago      Up 13 minutes                           schema-registry\n\ndocker inspect 3f6822d8f262 | grep -i \"network\"\n            \"NetworkMode\": \"host\",\n        \"NetworkSettings\": {\n            \"Networks\": {\n                    \"NetworkID\": \"5d40a7d178679339f87cc31965ba9a1c662c74ccea853945967d4303e4f9acc0\",\n\n```\n\ndocker总共有4种网络模式，从上到下隔离度下降：\n\n1.Close容器，即**none模式**，运行在Close容器中的进程只能访问本地回环接口，隔离度最高\n\n2.Bridge容器，即**bridge模式**，当容器中的进程需要访问外部网络的时候应该使用，且bridge是docker的default网络模式。\n\n<!--more-->\n&nbsp;\n\nbridge容器拥有2个接口，一个是私有的本地回环接口，另一个是私有接口通过网桥连接到主机的其他容器。\n\n3.Joined容器，\n\n4.Open容器，即**host模式**，该模式在启动docker的时候加上 --net=host 参数，比如\n\n```\ndocker run  --network=host xxxx:v1.0.0dev\n\n```\n\n该模式容器会占用宿主机的对应端口，且可以访问宿主机的host\n\n在host模式下，如果使用-p参数来定义端口，会报\n\n```\nWARNING: Published ports are discarded when using host network mode\n\n```\n\n但是-p参数还是会生效\n","tags":["docker"]},{"title":"上传snapshots jar包到nexus仓库","url":"/上传snapshots jar包到nexus仓库.html","content":"在nexus界面上可以手动上传release和hosted的包，但是无法手动上传snapshots的包\n\n<img src=\"/images/517519-20210520173339863-599770525.png\" alt=\"\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n&nbsp;\n\n&nbsp;需要使用&nbsp;mvn deploy 来上传snapshots包，比如\n\n```\nmvn deploy:deploy-file -DgroupId=org.apache.impala -DartifactId=impala-frontend -Dversion=0.1-SNAPSHOT -Dpackaging=jar -Dfile=/home/lintong/下载/impala-frontend-0.1-SNAPSHOT.jar -Durl=http://ip:8081/nexus/repository/maven-snapshots/ -DrepositoryId=nexus-snapshots\n\n```\n\n其中\n\n-Durl 指定的是仓库的URL地址\n\n<img src=\"/images/517519-20210520173624873-362576549.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n-DrepositoryId 指的是maven settings.xml中的repository的id\n\n注意<server></server>中的账号密码的id需要和<repository></repository>中的id对的上\n\n<img src=\"/images/517519-20210520173955798-1348933507.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;<img src=\"/images/517519-20210520173900490-362317745.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["Nexus"]},{"title":"MySQL学习笔记——binlog","url":"/MySQL学习笔记——binlog.html","content":"## 1.docker部署MySQL\n\namd64的机器可以使用centos的MySQL5.7的镜像：[https://hub.docker.com/r/centos/mysql-57-centos7/](https://hub.docker.com/r/centos/mysql-57-centos7/)\n\narm64和amd64的机器也可以使用MySQL8.0的镜像：[https://hub.docker.com/layers/library/mysql/8.0.29/images/sha256-44f98f4dd825a945d2a6a4b7b2f14127b5d07c5aaa07d9d232c2b58936fb76dc](https://hub.docker.com/layers/library/mysql/8.0.29/images/sha256-44f98f4dd825a945d2a6a4b7b2f14127b5d07c5aaa07d9d232c2b58936fb76dc)\n\n启动MySQL5.7的容器\n\n```\ndocker run --name mysqltest -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.7.44\n\n```\n\n启动MySQL8.0的容器\n\n```\ndocker run --name mysqltest -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:8.0.29\n\n```\n\n如果想指定mysql配置和data挂载路径，可以先进入容器中将mysql的配置先拷贝出来\n\n进入容器查看MySQL的配置路径\n\n```\nsh-4.4# mysql --help | grep my.cnf\n                      order of preference, my.cnf, $MYSQL_TCP_PORT,\n/etc/my.cnf /etc/mysql/my.cnf /usr/etc/my.cnf ~/.my.cnf \n\n```\n\n参考：[Docker安装MySQL 并挂载数据及配置文件,设置远程访问权限](https://blog.csdn.net/qq_41266196/article/details/130164908)\n\n将配置/etc/my.cnf拷贝到宿主机\n\n```\ndocker cp mysqltest:/etc/my.cnf /Users/lintong/Downloads/mysql8.0/config/\n\n```\n\n指定mysql配置和data挂载路径启动docker mysql\n\nmysql8.0\n\n```\ndocker run --name mysqltest \\\n-v /Users/lintong/Downloads/mysql8.0/config/my.cnf:/etc/my.cnf \\\n-v /Users/lintong/Downloads/mysql8.0/data:/var/lib/mysql \\\n-p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:8.0.29\n\n```\n\nmysql5.7\n\n```\ndocker run --name mysqltest \\\n-v /Users/lintong/Downloads/mysql5.7/config/my.cnf:/etc/my.cnf \\\n-v /Users/lintong/Downloads/mysql5.7/data:/var/lib/mysql \\\n-p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.7.44\n\n```\n\n## 2.开启binlog\n\n查看binlog是否开启，MySQL8.0默认是开启的\n\n```\nmysql>  show variables like '%log_bin%';\n+---------------------------------+--------------------------------+\n| Variable_name                   | Value                          |\n+---------------------------------+--------------------------------+\n| log_bin                         | ON                             |\n| log_bin_basename                | /var/lib/mysql/mysql-bin       |\n| log_bin_index                   | /var/lib/mysql/mysql-bin.index |\n| log_bin_trust_function_creators | OFF                            |\n| log_bin_use_v1_row_events       | OFF                            |\n| sql_log_bin                     | ON                             |\n+---------------------------------+--------------------------------+\n\n```\n\n低版本默认是关闭的\n\n<img src=\"/images/517519-20240813000924143-1457317742.png\" width=\"300\" height=\"56\" loading=\"lazy\" />\n\n如果是mysql5.7的话，需要在my.cnf配置中添加如下配置，参考：[MySQL-开启binlog](https://www.cnblogs.com/yeyuzhuanjia/p/18047618)\n\n```\n[mysqld]<br />log-bin=mysql-bin<br />server-id=1\n\n```\n\n其他配置\n\n```\n#设置日志格式\nbinlog_format = mixed\n#设置binlog清理时间\nexpire_logs_days = 5\n#binlog每个日志文件大小\nmax_binlog_size = 50m\n#binlog缓存大小\nbinlog_cache_size = 4m\n#最大binlog缓存大小\nmax_binlog_cache_size = 512m\n\n```\n\n参考：[Docker内部MySQL开启binlog日志](https://www.cnblogs.com/stcweb/articles/15141385.html)\n\n可以在data目录下看到生成的binlog文件\n\n<img src=\"/images/517519-20240815000607406-1331184034.png\" width=\"400\" height=\"202\" loading=\"lazy\" />\n\n使用命令查看binlog列表\n\n```\nmysql> show binary logs;\n+------------------+-----------+\n| Log_name         | File_size |\n+------------------+-----------+\n| mysql-bin.000001 |       177 |\n| mysql-bin.000002 |   2947794 |\n| mysql-bin.000003 |       154 |\n+------------------+-----------+\n3 rows in set (0.01 sec)\n\n```\n\n查看当前记录的binlog文件的文件名和偏移\n\n```\nmysql> show master status;\n+------------------+----------+--------------+------------------+-------------------+\n| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |\n+------------------+----------+--------------+------------------+-------------------+\n| mysql-bin.000004 |      154 |              |                  |                   |\n+------------------+----------+--------------+------------------+-------------------+\n1 row in set (0.00 sec)\n\n```\n\n可以使用mysqlbinlog命令来查看binlog，如果想镜像中自带mysqlbinlog命令，可以使用debian的镜像，比如\n\n```\ndocker run --name mysqltest \\\n-v /Users/lintong/Downloads/mysql5.7/config/my.cnf:/etc/my.cnf \\\n-v /Users/lintong/Downloads/mysql5.7/data:/var/lib/mysql \\\n-p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.7-debian\n\n```\n\n查看指定binlog文件的内容，可以看到这里先创建了一个test database，然后create了一张名为user的表\n\n```\nmysql> show binlog events in 'mysql-bin.000005';\n+------------------+-----+----------------+-----------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Log_name         | Pos | Event_type     | Server_id | End_log_pos | Info                                                                                                                                                                                                                                        |\n+------------------+-----+----------------+-----------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| mysql-bin.000005 |   4 | Format_desc    |         1 |         123 | Server ver: 5.7.42-log, Binlog ver: 4                                                                                                                                                                                                       |\n| mysql-bin.000005 | 123 | Previous_gtids |         1 |         154 |                                                                                                                                                                                                                                             |\n| mysql-bin.000005 | 154 | Anonymous_Gtid |         1 |         219 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS'                                                                                                                                                                                                        |\n| mysql-bin.000005 | 219 | Query          |         1 |         313 | create database test                                                                                                                                                                                                                        |\n| mysql-bin.000005 | 313 | Anonymous_Gtid |         1 |         378 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS'                                                                                                                                                                                                        |\n| mysql-bin.000005 | 378 | Query          |         1 |         675 | use `test`; create table user\n(\n    id       bigint unsigned auto_increment comment ''\n        primary key,\n    username varchar(128) not null comment '',\n    email    varchar(128) not null comment ''\n)\n    comment '' charset = utf8mb4 |\n+------------------+-----+----------------+-----------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n6 rows in set (0.00 sec)\n\n```\n\n查看binlog的格式，binlog的格式有3种，分别为STATEMENT，ROW和MIXED\n\n```\nmysql> SHOW VARIABLES LIKE '%binlog_format%';\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| binlog_format | ROW   |\n+---------------+-------+\n1 row in set (0.01 sec)\n\n```\n\n可以使用mysqlbinlog命令将binlog导成sql文件\n\n```\nmysqlbinlog --no-defaults --base64-output=decode-rows -v /var/lib/mysql/mysql-bin.000005 > /tmp/binlog005.sql\n\n```\n\n也可以指定开始和结束时间来导出binlog，或者指定position\n\n```\nmysqlbinlog --no-defaults --base64-output=decode-rows -v --start-datetime='2024-08-16 00:00:00' --stop-datetime='2024-08-16 23:00:00' /var/lib/mysql/mysql-bin.000005 > /tmp/binlog005.sql\n\n```\n\n查看mysql的时区，可以看出使用的是UTC时间\n\n```\nmysql> show variables like '%time_zone%';\n+------------------+--------+\n| Variable_name    | Value  |\n+------------------+--------+\n| system_time_zone | UTC    |\n| time_zone        | SYSTEM |\n+------------------+--------+\n2 rows in set (0.02 sec)\n\nmysql> select now();\n+---------------------+\n| now()               |\n+---------------------+\n| 2024-08-17 15:29:48 |\n+---------------------+\n1 row in set (0.00 sec)\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["MySQL"]},{"title":"Flink学习笔记——内存调优","url":"/Flink学习笔记——内存调优.html","content":"## flink内存分布\n\ntask manager\n\n<img src=\"/images/517519-20230518113912532-567190663.png\" alt=\"\" loading=\"lazy\" />\n\n参考：[Flink重点难点：Flink任务综合调优(Checkpoint/反压/内存)](https://www.cnblogs.com/importbigdata/articles/15617347.html)\n\n## 1.堆外内存不足：java.lang.OutOfMemoryError: Direct buffer memory\n\n报错如下\n\n```\nCaused by: java.lang.OutOfMemoryError: Direct buffer memory. The direct out-of-memory error has occurred. This can mean two things: either job(s) require(s) a larger size of JVM direct memory or there is a direct memory leak. <br />The direct memory can be allocated by user code or some of its dependencies. <br />In this case 'taskmanager.memory.task.off-heap.size' configuration option should be increased. Flink framework and its dependencies also consume the direct memory, mostly for network communication. <br />The most of network memory is managed by Flink and should not result in out-of-memory error. In certain special cases, in particular for jobs with high parallelism, the framework may require more direct memory which is not managed by Flink. <br />In this case 'taskmanager.memory.framework.off-heap.size' configuration option should be increased. If the error persists then there is probably a direct memory leak in user code or some of its dependencies which has to be investigated and fixed. The task executor has to be shutdown...\n\tat java.nio.Bits.reserveMemory(Bits.java:695)\n\tat java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:123)\n\tat java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311)\n\tat sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:247)\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:58)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n\tat java.nio.channels.Channels.writeFullyImpl(Channels.java:78)\n\tat java.nio.channels.Channels.writeFully(Channels.java:101)\n\tat java.nio.channels.Channels.access$000(Channels.java:61)\n\tat java.nio.channels.Channels$1.write(Channels.java:174)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n\tat java.security.DigestOutputStream.write(DigestOutputStream.java:145)\n\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.write(MultipartUploadOutputStream.java:172)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:63)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:63)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.hudi.common.fs.SizeAwareFSDataOutputStream.lambda$write$0(SizeAwareFSDataOutputStream.java:58)\n\tat org.apache.hudi.common.fs.HoodieWrapperFileSystem.executeFuncWithTimeMetrics(HoodieWrapperFileSystem.java:106)\n\tat org.apache.hudi.common.fs.HoodieWrapperFileSystem.executeFuncWithTimeAndByteMetrics(HoodieWrapperFileSystem.java:124)\n\tat org.apache.hudi.common.fs.SizeAwareFSDataOutputStream.write(SizeAwareFSDataOutputStream.java:55)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:63)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n\tat org.apache.hudi.common.table.log.HoodieLogFormatWriter.appendBlocks(HoodieLogFormatWriter.java:175)\n\tat org.apache.hudi.io.HoodieAppendHandle.appendDataAndDeleteBlocks(HoodieAppendHandle.java:404)\n\tat org.apache.hudi.io.HoodieAppendHandle.close(HoodieAppendHandle.java:439)\n\tat org.apache.hudi.io.FlinkAppendHandle.close(FlinkAppendHandle.java:99)\n\tat org.apache.hudi.execution.ExplicitWriteHandler.closeOpenHandle(ExplicitWriteHandler.java:62)\n\tat org.apache.hudi.execution.ExplicitWriteHandler.finish(ExplicitWriteHandler.java:52)\n\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueueConsumer.consume(BoundedInMemoryQueueConsumer.java:41)\n\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$2(BoundedInMemoryExecutor.java:135)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n```\n\n可能需要调整的是taskmanager的内存参数， taskmanager.memory.task.off-heap.size 或者 taskmanager.memory.framework.off-heap.size，在启动flink session cluster的时候添加如下配置\n\n需要注意的是，需要在启动session cluster的时候配置-D参数，在flink run的时候添加内存参数是无法生效的\n\n```\n/usr/lib/flink/bin/yarn-session.sh -s 1 -jm 51200 -tm 51200 -qu data -D taskmanager.memory.task.off-heap.size=4G -D taskmanager.memory.framework.off-heap.size=4G --detached\n\n```\n\n点到task manager的页面查看，配置的4G内存已经生效\n\n<img src=\"/images/517519-20230428162823889-1458699743.png\" alt=\"\" loading=\"lazy\" />\n\n这是由于flink off-heap size默认只有128M，需要进行调整，如下\n\n<img src=\"/images/517519-20230427161517997-1136953965.png\" alt=\"\" loading=\"lazy\" />\n\n参考：[Flink 运行错误 java.lang.OutOfMemoryError: Direct buffer memory](https://www.cnblogs.com/aquester/p/16986545.html)\n\n其他调优：[Flink性能调优](https://blog.csdn.net/xiaoyixiao_/article/details/122928240)\n\n在flink cdc写hudi的场景下，建议使用BUCKET index type替换默认的FLINK STATE index type，FLINK STATE index type是in-memory的，十分消耗内存\n\n参考：[HUDI-0.11.0 BUCKET index on Flink 新特性试用](https://www.cnblogs.com/magic-x/articles/16114261.html)\n\n<!--more-->\n&nbsp;\n\n## 2.磁盘空间不足：No space left on device\n\n报错如下\n\n```\nCaused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for KeyedProcessOperator_90bea66de1c231edf33913ecd54406c1_(10/25) from any of the 1 provided restore options.\n\tat org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:160)\n\tat org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:346)\n\tat org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:164)\n\t... 11 more\nCaused by: org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.\n\tat org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:395)\n\tat org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:483)\n\tat org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:97)\n\tat org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:329)\n\tat org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168)\n\tat org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)\n\t... 13 more\nCaused by: java.io.IOException: No space left on device\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:65)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n\tat java.nio.channels.Channels.writeFullyImpl(Channels.java:78)\n\tat java.nio.channels.Channels.writeFully(Channels.java:101)\n\tat java.nio.channels.Channels.access$000(Channels.java:61)\n\tat java.nio.channels.Channels$1.write(Channels.java:174)\n\tat org.apache.flink.contrib.streaming.state.RocksDBStateDownloader.downloadDataForStateHandle(RocksDBStateDownloader.java:141)\n\tat org.apache.flink.contrib.streaming.state.RocksDBStateDownloader.lambda$createDownloadRunnables$0(RocksDBStateDownloader.java:110)\n\tat org.apache.flink.util.function.ThrowingRunnable.lambda$unchecked$0(ThrowingRunnable.java:49)\n\tat java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n```\n\n解决方法：适当增加机器磁盘的空间\n\n&nbsp;\n\n## 3.堆内存不足：Caused by: java.lang.OutOfMemoryError:&nbsp;Java heap space\n\n1.使用rocksdb作用flink任务的状态后端，flink默认使用的是in memory状态后端，十分消耗内存\n\n参考：[https://hudi.apache.org/cn/docs/0.9.0/flink-quick-start-guide/#%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96](https://hudi.apache.org/cn/docs/0.9.0/flink-quick-start-guide/#%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96)\n\n2.其他一些由于内存oom导致的GC问题\n\n参考：[FlinkCDC-Hudi:Mysql数据实时入湖全攻略六：极限压测下炸出来的FlinkCDC-Hudi坑，真多](https://blog.csdn.net/m0_66705151/article/details/123071987)\n","tags":["Flink"]},{"title":"pycharm连接远程python环境","url":"/pycharm连接远程python环境.html","content":"## 1.配置ssh configuration\n\n<img src=\"/images/517519-20230319203130112-1896190781.png\" width=\"700\" height=\"506\" loading=\"lazy\" />\n\n2.给项目配置远程python环境\n\n<img src=\"/images/517519-20230319202558093-1519646490.png\" width=\"800\" height=\"577\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n## 2.配置python interpreter以及代码同步的路径\n\n<img src=\"/images/517519-20230319203318450-1879531132.png\" width=\"700\" height=\"453\" loading=\"lazy\" />\n\n&nbsp;\n\n最后选择remote的python环境来运行代码\n\n<img src=\"/images/517519-20230319203727208-955746729.png\" width=\"700\" height=\"201\" loading=\"lazy\" />\n\n&nbsp;\n\nremote的python环境上有显卡，可以成功查询到\n\n<img src=\"/images/517519-20230319203835231-1048342835.png\" width=\"200\" height=\"43\" loading=\"lazy\" />\n\n&nbsp;\n\n参考：[Pycharm远程调试及远程虚拟环境（包括Docker）](https://zhuanlan.zhihu.com/p/70903277)\n\n&nbsp;\n","tags":["Python"]},{"title":"使用joda-time处理时间","url":"/使用joda-time处理时间.html","content":"引入joda-time\n\n```\n<!--jodatime-->\n<dependency>\n    <groupId>joda-time</groupId>\n    <artifactId>joda-time</artifactId>\n    <version>2.10</version>\n</dependency>\n\n```\n\n1.字符串转joda-time的DateTime\n\nparse日期\n\n```\nDateTimeFormatter fmt = DateTimeFormat.forPattern(\"yyyy-MM-dd\");\nDateTime dateTime = DateTime.parse(date, fmt);\n\n```\n\nparse时间戳\n\n```\nDateTimeFormatter format = DateTimeFormat.forPattern(\"yyyy-MM-dd HH:mm:ss\");\nDateTime dt = DateTime.parse(dateStr, format);\n\n```\n\n转换时区\n\n```\nDateTimeFormatter format = DateTimeFormat.forPattern(\"EEE MMM dd HH:mm:ss yyyy\").withLocale(Locale.ENGLISH);\nDateTime dt = DateTime.parse(dateStr, format);\n\n```\n\n2.joda-time的DateTime转字符串\n\n```\nString date = dt.toString(\"yyyy-MM-dd\");\nString datetime = dt.toString(\"yyyy-MM-dd HH:mm:ss\");\nString datetime = dt.toString(\"yyyy-MM-dd HH:00:00\"); // 整点\n\n```\n\n3.获取当前时间\n\n```\nDateTime now = new DateTime();\n\n```\n\n4.比较2个DateTime的时间差\n\n```\nDays.daysBetween(dateTime, now).getDays() > 7\n\n```\n\n5.jodatime添加时区\n\n```\nDateTime now = new DateTime(DateTimeZone.UTC);\n\n```\n\n　　\n","tags":["开发工具"]},{"title":"Java泛型","url":"/Java泛型.html","content":"**泛型**就是指在对象建立时不指定类中属性的具体类型，而由外部在声明及实例化对喜爱时指定类型。\n\n在泛型的指定中无法指定基本数据类型的，必须设置成一个类，这样在设置一个数字时就必须使用包装类。\n\n```\nclass Point<T>{\t\t//此处T可以是任意的标识符号，T是type的简称\n\tprivate T var;\t\t//此变量的类型由外部决定\n\n\tpublic T getVar() {\t\t//返回值的类型由外部决定\n\t\treturn var;\n\t}\n\n\tpublic void setVar(T var) {\t//设置的类型由外部指定\n\t\tthis.var = var;\n\t}\n}\n\npublic class Generics_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n//\t\tPoint<Integer> p = new Point<Integer>();\t//里面的var类型为Integer类型\n//\t\tp.setVar(30);  \t\t\t\t\t\t\t\t\t\t\t\t\t\t//设置数字，自动装箱\n//\t\tSystem.out.println(p.getVar()*2);\t\t\t\t\t//计算结果，按数字取出\n\t\tPoint<String> p = new Point<String>();\t//里面的var类型为Integer类型\n\t\tp.setVar(\"张三\");  \t\t\t\t\t\t\t\t\t\t\t\t\t\t//设置数字，自动装箱\n\t\tSystem.out.println(p.getVar().length());\t\t\t\t\t//计算结果，按数字取出\n\t}\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n\n**泛型的构造方法**\n\n```\nclass Point<T>{\t\t\t//此处T可以是任意的标识符号，T是type的简称\n\tprivate T var;\t\t//此变量的类型由外部决定\n\t\n\tpublic Point(T var) {\t\t\t\t\t//构造方法\n\t\tsuper();\n\t\tthis.var = var;\n\t}\n\n\tpublic T getVar() {\t\t\t\t\t//返回值的类型由外部决定\n\t\treturn var;\n\t}\n\n\tpublic void setVar(T var) {\t//设置的类型由外部指定\n\t\tthis.var = var;\n\t}\n\n}\n\npublic class Generics_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n//\t\tPoint<Integer> p = new Point<Integer>();\t//里面的var类型为Integer类型\n//\t\tp.setVar(30);  \t\t\t\t\t\t\t\t\t\t\t\t\t\t//设置数字，自动装箱\n//\t\tSystem.out.println(p.getVar()*2);\t\t\t\t\t//计算结果，按数字取出\n\t\t\n//\t\tPoint<String> p = new Point<String>();\t//里面的var类型为Integer类型\n//\t\tp.setVar(\"张三\");  \t\t\t\t\t\t\t\t\t\t\t\t\t\t//设置数字，自动装箱\n//\t\tSystem.out.println(p.getVar().length());\t\t\t\t\t//计算结果，按数字取出\n\t\t\n\t\tPoint<String> p = new Point<String>(\"张三\");\n\t\tSystem.out.println(\"内容：\"+p.getVar());\t\n\t}\n\n}\n\n```\n\n&nbsp;\n\n指定多个泛型类型\n\n```\nclass Point<T>{\t\t//此处T可以是任意的标识符号，T是type的简称\n\tprivate T var;\t//此变量的类型由外部决定\n\t\n\tpublic Point(T var) {\t\t//构造方法\n\t\tsuper();\n\t\tthis.var = var;\n\t}\n\n\tpublic T getVar() {\t\t\t//返回值的类型由外部决定\n\t\treturn var;\n\t}\n\n\tpublic void setVar(T var) {\t//设置的类型由外部指定\n\t\tthis.var = var;\n\t}\n\t\n}\n\nclass Notepad<K,V>{\t\t\t//此处T可以是任意的标识符号，T是type的简称\n\tprivate K key;\t\t\t//此变量的类型由外部决定\n\tprivate V value;\t\t//此变量的类型由外部决定\n\t\n\tpublic Notepad(K key, V value) {\t\t//构造方法\n\t\tsuper();\n\t\tthis.key = key;\n\t\tthis.value = value;\n\t}\n\tpublic K getKey() {\n\t\treturn key;\n\t}\n\tpublic void setKey(K key) {\n\t\tthis.key = key;\n\t}\n\tpublic V getValue() {\n\t\treturn value;\n\t}\n\tpublic void setValue(V value) {\n\t\tthis.value = value;\n\t}\n\t\n}\n\npublic class Generics_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n//\t\tPoint<Integer> p = newdd Point<Integer>();\t//里面的var类型为Integer类型\n//\t\tp.setVar(30);  \t\t\t\t\t\t\t\t\t\t\t\t\t\t//设置数字，自动装箱\n//\t\tSystem.out.println(p.getVar()*2);\t\t\t\t\t//计算结果，按数字取出\n\t\t\n//\t\tPoint<String> p = new Point<String>();\t//里面的var类型为Integer类型\n//\t\tp.setVar(\"张三\");  \t\t\t\t\t\t\t\t\t\t\t\t\t\t//设置数字，自动装箱\n//\t\tSystem.out.println(p.getVar().length());\t\t\t\t\t//计算结果，按数字取出\n\t\t\n//\t\tPoint<String> p = new Point<String>(\"张三\");\n//\t\tSystem.out.println(\"内容：\"+p.getVar());\n\t\t\n\t\tNotepad<String,Integer> t = new Notepad<String,Integer>(\"张三\",18);\n\t\tSystem.out.println(t.getKey());\n\t\tSystem.out.println(t.getValue());\n\t\t\n\t}\n}\n\n```\n\n&nbsp;\n\n**通配符**\n\n```\nclass Point<T>{\t\t\t\t\t\t\t\t//此处T可以是任意的标识符号，T是type的简称\n\tprivate T var;\t\t\t\t\t\t\t\t//此变量的类型由外部决定\n\t\n\tpublic Point(T var) {\t\t\t\t\t//构造方法\n\t\tsuper();\n\t\tthis.var = var;\n\t}\n\t\n\tpublic String toString(){\t\t\t//覆写Object类中的toString()方法\n\t\treturn this.var.toString();\n\t}\n\n\tpublic T getVar() {\t\t\t\t\t//返回值的类型由外部决定\n\t\treturn var;\n\t}\n\n\tpublic void setVar(T var) {\t//设置的类型由外部指定\n\t\tthis.var = var;\n\t}\n\t\n\n}\n\nclass Notepad<K,V>{\t\t\t\t\t\t\t\t//此处T可以是任意的标识符号，T是type的简称\n\tprivate K key;\t\t\t\t\t\t\t\t\t\t\t//此变量的类型由外部决定\n\tprivate V value;\t\t\t\t\t\t\t\t\t\t//此变量的类型由外部决定\n\t\n\tpublic Notepad(K key, V value) {\t\t//构造方法\n\t\tsuper();\n\t\tthis.key = key;\n\t\tthis.value = value;\n\t}\n\tpublic K getKey() {\n\t\treturn key;\n\t}\n\tpublic void setKey(K key) {\n\t\tthis.key = key;\n\t}\n\tpublic V getValue() {\n\t\treturn value;\n\t}\n\tpublic void setValue(V value) {\n\t\tthis.value = value;\n\t}\n\t\n}\n\npublic class Generics_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\n\t\tPoint<String> p = new Point<String>(\"张三\");\n\t\tfun(p);\n\t}\n\t\n\tpublic static void fun(Point<?> point){\t\t\t//使用泛型接收Point的对象\n\t\tSystem.out.println(\"内容：\" + point);\n\t}\n\t\n}\n\n```\n\n&nbsp;\n\n**受限泛型**\n\n在引用传递中，在泛型操作中也可以设置一个泛型对象的范围上限和范围下限。\n\n**范围上限**使用extends关键字声明，标识参数化的类型可能是所指定的类型或者是此类型的子类。\n\n**范围下限**使用super关键字声明，标识参数化的类型可能是所指定的类型，或者是此类型的父类型，或是Object类。\n\n&nbsp;\n\n**范围上限**\n\n```\nimport java.awt.datatransfer.FlavorTable;\n\nclass info<T extends Number>{\t//在声明的地方指定泛型的上限范围\n//class info<T>{\t\t\t\t\t\t\t\t//此处T可以是任意的标识符号，T是type的简称\n\tprivate T var;\t\t\t\t\t\t\t\t//此变量的类型由外部决定\n\t\n\tpublic info(T var) {\t\t\t\t\t//构造方法\n\t\tsuper();\n\t\tthis.var = var;\n\t}\n\t\n\tpublic String toString(){\t\t\t//覆写Object类中的toString()方法\n\t\treturn this.var.toString();\n\t}\n\n\tpublic T getVar() {\t\t\t\t\t//返回值的类型由外部决定\n\t\treturn var;\n\t}\n\n\tpublic void setVar(T var) {\t//设置的类型由外部指定\n\t\tthis.var = var;\n\t}\n}\n\npublic class Generics_extends_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tinfo<Float> i1 = new info<Float>(10.1f);\t\t\t//声明并实例化Float类型的泛型对象\n\t\tinfo<Integer> i2 = new info<Integer>(10);\t\t//声明并实例化Integer类型的泛型对象\n\t\tfun(i1);\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//是数字可以传递\n\t\tfun(i2);\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//是数字可以传递\n\t}\n\t\n\tpublic static void fun(info<?> temp){\t\t\t//使用泛型接收info的对象\n//\tpublic static void fun(info<? extends Number> temp){\t\t\t//使用泛型接收info的对象\n\t\tSystem.out.println(\"temp:\" + temp);\n\t}\n}\n\n```\n\n&nbsp;\n\n**范围下限**\n\n```\nimport java.awt.datatransfer.FlavorTable;\n\n//class info<T extends Number>{\t//在声明的地方指定泛型的上限范围\nclass info<T>{\t\t\t\t\t\t\t\t//此处T可以是任意的标识符号，T是type的简称\n\tprivate T var;\t\t\t\t\t\t\t\t//此变量的类型由外部决定\n\t\n\tpublic info(T var) {\t\t\t\t\t//构造方法\n\t\tsuper();\n\t\tthis.var = var;\n\t}\n\t\n\tpublic String toString(){\t\t\t//覆写Object类中的toString()方法\n\t\treturn this.var.toString();\n\t}\n\n\tpublic T getVar() {\t\t\t\t\t//返回值的类型由外部决定\n\t\treturn var;\n\t}\n\n\tpublic void setVar(T var) {\t//设置的类型由外部指定\n\t\tthis.var = var;\n\t}\n}\n\npublic class Generics_extends_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n//\t\tinfo<Float> i1 = new info<Float>(10.1f);\t\t\t//声明并实例化Float类型的泛型对象\n//\t\tinfo<Integer> i2 = new info<Integer>(10);\t\t//声明并实例化Integer类型的泛型对象\n\t\t\n\t\tinfo<Object> i1 = new info<Object>(new Object());\t//声明并实例化Object类型的泛型对象\n\t\tinfo<String> i2 = new info<String>(\"张三\");\t\t\t\t\t//声明并实例化String类型的泛型对象\n\t\tfun(i1);\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//是数字可以传递\n\t\tfun(i2);\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//是数字可以传递\n\t}\n\t\n//\tpublic static void fun(info<?> temp){\t\t\t//使用泛型接收info的对象\n//\tpublic static void fun(info<? extends Number> temp){\t\t\t//使用泛型接收info的对象\n\tpublic static void fun(info<? super String> temp){\t\t\t//使用泛型接收info的对象\n\t\tSystem.out.println(\"temp:\" + temp);\n\t}\n}\n\n```\n\n&nbsp;\n\n**泛型接口**\n\n定义泛型接口\n\n<img src=\"/images/517519-20160309155804288-131913529.png\" alt=\"\" />\n\n定义子类方式<1>&mdash;&mdash;**在子类的定义上声明泛型类型**\n\n```\ninterface Info_1<T>{\n\tpublic T getVar();\n}\n\nclass InfoImp<T> implements Info_1<T>{\n\t\n\tprivate T var;\t\t\t\t\t\t\t//定义属性\n\t\n\tpublic InfoImp(T var) {\t\t//构造方法\n\t\tsuper();\n\t\tthis.var = var;\n\t}\n\n\tpublic void setVar(T var) {\n\t\tthis.var = var;\n\t}\n\n\t@Override\n\tpublic T getVar() {\n\t\t// TODO 自动生成的方法存根\n\t\treturn this.var;\n\t}\n\t\n}\n\npublic class Generics_Interface_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tInfoImp<String> p = new InfoImp<String>(\"张三\");\n\t\tSystem.out.println(\"内容：\"+p.getVar());\n\t}\n\n}\n\n```\n\n定义子类方式<2>&mdash;&mdash;**直接在接口中指定具体类型**\n\n```\ninterface Info_1<T>{\n\tpublic T getVar();\n}\n\nclass InfoImp implements Info_1<String>{//定义泛型接口的子类，指定类型为String\n//class InfoImp<T> implements Info_1<T>{\t//定义泛型接口的子类\n\t\n//\tprivate T var;\t\t\t\t\t\t\t//定义属性\n\tprivate String var;\t\t\t\t\t\t\t//定义属性\n\t\n//\tpublic InfoImp(T var) {\t\t//构造方法\n\tpublic InfoImp(String var) {\t\t//构造方法\n\t\tsuper();\n\t\tthis.var = var;\n\t}\n\n//\tpublic void setVar(T var) {\n\tpublic void setVar(String var) {\n\t\tthis.var = var;\n\t}\n\n\t@Override\n//\tpublic T getVar() {\n\tpublic String getVar() {\n\t\t// TODO 自动生成的方法存根\n\t\treturn this.var;\n\t}\n\t\n}\n\npublic class Generics_Interface_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tInfoImp p = new InfoImp(\"张三\");\n\t\tSystem.out.println(\"内容：\"+p.getVar());\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**定义泛型方法**\n\n```\nclass Demo_Generics{\n\tpublic <T> T fun(T t){\t\t//可以接收任意类型的数据\n\t\treturn t;\n\t}\n}\n\ninterface Info_1<T>{\n\tpublic T getVar();\n}\n\nclass InfoImp implements Info_1<String>{//定义泛型接口的子类，指定类型为String\n//class InfoImp<T> implements Info_1<T>{\t//定义泛型接口的子类\n\t\n//\tprivate T var;\t\t\t\t\t\t\t//定义属性\n\tprivate String var;\t\t\t\t\t\t\t//定义属性\n\t\n//\tpublic InfoImp(T var) {\t\t//构造方法\n\tpublic InfoImp(String var) {\t\t//构造方法\n\t\tsuper();\n\t\tthis.var = var;\n\t}\n\n//\tpublic void setVar(T var) {\n\tpublic void setVar(String var) {\n\t\tthis.var = var;\n\t}\n\n\t@Override\n//\tpublic T getVar() {\n\tpublic String getVar() {\n\t\t// TODO 自动生成的方法存根\n\t\treturn this.var;\n\t}\n\t\n}\n\npublic class Generics_Interface_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n//\t\tInfoImp p = new InfoImp(\"张三\");\n//\t\tSystem.out.println(\"内容：\"+p.getVar());\n\t\t\n\t\tDemo_Generics d = new Demo_Generics();\n\t\tString str = d.fun(\"张三\");\t\t//传递字符串\n\t\tint i = d.fun(30);\t\t\t\t\t\t\t//传递数字，自动装箱\n\t\tSystem.out.println(str);\n\t\tSystem.out.println(i);\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**通过泛型方法返回泛型类实例**\n\n```\nclass info<T extends Number>{\t//在声明的地方指定泛型的上限范围\n//class info<T>{\t\t\t\t\t\t\t\t//此处T可以是任意的标识符号，T是type的简称\n\tprivate T var;\t\t\t\t\t\t\t\t//此变量的类型由外部决定\n\t\n//\tpublic info(T var) {\t\t\t\t\t//构造方法\n//\t\tsuper();\n//\t\tthis.var = var;\n//\t}\n\t\n\tpublic String toString(){\t\t\t//覆写Object类中的toString()方法\n\t\treturn this.var.toString();\n\t}\n\n\tpublic T getVar() {\t\t\t\t\t//返回值的类型由外部决定\n\t\treturn var;\n\t}\n\n\tpublic void setVar(T var) {\t//设置的类型由外部指定\n\t\tthis.var = var;\n\t}\n}\n\npublic class Generics_extends_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\t\t\n\t\tinfo<Integer> i = fun(30);\t\t\t\t\t\t//传递整数到fun()方法\n\t\tSystem.out.println(i.getVar());\n\n\t}\n\t\t\n\tpublic static <T extends Number> info<T> fun (T param){\t//fun返回的参数类型是info<T>\n\t\tinfo<T> temp = new info<T>();\t\t//根据传入的数据类型实例化info对象\n\t\ttemp.setVar(param);\t\t\t\t\t\t\t//将传递的内容设置到info类中的var属性之中\n\t\treturn temp;\t\t\t\t\t\t\t\t\t\t//返回实例化对象\n\t\t\n\t}\n\n}\n\n```\n\n&nbsp;\n\n```\n//class info<T extends Number>{\t//在声明的地方指定泛型的上限范围\nclass info<T>{\t\t\t\t\t\t\t\t//此处T可以是任意的标识符号，T是type的简称\n\tprivate T var;\t\t\t\t\t\t\t\t//此变量的类型由外部决定\n\t\n//\tpublic info(T var) {\t\t\t\t\t//构造方法\n//\t\tsuper();\n//\t\tthis.var = var;\n//\t}\n\t\n\tpublic String toString(){\t\t\t//覆写Object类中的toString()方法\n\t\treturn this.var.toString();\n\t}\n\n\tpublic T getVar() {\t\t\t\t\t//返回值的类型由外部决定\n\t\treturn var;\n\t}\n\n\tpublic void setVar(T var) {\t//设置的类型由外部指定\n\t\tthis.var = var;\n\t}\n}\n\npublic class Generics_extends_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\t\n\t\tinfo<String> i1 = new info<String>();\t\t//设置String为泛型类型\n\t\tinfo<String> i2 = new info<String>();\t\t//设置String为泛型类型\n\t\ti1.setVar(\"HELLO\");\t\t\t\t\t\t\t\t\t//设置内容\n\t\ti2.setVar(\"张三\");\t\t\t\t\t\t\t\t\t\t//设置内容\n\t\tadd(i1,i2);\n\t}\n\t\t\n\tpublic static <T> void add(info<T> i1,info<T> i2){\n\t\tSystem.out.println(i1.getVar()+\"\"+i2.getVar());\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**泛型数组**\n\n程序从fun1()方法返回一个泛型数组，在fun1()方法接收参数时使用了可变参数传递方式，然后将fun1()返回的泛型数组内容交给fun2()方法进行输出。\n\n```\npublic class Generics_array_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tInteger i[] = fun1(1,2,3,4,5);\t\t\t\t//返回泛型数组\n\t\tfun2(i);\t\t\t\t\t\t\t\t\t\t\t\t//输出数组内容\n\t}\n\t\n\tpublic static <T> T[] fun1(T...arg){\t//接收可变参数，返回泛型数组\n\t\treturn arg;\t\t\t\t\t\t\t\t\t\t\t//返回泛型数组\n\t}\n\n\tpublic static <T> void fun2(T param[]){\n\t\tSystem.out.println(\"接收泛型数组：\");\t//接收泛型数组\n\t\tfor(T t:param){\n\t\t\tSystem.out.println(t+\"、\");\n\t\t}\n\t\tSystem.out.println();\n\t}\n\t\n}\n\n```\n\n&nbsp;\n\n**泛型应用**\n\n一个人有联系方式、基本信息等，这些信息的类型可以通过泛型进行声明，然后传给Person\n\n```\ninterface Generics_array_interface{\t\t//定义一个标识接口，此接口没有定义任何方法\n\t\n}\n\nclass Contact implements Generics_array_interface{\t\t//实现Generics_array_interface接口\n\n\tprivate String address;\t\t\t\t//联系地址\n\tprivate String telephone;\t\t\t\t//联系方式\n\tprivate String zipcode;\t\t\t\t\t//邮政编码\n\t\n\tpublic Contact(String address, String telephone, String zipcode) {\t//构造方法\n\t\tsuper();\n\t\tthis.address = address;\n\t\tthis.telephone = telephone;\n\t\tthis.zipcode = zipcode;\n\t}\n\n\tpublic String getAddress() {\n\t\treturn address;\n\t}\n\n\tpublic void setAddress(String address) {\n\t\tthis.address = address;\n\t}\n\n\tpublic String getTelephone() {\n\t\treturn telephone;\n\t}\n\n\tpublic void setTelephone(String telephone) {\n\t\tthis.telephone = telephone;\n\t}\n\n\tpublic String getZipcode() {\n\t\treturn zipcode;\n\t}\n\n\tpublic void setZipcode(String zipcode) {\n\t\tthis.zipcode = zipcode;\n\t}\n\t\n\tpublic String toString(){\t\t\t//覆写Object类中的toString()方法\n\t\t\treturn \"联系方式：\"+\"\\n\"+\"联系电话：\"+this.telephone+\"联系地址：\"+this.address+\"邮政编码：\"+this.zipcode;\n\t}\n\t\n\t\n}\n\nclass Introduction implements Generics_array_interface{\n\tprivate String name;\n\tprivate String sex;\n\tprivate int age;\n\t\n\tpublic Introduction(String name, String sex, int age) {\t//构造方法\n\t\tsuper();\n\t\tthis.name = name;\n\t\tthis.sex = sex;\n\t\tthis.age = age;\n\t}\n\tpublic String getName() {\n\t\treturn name;\n\t}\n\tpublic void setName(String name) {\n\t\tthis.name = name;\n\t}\n\tpublic String getSex() {\n\t\treturn sex;\n\t}\n\tpublic void setSex(String sex) {\n\t\tthis.sex = sex;\n\t}\n\tpublic int getAge() {\n\t\treturn age;\n\t}\n\tpublic void setAge(int age) {\n\t\tthis.age = age;\n\t}\n\t\n\tpublic String toString(){\t\t\t//覆写Object类中的toString()方法\n\t\treturn \"基本信息：\"+\"\\n\"+\"姓名：\"+this.name+\"性别：\"+this.sex+\"年龄：\"+this.age;\n\t}\n\t\n}\n\nclass Person_ <T extends Generics_array_interface>{\n\tprivate T info;\n\n\tpublic Person_(T info) {\t//构造方法\n\t\tsuper();\n\t\tthis.info = info;\n\t}\n\n\tpublic T getInfo() {\n\t\treturn info;\n\t}\n\n\tpublic void setInfo(T info) {\n\t\tthis.info = info;\n\t}\n\n\t@Override\n\tpublic String toString() {\t\t\t//覆写Object类中的toString()方法\n\t\treturn this.info.toString();\n\t}\n\t\n}\n\npublic class Generics_array_demo {\n\n\tpublic static void main(String[] args) {\n//\t\tInteger i[] = fun1(1,2,3,4,5);\t\t\t\t//返回泛型数组\n//\t\tfun2(i);\t\t\t\t\t\t\t\t\t\t\t\t//输出数组内容\n\t\t\n\t\tPerson_ <Contact> per = null;\t//声明Person_对象，同时指定Contact类型\n\t\tper = new Person_<Contact>(new Contact(\"北京市\",\"010010101010\",\"100876\"));//实例化Person_对象，同时设置信息\n\t\tSystem.out.println(per);\n\t}\n\t\n//\tpublic static <T> T[] fun1(T...arg){\t//接收可变参数，返回泛型数组\n//\t\treturn arg;\t\t\t\t\t\t\t\t\t\t\t//返回泛型数组\n//\t}\n//\n//\tpublic static <T> void fun2(T param[]){\n//\t\tSystem.out.println(\"接收泛型数组：\");\t//接收泛型数组\n//\t\tfor(T t:param){\n//\t\t\tSystem.out.println(t+\"、\");\n//\t\t}\n//\t\tSystem.out.println();\n//\t}\n\t\n}\n\n```\n\n&nbsp;\n","tags":["Java"]},{"title":"Java同步synchronized与死锁","url":"/Java同步synchronized与死锁.html","content":"多个线程要**操作同一资源**时就有可能出现**资源的同步问题**。\n\n**同步**就是指多个操作在同一个时间段内只能有一个线程进行，其他线程要等待此线程完成之后才可以继续执行。\n\n解决资源共享的同步操作，可以使用**同步代码块**和**同步方法**两种方式完成。\n\n<!--more-->\n&nbsp;\n\n<1>**同步代码块**\n\n<img src=\"/images/517519-20160308115555022-1178560527.png\" alt=\"\" />\n\n所谓代码块就是指使用&ldquo;{}\"括起来的一段代码，根据其位置和声明的不同，可以分为普通代码块、构造块、静态块3种，如果在代码块上加上**synchronized关键字**，则此代码块就称为同步代码块。\n\n```\npackage java_thread;\n\nclass MyThread_2 implements Runnable{\n\tprivate int ticket = 5;\t\n\t\n\t@Override\n\tpublic void run() {\t\t\t\t\t\t\t\t//覆写Thread类中的run()方法\n\t\t// TODO 自动生成的方法存根\n\t\tfor (int i=0;i<10;i++){\n\t\t\tsynchronized (this) {\t\t\t\t\t//设置需要同步的操作\n\t\t\t\tif(ticket>0){\n\t\t\t\t\ttry{\n\t\t\t\t\t\tThread.sleep(300);\n\t\t\t\t\t}catch(InterruptedException e){\n\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t}\n\t\t\t\t\tSystem.out.println(\"卖票：ticket=\"+ticket--);\n\t\t\t\t}\n\t\t\t}\n//\t\t\tthis.sale();\t\t\t\t\t\t\t\t\t\t\t\t//调用同步方法\n\t\t}\n\t}\n\t\n}\n\npublic class Runnable_demo2 {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tMyThread_2 mt = new MyThread_2();\t//实例化Runnable子类对象\n\t\tThread t1 = new Thread(mt);\t\t\t//实例化Thread类对象\n\t\tThread t2 = new Thread(mt);\t\t\t//实例化Thread类对象\n\t\tThread t3 = new Thread(mt);\t\t\t//实例化Thread类对象\n\t\tt1.start();\t\t\t\t//启动线程\n\t\tt2.start();\t\t\t\t//启动线程\n\t\tt3.start();\t\t\t\t//启动线程\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20170107113230284-1521636617.png\" alt=\"\" />\n\n```\npackage java_thread;\n\nclass Output{\n\tpublic void output(String name){\n\t\tint len = name.length();\n\t\tsynchronized(this){\t\t//不能使用name，因为&ldquo;输出1&rdquo;和\"输出2\"两个字符串不是同一个对象\n\t\t\tfor(int i=0;i<len;i++){\n\t\t\t\tSystem.out.print(name.charAt(i));\n\t\t\t}\n\t\t\tSystem.out.println();\n\t\t}\n\t}\n}\n\n\npublic class Huchi {\n\n\tprivate void init(){\n\t\tfinal Output outputer = new Output();\n\t\t//线程1\n\t\tnew Thread(new Runnable(){\n\t\t\t@Override\n\t\t\tpublic void run() {\t\t\t\t\t\t\t\t//覆写Thread类中的run()方法\n\t\t\t\twhile(true){\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tThread.sleep(10);\n\t\t\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\t\t\t// TODO 自动生成的 catch 块\n\t\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t\t}\n\t\t\t\t\t\t//new Output().output(\"输出1\");\t\t//也不能new对象，new对象的话，this就不代表同一个对象了\n\t\t\t\t\t\toutputer.output(\"输出1\");\n\t\t\t\t}\n\t\t\t}\n\t\t}).start();\n\t\t\n\t\t//线程2\n\t\tnew Thread(new Runnable(){\n\t\t\t@Override\n\t\t\tpublic void run() {\t\t\t\t\t\t\t\t//覆写Thread类中的run()方法\n\t\t\t\twhile(true){\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tThread.sleep(10);\n\t\t\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\t\t\t// TODO 自动生成的 catch 块\n\t\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t\t}\n\t\t\t\t\t\t//new Output().output(\"输出1\");\t\t//也不能new对象，new对象的话，this就不代表同一个对象了\n\t\t\t\t\t\toutputer.output(\"输出2\");\n\t\t\t\t}\n\t\t\t}\n\t\t}).start();\n\t\t\n\t}\n\t\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tnew Huchi().init();\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20170107192256331-1452409341.png\" alt=\"\" />\n\n&nbsp;\n\n**<2>同步方法**\n\n也可以使用**synchronized关键字**将一个方法声明成同步方法\n\n```\nclass MyThread_2 implements Runnable{\n\tprivate int ticket = 5;\t\n\t\n\t@Override\n\tpublic void run() {\t\t\t\t\t\t\t\t//覆写Thread类中的run()方法\n\t\t// TODO 自动生成的方法存根\n\t\tfor (int i=0;i<10;i++){\n\t\t\tthis.sale();\t\t\t\t\t\t\t\t\t\t\t\t//调用同步方法\n\t\t}\n\t}\n\t\n\tpublic synchronized void sale(){\t\t\t//声明同步方法\n\t\tif(ticket>0){\n\t\t\ttry{\n\t\t\t\tThread.sleep(300);\n\t\t\t}catch(InterruptedException e){\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t\t\tSystem.out.println(\"卖票：ticket=\"+ticket--);\n\t\t}\n\t}\n\t\n}\n\npublic class Runnable_demo2 {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tMyThread_2 mt = new MyThread_2();\t//实例化Runnable子类对象\n\t\tThread t1 = new Thread(mt);\t\t\t//实例化Thread类对象\n\t\tThread t2 = new Thread(mt);\t\t\t//实例化Thread类对象\n\t\tThread t3 = new Thread(mt);\t\t\t//实例化Thread类对象\n\t\tt1.start();\t\t\t\t//启动线程\n\t\tt2.start();\t\t\t\t//启动线程\n\t\tt3.start();\t\t\t\t//启动线程\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**死锁**就是指两个线程都在等待彼此先完成，造成了程序的停滞,一般程序的死锁都是在程序运行时出现的。\n\n多个线程共享同一资源时需要进行同步，以保证资源操作的完整性，但是过多的同步就有可能产生死锁。\n\n&nbsp;\n\n生产者不断生产，消费者不断取走生产者生产的产品\n\n```\nclass Info{\n\tprivate String name = \"张三\";\n\tprivate String content = \"学生\";\n\tprivate boolean flag = false;\n\t\n\tpublic synchronized void set(String name,String content){\t//设置信息名称及内容\n\t\tif(!flag){\t\t\t\t\t\t\t\t\t\t\t\t//标志位为false，不可以生产，在这里等待取走\n\t\t\ttry{\n\t\t\t\tsuper.wait();\t\t\t\t\t\t//等待消费者取走\n\t\t\t}catch(InterruptedException e){\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t\t}\t\n\t\tthis.setName(name);\t\t\t\t\t\t\t\t//设置信息名称\t\t\t\t\t\t\t\n\t\ttry{\n\t\t\tThread.sleep(300);\t\t\t\t\t\t\t//加入延迟\n\t\t}catch(InterruptedException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\tthis.setContent(content);\t\t\t\t\t\t//设置信息内容\n\t\tflag = false;\t\t\t\t\t\t\t\t\t//标志位为true，表示可以取走\n\t\tsuper.notify();\t\t\t\t\t\t\t\t\t//唤醒等待线程\n\t}\n\t\n\tpublic synchronized void get(){\t\t\t//取得信息内容\n\t\tif(flag){\t\t\t\t\t\t\t//标志位为true，不可以取走\n\t\t\ttry{\n\t\t\t\tsuper.wait();\t\t\t\t//等待生产者生产\n\t\t\t}catch(InterruptedException e){\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t\t}\t\t\n\t\ttry {\n\t\t\tThread.sleep(300);\t\t\t\t\t\t\t//加入延迟\n\t\t} catch (InterruptedException e) {\n\t\t\te.printStackTrace();\n\t\t}\n\t\tSystem.out.println(this.getName()+\"-->\"+this.getContent());\t//输出信息\n\t\tflag = true;\t\t\t\t\t\t\t\t\t//修改标志位为true，表示可以生产\n\t\tsuper.notify();\t\t\t\t\t\t\t\t\t//唤醒等待线程\n\t}\n\n\tpublic String getName() {\n\t\treturn name;\n\t}\n\n\tpublic void setName(String name) {\n\t\tthis.name = name;\n\t}\n\n\tpublic String getContent() {\n\t\treturn content;\n\t}\n\n\tpublic void setContent(String content) {\n\t\tthis.content = content;\n\t}\n\t\n}\n\nclass Producer implements Runnable{\t\t//定义生产者线程\n\n\tprivate Info info = null;\t\t//保存Info引用\n\t\n\tpublic Producer(Info info) {\t//构造函数\n\t\tsuper();\n\t\tthis.info = info;\n\t}\n\n\t@Override\n\tpublic void run() {\n\t\t// TODO 自动生成的方法存根\n\t\tboolean flag = false;\n\t\tfor(int i=0;i<50;i++){\t\t//50次反复修改name和content的值\n\t\t\tif(flag){\n\t\t\t\tthis.info.set(\"张三\", \"学生\");\n\t\t\t\tflag = false;\n\t\t\t}else{\n\t\t\t\tthis.info.set(\"李四\", \"老师\");\n\t\t\t\tflag = true;\n\t\t\t}\n\t\t}\n\t}\n\t\n}\n\nclass Consumer implements Runnable{\t\t//定义生产者线程\n\n\tprivate Info info = null;\t\t\t\t\t\t\t\t//保存Info引用\n\t\n\tpublic Consumer(Info info) {\t\t\t\t\t\t//构造函数\n\t\tsuper();\n\t\tthis.info = info;\n\t}\n\n\t@Override\n\tpublic void run() {\n\t\t// TODO 自动生成的方法存根\n\t\tfor(int i=0;i<50;i++){\t\t\t\t//50次反复修改name和content的值\n\t\t\ttry {\n\t\t\t\tThread.sleep(100);\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t\t\tthis.info.get();\n\t\t}\n\t}\n\t\n}\n\npublic class ThreadInfo_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tInfo i = new Info();\n\t\tProducer pro = new Producer(i);\n\t\tConsumer con = new Consumer(i);\n\t\tnew Thread(pro).start();\n\t\tnew Thread(con).start();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**停止线程运行**\n\n在多线程的开发中可以通过设置标志位的方式停止一个线程的运行\n\n```\nclass MYThread implements Runnable{\n\t\n\tprivate boolean flag = true;\t\t\t//定义标志位属性\n\t\n\t@Override\n\tpublic void run() {\n\t\t// TODO 自动生成的方法存根\n\t\tint i = 0;\n\t\twhile(this.flag){\t\t\t\t\t\t\t\t//循环输出\n\t\t\twhile(true){\n\t\t\t\tSystem.out.println(Thread.currentThread().getName()+(i++));\t//输出当前线程名称\n\t\t\t}\n\t\t}\n\t}\n\t\n\tpublic void stop(){\t\t\t\t\t\t\t//编写停止方法\n\t\tthis.flag = false;\t\t\t\t\t\t\t//修改标志位\n\t}\n\t\n}\n\npublic class ThreadStop_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tMYThread my = new MYThread();\n\t\tThread t = new Thread(my,\"线程\");\n\t\tt.start();\n\t\tmy.stop();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**互斥性**\n\n**output1和output2两段代码互斥，检查的都是outputer这个对象**\n\n&nbsp;\n\n```\npackage java_thread;\n\nclass Output{\n\tpublic void output(String name){\n\t\tint len = name.length();\n\t\tsynchronized(this){\t\t//不能使用name，因为&ldquo;输出1&rdquo;和\"输出2\"两个字符串不是同一个对象\n\t\t\tfor(int i=0;i<len;i++){\n\t\t\t\tSystem.out.print(name.charAt(i));\n\t\t\t}\n\t\t\tSystem.out.println();\n\t\t}\n\t}\n\t\n\tpublic synchronized void output2(String name){\t//output1和output2两段代码互斥，检查的都是outputer这个对象\n\t\tint len = name.length();\n\t\tfor(int i=0;i<len;i++){\n\t\t\tSystem.out.print(name.charAt(i));\n\t\t}\n\t\tSystem.out.println();\n\t}\n}\n\n\npublic class Huchi {\n\n\tprivate void init(){\n\t\tfinal Output outputer = new Output();\n\t\t//线程1\n\t\tnew Thread(new Runnable(){\n\t\t\t@Override\n\t\t\tpublic void run() {\t\t\t\t\t\t\t\t//覆写Thread类中的run()方法\n\t\t\t\twhile(true){\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tThread.sleep(10);\n\t\t\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\t\t\t// TODO 自动生成的 catch 块\n\t\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t\t}\n\t\t\t\t\t\t//new Output().output(\"输出1\");\t\t//也不能new对象，new对象的话，this就不代表同一个对象了\n\t\t\t\t\t\toutputer.output(\"输出1\");\n\t\t\t\t}\n\t\t\t}\n\t\t}).start();\n\t\t\n\t\t//线程2\n\t\tnew Thread(new Runnable(){\n\t\t\t@Override\n\t\t\tpublic void run() {\t\t\t\t\t\t\t\t//覆写Thread类中的run()方法\n\t\t\t\twhile(true){\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tThread.sleep(10);\n\t\t\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\t\t\t// TODO 自动生成的 catch 块\n\t\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t\t}\n\t\t\t\t\t\t//new Output().output(\"输出1\");\t\t//也不能new对象，new对象的话，this就不代表同一个对象了\n\t\t\t\t\t\toutputer.output2(\"输出2\");\n\t\t\t\t}\n\t\t\t}\n\t\t}).start();\n\t\t\n\t}\n\t\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tnew Huchi().init();\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20170107193337581-1817217802.png\" alt=\"\" />\n\n**static静态方法调用的对象是字节码对象，所以要把output1的this改成Output.class**\n\n```\npackage java_thread;\n\nclass Output{\n\tpublic void output1(String name){\n\t\tint len = name.length();\n\t\tsynchronized(this){\t\t//不能使用name，因为&ldquo;输出1&rdquo;和\"输出2\"两个字符串不是同一个对象\n\t\t\tfor(int i=0;i<len;i++){\n\t\t\t\tSystem.out.print(name.charAt(i));\n\t\t\t}\n\t\t\tSystem.out.println();\n\t\t}\n\t}\n\t\n\t//static静态方法调用的对象是字节码对象，所以要把output1的this改成Output.class\n\tpublic static synchronized void output3(String name){\t//output1和output3不同步，除非把output1的this改成Output.class\n\t\tint len = name.length();\n\t\tfor(int i=0;i<len;i++){\n\t\t\tSystem.out.print(name.charAt(i));\n\t\t}\n\t\tSystem.out.println();\n\t}\n\t\n}\n\n\npublic class Huchi {\n\n\tprivate void init(){\n\t\tfinal Output outputer = new Output();\n\t\t//线程1\n\t\tnew Thread(new Runnable(){\n\t\t\t@Override\n\t\t\tpublic void run() {\t\t\t\t\t\t\t\t//覆写Thread类中的run()方法\n\t\t\t\twhile(true){\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tThread.sleep(10);\n\t\t\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\t\t\t// TODO 自动生成的 catch 块\n\t\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t\t}\n\t\t\t\t\t\t//new Output().output(\"输出1\");\t\t//也不能new对象，new对象的话，this就不代表同一个对象了\n\t\t\t\t\t\toutputer.output1(\"输出1\");\n\t\t\t\t}\n\t\t\t}\n\t\t}).start();\n\t\t\n\t\t//线程2\n\t\tnew Thread(new Runnable(){\n\t\t\t@Override\n\t\t\tpublic void run() {\t\t\t\t\t\t\t\t//覆写Thread类中的run()方法\n\t\t\t\twhile(true){\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tThread.sleep(10);\n\t\t\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\t\t\t// TODO 自动生成的 catch 块\n\t\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t\t}\n\t\t\t\t\t\t//new Output().output(\"输出1\");\t\t//也不能new对象，new对象的话，this就不代表同一个对象了\n\t\t\t\t\t\toutputer.output3(\"输出2\");\n\t\t\t\t}\n\t\t\t}\n\t\t}).start();\n\t\t\n\t}\n\t\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tnew Huchi().init();\n\t}\n\n}\n\n```\n\n**&nbsp;this　　　　　　Output.class**\n\n<img src=\"/images/517519-20170107193952175-906910675.png\" alt=\"\" />　　　　<img src=\"/images/517519-20170107194032644-1756522005.png\" alt=\"\" />\n\n&nbsp;\n\n**面试题：子线程循环 10 次,接着主线程循环 100,接着又回到子线程循环 10 次,接着再回到主线程又循环 100,如此循环50 次,请写出程序**\n\n```\npackage java_thread;\n\nimport java.util.concurrent.atomic.AtomicInteger;\n//张孝祥java面试题28\n//子线程循环 10 次,接着主线程循环 100,接着又回到子线程循环 10 次,接着再回到主线程又循环 100,如此循环50 次,请写出程序\n\npublic class TraditionalThreadCommunication {\n\n\t/**\n\t * @param args\n\t */\n\tpublic static void main(String[] args) {\n\t\tfinal Business business = new Business();\n\t\tnew Thread(\n\t\t\t\tnew Runnable() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void run() {\n\t\t\t\t\t\tfor(int i=1;i<=50;i++){\n\t\t\t\t\t\t\tbusiness.sub(i);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t).start();\n\t\t\n\t\tfor(int i=1;i<=50;i++){\n\t\t\tbusiness.main(i);\n\t\t}\n\t\t\n\t}\n}\n\nclass Business {\n\tprivate boolean bShouldSub = true;\n\tpublic synchronized void sub(int i){\n\t\twhile(!bShouldSub){\n\t\t\ttry {\n\t\t\t\tthis.wait();\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\t// TODO Auto-generated catch block\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t\t}\n\t\tfor(int j=1;j<=10;j++){\n\t\t\tSystem.out.println(\"sub thread sequence of \" + j + \",loop of \" + i);\n\t\t}\n\t\tbShouldSub = false;\n\t\tthis.notify();\n\t}\n\t  \n\tpublic synchronized void main(int i){\n\t\twhile(bShouldSub){\n\t\t\ttry {\n\t\t\t\tthis.wait();\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\t// TODO Auto-generated catch block\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t\t}\n\t\tfor(int j=1;j<=100;j++){\n\t\t\tSystem.out.println(\"main thread sequence of \" + j + \",loop of \" + i);\n\t\t}\n\t\tbShouldSub = true;\n\t\tthis.notify();\n\t}\n}\n\n```\n\n&nbsp;\n","tags":["多线程"]},{"title":"airflow学习笔记——DAG","url":"/airflow学习笔记——DAG.html","content":"DAG参数含义\n\n```\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/dag/index.html\n\n```\n\n　　\n","tags":["Airbnb"]},{"title":"CDH学习笔记——角色组","url":"/CDH学习笔记——角色组.html","content":"1.对于机型不同的机器，可以通过角色组来进行统一归类管理\n\n比如对于HDFS组件，有的机型的磁盘为12块，有的机型的磁盘为16块，那么可以通过角色组将配置一致的机器分到一起\n\n在HDFS组件下，选择实例\n\n<img src=\"/images/517519-20210511160441198-655150911.png\" width=\"700\" height=\"149\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n再点击角色组，可以选择创建角色组\n\n<img src=\"/images/517519-20210511160644964-1436856810.png\" width=\"300\" height=\"412\" loading=\"lazy\" />\n\n其中DataNode Default Group有100台，每台有12块磁盘\n\n其中DataNode Group 1有50台，每台有16块磁盘\n\n此时在&nbsp;dfs.datanode.data.dir 参数可以分别对2个角色组进行配置，default组有12块盘，group 1有16块盘\n\n<img src=\"/images/517519-20210511164904921-1401188867.png\" width=\"800\" height=\"151\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["CDH"]},{"title":"Flink学习笔记——远程debug flink任务","url":"/Flink学习笔记——远程debug flink任务.html","tags":["Flink"]},{"title":"Java关键字","url":"/Java关键字.html","content":"## **private default protected public的访问控制权限**\n\n**<img src=\"/images/517519-20161029172518468-221254670.png\" alt=\"\" />**\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20160307212336007-2045838380.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160307212402725-349855913.png\" alt=\"\" />\n\nprotected范例<img src=\"/images/517519-20160307212509413-679971924.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160307212536429-210744914.png\" alt=\"\" />\n\n## transient关键字\n\n当使用Serializable接口实现序列化操作时，如果一个对象中的某一属性不希望被序列化，则可以使用transient关键字进行声明\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileNotFoundException;\nimport java.io.FileOutputStream;\nimport java.io.InputStream;\nimport java.io.ObjectInputStream;\nimport java.io.ObjectOutputStream;\nimport java.io.OutputStream;\nimport java.io.Serializable;\n\nclass Person_3 implements Serializable{\t\t//此类的对象可以被序列化\n\tprivate transient String name;\n\tprivate int age;\n\t\n\tpublic Person_3(String name, int age) {\n\t\tsuper();\n\t\tthis.name = name;\n\t\tthis.age = age;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn \"姓名：\" + name + \", 年龄：\" + age;\n\t}\n\t\n\t\n}\n\npublic class Serializable_demo {\n\n\tpublic static void main(String[] args) throws Exception {\n\t\t// TODO 自动生成的方法存根\n//\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.txt\");//路径\n//\t\tObjectOutputStream oos = null;\n//\t\tOutputStream out = new FileOutputStream(f);\t\t//文件输出流\n//\t\toos = new ObjectOutputStream(out);\t\t\t\t\t\t//为对象输出流实例化\n//\t\toos.writeObject(new Person_3(\"张三\", 30));\n//\t\toos.close();\n\t\t\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.txt\");//路径\n\t\tObjectInputStream ois = null;\n\t\tInputStream input = new FileInputStream(f);\t\t//文件输入流\n\t\tois = new ObjectInputStream(input);\t\t\t\t\t\t//为对象输入流实例化\n\t\tObject obj = ois.readObject();\t\t\t\t\t\t\t\t\t//读取对象\n\t\tois.close();\n\t\tSystem.out.println(obj);\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**<3>序列化一组对象**\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileNotFoundException;\nimport java.io.FileOutputStream;\nimport java.io.InputStream;\nimport java.io.ObjectInputStream;\nimport java.io.ObjectOutputStream;\nimport java.io.OutputStream;\nimport java.io.Serializable;\n\nclass Person_3 implements Serializable{\t\t//此类的对象可以被序列化\n//\tprivate transient String name;\n\tprivate String name;\n\tprivate int age;\n\t\n\tpublic Person_3(String name, int age) {\n\t\tsuper();\n\t\tthis.name = name;\n\t\tthis.age = age;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn \"姓名：\" + name + \", 年龄：\" + age;\n\t}\n\t\n\t\n}\n\npublic class Serializable_demo {\n\n\tpublic static void main(String[] args) throws Exception {\n\t\t// TODO 自动生成的方法存根\n//\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.txt\");//路径\n//\t\tObjectOutputStream oos = null;\n//\t\tOutputStream out = new FileOutputStream(f);\t\t//文件输出流\n//\t\toos = new ObjectOutputStream(out);\t\t\t\t\t\t//为对象输出流实例化\n//\t\toos.writeObject(new Person_3(\"张三\", 30));\n//\t\toos.close();\n\t\t\n//\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.txt\");//路径\n//\t\tObjectInputStream ois = null;\n//\t\tInputStream input = new FileInputStream(f);\t\t//文件输入流\n//\t\tois = new ObjectInputStream(input);\t\t\t\t\t\t//为对象输入流实例化\n//\t\tObject obj = ois.readObject();\t\t\t\t\t\t\t\t\t//读取对象\n//\t\tois.close();\n//\t\tSystem.out.println(obj);\n\t\t\n\t\tPerson_3 per[] = {new Person_3(\"张三\",30),new Person_3(\"李四\",31),new Person_3(\"王五\",32)};//定义对象数组\n\t\tser(per);\t\t\t\t\t//序列化对象数组\n\t\tObject o[] = dser();\n\t\tfor(int i=0;i<o.length;i++){\n\t\t\tPerson_3 p = (Person_3) o[i];\n\t\t\tSystem.out.println(p);\n\t\t}\n\t}\n\t\n\tpublic static void ser(Object obj[]) throws Exception{\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.txt\");//路径\n\t\tObjectOutputStream oos = null;\n\t\tOutputStream out = new FileOutputStream(f);\t\t//文件输出流\n\t\toos = new ObjectOutputStream(out);\t\t\t\t\t\t//为对象输出流实例化\n\t\toos.writeObject(obj);\n\t\toos.close();\n\t}\n\t\n\tpublic static Object[] dser() throws Exception{\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.txt\");//路径\n\t\tObjectInputStream ois = null;\n\t\tInputStream input = new FileInputStream(f);\t\t//文件输入流\n\t\tois = new ObjectInputStream(input);\t\t\t\t\t\t//为对象输入流实例化\n\t\tObject obj[] = (Object[])ois.readObject();\t\t\t\t//读取对象数组\n\t\tois.close();\n\t\treturn obj;\n\t}\n\n}\n\n```\n\n## volatile关键字 \n\n一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义：\n\n1. 保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。\n\n2. 禁止进行指令重排序。\n\n参考：[Java并发编程：volatile关键字解析](https://www.cnblogs.com/dolphin0520/p/3920373.html)\n\n## final关键字\n\nfinal在Java中表示的意思是最终，使用final关键字声明类属性、方法，注意：\n\n1、使用final声明的类不能有子类\n\n2、使用final声明的方法不能被子类所覆写\n\n3、使用final声明的变量即成为常量、常量不可以修改\n\n&nbsp;\n\n注意：final变量的命名规则\n\n　　在使用final声明变量的时候，**要求全部的字母大写**\n\n　　如果一个程序中的变量使用public static final声明，则此变量将称为**全局常量**\n\n## super关键字\n\n使用super关键字可以从子类中调用父类中的构造方法、普通方法和属性\n\n与this调用构造方法的要求一样，语句必须放在子类构造方法的首行\n\nthis和super都可以调用构造方法，但是两者不能同时出现，调用构造的时候都必须放在构造方法的首行\n\n```\nclass person{\n\tprivate String name;\n\tprivate int age;\n\t\n\tpublic person(String name,int age){\t//构造方法\n\t\tthis.name = name;\n\t\tthis.age = age;\n\t}\n\t\n\tpublic String getName() {\n\t\treturn name;\n\t}\n\tpublic void setName(String name) {\n\t\tthis.name = name;\n\t}\n\tpublic int getAge() {\n\t\treturn age;\n\t}\n\tpublic void setAge(int age) {\n\t\tthis.age = age;\n\t}\n\t\n//\tvoid print(){\t//定义一个默认访问权限的方法\n//\t\tSystem.out.println(\"Person---->void print()\");\n//\t}\n\t\n\tpublic String getInfo(){\n\t\treturn \"姓名\"+this.name+\"年龄\"+this.age;\n\t}\n}\n\nclass student extends person{\n\n\tprivate String school;\t\t//新定义的属性school\n\t\n\tpublic student(String name, int age,String school) {\n\t\tsuper(name, age);\t\t//指定调用父类中的构造方法\n\t\tthis.school = school;\t\t\n\t\t// TODO 自动生成的构造函数存根\n\t}\n\t\n\tpublic String getSchool() {\n\t\treturn school;\n\t}\n\n\tpublic void setSchool(String school) {\n\t\tthis.school = school;\n\t}\n\t\n//\tpublic void print(){\t//覆写父类中的方法，扩大了权限\n//\t\tsuper.print();\t\t\t//调用父类中的print()方法\n//\t\tSystem.out.println(\"student---->void print()\");\n//\t}\n\t\n\tpublic String getInfo(){\t\t\t\t\t\t\t\t\t\t\t//覆写父类中的方法\n\t\treturn super.getInfo()+\"学校\"+this.school;\t\t//扩充父类中的方法\n\t}\n}\n\n\npublic class extends_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tstudent person_1 = new student(\"李四\",18,\"清华大学\");\n//\t\tperson_1.setName(\"张三\");\n//\t\tperson_1.setAge(10);\n//\t\tperson_1.setSchool(\"涵三中\");\n//\t\tSystem.out.println(\"姓名:\"+person_1.getName()+\"\\n\"+\"年龄:\"+person_1.getAge()+\"\\n\"+\"学校:\"+person_1.getSchool());\n//\t\tnew student(\"张三\",11,\"三中\").print();\n\t\tSystem.out.println(person_1.getInfo());\t\t//打印信息，调用覆写过的方法\n\t\tperson person_2 = new person(\"张三\",18);\n\t\tSystem.out.println(person_2.getInfo());\n\t}\n\n}\n\n```\n\nthis和super的区别\n\n\n\n|区别|this|super\n| ---- | ---- | ---- \n|1.属性访问|访问本类中的属性，如果本类中没有此属性，则从父类中继续查找|访问父类中的属性\n|2.方法|访问本类中的方法，如果本类中没有此方法，则从父类中继续查找|直接访问父类中的方法\n|3.调用构造|调用本类构造，必须放在构造方法的首行|调用父类构造，必须放在子类构造方法的首行\n|4.特殊|表示当前对象|无此概念\n\n## static关键字\n\n### **1.static申明属性**\n\n如果有属性希望被所有对象共享，则必须将其申明为static属性。\n\n使用static声明属性，则此属性称为全局属性，有时候也称为静态属性。\n\n&nbsp;\n\n当一个类的属性申明为static的时候，由这个类产生的多个对象中属性，只需要**对其中一个对象的该属性进行修改**，即**可以修改所有对象的这个属性**。\n\n若只申明为public，没有static的时候，则修改申明的对象的属性只修改一个，申明为private的时候报错，因为该属性私有化，不能被方法所调用。\n\n在调用static申明的属性的时候，最好通过类名称来直接调用，因为通过对象来调用不知道该类产生了多少的对象，这样子不太好，所以又把static声明的属性称为类属性，调用的格式位Person_1.coountry=\"B city\";\n\n```\nclass Person_1{\n\tprivate String name;\n\tprivate int age;\n\tstatic String country = \"A city\";\n\t\n\tpublic Person_1(String n,int a){\n\t\tthis.name = n;\n\t\tthis.age = a;\n\t}\n\t\n\tpublic void info(){\n\t\tSystem.out.println(\"name:\"+this.name+\"\\t\"+\"age:\"+this.age+\"\\t\"+\"city:\"+this.country);\n\t}\n};\n\n\npublic class static_test {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tPerson_1 p1 = new Person_1(\"zhangsan\",30);\n\t\tPerson_1 p2 = new Person_1(\"wangwu\",40);\n\t\tp1.info();\n\t\tp2.info();\n\t\tp1.country = \"B city\";\n\t\tp1.info();\n\t\tp2.info();\n\t}\n\n}\n\n```\n\n输出\n\n```\nname:zhangsan\tage:30\tcity:A city\nname:wangwu\tage:40\tcity:A city\nname:zhangsan\tage:30\tcity:B city\nname:wangwu\tage:40\tcity:B city\n\n```\n\n### **2.static申明方法<br />**\n\n使用static申明的方法又称为类方法，Person_1.setCountry(\"B city\"); 同时修改多个对象的属性\n\n非static声明的方法可以去调用static声明的属性或方法\n\n但是static声明的方法是不能调用非static类型声明的属性或者方法的\n\n```\nclass Person_1{\n\tprivate String name;\n\tprivate int age;\n\tpublic static  String country = \"A city\";\n\t\n\tpublic static void setCountry(String c){\n\t\tcountry = c;\n\t}\n\t\n\tpublic static String getCountry(){\n\t\treturn country;\n\t}\n\t\n\tpublic Person_1(String n,int a){\n\t\tthis.name = n;\n\t\tthis.age = a;\n\t}\n\t\n\tpublic void info(){\n\t\tSystem.out.println(\"name:\"+this.name+\"\\t\"+\"age:\"+this.age+\"\\t\"+\"city:\"+this.country);\n\t}\n};\n\n\npublic class static_test {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tPerson_1 p1 = new Person_1(\"zhangsan\",30);\n\t\tPerson_1 p2 = new Person_1(\"wangwu\",40);\n\t\tp1.info();\n\t\tp2.info();\n\t\t//p1.country = \"B city\";\n\t\tPerson_1.setCountry(\"B city\");\n\t\tp1.info();\n\t\tp2.info();\n\t}\n\n}\n\n```\n\n可以通过static还统计实例化了多少个对象\n\n```\nclass demo{\n\tprivate static int count = 0;\n\tpublic demo(){\n\t\tcount++;\n\t\tSystem.out.println(\"No.\"+count);\n\t}\n}\n\n\npublic class static_count {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tnew demo();\n\t\tnew demo();\n\t\tnew demo();\n\t}\n\n}\n\n```\n\n&nbsp;给主方法的args传递参数，然后统计传递的参数的个数\n\n```\npublic class HelloWprdApp {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO Auto-generated method stub\n\t\tint num = 0,sum = 0;\n\t\tchar num1 = 97;\n\n\t\tif(args.length != 3){\n\t\t\tSystem.out.println(\"<3\");\n\t\t\tSystem.exit(1);\n\t\t}\n\t\t\n\t\tfor(int i=0;i<args.length;i++){\n\t\t\tSystem.out.println(\"name:\"+args[i]);\n\t\t}\n\t}\n}\n\n```\n\n&nbsp;\n\n## **Java其他关键字**\n\npublic：表示此方法可以被外部调用\n\nstatic：表示此方法可以由类名称直接调用\n\nvoid：主方法是程序的起点，所以不需要任何的返回值\n\nmain：系统规定好默认调用的方法名称，执行时默认找到main方法名称\n\nString arg[]：表示的是运行 时的参数。参数传递的形式为&ldquo;Java类名称 参数1 参数2...&rdquo;\n","tags":["Java"]},{"title":"Java线程操作方法","url":"/Java线程操作方法.html","content":"<img src=\"/images/517519-20160307231841069-1220301823.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n**取得和设置线程名称**\n\n```\n//=================================================\n// File Name       :\tThread_demo\n//------------------------------------------------------------------------------\n// Author          :\tCommon\n\n\n// 接口名：MyThread\n// 属性：\n// 方法：\nclass MyThread_1 implements Runnable{\t//实现Runnable接口\n\tprivate String name;\n\t\n//\tpublic MyThread_1(String name) {\t//构造方法\n//\t\tsuper();\n//\t\tthis.name = name;\n//\t}\n\t\n\t@Override\n\tpublic void run() {\t\t\t\t\t\t\t\t//覆写Thread类中的run()方法\n\t\t// TODO 自动生成的方法存根\n\t\tfor (int i=0;i<10;i++){\n//\t\t\tSystem.out.println(name+\"运行，i=\"+i);\n\t\t\tSystem.out.println(Thread.currentThread().getName()+\",i=\"+i);\t//取出当前线程的名称\n\t\t}\n\t}\n\t\n}\n\n\n\n//主类\n//Function        : \tThread_demo\npublic class Runnable_demo {\n\n\tpublic static void main(String[] args) {\n\t\t\n\t\tMyThread_1 mt1 = new MyThread_1();\t//实例化Runnable子类对象\n\t\tnew Thread(mt1).start();\t\t\t\t\t\t\t\t//系统自动设置线程名称\n\t\tnew Thread(mt1,\"线程A\").start();\t\t\t\t//手工自动设置线程名称\n\t\tnew Thread(mt1,\"线程B\").start();\t\t\t\t//手工自动设置线程名称\n\t\tnew Thread(mt1).start();\t\t\t\t\t\t\t\t//系统自动设置线程名称\n\t\tnew Thread(mt1).start();\t\t\t\t\t\t\t\t//系统自动设置线程名称\n\t}\n\n}\n\n```\n\n**手工设置线程名称　　　　系统自动设置线程名称**\n\n<img src=\"/images/517519-20170107140247206-83797176.png\" alt=\"\" />　　　　　　　　<img src=\"/images/517519-20170107140335534-1375081390.png\" alt=\"\" />\n\n&nbsp;\n\n**判断线程是否启动**\n\n使用**isAlive()方法**来判断线程是否已经启动而且仍然在启动\n\n```\n//=================================================\n// File Name       :\tThread_demo\n//------------------------------------------------------------------------------\n// Author          :\tCommon\n\n\n// 接口名：MyThread\n// 属性：\n// 方法：\nclass MyThread_1 implements Runnable{\t//实现Runnable接口\n\tprivate String name;\n\t\n\tpublic MyThread_1(String name) {\t//构造方法\n\t\tsuper();\n\t\tthis.name = name;\n\t}\n\t\n\t@Override\n\tpublic void run() {\t\t\t\t\t\t\t\t//覆写Thread类中的run()方法\n\t\t// TODO 自动生成的方法存根\n\t\tfor (int i=0;i<10;i++){\n//\t\t\tSystem.out.println(name+\"运行，i=\"+i);\n\t\t\tSystem.out.println(Thread.currentThread().getName()+\",i=\"+i);\t//取出当前线程的名称\n\t\t}\n\t}\n\t\n}\n\n\n\n//主类\n//Function        : \tThread_demo\npublic class Runnable_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tMyThread_1 mt1 = new MyThread_1(\"线程A \");\t//实例化Runnable子类对象\n\t\tMyThread_1 mt2 = new MyThread_1(\"线程B \");\t//实例化Runnable子类对象\n\t\tThread t1 = new Thread(mt1);\t\t\t\t\t\t\t//实例化Thread类对象\n\t\tThread t2 = new Thread(mt2);\t\t\t\t\t\t\t//实例化Thread类对象\n\t\tSystem.out.println(\"线程开始执行之前-->\"+t1.isAlive());\n\t\tt1.start();\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//启动线程\n\t\tSystem.out.println(\"线程开始执行之后-->\"+t1.isAlive());\n\t\tt2.start();\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//启动线程\n\n\t}\n\n}\n\n```\n\n&nbsp;主线程有可能比其他线程先执行完\n\n<img src=\"/images/517519-20170107140859519-2033832084.png\" alt=\"\" />\n\n&nbsp;\n\n**线程的强制运行**\n\n在线程操作中，可以使用**join()方法**让一个线程强制运行，线程强制运行期间，期间线程无法运行，必须等待此线程完成之后才可以继续执行。\n\n<img src=\"/images/517519-20160308100922460-124236205.png\" alt=\"\" />\n\n&nbsp;\n\n**线程的休眠**\n\n在程序中允许一个线程进行暂时的休眠，直接使用**Thread.sleep()方法**即可实现休眠\n\n程序在执行的时候，每次的输出都会间隔500ms，达到了延时操作的效果。\n\n**Thread.sleep()方法**要用try和catch语句包围\n\n```\n//=================================================\n// File Name       :\tThread_demo\n//------------------------------------------------------------------------------\n// Author          :\tCommon\n\n\n// 接口名：Mythread\n// 属性：\n// 方法：\nclass Mythread implements Runnable{\t//实现Runnable接口\n\t\n\t@Override\n\tpublic void run() {\t\t\t\t\t\t\t\t//覆写Thread类中的run()方法\n\t\t// TODO 自动生成的方法存根\n\t\tfor (int i=0;i<5;i++){\n\t\t\ttry{\n\t\t\t\tThread.sleep(500);\t\t\t\t\t\t//线程休眠\n\t\t\t}catch (Exception e){}\t\t\t\t//需要异常处理\n\t\t\t\tSystem.out.println(Thread.currentThread().getName()+\",i=\"+i);\t//取出当前线程的名称\n\t\t}\n\t}\n\t\n}\n\n//主类\n//Function        : \tThreadSleep_demo\npublic class ThreadSleep_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tMythread m = new Mythread();\n\t\tnew Thread(m,\"线程\").start();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**中断线程**\n\n当一个线程运行时，另外一个线程可以直接通过**interrupt()方法**中断其运行状态。\n\n一个线程启动之后进入了休眠状态，原来是要休眠10s之后再继续执行，但是主方法在线程启动之后的2s之后就将其中断，休眠一旦中断之后将执行catch中的代码。\n\n```\n//=================================================\n// File Name       :\tThread_demo\n//------------------------------------------------------------------------------\n// Author          :\tCommon\n\n\n// 接口名：Mythread_1\n// 属性：\n// 方法：\nclass Mythread_1 implements Runnable{\t//实现Runnable接口\n\t\n\t@Override\n\tpublic void run() {\t\t\t\t\t\t\t\t//覆写Thread类中的run()方法\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"进入run方法\");\n\t\ttry{\n\t\t\tThread.sleep(10000);\t\t\t\t\t\t//线程休眠\n\t\t\tSystem.out.println(\"休眠完成\");\n\t\t\t\t}catch (Exception e){\t\t\t\t//需要异常处理\n\t\t\tSystem.out.println(\"休眠被终止\");\n\t\t\treturn;\t\t\t\t\t\t\t\t\t\t\t\t//让程序返回被调用处\n\t\t}\n\t\tSystem.out.println(\"run方法结束\");\n\t}\n\t\n}\n\n//主类\n//Function        : \tThreadSleep_demo\npublic class ThreadInterrupt_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tMythread_1 m = new Mythread_1();\n\t\tThread t = new Thread(m,\"线程\");\n\t\tt.start();\n\t\ttry{\n\t\t\tThread.sleep(2000);\t\t\t//主线程2s之后再执行中断\n\t\t}catch(Exception e){}\n\t\tt.interrupt();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**后台线程**\n\n在Java程序中，只要前台有一个线程在运行，则整个Java进程都不会消失，所以此时可以设置一个后台线程，这样即使Java进程结束了，此后台线程依然会继续执行。要想实现这样的操作，直接使用**setDaemon()方法**即可。\n\n<img src=\"/images/517519-20160308104627257-1800492555.png\" alt=\"\" />\n\n&nbsp;\n\n**线程的优先级**\n\n在Java的线程中使用**setPriority()方法**可以设置一个线程的优先级，在Java的线程中一共有3种优先级。<img src=\"/images/517519-20160308110823616-435295338.png\" alt=\"\" />\n\n```\n//=================================================\n// File Name       :\tThread_demo\n//------------------------------------------------------------------------------\n// Author          :\tCommon\n\n\n// 接口名：MyThread\n// 属性：\n// 方法：\nclass MyThread_1 implements Runnable{\t//实现Runnable接口\n\tprivate String name;\n\t\n//\tpublic MyThread_1(String name) {\t//构造方法\n//\t\tsuper();\n//\t\tthis.name = name;\n//\t}\n\t\n\t@Override\n\tpublic void run() {\t\t\t\t\t\t\t\t//覆写Thread类中的run()方法\n\t\t// TODO 自动生成的方法存根\n\t\tfor (int i=0;i<10;i++){\n\t\t\t//System.out.println(name+\"运行，i=\"+i);\n\t\t\tSystem.out.println(Thread.currentThread().getName()+\",i=\"+i);\t//取出当前线程的名称\n\t\t}\n\t}\n\t\n}\n\n\n\n//主类\n//Function        : \tThread_demo\npublic class Runnable_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tMyThread_1 mt1 = new MyThread_1();\t//实例化Runnable子类对象\n\t\tMyThread_1 mt2 = new MyThread_1();\t//实例化Runnable子类对象\n\t\tMyThread_1 mt3 = new MyThread_1();\t//实例化Runnable子类对象\n\t\tThread t1 = new Thread(mt1,\"线程A\");\t\t\t\t\t\t\t//实例化Thread类对象\n\t\tThread t2 = new Thread(mt2,\"线程B\");\t\t\t\t\t\t\t//实例化Thread类对象\n\t\tThread t3 = new Thread(mt3,\"线程C\");\t\t\t\t\t\t\t//实例化Thread类对象\n//\t\tSystem.out.println(\"线程开始执行之前-->\"+t1.isAlive());\n\t\tt1.setPriority(Thread.MIN_PRIORITY);\n\t\tt2.setPriority(Thread.NORM_PRIORITY);\n\t\tt3.setPriority(Thread.MAX_PRIORITY);\n\t\tt1.start();\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//启动线程\n//\t\tSystem.out.println(\"线程开始执行之前-->\"+t1.isAlive());\n\t\tt2.start();\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//启动线程\n\t\tt3.start();\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//启动线程\n\t\t\n//\t\tMyThread_1 mt1 = new MyThread_1();\t//实例化Runnable子类对象\n//\t\tnew Thread(mt1).start();\t\t\t\t\t\t\t\t//系统自动设置线程名称\n//\t\tnew Thread(mt1,\"线程A\").start();\t\t\t\t//手工自动设置线程名称\n//\t\tnew Thread(mt1,\"线程B\").start();\t\t\t\t//手工自动设置线程名称\n//\t\tnew Thread(mt1).start();\t\t\t\t\t\t\t\t//系统自动设置线程名称\n//\t\tnew Thread(mt1).start();\t\t\t\t\t\t\t\t//系统自动设置线程名称\n\t}\n\n}\n\n```\n\n&nbsp;线程将根据优先级的大小来决定哪个线程会先运行，**但是并非线程的优先级越高就一定会先执行**，哪个线程先执行将由CPU的调度决定。\n\n主方法的优先级是NORM，通过Thread.currentThread().getPriority()来取得主方法的优先级，结果是5\n\n&nbsp;\n\n**线程的礼让**\n\n在线程的操作中，可以使用**yield()方法**将一个线程的操作暂时让给其他线程执行。本线程暂停，让其他进程先执行。\n\n```\n//=================================================\n// File Name       :\tThread_demo\n//------------------------------------------------------------------------------\n// Author          :\tCommon\n\n\n// 接口名：MyThread\n// 属性：\n// 方法：\nclass MyThread_1 implements Runnable{\t//实现Runnable接口\n\tprivate String name;\n\t\n//\tpublic MyThread_1(String name) {\t//构造方法\n//\t\tsuper();\n//\t\tthis.name = name;\n//\t}\n\t\n\t@Override\n\tpublic void run() {\t\t\t\t\t\t\t\t//覆写Thread类中的run()方法\n\t\t// TODO 自动生成的方法存根\n\t\tfor (int i=0;i<10;i++){\n\t\t\t//System.out.println(name+\"运行，i=\"+i);\n\t\t\tSystem.out.println(Thread.currentThread().getName()+\",i=\"+i);\t//取出当前线程的名称\n\t\t\tif(i==3){\n\t\t\t\tSystem.out.println(\"线程礼让：\");\n\t\t\t\tThread.currentThread().yield();\t//线程礼让\n\t\t\t}\n\t\t}\n\t}\n\t\n}\n\n\n\n//主类\n//Function        : \tThread_demo\npublic class Runnable_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tMyThread_1 mt1 = new MyThread_1();\t//实例化Runnable子类对象\n\t\tMyThread_1 mt2 = new MyThread_1();\t//实例化Runnable子类对象\n\t\tMyThread_1 mt3 = new MyThread_1();\t//实例化Runnable子类对象\n\t\tThread t1 = new Thread(mt1,\"线程A\");\t\t\t\t\t\t\t//实例化Thread类对象\n\t\tThread t2 = new Thread(mt2,\"线程B\");\t\t\t\t\t\t\t//实例化Thread类对象\n\t\tThread t3 = new Thread(mt3,\"线程C\");\t\t\t\t\t\t\t//实例化Thread类对象\n//\t\tSystem.out.println(\"线程开始执行之前-->\"+t1.isAlive());\n\t\tt1.setPriority(Thread.MIN_PRIORITY);\n\t\tt2.setPriority(Thread.NORM_PRIORITY);\n\t\tt3.setPriority(Thread.MAX_PRIORITY);\n\t\tt1.start();\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//启动线程\n//\t\tSystem.out.println(\"线程开始执行之前-->\"+t1.isAlive());\n\t\tt2.start();\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//启动线程\n\t\tt3.start();\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//启动线程\n\t\t\n//\t\tMyThread_1 mt1 = new MyThread_1();\t//实例化Runnable子类对象\n//\t\tnew Thread(mt1).start();\t\t\t\t\t\t\t\t//系统自动设置线程名称\n//\t\tnew Thread(mt1,\"线程A\").start();\t\t\t\t//手工自动设置线程名称\n//\t\tnew Thread(mt1,\"线程B\").start();\t\t\t\t//手工自动设置线程名称\n//\t\tnew Thread(mt1).start();\t\t\t\t\t\t\t\t//系统自动设置线程名称\n//\t\tnew Thread(mt1).start();\t\t\t\t\t\t\t\t//系统自动设置线程名称\n\t}\n\n}\n\n```\n\n**&nbsp;<img src=\"/images/517519-20170107141807253-1182441932.png\" alt=\"\" />　　<img src=\"/images/517519-20170107141812800-1092290694.png\" alt=\"\" />　　线程礼让也是不一定的**\n","tags":["多线程"]},{"title":"SpringBoot学习笔记——校验","url":"/SpringBoot学习笔记——校验.html","content":"JSR-303提供了一些注解，将其放到属性上，可以限制这些属性的值。\n\n参考：[Spring MVC学习笔记&mdash;&mdash;JSR303介绍及最佳实践](https://www.cnblogs.com/tonglin0325/p/5516289.html)\n\n校验放在DTO层上，不要和数据库交互的model层混用\n\n关于model，VO等的区别，参考：[Spring MVC学习笔记&mdash;&mdash;POJO和DispatcherServlet](https://www.cnblogs.com/tonglin0325/p/5495899.html)\n\n如何赋值，参考：[优雅的使用BeanUtils对List集合的操作](https://www.cnblogs.com/Johnson-lin/p/12123012.html)\n\nDTO和DO的转换，可以使用`BeanUtils`，参考：[设计之道－controller层的设计](https://www.jianshu.com/p/654f4589eb8e)\n\n也可以使用ModelMapper，参考：[Spring Boot DTO示例：实体到DTO的转换](https://www.javaguides.net/2021/02/spring-boot-dto-example-entity-to-dto.html?spref=tw)\n\n如果使用的springboot版本大于2.3.x，需要额外引用依赖\n\n```\n<dependency>\n    <groupId>org.hibernate</groupId>\n    <artifactId>hibernate-validator</artifactId>\n    <version>6.0.1.Final</version>\n</dependency>\n\n```\n\n参考：[使用SpringBoot进行优雅的数据验证](https://www.cnblogs.com/54chensongxia/p/14016179.html)\n\n定义dto层或者vo层，添加 @NotEmpty注解 和 @Size注解，并设置分组校验，即在Post请求或者Put请求的时候进行校验\n\n```\nimport com.example.demo.core.valid.Post;\nimport com.example.demo.core.valid.Put;\nimport com.example.demo.model.User;\nimport lombok.AllArgsConstructor;\nimport lombok.Data;\nimport lombok.NoArgsConstructor;\nimport org.springframework.beans.BeanUtils;\n\nimport javax.validation.constraints.NotEmpty;\nimport javax.validation.constraints.Size;\n\n@Data\n@AllArgsConstructor\n@NoArgsConstructor\npublic class UserDTO {\n\n    @NotEmpty(groups = Post.class, message = \"注册时username字段不能为空\")\n    @Size(groups = {Post.class, Put.class}, min = 3, max = 120)\n    private String username;\n\n    private String password;\n\n    public static User convert(UserDTO dto) {\n        User user = new User();\n        BeanUtils.copyProperties(dto, user);\n        return user;\n    }\n\n    public static UserDTO convertDTO(User user) {\n        UserDTO dto = new UserDTO();\n        BeanUtils.copyProperties(user, dto);\n        return dto;\n    }\n\n}\n\n```\n\n定义Post分组接口\n\n```\npackage com.example.demo.core.valid;\n\npublic interface Post {\n}\n\n```\n\n定义Put分组接口\n\n```\npackage com.example.demo.core.valid;\n\npublic interface Put {\n}\n\n```\n\n设置全局异常处理\n\n```\n    @ResponseStatus(HttpStatus.BAD_REQUEST)\n    @ExceptionHandler(MethodArgumentNotValidException.class)\n    public ControllerResponseT methodArgumentNotValidException(MethodArgumentNotValidException e) {\n        String message = ResultCode.METHOD_ARGUMENT_NOT_VALID.getMessage();\n        log.error(\"参数验证失败 => {}\", e.getMessage());\n        BindingResult bindingResult = e.getBindingResult();\n        List<ObjectError> allErrors = bindingResult.getAllErrors();\n        return new ControllerResponseT<>(ResultCode.METHOD_ARGUMENT_NOT_VALID.getCode(), message, allErrors);\n    }\n\n    @ResponseStatus(HttpStatus.INTERNAL_SERVER_ERROR)\n    @ExceptionHandler({SQLException.class, DataAccessException.class})\n    public ControllerResponseT databaseException(final Throwable e) {\n        String message = ResultCode.DATABASE_ERROR.getMessage();\n        log.error(\"数据库错误 => {}\", e.getMessage());\n        return ControllerResponseT.ofFail(ResultCode.DATABASE_ERROR.getCode(), message, e.getMessage());\n    }\n\n```\n\ncontroller层\n\n```\n    @ApiImplicitParams({\n            @ApiImplicitParam(paramType = \"body\", dataType = \"UserDTO\", name = \"userDTO\", value = \"用户\", required = true)\n    })\n    @RequestMapping(path = \"/user\", method = RequestMethod.POST)\n    public ControllerResponseT create(@Validated({Post.class}) @RequestBody UserDTO userDTO) {\n        int result = userService.save(UserDTO.convert(userDTO));\n        return ControllerResponseT.ofSuccess(\"success\");\n    }\n```\n\n如果参数验证错误，则接口返回结果如下\n\n<img src=\"/images/517519-20210629162110731-245556429.png\" alt=\"\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n","tags":["SpringBoot"]},{"title":"Java多线程","url":"/Java多线程.html","content":"## **Java进程与线程**\n\n**进程**是程序的一次动态执行过程，它经历了从代码加载、执行到执行完毕的一个完整过程，这个过程也是进程本身从产生、发展到最终消亡的过程。\n\n**多进程**操作系统能同时运行多个进程(程序)，由于CPU具备分时机制，所以每个进程都能循环获得自己的CPU时间片。\n\n**多线程**是指一个进程在执行过程中可以产生多个线程，这些线程可以同时存在、同时运行，**一个进程**可能包含了**多个同时执行的线程**。\n\n比如JVM就是一个操作系统，每当使用java命令执行一个类时，实际上都会启动一个jvm,每一个JVM实际上就是在操作系统中启动一个进程，java本身具备了垃圾回收机制，所以每个java运行时**至少会启动两个线程**，**一个main线程**，**另外一个是垃圾回收机制**。\n\n## **Java中线程的实现**\n\n在Java中要想实现多线程代码有两种手段，**一种是继承Thread类**，**另一种就是实现Runnable接口**。\n\n### **1.继承Thread类**\n\n```\nclass MyThread extends Thread{\n\tprivate String name;\n\n\tpublic MyThread(String name) {\t//构造方法\n\t\tsuper();\n\t\tthis.name = name;\n\t}\n\t\n\tpublic void run(){\t//覆写Thread类中的run()方法\n\t\tfor (int i=0;i<10;i++){\n\t\t\tSystem.out.println(name+\"运行，i=\"+i);\n\t\t}\n\t}\n\t\n}\n\npublic class Thread_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tMyThread mt1 = new MyThread(\"线程A \");\n\t\tMyThread mt2 = new MyThread(\"线程B \");\n\t\tmt1.start();\n\t\tmt2.start();\n\t}\n\n}\n\n```\n\n输出的结果可能是**A线程和B线程交替进行**，哪一个线程对象抢到了CPU资源，哪个线程就可以运行，在线程启动时虽然调用的是start()方法，但是实际上调用的却是run()方法的主体\n\n**如果一个类通过Thread类来实现，那么只能调用一次start()方法**，如果调用多次，则将会抛出\"IllegalThreadStateException\"异常。\n\n<img src=\"/images/517519-20170106163040644-1295118209.png\" alt=\"\" /> <img src=\"/images/517519-20170106163112691-1233037183.png\" alt=\"\" /><!--more-->\n&nbsp;<img src=\"/images/517519-20170106163131909-1901112296.png\" alt=\"\" />\n\n### **2.实现Runnable接口**\n\n仍然要依靠Thread类完成启动，在Thread类中提供了public Thread(Runnable target)和public Thread(Runnable target,String name)两个构造方法。\n\n这两个构造方法都可以接受Runnable的子类实例对象。\n\n```\nclass MyThread_1 implements Runnable{\n    private String name;\n    \n    public MyThread_1(String name) {    //构造方法\n        super();\n        this.name = name;\n    }\n    \n    @Override\n    public void run() {     //覆写Thread类中的run()方法\n        // TODO 自动生成的方法存根\n        for (int i=0;i<10;i++){\n            System.out.println(name+\"运行，i=\"+i);\n        }\n    }\n    \n}\n\npublic class Runnable_demo {\n\n    public static void main(String[] args) {\n        // TODO 自动生成的方法存根\n        MyThread_1 mt1 = new MyThread_1(\"线程A \");    //实例化Runnable子类对象\n        MyThread_1 mt2 = new MyThread_1(\"线程B \");    //实例化Runnable子类对象\n        Thread t1 = new Thread(mt1);                            //实例化Thread类对象\n        Thread t2 = new Thread(mt2);                            //实例化Thread类对象\n        t1.start();                                                                    //启动线程\n        t2.start();                                                                    //启动线程\n    }\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20170106194149722-1446980436.png\" alt=\"\" />\n\n通过Thread和Runnable接口都可以实现多线程，其中**Thread类也是Runnable接口的子类**，但在Thread类中并没有完全地实现Runnable接口中的run()方法。\n\n**区别**：如果一个类继承了Thread类，则不适合多个线程共享资源，而实现了Runnable接口，就可以方便地实现资源的共享。\n\n如果在Thread子类覆盖的run方法中编写了代码，也为Thread子类对象传递了一个Runnable对象，线程运行的时候执行的是子类的run方法（匿名内部类对象的构造方法如何调用非默认构造方法）\n\n&nbsp;\n\n```\nclass MyThread_2 implements Runnable{\n\tprivate int ticket = 5;\t\n\t\n\t@Override\n\tpublic void run() {\t\t//覆写Thread类中的run()方法\n\t\t// TODO 自动生成的方法存根\n\t\tfor (int i=0;i<10;i++){\n\t\t\tif(ticket>0){\n\t\t\t\tSystem.out.println(\"卖票：ticket=\"+ticket--);\n\t\t\t}\n\t\t}\n\t}\n\t\n}\n\npublic class Runnable_demo2 {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tMyThread_2 mt = new MyThread_2();\t//实例化Runnable子类对象\n\t\tThread t1 = new Thread(mt);\t\t//实例化Thread类对象\n\t\tThread t2 = new Thread(mt);\t\t//实例化Thread类对象\n\t\tThread t3 = new Thread(mt);\t\t//实例化Thread类对象\n\t\tt1.start();\t\t\t\t//启动线程\n\t\tt2.start();\t\t\t\t//启动线程\n\t\tt3.start();\t\t\t\t//启动线程\n\t}\n\n}\n\n```\n\n**在没有同步之前会出现下面这种情况**\n\n<img src=\"/images/517519-20170107135152456-1338982113.png\" alt=\"\" />\n\n&nbsp;\n\n**实现Runnable接口相对于继承Thread类来说，有下列优势：**\n\n<1>适合多个相同程序代码的线程去处理同一资源的情况\n\n<2>可以避免由于Java的单继承特性带来的局限\n\n<3>增强了程序的健壮性，代码能够被多个线程共享，代码和数据是独立的\n\n&nbsp;\n\n## **线程的生命周期**\n\n<img src=\"/images/517519-20160308162330882-1429306344.png\" alt=\"\" />\n\n&nbsp;\n\n<1>设计4个线程对象，两个线程执行减操作，两个线程执行加操作\n\n```\nclass Operator{\n\t\n\tprivate static int i;\n\t\n\tclass Inc implements Runnable{\n\t\t\n\t\t@Override\n\t\tpublic void run() {\n\t\t\t// TODO 自动生成的方法存根\n\t\t\tfor(int j=0;j<10;j++){\n\t\t\t\tthis.inc();\n\t\t\t\tSystem.out.println(Thread.currentThread().getName()+\",i=\"+i);\n\t\t\t}\n\t\t}\n\t\t\n\t\tpublic synchronized void inc(){\n\t\t\ti++;\n\t\t}\n\t}\n\t\n\tclass Dec implements Runnable{\n\t\t\n\t\t@Override\n\t\tpublic void run() {\n\t\t\t// TODO 自动生成的方法存根\n\t\t\tfor(int j=0;j<10;j++){\n\t\t\t\tthis.dec();\n\t\t\t\tSystem.out.println(Thread.currentThread().getName()+\",i=\"+i);\n\t\t\t}\n\t\t}\n\t\t\n\t\tpublic synchronized void dec(){\n\t\t\ti--;\n\t\t}\n\t}\n}\n\npublic class Thread4_demo {\n\t\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tOperator.Inc inc1 = new Operator().new Inc();\t\t//实例化内部类对象\n\t\tOperator.Inc inc2 = new Operator().new Inc();\t\t//实例化内部类对象\n\t\tOperator.Dec dec1 = new Operator().new Dec();\t\t//实例化内部类对象\n\t\tOperator.Dec dec2 = new Operator().new Dec();\t\t//实例化内部类对象\n\t\t\n\t\tThread t1 = new Thread(inc1);\t\t\t\t//实例化Thread类对象\n\t\tThread t2 = new Thread(inc2);\t\t\t\t//实例化Thread类对象\n\t\tThread t3 = new Thread(dec1);\t\t\t\t//实例化Thread类对象\n\t\tThread t4 = new Thread(dec2);\t\t\t\t//实例化Thread类对象\n\t\t\n\t\tt1.start();\n\t\tt2.start();\n\t\tt3.start();\n\t\tt4.start();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n<2>设计一个生产电脑和搬运电脑类，要求生产出一台电脑就搬走一台电脑，如果没有新的电脑生产出来，则搬运工要等待新电脑产出；如果生产出的电脑没有搬走，则要等待电脑搬走之后再生产，并统计出生产的电脑数量。\n\n```\nclass Computer {\n\tprivate String name = \"未生产\";\n\tprivate boolean flag = true;\n\t\n\tpublic String getName() {\n\t\treturn name;\n\t}\n\n\tpublic void setName(String name) {\n\t\tthis.name = name;\n\t}\n\t\n\tpublic synchronized void set(String name){\t//设置信息名称及内容\n\t\tif(!flag){\t//标志位为false，不可以生产，在这里等待取走\n\t\t\ttry{\n\t\t\t\tsuper.wait();\t\t//等待搬运者取走\n\t\t\t}catch(InterruptedException e){\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t\t}\t\n\t\tthis.setName(name);\t\t\t//设置信息名称\t\n\t\tSystem.out.println(this.getName());\t//输出信息\n\t\ttry{\n\t\t\tThread.sleep(300);\t\t//加入延迟\n\t\t}catch(InterruptedException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\tflag = false;\t\t\t\t//标志位为true，表示可以取走\n\t\tsuper.notify();\t\t\t\t//唤醒等待线程\n\t}\n\t\n\tpublic synchronized void get(){\t\t\t//取得信息内容\n\t\tif(flag){\t\t\t\t//标志位为true，不可以取走\n\t\t\ttry{\n\t\t\t\tsuper.wait();\t\t//等待生产者生产\n\t\t\t}catch(InterruptedException e){\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t\t}\t\t\n\t\ttry {\n\t\t\tThread.sleep(300);\t\t//加入延迟\n\t\t} catch (InterruptedException e) {\n\t\t\te.printStackTrace();\n\t\t}\n\t\tthis.setName(\"已经搬运完毕\");\n\t\tSystem.out.println(this.getName());\t//输出信息\n\t\tflag = true;\t\t\t\t//修改标志位为true，表示可以生产\n\t\tsuper.notify();\t\t\t\t//唤醒等待线程\n\t}\n\t\n}\n\nclass producer implements Runnable{\t\t//定义生产者线程\n\n\tprivate Computer com = null;\t\t//保存Computer引用\n\t\n\tpublic producer(Computer com) {\t\t//构造函数\n\t\tsuper();\n\t\tthis.com = com;\n\t}\n\n\t@Override\n\tpublic void run() {\n\t\tint count = 0;\n\t\t// TODO 自动生成的方法存根\n\t\tfor(int i=0;i<10;i++){\n\t\t\tthis.com.set(\"已经生产完毕\");\n\t\t\tcount++;\n\t\t}\n\t\tSystem.out.println(\"生产的电脑数量:\"+count);\n\t}\n\t\n}\n\nclass transfer implements Runnable{\t\t//定义生产者线程\n\n\tprivate Computer com = null;\t\t//保存Computer引用\n\t\n\tpublic transfer(Computer com) {\t\t//构造函数\n\t\tsuper();\n\t\tthis.com = com;\n\t}\n\n\t@Override\n\tpublic void run() {\n\t\t// TODO 自动生成的方法存根\n\t\tfor(int i=0;i<10;i++){\n\t\t\tthis.com.get();\n\t\t\t\n\t\t}\n\t}\n\t\n}\n\npublic class computer_transfer_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tComputer c = new Computer();\n\t\tproducer pro = new producer(c);\n\t\ttransfer tra = new transfer(c);\n\t\tnew Thread(pro).start();\n\t\tnew Thread(tra).start();\n\t}\n\n}\n\n```\n\n## java多线程操作方法\n\n<img src=\"/images/517519-20160307231841069-1220301823.png\" alt=\"\" />\n\n&nbsp;\n\n**取得和设置线程名称**\n\n```\nclass MyThread_1 implements Runnable{\t//实现Runnable接口\n\tprivate String name;\n\t\n//\tpublic MyThread_1(String name) {\t//构造方法\n//\t\tsuper();\n//\t\tthis.name = name;\n//\t}\n\t\n\t@Override\n\tpublic void run() {\t\t\t//覆写Thread类中的run()方法\n\t\t// TODO 自动生成的方法存根\n\t\tfor (int i=0;i<10;i++){\n//\t\t\tSystem.out.println(name+\"运行，i=\"+i);\n\t\t\tSystem.out.println(Thread.currentThread().getName()+\",i=\"+i);\t//取出当前线程的名称\n\t\t}\n\t}\n\t\n}\n\npublic class Runnable_demo {\n\n\tpublic static void main(String[] args) {\n\t\t\n\t\tMyThread_1 mt1 = new MyThread_1();\t//实例化Runnable子类对象\n\t\tnew Thread(mt1).start();\t\t//系统自动设置线程名称\n\t\tnew Thread(mt1,\"线程A\").start();\t\t//手工自动设置线程名称\n\t\tnew Thread(mt1,\"线程B\").start();\t\t//手工自动设置线程名称\n\t\tnew Thread(mt1).start();\t\t//系统自动设置线程名称\n\t\tnew Thread(mt1).start();\t\t//系统自动设置线程名称\n\t}\n\n}\n\n```\n\n**手工设置线程名称　　　　系统自动设置线程名称**\n\n<img src=\"/images/517519-20170107140247206-83797176.png\" alt=\"\" />　　　　　　　　<img src=\"/images/517519-20170107140335534-1375081390.png\" alt=\"\" />\n\n### **判断线程是否启动**\n\n使用**isAlive()方法**来判断线程是否已经启动而且仍然在启动\n\n```\nclass MyThread_1 implements Runnable{\t//实现Runnable接口\n\tprivate String name;\n\t\n\tpublic MyThread_1(String name) {\t//构造方法\n\t\tsuper();\n\t\tthis.name = name;\n\t}\n\t\n\t@Override\n\tpublic void run() {\t\t\t//覆写Thread类中的run()方法\n\t\t// TODO 自动生成的方法存根\n\t\tfor (int i=0;i<10;i++){\n//\t\t\tSystem.out.println(name+\"运行，i=\"+i);\n\t\t\tSystem.out.println(Thread.currentThread().getName()+\",i=\"+i);\t//取出当前线程的名称\n\t\t}\n\t}\n\t\n}\n\npublic class Runnable_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tMyThread_1 mt1 = new MyThread_1(\"线程A \");\t//实例化Runnable子类对象\n\t\tMyThread_1 mt2 = new MyThread_1(\"线程B \");\t//实例化Runnable子类对象\n\t\tThread t1 = new Thread(mt1);\t\t\t//实例化Thread类对象\n\t\tThread t2 = new Thread(mt2);\t\t\t//实例化Thread类对象\n\t\tSystem.out.println(\"线程开始执行之前-->\"+t1.isAlive());\n\t\tt1.start();\t\t\t\t\t\t//启动线程\n\t\tSystem.out.println(\"线程开始执行之后-->\"+t1.isAlive());\n\t\tt2.start();\t\t\t\t\t\t//启动线程\n\n\t}\n\n}\n\n```\n\n&nbsp;主线程有可能比其他线程先执行完\n\n<img src=\"/images/517519-20170107140859519-2033832084.png\" alt=\"\" />\n\n&nbsp;\n\n### **线程的强制运行**\n\n在线程操作中，可以使用**join()方法**让一个线程强制运行，线程强制运行期间，期间线程无法运行，必须等待此线程完成之后才可以继续执行。\n\n<img src=\"/images/517519-20160308100922460-124236205.png\" alt=\"\" />\n\n### **线程的休眠**\n\n在程序中允许一个线程进行暂时的休眠，直接使用**Thread.sleep()方法**即可实现休眠\n\n程序在执行的时候，每次的输出都会间隔500ms，达到了延时操作的效果。\n\n**Thread.sleep()方法**要用try和catch语句包围\n\n```\nclass Mythread implements Runnable{\t//实现Runnable接口\n\t\n\t@Override\n\tpublic void run() {\t\t//覆写Thread类中的run()方法\n\t\t// TODO 自动生成的方法存根\n\t\tfor (int i=0;i<5;i++){\n\t\t\ttry{\n\t\t\t\tThread.sleep(500);\t\t\t//线程休眠\n\t\t\t}catch (Exception e){}\t\t\t\t//需要异常处理\n\t\t\t\tSystem.out.println(Thread.currentThread().getName()+\",i=\"+i);\t//取出当前线程的名称\n\t\t}\n\t}\n\t\n}\n\npublic class ThreadSleep_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tMythread m = new Mythread();\n\t\tnew Thread(m,\"线程\").start();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n### **中断线程**\n\n当一个线程运行时，另外一个线程可以直接通过**interrupt()方法**中断其运行状态。\n\n一个线程启动之后进入了休眠状态，原来是要休眠10s之后再继续执行，但是主方法在线程启动之后的2s之后就将其中断，休眠一旦中断之后将执行catch中的代码。\n\n```\nclass Mythread_1 implements Runnable{\t//实现Runnable接口\n\t\n\t@Override\n\tpublic void run() {\t\t//覆写Thread类中的run()方法\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"进入run方法\");\n\t\ttry{\n\t\t\tThread.sleep(10000);\t\t//线程休眠\n\t\t\tSystem.out.println(\"休眠完成\");\n\t\t\t\t}catch (Exception e){\t//需要异常处理\n\t\t\tSystem.out.println(\"休眠被终止\");\n\t\t\treturn;\t\t\t\t//让程序返回被调用处\n\t\t}\n\t\tSystem.out.println(\"run方法结束\");\n\t}\n\t\n}\n\npublic class ThreadInterrupt_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tMythread_1 m = new Mythread_1();\n\t\tThread t = new Thread(m,\"线程\");\n\t\tt.start();\n\t\ttry{\n\t\t\tThread.sleep(2000);\t\t//主线程2s之后再执行中断\n\t\t}catch(Exception e){}\n\t\tt.interrupt();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n### **后台线程**\n\n在Java程序中，只要前台有一个线程在运行，则整个Java进程都不会消失，所以此时可以设置一个后台线程，这样即使Java进程结束了，此后台线程依然会继续执行。要想实现这样的操作，直接使用**setDaemon()方法**即可。\n\n<img src=\"/images/517519-20160308104627257-1800492555.png\" alt=\"\" />\n\n&nbsp;\n\n### **线程的优先级**\n\n在Java的线程中使用**setPriority()方法**可以设置一个线程的优先级，在Java的线程中一共有3种优先级。\n\n<img src=\"/images/517519-20160308110823616-435295338.png\" alt=\"\" />\n\n```\nclass MyThread_1 implements Runnable{\t//实现Runnable接口\n\tprivate String name;\n\t\n//\tpublic MyThread_1(String name) {\t//构造方法\n//\t\tsuper();\n//\t\tthis.name = name;\n//\t}\n\t\n\t@Override\n\tpublic void run() {\t\t\t//覆写Thread类中的run()方法\n\t\t// TODO 自动生成的方法存根\n\t\tfor (int i=0;i<10;i++){\n\t\t\t//System.out.println(name+\"运行，i=\"+i);\n\t\t\tSystem.out.println(Thread.currentThread().getName()+\",i=\"+i);\t//取出当前线程的名称\n\t\t}\n\t}\n\t\n}\n\npublic class Runnable_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tMyThread_1 mt1 = new MyThread_1();\t//实例化Runnable子类对象\n\t\tMyThread_1 mt2 = new MyThread_1();\t//实例化Runnable子类对象\n\t\tMyThread_1 mt3 = new MyThread_1();\t//实例化Runnable子类对象\n\t\tThread t1 = new Thread(mt1,\"线程A\");\t/实例化Thread类对象\n\t\tThread t2 = new Thread(mt2,\"线程B\");\t//实例化Thread类对象\n\t\tThread t3 = new Thread(mt3,\"线程C\");\t//实例化Thread类对象\n//\t\tSystem.out.println(\"线程开始执行之前-->\"+t1.isAlive());\n\t\tt1.setPriority(Thread.MIN_PRIORITY);\n\t\tt2.setPriority(Thread.NORM_PRIORITY);\n\t\tt3.setPriority(Thread.MAX_PRIORITY);\n\t\tt1.start();\t\t\t\t\t\t//启动线程\n//\t\tSystem.out.println(\"线程开始执行之前-->\"+t1.isAlive());\n\t\tt2.start();\t\t\t\t\t\t//启动线程\n\t\tt3.start();\t\t\t\t\t\t//启动线程\n\t\t\n//\t\tMyThread_1 mt1 = new MyThread_1();\t//实例化Runnable子类对象\n//\t\tnew Thread(mt1).start();\t\t\t\t//系统自动设置线程名称\n//\t\tnew Thread(mt1,\"线程A\").start();\t\t\t\t//手工自动设置线程名称\n//\t\tnew Thread(mt1,\"线程B\").start();\t\t\t\t//手工自动设置线程名称\n//\t\tnew Thread(mt1).start();\t\t\t\t//系统自动设置线程名称\n//\t\tnew Thread(mt1).start();\t\t\t\t//系统自动设置线程名称\n\t}\n\n}\n\n```\n\n&nbsp;线程将根据优先级的大小来决定哪个线程会先运行，**但是并非线程的优先级越高就一定会先执行**，哪个线程先执行将由CPU的调度决定。\n\n主方法的优先级是NORM，通过Thread.currentThread().getPriority()来取得主方法的优先级，结果是5\n\n&nbsp;\n\n### **线程的礼让**\n\n在线程的操作中，可以使用**yield()方法**将一个线程的操作暂时让给其他线程执行。本线程暂停，让其他进程先执行。\n\n```\nclass MyThread_1 implements Runnable{\t//实现Runnable接口\n\tprivate String name;\n\t\n//\tpublic MyThread_1(String name) {\t//构造方法\n//\t\tsuper();\n//\t\tthis.name = name;\n//\t}\n\t\n\t@Override\n\tpublic void run() {\t\t\t\t\t\t\t\t//覆写Thread类中的run()方法\n\t\t// TODO 自动生成的方法存根\n\t\tfor (int i=0;i<10;i++){\n\t\t\t//System.out.println(name+\"运行，i=\"+i);\n\t\t\tSystem.out.println(Thread.currentThread().getName()+\",i=\"+i);\t//取出当前线程的名称\n\t\t\tif(i==3){\n\t\t\t\tSystem.out.println(\"线程礼让：\");\n\t\t\t\tThread.currentThread().yield();\t//线程礼让\n\t\t\t}\n\t\t}\n\t}\n\t\n}\n\npublic class Runnable_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tMyThread_1 mt1 = new MyThread_1();\t//实例化Runnable子类对象\n\t\tMyThread_1 mt2 = new MyThread_1();\t//实例化Runnable子类对象\n\t\tMyThread_1 mt3 = new MyThread_1();\t//实例化Runnable子类对象\n\t\tThread t1 = new Thread(mt1,\"线程A\");\t//实例化Thread类对象\n\t\tThread t2 = new Thread(mt2,\"线程B\");\t//实例化Thread类对象\n\t\tThread t3 = new Thread(mt3,\"线程C\");\t//实例化Thread类对象\n//\t\tSystem.out.println(\"线程开始执行之前-->\"+t1.isAlive());\n\t\tt1.setPriority(Thread.MIN_PRIORITY);\n\t\tt2.setPriority(Thread.NORM_PRIORITY);\n\t\tt3.setPriority(Thread.MAX_PRIORITY);\n\t\tt1.start();\t\t\t\t//启动线程\n//\t\tSystem.out.println(\"线程开始执行之前-->\"+t1.isAlive());\n\t\tt2.start();\t\t\t\t//启动线程\n\t\tt3.start();\t\t\t\t//启动线程\n\t\t\n//\t\tMyThread_1 mt1 = new MyThread_1();\t//实例化Runnable子类对象\n//\t\tnew Thread(mt1).start();\t\t\t\t//系统自动设置线程名称\n//\t\tnew Thread(mt1,\"线程A\").start();\t\t\t\t//手工自动设置线程名称\n//\t\tnew Thread(mt1,\"线程B\").start();\t\t\t\t//手工自动设置线程名称\n//\t\tnew Thread(mt1).start();\t\t\t\t//系统自动设置线程名称\n//\t\tnew Thread(mt1).start();\t\t\t\t//系统自动设置线程名称\n\t}\n\n}\n\n```\n\n**&nbsp;<img src=\"/images/517519-20170107141807253-1182441932.png\" alt=\"\" />　　<img src=\"/images/517519-20170107141812800-1092290694.png\" alt=\"\" />　　**\n\n**线程礼让也是不一定的**\n","tags":["多线程"]},{"title":"HBase学习笔记——存储结构","url":"/HBase学习笔记——存储结构.html","content":"参考：[一文讲清HBase存储结构<!--more-->\n&nbsp;](https://juejin.cn/post/6844903753271754759)\n","tags":["HBase"]},{"title":"ubuntu删除输入法后，循环登陆","url":"/ubuntu删除输入法后，循环登陆.html","content":"在登陆界面ctrl+alt+F1进入tty界面，登陆账号，然后输入\n\n```\ndpkg -l |grep ^rc|awk '{print $2}' |sudo xargs dpkg -P\n\n```\n\n<!--more-->\n&nbsp;\n\n主要就是 sudo chown root:root .Xauthority\n\n应该改成 sudo chown 图形界面的own:图形界面的grp .Xauthority\n\n&nbsp;\n\n登陆之后没有中文输入法\n\n首先安装fcitx\n\nsudo apt-get install fcitx libssh2-1到搜狗输入法的首页下载deb的安装文件\n\n然后安装搜狗输入法，双击安装或者sudo dpkg -i XXXX.deb\n\n在系统设置>语言支持>键盘输入方式系统中选择fcitx\n\n如果没有语言支持的话，sudo apt-get install language-selector-gnome\n\n然后即可输入中文\n","tags":["Linux"]},{"title":"ElasticSearch学习笔记——Request请求的类型","url":"/ElasticSearch学习笔记——Request请求的类型.html","content":"1. TransportNodesListGatewayMetaState.Request\n\n`获取各个节点的元信息的请求`\n\n2.<!--more-->\n&nbsp;TransportNodesListGatewayStartedShards.Request\n\n获取started的shard的列表的请求\n\n3.&nbsp;org.elasticsearch.action.admin.cluster.node.stats.NodesStatsRequest\n\n获取节点状态的请求，es默认会每隔30s去检查一遍节点的状态\n\n4.&nbsp;org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest\n\n获取索引状态的请求，es默认会每隔30s去检查一遍index的状态\n\n5.&nbsp;org.elasticsearch.action.search.SearchRequest\n\nsearch query的请求\n\n6.&nbsp;org.elasticsearch.action.get.GetRequest\n\nget query的请求\n\n7.&nbsp;org.elasticsearch.action.admin.indices.template.put.PutIndexTemplateRequest\n\n更新索引模板的请求\n\n&nbsp;\n","tags":["ELK"]},{"title":"HAProxy+Keepalive实现HA","url":"/HAProxy+Keepalive实现HA.html","content":"keepalive原理可以参考：[Ubuntu安装keepalived](https://www.cnblogs.com/guoximing/p/9390269.html)\n\n1. 首先需要安装keepalived\n\n```\nsudo apt-get install keepalived\n\n```\n\n2. 编辑 /etc/keepalived/keepalived.conf 配置，参考：[16.6<!--more-->\n&nbsp;Configuring Simple Virtual IP Address Failover Using Keepalived](https://docs.oracle.com/cd/E37670_01/E41138/html/section_uxg_lzh_nr.html)\n\n**master配置**\n\n将master的一个网卡的ip绑定到一个虚拟ip上，其中&nbsp;interface 是绑定的网卡，virtual_ipaddress 是绑定的虚拟ip的地址\n\n```\nglobal_defs {\n   notification_email {\n     root@mydomain.com\n   }\n   notification_email_from svr1@mydomain.com\n   smtp_server localhost\n   smtp_connect_timeout 30\n}\n\nvrrp_instance VRRP1 {\n    state MASTER\n#   Specify the network interface to which the virtual address is assigned\n    interface enp3s0\n#   The virtual router ID must be unique to each VRRP instance that you define\n    virtual_router_id 41\n#   Set the value of priority higher on the master server than on a backup server\n    priority 200\n    advert_int 1\n    authentication {\n        auth_type PASS\n        auth_pass 1066\n    }\n    virtual_ipaddress {\n        10.0.0.100/24\n    }\n}\n\n```\n\n启动\n\n```\nsudo service keepalived start\n\n```\n\n<img src=\"/images/517519-20210521174001427-1663370043.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n查看网卡信息\n\n```\nip addr list\n\n```\n\n看到&nbsp;enp3s0 网卡下面已经出现 10.0.0.100 的虚拟ip\n\n<img src=\"/images/517519-20210521174133371-1648864457.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n可以ping通\n\n```\nping 10.0.0.100\nPING 10.0.0.100 (10.0.0.100) 56(84) bytes of data.\n64 bytes from 10.0.0.100: icmp_seq=1 ttl=64 time=0.026 ms\n64 bytes from 10.0.0.100: icmp_seq=2 ttl=64 time=0.023 ms\n\n```\n\n**backup配置**\n\n将backup的一个网卡的ip也绑定到10.0.0.100这个虚拟ip上，来实现HA\n\n```\nglobal_defs {\n   notification_email {\n     root@mydomain.com\n   }\n   notification_email_from svr2@mydomain.com\n   smtp_server localhost\n   smtp_connect_timeout 30\n}\n\nvrrp_instance VRRP1 {\n    state BACKUP\n#   Specify the network interface to which the virtual address is assigned\n    interface eth0\n    virtual_router_id 41\n#   Set the value of priority lower on the backup server than on the master server\n    priority 100\n    advert_int 1\n    authentication {\n        auth_type PASS\n        auth_pass 1066\n    }\n    virtual_ipaddress {\n        10.0.0.100/24\n    }\n}\n\n```\n\n这样，master和backup的ip就都绑定到 10.0.0.100 这个虚拟ip上了\n\n&nbsp;\n\n2. 使用haproxy，将两个webserver 的ip 192.168.1.71和192.168.1.72绑定到Keepalive管理的虚拟ip 10.0.0.100上，从而来实现高可用\n\n```\nglobal\n    daemon\n    log 127.0.0.1 local0 debug\n    maxconn 50000\n    nbproc 1\n\ndefaults\n    mode http\n    timeout connect 5s\n    timeout client 25s\n    timeout server 25s\n    timeout queue 10s\n\n# Handle Incoming HTTP Connection Requests on the virtual IP address controlled by Keepalived\nlisten  http-incoming\n    mode http\n    bind 10.0.0.10:80\n# Use each server in turn, according to its weight value\n    balance roundrobin\n# Verify that service is available\n    option httpchk OPTIONS * HTTP/1.1\\r\\nHost:\\ www\n# Insert X-Forwarded-For header\n    option forwardfor\n# Define the back-end servers, which can handle up to 512 concurrent connections each\n    server websvr1 192.168.1.71:80 weight 1 maxconn 512 check\n    server websvr2 192.168.1.72:80 weight 1 maxconn 512 check\n\n```\n\n参考文档：[16.10&nbsp;Making HAProxy Highly Available Using Keepalived](https://docs.oracle.com/cd/E37670_01/E41138/html/section_sm3_svy_4r.html)\n\n&nbsp;\n\nkeepalived有主主模式和主从模式，参考：[HAProxy &amp; Keepalived L4-L7 高可用负载均衡解决方案](https://www.cnblogs.com/jmilkfan-fanguiju/p/10589765.html)\n\n&nbsp;\n","tags":["HAProxy"]},{"title":"Hive学习笔记——在Hive中使用AvroSerde","url":"/Hive学习笔记——在Hive中使用AvroSerde.html","content":"Hive支持使用avro serde作为序列化的方式，参考：\n\n```\nhttps://cwiki.apache.org/confluence/display/hive/avroserde\nhttps://www.docs4dev.com/docs/zh/apache-hive/3.1.1/reference/AvroSerDe.html\nhttps://github.com/jghoman/haivvreo\n\n```\n\n以及CDH官方的文档\n\n```\nhttps://docs.cloudera.com/documentation/enterprise/6/6.3/topics/cdh_ig_avro_usage.html\n\n```\n\n　　\n\n1.定义avro schema，kst.avsc\n\n```\n{\n  \"namespace\": \"com.linkedin.haivvreo\",\n  \"name\": \"test_serializer\",\n  \"type\": \"record\",\n  \"fields\": [\n    { \"name\":\"string1\", \"type\":\"string\" },\n    { \"name\":\"int1\", \"type\":\"int\" },\n    { \"name\":\"tinyint1\", \"type\":\"int\" },\n    { \"name\":\"smallint1\", \"type\":\"int\" },\n    { \"name\":\"bigint1\", \"type\":\"long\" },\n    { \"name\":\"boolean1\", \"type\":\"boolean\" },\n    { \"name\":\"float1\", \"type\":\"float\" },\n    { \"name\":\"double1\", \"type\":\"double\" },\n    { \"name\":\"list1\", \"type\":{\"type\":\"array\", \"items\":\"string\"} },\n    { \"name\":\"map1\", \"type\":{\"type\":\"map\", \"values\":\"int\"} },\n    { \"name\":\"struct1\", \"type\":{\"type\":\"record\", \"name\":\"struct1_name\", \"fields\": [\n          { \"name\":\"sInt\", \"type\":\"int\" }, { \"name\":\"sBoolean\", \"type\":\"boolean\" }, { \"name\":\"sString\", \"type\":\"string\" } ] } },\n    { \"name\":\"union1\", \"type\":[\"float\", \"boolean\", \"string\"] },\n    { \"name\":\"enum1\", \"type\":{\"type\":\"enum\", \"name\":\"enum1_values\", \"symbols\":[\"BLUE\",\"RED\", \"GREEN\"]} },\n    { \"name\":\"nullableint\", \"type\":[\"int\", \"null\"] },\n    { \"name\":\"bytes1\", \"type\":\"bytes\" },\n    { \"name\":\"fixed1\", \"type\":{\"type\":\"fixed\", \"name\":\"threebytes\", \"size\":3} }\n  ] }\n\n```\n\n将schema文件其放到HDFS上\n\n```\nhadoop fs -ls /user/hive/schema\nFound 1 items\n-rw-r--r--   3 hive hive       1131 2021-12-04 13:53 /user/hive/schema/kst.avsc\n\n```\n\n2.建Hive表\n\n```\nCREATE TABLE default.kst\n  PARTITIONED BY (ds string)\n  ROW FORMAT SERDE\n  'org.apache.hadoop.hive.serde2.avro.AvroSerDe'\n  STORED AS INPUTFORMAT\n  'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\n  OUTPUTFORMAT\n  'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\n  TBLPROPERTIES (\n    'avro.schema.url'='hdfs:///user/hive/schema/kst.avsc');\n    \n\n```\n\n或者直接在TBLPROPERTIES中指定schema\n\n```\nCREATE TABLE default.kst\n  PARTITIONED BY (ds string)\n  ROW FORMAT SERDE\n  'org.apache.hadoop.hive.serde2.avro.AvroSerDe'\n  STORED AS INPUTFORMAT\n  'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\n  OUTPUTFORMAT\n  'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\n  TBLPROPERTIES (\n    'avro.schema.literal'='{\n  \"namespace\": \"com.linkedin.haivvreo\",\n  \"name\": \"test_serializer\",\n  \"type\": \"record\",\n  \"fields\": [\n    { \"name\":\"string1\", \"type\":\"string\" },\n    { \"name\":\"int1\", \"type\":\"int\" },\n    { \"name\":\"tinyint1\", \"type\":\"int\" },\n    { \"name\":\"smallint1\", \"type\":\"int\" },\n    { \"name\":\"bigint1\", \"type\":\"long\" },\n    { \"name\":\"boolean1\", \"type\":\"boolean\" },\n    { \"name\":\"float1\", \"type\":\"float\" },\n    { \"name\":\"double1\", \"type\":\"double\" },\n    { \"name\":\"list1\", \"type\":{\"type\":\"array\", \"items\":\"string\"} },\n    { \"name\":\"map1\", \"type\":{\"type\":\"map\", \"values\":\"int\"} },\n    { \"name\":\"struct1\", \"type\":{\"type\":\"record\", \"name\":\"struct1_name\", \"fields\": [\n          { \"name\":\"sInt\", \"type\":\"int\" }, { \"name\":\"sBoolean\", \"type\":\"boolean\" }, { \"name\":\"sString\", \"type\":\"string\" } ] } },\n    { \"name\":\"union1\", \"type\":[\"float\", \"boolean\", \"string\"] },\n    { \"name\":\"enum1\", \"type\":{\"type\":\"enum\", \"name\":\"enum1_values\", \"symbols\":[\"BLUE\",\"RED\", \"GREEN\"]} },\n    { \"name\":\"nullableint\", \"type\":[\"int\", \"null\"] },\n    { \"name\":\"bytes1\", \"type\":\"bytes\" },\n    { \"name\":\"fixed1\", \"type\":{\"type\":\"fixed\", \"name\":\"threebytes\", \"size\":3} }\n  ] }');\n\n```\n\n3.查看Hive表的schema\n\n```\ndescribe kst;\n\n```\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20211204140945371-820271955.png\" width=\"800\" height=\"546\" loading=\"lazy\" />\n\n使用show create table查看表结构\n\n```\nCREATE TABLE `default.kst`(\n\t  `string1` string COMMENT '', \n\t  `int1` int COMMENT '', \n\t  `tinyint1` int COMMENT '', \n\t  `smallint1` int COMMENT '', \n\t  `bigint1` bigint COMMENT '', \n\t  `boolean1` boolean COMMENT '', \n\t  `float1` float COMMENT '', \n\t  `double1` double COMMENT '', \n\t  `list1` array<string> COMMENT '', \n\t  `map1` map<string,int> COMMENT '', \n\t  `struct1` struct<sint:int,sboolean:boolean,sstring:string> COMMENT '', \n\t  `union1` uniontype<float,boolean,string> COMMENT '', \n\t  `enum1` string COMMENT '', \n\t  `nullableint` int COMMENT '', \n\t  `bytes1` binary COMMENT '', \n\t  `fixed1` binary COMMENT '')\n\tPARTITIONED BY ( \n\t  `ds` string)\n\tROW FORMAT SERDE \n\t  'org.apache.hadoop.hive.serde2.avro.AvroSerDe' \n\tSTORED AS INPUTFORMAT \n\t  'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat' \n\tOUTPUTFORMAT \n\t  'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\n\tLOCATION\n\t  'hdfs://master:8020/user/hive/warehouse/kst'\n\tTBLPROPERTIES (\n\t  'avro.schema.url'='hdfs:///user/hive/schema/kst.avsc', \n\n```\n\n4.插入数据\n\n```\ninsert into table default.kst partition(ds=\"2019-08-20\") select \"test\",2,1Y,2S,1000L,true,1.111,2.22222,array(\"test1\",\"test2\",\"test3\"),map(\"test123\",2222,\"test321\",4444),\nnamed_struct('sInt',123,'sBoolean',true, 'sString','London') as struct1,create_union(0,cast(0.2 as float),false,\"test3\"),\"BLUE\",12345,\"00008DAC\",'111';\n\n```\n\n5.查看刚刚insert的数据\n\n<img src=\"/images/517519-20211205145135554-293195558.png\" width=\"800\" height=\"150\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["avro","Hive"]},{"title":"Java抽象类与接口的关系","url":"/Java抽象类与接口的关系.html","content":"<img src=\"/images/517519-20160304113232487-469673318.png\" alt=\"\" />\n\n<!--more-->\n&nbsp;\n\n<img src=\"/images/517519-20161029120119921-647599091.png\" alt=\"\" />\n\n&nbsp;\n\n**1.抽象类：**\n\nJava可以创建一种类专门用来当做父类，这种类称为&ldquo;抽象类&rdquo;。\n\n&ldquo;抽象类&rdquo;的作用类似&ldquo;模板&rdquo;，其目的是要设计者依据它的格式来修改并创建新的类。但是不能直接由抽象类创建对象，只能通过抽象类派生出新的类，再由它来创建对象。\n\n抽象类的定义及使用规则：\n\n<1>包含一个抽象方法的类必须是抽象类\n\n<2>抽象类和抽象方法都要使用abstract关键字声明\n\n<3>抽象方法只需声明而不需要实现\n\n<4>抽象类必须被子继承、子类（如果不是抽象类）必须覆写抽象类中的全部抽象方法\n\n&nbsp;\n\n抽象类不能使用final关键字声明，因为使用final关键字声明的类不能被子类所继承\n\n抽象方法不要使用private声明，因为使用private声明了就不能被子类覆写\n\n一个抽象类中可以定义构造方法\n\n&nbsp;\n\n```\nabstract class A{\n\tpublic static final String FLAG = \"CHINA\";\n\tprivate String name = \"张三\";\n\t\n\tpublic String getName() {\n\t\treturn name;\n\t}\n\tpublic void setName(String name) {\n\t\tthis.name = name;\n\t}\n\t\n\tpublic abstract void print();\t\t//定义一个抽象方法\n}\n\nclass B extends A{\t\t\t\t\t//继承抽象类，覆写全部的抽象方法\n\tpublic void print(){\n\t\tSystem.out.println(\"FLAG=\"+FLAG);\n\t\tSystem.out.println(\"姓名=\"+super.getName());\n\t}\n}\n\npublic class abstract_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tB b = new B();\n\t\tb.print();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n假设人分为学生和工人，学生和工人可以说话，但是学生和工人说话的内容不一样，使用抽象类实现这样一个场景\n\n```\nabstract class People{\n\tprivate String name;\n\tprivate int age;\n\t\n\tpublic String getName() {\n\t\treturn name;\n\t}\n\tpublic void setName(String name) {\n\t\tthis.name = name;\n\t}\n\tpublic int getAge() {\n\t\treturn age;\n\t}\n\tpublic void setAge(int age) {\n\t\tthis.age = age;\n\t}\n\t\n\tpublic People(String name, int age) {\t//构造函数\n\t\tthis.name = name;\n\t\tthis.age = age;\n\t}\n\t\n\tpublic void say(){\t\t\t\t\t\t\t\t\t\n\t\tSystem.out.println(this.getContent());\n\t}\n\t\n\tpublic abstract String getContent();\t//get方法，说话的内容由子类决定\n\t\n}\n\nclass Student extends People{\n\t\n\tprivate float score;\n\t\n\tpublic Student(String name, int age,float score) {\n\t\tsuper(name, age);\t\t\t\t\t//调用父类的构造方法\n\t\t// TODO 自动生成的构造函数存根\n\t\tthis.score = score;\n\t}\n\n\t@Override\n\tpublic String getContent() {\t\t//覆写父类中的get抽象方法\n\t\t// TODO 自动生成的方法存根\n\t\treturn \"学生信息-->姓名：\"+super.getName()+\"年龄：\"+super.getAge()+\"成绩：\"+this.score;\n\t}\n\t\n}\n\nclass Worker extends People{\n\t\n\tprivate float salary;\n\t\n\tpublic Worker(String name, int age,float salary) {\n\t\tsuper(name, age);\t\t\t\t\t//调用父类的构造方法\n\t\t// TODO 自动生成的构造函数存根\n\t\tthis.salary = salary;\n\t}\n\n\t@Override\n\tpublic String getContent() {\t\t//覆写父类中的get抽象方法\n\t\t// TODO 自动生成的方法存根\n\t\treturn \"工人信息-->姓名：\"+super.getName()+\"年龄：\"+super.getAge()+\"薪水：\"+this.salary;\n\t}\n\t\n}\n\npublic class abstract_demo2 {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tStudent stu = new Student(\"张三\",18,99.0f);\n\t\tWorker wor = new Worker(\"张三\",18,1000.0f);\n\t\tstu.say();\n\t\twor.say();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**2.接口：**\n\nJava接口可以理解为一种特殊的类，是由**全局常量**和**公共的抽象方法**所组成，定义在接口中的方法默认是**public**的\n\n接口如果不写public，则也是public访问权限，不是default\n\n与抽象类一样，接口若要使用也必须通过子类，子类通过**implements关键字**实现接口。\n\n一个子类可以同时实现多个接口，摆脱了Java的单继承局限\n\n&nbsp;\n\n```\ninterface A_1{\t\t\t\t\t\t\t\t\t\t//定义接口A_1\n\tpublic String AUTHOR = \"张三\";\t\t//定义全局常量,等价于：public static final String AUTHOR = \"张三\";\t\n\tpublic void print();\t\t\t\t\t\t\t//定义抽象方法,等价于：public abstract void print();\n\tpublic String getInfo();\t\t\t\t\t//定义抽象方法,等价于：public abstract String getInfo();\n}\n\ninterface B_1{\t\t\t\t\t\t\t\t\t\t//定义接口B_1\n\tpublic void say();\t\t\t\t\t\t\t//定义抽象方法\n}\n\nclass X implements A_1,B_1{\t\t\t\t//子类同时实现两个接口\n\n\t@Override\n\tpublic void say() {\t\t\t\t\t\t\t//覆写B接口中的抽象方法\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"Hello World!\");\n\t}\n\n\t@Override\n\tpublic void print() {\t\t\t\t\t\t//覆写A接口中的抽象方法\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"作者:\"+AUTHOR);\n\t}\n\n\t@Override\n\tpublic String getInfo() {\t\t\t\t\t//覆写A接口中的抽象方法\n\t\t// TODO 自动生成的方法存根\n\t\treturn \"HELLO\";\n\t}\n\t\n}\n\npublic class interface_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tX x = new X();\t\t\t\t//实例化子类对象\n\t\tx.say();\t\t\t\t\t\t//调用被覆写过的方法\n\t\tx.print();\t\t\t\t\t\t//调用被覆写过的方法\n\t}\n\n}\n\n```\n\n&nbsp;\n\n&nbsp;\n\n同时实现**继承和接口**\n\n```\ninterface A_2{\t//定义接口A_1\n\tpublic String AUTHOR = \"张三\";\t/定义全局常量\n\tpublic void print();\t//定义抽象方法\n\tpublic String getInfo();\t//定义抽象方法\n}\n\nabstract class B_2{\t//定义一个抽象类\n\tpublic abstract void say();\t//定义一个抽象方法\n}\n\nabstract class B_3 implements A_2{\t//定义一个抽象类，并实现接口\n\tpublic abstract void say();\t//定义一个抽象方法\n}\n\nclass X_2 extends B_2 implements A_2{\t//同时实现继承和接口\n\n\t@Override\n\tpublic String getInfo() {\t//覆写A_2接口中的抽象方法\n\t\t// TODO 自动生成的方法存根\n\t\treturn \"HELLO\";\n\t}\n\n\t@Override\n\tpublic void say() {\t//覆写B_2抽象类中的抽象方法\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"Hello World!\");\n\t}\t\t\t\t\n\n\t@Override\n\tpublic void print() {\t//覆写A_2接口中的抽象方法\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"作者:\"+AUTHOR);\n\t}\n}\n\nclass X_3 extends B_3{\t//继承抽象类，覆写全部的抽象方法\n\n\t@Override\n\tpublic void print() {\t//覆写B_3抽象类中的抽象方法\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"作者:\"+AUTHOR);\n\t}\n\n\t@Override\n\tpublic String getInfo() {\t//覆写B_3抽象类中的抽象方法\n\t\t// TODO 自动生成的方法存根\n\t\treturn \"HELLO\";\n\t}\n\n\t@Override\n\tpublic void say() {\t//覆写B_3抽象类中的抽象方法\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"Hello World!\");\n\t}\n\t\n}\n\npublic class extends_implements_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tX_2 x = new X_2();\t//实例化子类对象\n\t\tx.say();\t//调用被覆写过的方法\n\t\tx.print();\t//调用被覆写过的方法\n\t\tX_3 x_1 = new X_3();\n\t\tx_1.say();\n\t\tx_1.print();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**接口的继承**\n\n```\ninterface C{\t// 定义接口C\n\tpublic String AUTHOR = \"张三\";\t//定义全局常量\n\tpublic void printC();\t//定义抽象方法\n}\n\ninterface D{\t//定义接口D\n\tpublic void printD();\t//定义抽象方法\n}\n\ninterface E extends C,D{\t// 定义接口E，同时继承接口C和D\n\tpublic void printE();\t//定义抽象方法\n}\n\nclass F implements E{\n\n\t@Override\n\tpublic void printC() {\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"C--Hello World\");\n\t}\n\n\t@Override\n\tpublic void printD() {\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"D--Hello World\");\n\t}\n\n\t@Override\n\tpublic void printE() {\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"E--Hello World\");\n\t}\t\t\t\t\t\t\t\n\t\n}\n\npublic class extends_implements_demo2 {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tF f = new F();\n\t\tf.printC();\n\t\tf.printD();\n\t\tf.printE();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**接口的实际应用**\n\n接口在实际中更多的作用是用来制订标准，例子USB设备\n\n```\ninterface USB{\n\tpublic void start();\n\tpublic void stop();\n}\n\nclass computer {\n\tpublic static void plugin(USB usb){\n\t\tusb.start();\n\t\tSystem.out.println(\"=====USB 设备工作=====\");\n\t\tusb.stop();\n\t}\n}\n\nclass FLASH implements USB{\n\n\t@Override\n\tpublic void start() {\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"U盘开始工作\");\n\t}\n\n\t@Override\n\tpublic void stop() {\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"U盘停止工作\");\n\t}\n\t\n}\n\nclass Print implements USB{\n\n\t@Override\n\tpublic void start() {\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"打印机开始工作\");\n\t}\n\n\t@Override\n\tpublic void stop() {\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"打印机停止工作\");\n\t}\n\t\n}\n\npublic class implements_usb_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tcomputer.plugin(new FLASH());\n\t\tcomputer.plugin(new Print());\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["Java"]},{"title":"Java工厂设计模式","url":"/Java工厂设计模式.html","content":"工厂模式的核心思想就是把创建对象和使用对象解藕，由工厂负责对象的创建，而用户只能通过接口来使用对象，这样就可以灵活应对变化的业务需求，方便代码管理、避免代码重复。\n\n## 1.工厂设计模式的例子：水果，苹果和橘子\n\n程序在**接口和子类**之间加入一个**过渡类**，通过此**过渡类**端取得**接口的实例化对象**，一般都会称这个过渡端为**工厂类**\n\n```\ninterface Fruit{\n\tpublic void eat();\n}\n\n// Apple类实现了Fruit接口\nclass Apple implements Fruit{\n\n\t@Override\n\tpublic void eat() {\n\t\tSystem.out.println(\"eat apple!\");\n\t}\n\t\n}\n\n// Orange类实现了Fruit接口\nclass Orange implements Fruit{\n\n\t@Override\n\tpublic void eat() {\n\t\tSystem.out.println(\"eat orange!\");\n\t}\n\t\n}\n\n// 定义工厂类\nclass Factory{\n\tpublic static Fruit getInstance(String className){\n\t\tFruit f = null;\t\t\t\t\t\t\t//定义接口对象\n\t\tif(\"apple\".equals(className)){\t\t\t//判断是哪个类的标记\n\t\t\tf = new Apple();\n\t\t}\n\t\tif(\"orange\".equals(className)){\t\t\t//判断是哪个类的标记\n\t\t\tf = new Orange();\n\t\t}\n\t\treturn f;\n\t}\n}\n\npublic class factory {\n\n\tpublic static void main(String[] args) {\n\t\tFruit f = null;\t\t\t\t\t\t\t//定义接口对象\n\t\tf = Factory.getInstance(\"apple\");\t\t//通过工厂取得实例\n\t\tf.eat();\t\t\t\t\t\t\t\t//调用方法\n\t}\n\n}\n\n```\n\n## 2.将反射应用在工厂模式上\n\n为了能不修改工厂方法\n\n<img src=\"/images/517519-20160320153700021-806115254.png\" alt=\"\" />\n\n```\ninterface Fruit{\n\tpublic void eat();\n}\n\nclass Apple implements Fruit{\n\n\t@Override\n\tpublic void eat() {\n\t\tSystem.out.println(\"eat apple!\");\n\t}\n\t\n}\n\nclass Orange implements Fruit{\n\n\t@Override\n\tpublic void eat() {\n\t\tSystem.out.println(\"eat orange!\");\n\t}\n\t\n}\n\n// 定义工厂类\nclass Factory{\n\tpublic static Fruit getInstance(String className){\n\t\tFruit f = null;\t\t\t//定义接口对象\n\t\ttry{\n\t\t\tf = (Fruit)Class.forName(className).newInstance();\t//实例化对象\n\t\t}catch(Exception e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\treturn f;\n\t}\n}\n\npublic class factory {\n\n\tpublic static void main(String[] args) {\n\t\tFruit f = null;\t\t\t\t\t\t\t//定义接口对象\n\t\tf = Factory.getInstance(\"Apple\");\t\t//通过工厂取得实例\n\t\tf.eat();\t\t\t\t\t\t\t\t//调用方法\n\t}\n\n}\n\n```\n\n## **3.结合属性文件的工厂模式**\n\n**<img src=\"/images/517519-20160320154157568-1545468328.png\" alt=\"\" />**\n\n<!--more-->\n&nbsp;\n\n&nbsp;<img src=\"/images/517519-20160320154240381-226064870.png\" alt=\"\" />\n","tags":["设计模式"]},{"title":"Java适配器设计模式","url":"/Java适配器设计模式.html","content":"适配器设计模式，一个接口首先被一个抽象类先实现（此抽象类通常称为**适配器类**，比如下面的**WindowAdapter**），并在此抽象类中实现若干方法（但是这个抽象类中的方法体是空的），则以后的子类直接继承此抽象类，就可以有选择地覆写所需要的方法。\n\n```\ninterface Window{\t\t\t\t\t//定义Window接口，表示窗口操作\n\tpublic void open();\t\t\t\t//窗口打开\n\tpublic void close();\t\t\t\t//窗口关闭\n\tpublic void activated();\t\t//窗口活动\n\tpublic void iconified();\t\t//窗口最小化\n\tpublic void deiconified();\t//窗口恢复大小\n}\n\nabstract class WindowAdapter implements Window{\t//定义抽象类实现接口，在此类中覆写方法，但是所有的方法体为空\n\tpublic void open() {}\t\t\t\t//窗口打开\n\tpublic void close() {};\t\t\t\t//窗口关闭\n\tpublic void activated() {};\t\t//窗口活动\n\tpublic void iconified() {};\t\t//窗口最小化\n\tpublic void deiconified() {};\t//窗口恢复大小\n}\n\nclass WindowImp1 extends WindowAdapter{\t//子类直接继承WindowAdapter类，有选择地实现需要的方法\n\tpublic void open() {\t\t\t//窗口打开\n\t\tSystem.out.println(\"窗口打开\");\n\t}\t\t\n\t\n\tpublic void close() {\t\t\t//窗口关闭\n\t\tSystem.out.println(\"窗口关闭\");\n\t}\t\t\n}\n\npublic class Adapter_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tWindow win = new WindowImp1();\t//实现接口对象\n\t\twin.open();\n\t\twin.close();\n\t}\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["设计模式"]},{"title":"Java自定义异常类","url":"/Java自定义异常类.html","content":"用户可以根据自己的需要定义自己的异常类，定义异常类只需要继承Exception类即可\n\n```\nclass MyException extends Exception{\t\t//自定义异常类，继承Exception类\n\tpublic MyException(String msg){\t\t\t//构造方法接受异常信息\n\t\tsuper(msg);\t\t\t\t\t\t\t\t\t\t\t//调用父类中的构造方法\n\t}\n}\n\n\n//主类\n//Function        : \tMyException_demo\npublic class MyException_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\ttry{\n\t\t\tthrow new MyException(\"自定义异常\");\t//抛出异常\n\t\t}catch(Exception e){\t\t\t\t\t\t\t\t\t\t\t//异常处理\n\t\t\tSystem.out.println(e);\n\t\t}\n\t\n\t}\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["Java"]},{"title":"k8s学习笔记——基本命令","url":"/k8s学习笔记——基本命令.html","content":"1. 进入pod，获取一个交互 TTY 并运行 /bin/bash\n\n```\nkubectl exec -it <pod-name> -n <namespace> bash\n\n```\n\n参考：[k8s 命令操作](https://zhuanlan.zhihu.com/p/343988121)\n\n2.创建namespace\n\n```\nkubectl create ns xxxx\n\n```\n\n3.查看所有namespace下的pod\n\n```\nkubectl get pod -A\n\n```\n\n查看特定namespace下的pod\n\n```\nkubectl get pod -n kube-system\n\n```\n\n查看所有的namespace\n\n```\nkubectl get namespace\nNAME              STATUS   AGE\ndefault           Active   4d\nkube-node-lease   Active   4d\nkube-public       Active   4d\nkube-system       Active   4d\n\n```\n\n查看所有的service\n\n```\nkubectl get svc -n kube-system\nNAME                                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                  AGE\nchart-1645713368-kubernetes-dashboard   NodePort    10.109.3.120   <none>        443:31392/TCP            4d\nkube-dns                                ClusterIP   10.96.0.10     <none>        53/UDP,53/TCP,9153/TCP   4d\n\n```\n\n编写svc\n\n```\nkubectl edit svc kubernetes-dashboard -n kube-system\n\n```\n\n查看所有的deployment，删除了deployment，pod也会自动删除\n\n```\nkubectl get deployment -A\n\n```\n\n查看所有的secret\n\n```\nkubectl get secrets -A\n\n```\n\n查看k8s的所有节点\n\n```\nkubectl get nodes --show-labels\n\n```\n\n查看所有k8s角色\n\n```\nkubectl get role -A\nNAMESPACE              NAME                                             CREATED AT\nkube-public            kubeadm:bootstrap-signer-clusterinfo             2022-02-24T13:55:25Z\nkube-public            system:controller:bootstrap-signer               2022-02-24T13:55:23Z\nkube-system            extension-apiserver-authentication-reader        2022-02-24T13:55:23Z\nkube-system            kube-proxy                                       2022-02-24T13:55:25Z\nkube-system            kubeadm:kubelet-config-1.21                      2022-02-24T13:55:24Z\nkube-system            kubeadm:nodes-kubeadm-config                     2022-02-24T13:55:24Z\nkube-system            system::leader-locking-kube-controller-manager   2022-02-24T13:55:23Z\nkube-system            system::leader-locking-kube-scheduler            2022-02-24T13:55:23Z\nkube-system            system:controller:bootstrap-signer               2022-02-24T13:55:23Z\nkube-system            system:controller:cloud-provider                 2022-02-24T13:55:23Z\nkube-system            system:controller:token-cleaner                  2022-02-24T13:55:23Z\nkube-system            system:persistent-volume-provisioner             2022-02-24T13:55:27Z\nkubernetes-dashboard   kubernetes-dashboard                             2022-03-02T16:10:23Z\n\n```\n\n查看所有的serviceaccount\n\n```\nkubectl get serviceaccount -A\n\n```\n\n<!--more-->\n&nbsp;\n\n4.删除pod\n\n```\nkubectl delete pod chart-1645714995-kubernetes-dashboard-6b6475d8db-kp7vc -n kube-system\n\n```\n\n删除pod后如果又重启，需要先删除deployment，副本中定义了pod的数量\n\n```\nkubectl get deployment -A\nNAMESPACE     NAME                                    READY   UP-TO-DATE   AVAILABLE   AGE\nkube-system   chart-1645714995-kubernetes-dashboard   1/1     1            1           4d1h\nkube-system   coredns                                 1/1     1            1           4d2h\n\nkubectl delete deployment chart-1645714995-kubernetes-dashboard -n kube-system\n\n```\n\n5.查看pod的信息\n\n```\nkubectl describe pod chart-1645714995-kubernetes-dashboard-6b6475d8db-x4jxw --namespace kube-system\n\n```\n\n6.查看k8s节点外部访问地址查询\n\n```\nkubectl cluster-info\n\n```\n\n7.如果pod的状态是CrashLoopBackOff，可以这样查看pod的日志\n\n```\nkubectl logs kubernetes-dashboard-764d688cff-b4rlz -n kube-system\n\n```\n\n8.滚动重启deployment\n\n```\nkubectl rollout restart -n your_ns deployment your_deployment\n\n```\n\n9.调整deployment的pod副本数量，--replicas后面跟想扩容或者缩容的pod数量\n\n```\nkubectl scale -n your_ns deployment your_deployment --replicas=2\n```\n\n　　\n\n&nbsp;\n","tags":["k8s"]},{"title":"mac安装多个版本python","url":"/mac安装多个版本python.html","content":"1.安装pyenv\n\n```\nbrew install pyenv\n\n```\n\n2.是否安装成功\n\n```\npyenv -v\npyenv 2.0.6\n\n```\n\n3.安装python3.8.10，2.7.15和miniconda3-4.7.12\n\n```\npyenv install 3.8.10\npyenv install 2.7.15\npyenv install miniconda3-4.7.12\n\n```\n\n查看可以安装的版本列表\n\n```\npyenv install --list\n\n```\n\n4.查看安装的python版本\n\n```\npyenv versions\n\n  system\n  2.7.15\n  3.7.10\n* 3.8.10 (set by /Users/lintong/.python-version)\n  miniconda3-4.7.12\n\n```\n\n即可选择现成的interpreter\n\n<img src=\"/images/517519-20210908150025300-339313529.png\" width=\"400\" height=\"50\" loading=\"lazy\" />\n\n5.目录切换interpreter\n\n```\npyenv local 3.8.10  # 当前目录及其目录切换\npython -V  # 验证一下是否切换成功\npyenv local --unset  # 解除local设置\n\n```\n\n全局`切换interpeter`\n\n```\npyenv global 3.8.10 # 不建议全局切换\npython -V  # 验证一下是否切换成功\npyevn global system  # 切换回系统版本\n\n```\n\nshell切换interpreter\n\n```\npyenv shell 3.8.10  # 当前shell会话切换\npython -V  # 验证一下是否切换成功\npyenv shell --unset  # 解除shell设置\n```\n\n如果遇到不能切换的情况，在~/.bash_profile添加\n\n```\n# pythpn\nexport PYENV_ROOT=\"$HOME/.pyenv\"\nexport PATH=\"$PYENV_ROOT/shims:$PATH\"\nif command -v pyenv 1>/dev/null 2>&amp;1; then\n eval \"$(pyenv init -)\"\nfi\n\n```\n\n6.使用中科大的源来pip install\n\n```\npip install -r ./requirements.txt -i https://pypi.mirrors.ustc.edu.cn/simple\n\n```\n\n或者使用清华的源\n\n```\npip install -i https://pypi.tuna.tsinghua.edu.cn/simple some-package\n\n```\n\n参考：[pypi 镜像使用帮助](https://mirrors.tuna.tsinghua.edu.cn/help/pypi/)\n\n7.如果需要创建虚拟环境，首先需要安装virtualenv\n\n```\nbrew install pyenv-virtualenv\n\n```\n\n8.创建和删除virtualenv\n\n```\npyenv virtualenv 3.8.10 env3.8.10\npyenv uninstall env3.8.10\n\n```\n\n<!--more-->\n&nbsp;\n\n参考：[Mac 安装和管理多个 Python 版本 ](https://juejin.cn/post/6844903861979709453)\n","tags":["mac"]},{"title":"使用Impala parser解析SQL","url":"/使用Impala parser解析SQL.html","content":"Impala对于hive引擎的语法和hive原生的有些许不同，虽然使用hive的parser也能部分兼容，但是由于impala的parser是使用**flex**（Fast Lexical Analyzer Generator，快速词法分析生成器）和**java cup**（Java Constructor of Useful Parsers，生成语法分析器（parser）的工具）开发的，所以对impala的query进行语法解析的时候建议还是使用Impala原生的parser\n\n1.在安装了impala的机器下找到impala-frontend的jar包（环境中的impala版本为2.12.0+cdh5.15.1+0）\n\n```\nlintong@master:/opt/cloudera/parcels/CDH/jars$ ls | grep impala-frontend\nimpala-frontend-0.1-SNAPSHOT.jar\n\n```\n\n2.使用mvn install安装到本地仓库中，或者上传到私服仓库中\n\n```\nmvn install:install-file -Dfile=/home/lintong/下载/impala-frontend-0.1-SNAPSHOT.jar -DgroupId=org.apache.impala -DartifactId=impala-frontend -Dversion=0.1-SNAPSHOT -Dpackaging=jar\n\n```\n\n3.在工程中引入impala-frontend和java-cup，java-cup的版本可以使用反编译工具打开impala-frontend的jar进行确认\n\n```\n<dependency>\n    <groupId>org.apache.impala</groupId>\n    <artifactId>impala-frontend</artifactId>\n    <version>0.1-SNAPSHOT</version>\n</dependency>\n<dependency>\n    <groupId>net.sourceforge.czt.dev</groupId>\n    <artifactId>java-cup</artifactId>\n    <version>0.11-a-czt02-cdh</version>\n</dependency>\n\n```\n\n在解析select语句的时候如果报\n\n```\njava.lang.NoClassDefFoundError: org/apache/sentry/core/model/db/DBModelAction\n\n\tat org.apache.impala.analysis.TableRef.<init>(TableRef.java:138)\n\tat org.apache.impala.analysis.CUP$SqlParser$actions.case421(SqlParser.java:18035)\n\tat org.apache.impala.analysis.CUP$SqlParser$actions.CUP$SqlParser$do_action(SqlParser.java:5976)\n\tat org.apache.impala.analysis.SqlParser.do_action(SqlParser.java:1349)\n\tat java_cup.runtime.lr_parser.parse(lr_parser.java:587)\n\tat com.xxxx.xx.core.parser.XXXXTest.getLineageInfo(XXXXTest.java:41)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:363)\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:137)\n\tat com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)\n\tat com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)\n\tat com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:220)\n\tat com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:53)\nCaused by: java.lang.ClassNotFoundException: org.apache.sentry.core.model.db.DBModelAction\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\t... 28 more\n\n\nProcess finished with exit code 255\n\n```\n\n在pom中添加\n\n```\n<dependency>\n    <groupId>org.apache.sentry</groupId>\n    <artifactId>sentry-core-model-db</artifactId>\n    <version>1.5.1-cdh5.15.1</version>\n</dependency>\n\n```\n\n4.参考Impala的源代码中parser的demo\n\n```\nhttps://github.com/cloudera/Impala/blob/master/fe/src/test/java/com/cloudera/impala/analysis/ParserTest.java\n\n```\n\n解析select和create kudu table等语句\n\n```\nimport org.apache.impala.analysis.*;\nimport java.io.StringReader;\n\n\nString impalaSelectQuery = \"SELECT `ds` FROM `db1`.`table1` WHERE (`ds`='test') OR (`ds`='2020-08-02') OR (`ds`='2020-08-01') LIMIT 100\"; // select语句\nString hiveSelectQuery = \"select city,array_contains(city, 'Paris') from default.arraydemo  limit 5\";\nString kuduCreateTableQuery = \"CREATE TABLE `db1`.`my_first_table`\\n\" +\n        \"(\\n\" +\n        \"  id BIGINT,\\n\" +\n        \"  name STRING,\\n\" +\n        \"  PRIMARY KEY(id)\\n\" +\n        \")\\n\" +\n        \"PARTITION BY HASH PARTITIONS 16\\n\" +\n        \"STORED AS KUDU\\n\" +\n        \"TBLPROPERTIES (\\n\" +\n        \"  'kudu.master_addresses' = 'hadoop01:7051,hadoop02:7051,hadoop03:7051', \\n\" +\n        \"  'kudu.table_name' = 'my_first_table'\\n\" +\n        \");\"; // kudu建表语句\nString invalidQuery = \"INVALIDATE METADATA db1.tb1\"; // 刷新元数据语句\nString refreshQuery = \"REFRESH db1.tb1 partition(ds='2021-05-02')\"; // 刷新元数据语句\nString computeQuery = \"COMPUTE INCREMENTAL STATS db1.tb1\"; // compute stats语句\nString describeQuery = \"Describe db1.tb1;\"; // describe语句\nString renameQuery = \"ALTER TABLE my_db.customers RENAME TO my_db.users;\"; // rename语句\nString addColQuery = \"ALTER TABLE db1.tb1 ADD COLUMNS (col1 string)\"; // add col语句\nString alterColQuery = \"ALTER TABLE db1.tb1 CHANGE col1 col2 bigint\"; // alter col语句\nString setQuery = \"set mem_limit = 5gb\";\nString useQuery = \"use default\";\nString query = impalaSelectQuery;\nSqlScanner input = new SqlScanner(new StringReader(query));\nSqlParser parser = new SqlParser(input);\nParseNode node = null;\ntry {\n    node = (ParseNode) parser.parse().value;\n    if (node instanceof SelectStmt) {\n        System.out.println(\"查询语句\"); // with语句也属于查询语句\n        SelectStmt selectStmt = (SelectStmt) node;\n        String databaseName = selectStmt.getTableRefs().get(0).getPath().get(0);\n        String tableName = selectStmt.getTableRefs().get(0).getPath().get(1);\n        System.out.println(databaseName);\n        System.out.println(tableName);\n    } else if (node instanceof CreateTableStmt) {\n        System.out.println(\"建表语句\");\n        CreateTableStmt createTableStmt = (CreateTableStmt) node;\n        System.out.println(createTableStmt.getTbl());\n        for (ColumnDef def : createTableStmt.getColumnDefs()) {\n            System.out.println(def.getColName() + \" \" + def.getTypeDef());\n        }\n    } else if (node instanceof ResetMetadataStmt) {\n        System.out.println(\"刷新元数据语句\");\n    } else if (node instanceof ComputeStatsStmt) {\n        System.out.println(\"compute stats语句\");\n    } else if (node instanceof DescribeTableStmt) {\n        System.out.println(\"describe语句\");\n    } else if (node instanceof AlterTableOrViewRenameStmt) {\n        System.out.println(\"rename语句\");\n    } else if (node instanceof AlterTableAddReplaceColsStmt) {\n        System.out.println(\"add col语句\");\n    } else if (node instanceof AlterTableAlterColStmt) {\n        System.out.println(\"alter col语句\");\n    } else if (node instanceof UseStmt) {\n        System.out.println(\"use语句\");\n    } else if (node instanceof SetStmt) {\n        System.out.println(\"set语句\");\n    } else {\n        System.out.println(node.getClass());\n    }\n} catch (Exception e) {\n    e.printStackTrace();\n    fail(\"\\nParser error:\\n\" + parser.getErrorMsg(query));\n}\n\n```\n\n输出\n\n```\n建表语句\nmy_first_table\nid BIGINT\nname STRING\n\n```\n\nimpala建textfile表语句\n\n```\ncreate table IF NOT EXISTS default.bbb (\n   column1 string,\n   column2 int,\n   column3 bigint\n);\n\n```\n\n不添加其他参数默认建立的是TEXTFILE格式的hive表\n\n```\n\tCREATE TABLE default.bbb (   column1 STRING,   column2 INT,   column3 BIGINT ) STORED AS TEXTFILE LOCATION 'hdfs://xx-nameservice/user/hive/warehouse/bbb'\n\n```\n\nimpala建parquet表语句\n\n```\ncreate table IF NOT EXISTS default.bbb (\n   column1 string,\n   column2 int,\n   column3 bigint\n)\nstored as parquet;\n\n```\n\n表结构\n\n```\nCREATE TABLE default.bbb (   column1 STRING,   column2 INT,   column3 BIGINT ) STORED AS PARQUET LOCATION 'hdfs://xx-nameservice/user/hive/warehouse/bbb'\n\n```\n\n　　\n\n<!--more-->\n&nbsp;\n","tags":["impala"]},{"title":"HTTP和websocket","url":"/HTTP和websocket.html","content":"参考：[[译] HTTP 的进化 - 0.9、1.0、1.1、Keep-Alive、Upgrade 和 HTTPS](https://juejin.cn/post/6844903604336197646)\n","tags":["计算机网络"]},{"title":"Java对象的多态性（转型）","url":"/Java对象的多态性（转型）.html","content":"**多态性**在面向对象中主要有两种体现：\n\n## <1>方法的重载与覆写\n\n## <2>对象的多态性\n\n向上转型：子类对象-->父类对象，向上转型会自动完成\n\n向下转型：父类对象-->子类对象，向下转型时，必须明确地指明转型的子类类型\n\n<!--more-->\n&nbsp;\n\n### **对象的向上转型**\n\n虽然使用的父类对象调用fun1方法，但是实际上调用的方法是被子类覆写过的方法，也就是说，如果对象发生了向上转型关系后，所调用的方法一定是被子类覆写过的方法。\n\n但是父类的a无法调用b类中的fun3方法，因为这个方法只在子类中定义，而没有在父类中定义。\n\n```\nclass C_1{\t\t\t\t\t\t\t\t\t\t\t\t// 定义接口C_1\n\tpublic void fun1(){\t\t\t\t\t\t\t//定义fun1()方法\n\t\tSystem.out.println(\"C_1--->public void fun1\");\n\t}\n\t\n\tpublic void fun2(){\t\t\t\t\t\t\t//定义fun2()方法\n\t\tthis.fun1();\n\t}\n}\n\nclass D_1 extends C_1{\t\t\t\t\t\t\t\t\t\t\t\t//子类D_1继承父类C_1\n\tpublic void fun1(){\n\t\tSystem.out.println(\"D_1--->public void fun1\");\t//覆写父类中的fun1()方法\n\t}\n\t\n\tpublic void fun3(){\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\tSystem.out.println(\"D_1--->public void fun3\");\t//子类自己定义方法\n\t}\n}\n\n//\t对象的多态性，对象向上转型\npublic class ploy_up_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tD_1 b = new D_1();\t\t\t//定义子类的实例化对象\n\t\tC_1 a = b;\t\t\t\t\t\t//声明一个父类，发生向上转型的关系，子类赋值给父类\n\t\ta.fun1();\t\t\t\t\t\t\t//此方法被子类覆写过，虽然a是父类，但是调用的是子类的fun1()方法\n\t}\n\n}\n\n```\n\n&nbsp;\n\n### **对象的向下转型**\n\n在子类中调用了父类中的fun2方法，fun2方法要调用fun1方法，但是此时fun1方法已经被子类覆写过了，所以调用fun2方法的时候还是调用被子类覆写过的方法\n\n在进行对象的向下转型之前，必须首先发生对象的向上转型，否则将出现对象转换异常\n\n```\nclass C_1{\t\t\t\t\t\t\t\t\t\t\t\t// 定义接口C_1\n\tpublic void fun1(){\t\t\t\t\t\t\t//定义fun1()方法\n\t\tSystem.out.println(\"C_1--->public void fun1\");\n\t}\n\t\n\tpublic void fun2(){\t\t\t\t\t\t\t//定义fun2()方法\n\t\tthis.fun1();\n\t}\n}\n\nclass D_1 extends C_1{\t\t\t\t\t\t\t\t\t\t\t\t//子类D_1继承父类C_1\n\tpublic void fun1(){\n\t\tSystem.out.println(\"D_1--->public void fun1\");\t//覆写父类中的fun1()方法\n\t}\n\t\n\tpublic void fun3(){\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\tSystem.out.println(\"D_1--->public void fun3\");\t//子类自己定义方法\n\t}\n}\n\n//\t对象的多态性，对象向上转型\npublic class ploy_up_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n//\t\tD_1 b = new D_1();\t\t\t//定义子类的实例化对象\n//\t\tC_1 a = b;\t\t\t\t\t\t//声明一个父类，发生向上转型的关系，子类赋值给父类\n//\t\ta.fun1();\t\t\t\t\t\t\t//此方法被子类覆写过，虽然a是父类，但是调用的是子类的fun1()方法\n\t\t\n\t\tC_1 c = new D_1();\t\t\t//声明一个父类，发生了向上转型，子类赋值给父类\n\t\tD_1 d = (D_1)c;\t\t\t\t//声明的父类强制转换成子类，发生了向下转型关系\n\t\td.fun1();\n\t\td.fun2();\n\t\td.fun3();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n多态的应用\n\n设计一个方法，要求此方法可以接受A类的任意子类对象，并调用方法。\n\n```\nclass C_1{\t\t\t\t\t\t\t\t\t\t\t\t// 定义接口C_1\n\tpublic void fun1(){\t\t\t\t\t\t\t//定义fun1()方法\n\t\tSystem.out.println(\"C_1--->public void fun1\");\n\t}\n\t\n\tpublic void fun2(){\t\t\t\t\t\t\t//定义fun2()方法\n\t\tthis.fun1();\n\t}\n}\n\nclass D_1 extends C_1{\t\t\t\t\t\t\t\t\t\t\t\t//子类D_1继承父类C_1\n\tpublic void fun1(){\n\t\tSystem.out.println(\"D_1--->public void fun1\");\t//覆写父类中的fun1()方法\n\t}\n\t\n\tpublic void fun3(){\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\tSystem.out.println(\"D_1--->public void fun3\");\t//子类自己定义方法\n\t}\n}\n\nclass E_1 extends C_1{\t\t\t\t\t\t\t\t\t\t\t\t//子类E_1继承父类C_1\n\tpublic void fun1(){\n\t\tSystem.out.println(\"E_1--->public void fun1\");\t//覆写父类中的fun1()方法\n\t}\n\t\n\tpublic void fun5(){\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\tSystem.out.println(\"E_1--->public void fun3\");\t//子类自己定义方法\n\t}\n}\n\n//\t对象的多态性，对象向上转型\npublic class ploy_up_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n//\t\tD_1 b = new D_1();\t\t\t//定义子类的实例化对象\n//\t\tC_1 a = b;\t\t\t\t\t\t//声明一个父类，发生向上转型的关系，子类赋值给父类\n//\t\ta.fun1();\t\t\t\t\t\t\t//此方法被子类覆写过，虽然a是父类，但是调用的是子类的fun1()方法\n\t\t\n//\t\tC_1 c = new D_1();\t\t\t//声明一个父类，发生了向上转型，子类赋值给父类\n//\t\tD_1 d = (D_1)c;\t\t\t\t//声明的父类强制转换成子类，发生了向下转型关系\n//\t\td.fun1();\n//\t\td.fun2();\n//\t\td.fun3();\n\t\t\n\t\tfun(new C_1());\t\t\t\t\t\t\t//传递C_1类的实例，产生向上转型\n\t\tfun(new D_1());\t\t\t\t\t\t\t//传递D_1类的实例，产生向上转型\n\t}\n\t\n\tpublic static void fun(C_1 c){\t//接收父类对象，不用写多次分别接收子类对象\n\t\tc.fun1();\n\t}\n}\n\n```\n\n&nbsp;\n","tags":["Java"]},{"title":"airflow学习笔记——operator","url":"/airflow学习笔记——operator.html","content":"operator用于产生特定的DAG节点\n\n```\nhttps://airflow.apache.org/docs/apache-airflow/stable/python-api-ref.html#operators\n\n```\n\n<!--more-->\n&nbsp;\n\n下面是常用的operator及其用法\n\n## BaseOperator\n\n```\nhttps://airflow.apache.org/docs/apache-airflow/stable/_modules/airflow/models/baseoperator.html\n\n```\n\n用法：\n\n&nbsp;\n\n## BaseSensorOperator\n\n```\nhttps://airflow.apache.org/docs/apache-airflow/stable/_modules/airflow/sensors/base.html#BaseSensorOperator.poke\n\n```\n\n用法：\n\n&nbsp;\n\n## MySQLToS3Operator\n\n这个已经过时了，建议使用SqlToS3Operator\n\n```\nhttps://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/_modules/airflow/providers/amazon/aws/transfers/mysql_to_s3.html\n\n```\n\n用法：\n\n&nbsp;\n\n## SqlToS3Operator\n\n```\nhttps://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/transfer/sql_to_s3.html#howto-operator-sqltos3operator\n\n```\n\n用法：\n\n&nbsp;\n\n## MongoToS3Operator\n\n```\nhttps://airflow.apache.org/docs/apache-airflow-providers-amazon/1.2.0/_modules/airflow/providers/amazon/aws/transfers/mongo_to_s3.html\n\n```\n\n用法：\n\n&nbsp;\n\n## BashOperator：执行bash命令\n\n```\nhttps://airflow.apache.org/docs/apache-airflow/stable/howto/operator/bash.html\n\n```\n\n&nbsp;\n\n## PythonOperator：调用python代码\n\n```\nhttps://airflow.apache.org/docs/apache-airflow/stable/howto/operator/python.html\n\n```\n\n&nbsp;\n\n## SparkOperators\n\n```\nhttps://airflow.apache.org/docs/apache-airflow-providers-apache-spark/stable/operators.html\n\n```\n\n&nbsp;\n\n## EmailOperator：发送邮件\n\n```\nhttps://bhavaniravi.com/blog/sending-emails-from-airflow/\n\n```\n\n&nbsp;\n\n## HTTPOperator : 发送 HTTP 请求\n\n```\nhttps://airflow.apache.org/docs/apache-airflow-providers-http/stable/operators.html\n\n```\n\n&nbsp;\n\n## MySqlOperator&nbsp;: 执行 SQL 命令\n\n```\nhttps://airflow.apache.org/docs/apache-airflow-providers-mysql/stable/operators.html\n\n```\n\n&nbsp;\n\n## SSHExecuteOperator，airflow1.9.0之后废除，改成&nbsp;SSHOperator\n\n```\nhttps://newbedev.com/airflow-how-to-ssh-and-run-bashoperator-from-a-different-server\n\n```\n\n&nbsp;\n\n## SparkSubmitOperator：提交spark任务\n\n```\nhttps://airflow.apache.org/docs/apache-airflow-providers-apache-spark/stable/_api/airflow/providers/apache/spark/operators/spark_submit/index.html#airflow.providers.apache.spark.operators.spark_submit.SparkSubmitOperator\n\n```\n\n　　\n\n除了这些基本的构建块之外，还有更多的特定处理器：DockerOperator，HiveOperator，S3FileTransferOperator，PrestoToMysqlOperator，SlackOperator\n\n&nbsp;\n","tags":["Airbnb"]},{"title":"airflow学习笔记——sensor","url":"/airflow学习笔记——sensor.html","content":"sensor也是airflow的一种operator，用于检测某个条件是否达到。如果条件满足，sensor将会执行成功；如果条件不满足，sensor将会重试，直到超时，task超时的时候状态就位skipped。\n\n下面是常用的几种sensor：\n\n- **The FileSensor**: Waits for a file or folder to land in a filesystem.\n- **The S3KeySensor**: Waits for a key to be present in a S3 bucket.\n- **The SqlSensor**: Runs a sql statement repeatedly until a criteria is met.\n- **The HivePartitionSensor**: Waits for a partition to show up in Hive.\n- **The ExternalTaskSensor**: Waits for a different DAG or a task in a different DAG to complete for a specific execution date. (Pretty useful that one 🤓 )\n- **The DateTimeSensor**: Waits until the specified datetime (Useful to add some delay to your DAGs)\n- **The TimeDeltaSensor**: Waits for a timedelta after the task&rsquo;s execution_date + schedule interval (Looks similar to the previous one no?)\n\n参考：[Airflow Sensors : What you need to know](https://marclamberti.com/blog/airflow-sensors/)\n\n以及<!--more-->\n&nbsp;[How Airbnb Built &ldquo;Wall&rdquo; to prevent data bugs](https://medium.com/airbnb-engineering/how-airbnb-built-wall-to-prevent-data-bugs-ad1b081d6e8f)\n\n&nbsp;\n","tags":["Airbnb"]},{"title":"Java数据结构——红黑树","url":"/Java数据结构——红黑树.html","content":"**二叉树：**查找时间复杂度：最好：<img src=\"/images/math?formula=O(lgn)\" alt=\"O(lgn)\" class=\"math-inline\" />,最差<img src=\"/images/math?formula=O(n)\" alt=\"O(n)\" class=\"math-inline\" />。最差情况是所有的数据全部在一端时。<br />\n\n\n**二叉搜索树（二叉排序树、二叉查找树）：**查找时间复杂度：最好：<img src=\"/images/math?formula=O(lgn)\" alt=\"O(lgn)\" class=\"math-inline\" />,最差<img src=\"/images/math?formula=O(n)\" alt=\"O(n)\" class=\"math-inline\" />。最差情况是所有的数据全部在一端时。<br />\n\n\n**平衡二叉树：**查找时间复杂度：<img src=\"/images/math?formula=O(lgn)\" alt=\"O(lgn)\" class=\"math-inline\" /><br />\n\n\n**红黑树：**查找删除插入时间复杂度：<img src=\"/images/math?formula=O(lgn)\" alt=\"O(lgn)\" class=\"math-inline\" /><!--more-->\n&nbsp; 红黑树是一种**自平衡的二叉排序树**，它是复杂的，但它的操作有着良好的最坏情况运行时间，并且在实践中是高效的: 它可以在O(logn)时间内做查找，插入和删除，这里的n是树中元素的数目。\n\n<img src=\"/images/517519-20221107204607580-564935163.png\" width=\"400\" height=\"313\" loading=\"lazy\" /><img src=\"/images/517519-20221107204628024-239438921.png\" width=\"400\" height=\"267\" loading=\"lazy\" />\n\n&nbsp;\n\n\n\n红黑树(一棵自平衡的排序二叉树)五大特性：\n\n1）每个结点要么是红的，要么是黑的。&nbsp;\n\n2）根结点是黑的。&nbsp;\n\n3）每个叶结点，即空结点是黑的。&nbsp;\n\n4）如果一个结点是红的，那么它的俩个儿子都是黑的。&nbsp;\n\n5）对每个结点，从该结点到其子孙结点的所有路径上包含相同数目的黑结点。\n\n&nbsp;\n\n场景\n\n1）广泛用于C++的STL中,map和set都是用红黑树实现的.\n\n2）著名的linux进程调度Completely Fair Scheduler,用红黑树管理进程控制块,进程的虚拟内存区域都存储在一颗红黑树上,每个虚拟地址区域都对应红黑树的一个节点,左指针指向相邻的地址虚拟存储区域,右指针指向相邻的高地址虚拟地址空间.\n\n3）IO多路复用epoll的实现采用红黑树组织管理sockfd，以支持快速的增删改查.\n\n4）ngnix中,用红黑树管理timer,因为红黑树是有序的,可以很快的得到距离当前最小的定时器.\n\n5）java中的TreeSet,TreeMap\n\n&nbsp;\n","tags":["数据结构"]},{"title":"Java继承","url":"/Java继承.html","content":"**Java是单继承，只能继承一个父类，但是可以实现多个接口**\n\n继承的子类不能直接访问父类中的私有属性，只能通过get和set方法来访问\n\n在继承的操作中，子类对象在实例化之前必须首先调用父类中的构造方法后再调用子类自己的构造方法。\n\n```\nclass person{\n\tprivate String name;\n\tprivate int age;\n\t\n\tpublic String getName() {\n\t\treturn name;\n\t}\n\tpublic void setName(String name) {\n\t\tthis.name = name;\n\t}\n\tpublic int getAge() {\n\t\treturn age;\n\t}\n\tpublic void setAge(int age) {\n\t\tthis.age = age;\n\t}\n}\n\nclass student extends person{\n\tprivate String school;\n\n\tpublic String getSchool() {\n\t\treturn school;\n\t}\n\n\tpublic void setSchool(String school) {\n\t\tthis.school = school;\n\t}\n}\n\n\npublic class extends_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tstudent person_1 = new student();\n\t\tperson_1.setName(\"张三\");\n\t\tperson_1.setAge(10);\n\t\tperson_1.setSchool(\"涵三中\");\n\t\tSystem.out.println(\"姓名:\"+person_1.getName()+\"\\n\"+\"年龄:\"+person_1.getAge()+\"\\n\"+\"学校:\"+person_1.getSchool());\n\t}\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n\n**方法的覆写**\n\n方法的覆写就是指子类定义了与父类中同名的方法，但是在方法覆写时候必须考虑到权限，即被子类覆写的方法不能拥有比父类方法更加严格的访问权限。\n\n访问权限：private default public　　private<default<public\n\n如果在父类中使用public定义的方法，则子类的访问权限必须是public，否则无法编译\n\n如果将父类的一个方法定义成private访问权限，在子类中将此方法声明位default访问权限，这不算是覆写\n\n**调用父类的属性时，直接使用super.XXX**\n\n```\nclass person{\n\tprivate String name;\n\tprivate int age;\n\t\n\tpublic String getName() {\n\t\treturn name;\n\t}\n\tpublic void setName(String name) {\n\t\tthis.name = name;\n\t}\n\tpublic int getAge() {\n\t\treturn age;\n\t}\n\tpublic void setAge(int age) {\n\t\tthis.age = age;\n\t}\n\t\n\t//方法：print\n\tvoid print(){\t//定义一个默认访问权限的方法\n\t\tSystem.out.println(\"Person---->void print()\");\n\t}\n}\n\nclass student extends person{\n\tprivate String school;\n\n\tpublic String getSchool() {\n\t\treturn school;\n\t}\n\n\tpublic void setSchool(String school) {\n\t\tthis.school = school;\n\t}\n\t\n\t//方法：print\n\tpublic void print(){\t//覆写父类中的方法，扩大了权限\n\t\tSystem.out.println(\"student---->void print()\");\n\t}\n\t\n}\n\n\npublic class extends_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tstudent person_1 = new student();\n\t\tperson_1.setName(\"张三\");\n\t\tperson_1.setAge(10);\n\t\tperson_1.setSchool(\"涵三中\");\n\t\tSystem.out.println(\"姓名:\"+person_1.getName()+\"\\n\"+\"年龄:\"+person_1.getAge()+\"\\n\"+\"学校:\"+person_1.getSchool());\n\t\tnew student().print();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n&nbsp;\n\n继承的应用\n\n定义一个整型数组类，要求包含构造方法，增加数据及输出数据成员方法，并利用数组实现动态内存分配。在此基础上定义出以下两个子类。\n\n<1>排序类：实现排序\n\n<2>反转类：实现数据反向存放\n\n```\nclass Array{\n\tprivate int temp[];\t//定义一个整形数组，此数字的大小由传入的len决定\n\tprivate int foot;\t\t//定义数组的当前元素下标\n\t\n\tpublic Array(int len){\t//数组的大小由len决定,构造方法\n\t\tif(len>0){\t\t\t\t\t\t//判断传入的长度是否大于0\n\t\t\tthis.temp = new int[len];\t\t//根据传入的大小开辟空间\n\t\t}else{\t\t\t\t\t\t\t\t\t\n\t\t\tthis.temp = new int[1];\t\t//否则只开辟长度为1的空间\n\t\t}\n\t}\n\t\n\tpublic boolean add(int i){\t\t\t\n\t\tif(this.foot < this.temp.length){\t//判断数组是否已经满了\n\t\t\tthis.temp[foot] = i;\t\t\t\t\t//没有存满则继续添加\n\t\t\tfoot++;\t\t\t\t\t\t\t\t\t\t//修改下标\n\t\t\treturn true;\t\t\t\t\t\t\t\t//添加成功\n\t\t}else{\n\t\t\treturn false;\t\t\t\t\t\t\t\t//添加失败，已经存满了\n\t\t}\n\t}\n\t\n\tpublic int[] getArray(){\t\t\t\t\t//得到全部的数组\n\t\treturn this.temp;\n\t}\n\t\n}\n\nclass array_Sort extends Array{\n\n\tpublic array_Sort(int len) {\n\t\tsuper(len);\n\t\t// TODO 自动生成的构造函数存根\n\t}\n\t\n\tpublic int[] getArray(){\t\t\t\t\t\t\t\t//覆写父类中的方法\n\t\tjava.util.Arrays.sort(super.getArray());\t//排序操作 \n\t\treturn super.getArray();\t\t\t\t\t\t\t//使用父类中的方法输出数组\n\t}\n\t\n\tpublic void print_array_Sort(){\t\t\t\t\t//输出排序之后的数组\n\t\tfor(int i=0;i<this.getArray().length;i++){\t\t//调用getArray()方法\n\t\tSystem.out.println(this.getArray()[i]);\n\t\t}\n\n\t}\n}\n\nclass array_Reverse extends Array{\n\n\tpublic array_Reverse(int len) {\n\t\tsuper(len);\n\t\t// TODO 自动生成的构造函数存根\n\t}\n\t\n\tpublic int[] getArray(){\t\t\t\t\t\t\t\t//覆写父类中的方法\n\t\tint temp_Reverse[] = new int[super.getArray().length];\n\t\tfor(int i=0;i<temp_Reverse.length;i++){\n\t\t\ttemp_Reverse[i] = super.getArray()[temp_Reverse.length-1-i];\n\t\t} \n\t\treturn temp_Reverse;\t\t\t\t\t\t\t//使用父类中的方法输出数组\n\t}\n\t\n\tpublic void print_array_Reverse(){\t\t\t\t\t//输出之后的数组\n\t\tfor(int i=0;i<this.getArray().length;i++){\t\t//调用getArray()方法\n\t\tSystem.out.println(this.getArray()[i]);\n\t\t}\n\t}\n}\n\npublic class array_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tarray_Sort array_1 = new array_Sort(5);\n\t\tarray_1.add(10);\n\t\tarray_1.add(9);\n\t\tarray_1.add(8);\n\t\tarray_1.add(7);\n\t\tarray_1.add(6);\n\t\tarray_1.print_array_Sort();\n\t\t\n\t\tarray_Reverse array_2 = new array_Reverse(5);\n\t\tarray_2.add(10);\n\t\tarray_2.add(9);\n\t\tarray_2.add(8);\n\t\tarray_2.add(7);\n\t\tarray_2.add(6);\n\t\tarray_2.print_array_Reverse();\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["Java"]},{"title":"gson学习笔记","url":"/gson学习笔记.html","content":"1. 字符串转JsonObject\n\n```\nJsonObject origJson = new JsonParser().parse(str).getAsJsonObject();\n\n```\n\n2. JsonObject转字符串\n\n```\nString str = outputJson.toString();\n\n```\n\n3. List转JsonArray\n\n```\nJsonArray array = new Gson().toJsonTree(list).getAsJsonArray();\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["google"]},{"title":"机器学习——LightGBM","url":"/机器学习——LightGBM.html","tags":["ML"]},{"title":"Thrift通信协议","url":"/Thrift通信协议.html","content":"Thrift的通信协议主要有下面几种：\n\nTBinaryProtocol：二进制协议\n\nTCompactProtocol：带压缩的二进制协议\n\nTJSONProtocol：Json协议，序列化结果例如\n\n```\n{\"2\":{\"i64\":1},\"3\":{\"str\":\"lintong\"},\"4\":{\"lst\":[\"i64\",3,1,2,3]}}\n\n```\n\nTSimpleJSONProtocol：simple Json协议，该协议为只写，即不可反序列化，序列化结果例如\n\n```\n{'name': 'XiaoMing', 'age': '20'}\n\n```\n\nTMultiplexedProtocol\n\nTTupleProtocol\n\n<!--more-->\n&nbsp;\n\n参考：[Thrift序列化协议浅析](https://andrewpqc.github.io/2019/02/24/thrift/)\n\n[Thrift的TBinaryProtocol二进制协议分析](https://www.cnblogs.com/voipman/p/5125278.html)\n","tags":["Thrift"]},{"title":"ElasticSearch学习笔记——配置参数","url":"/ElasticSearch学习笔记——配置参数.html","content":"## 集群参数\n\n### 1.cluster.name 集群名称\n\n也可以在注释后在es的启动命令添加<!--more-->\n&nbsp;-Ecluster.name=XXX，默认为elasticsearch\n\n&nbsp;\n\n## 节点参数\n\n### 1.node.name 节点名称\n\n也可以在注释后在es的启动命令添加 -Enode.name=XXX\n\n### 2.node.name 节点角色\n\n有如下参数：\n\nnode.master: false\n\nnode.data: true\n\nnode.ingest: false\n\nnode.ml: true\n\nxpack.ml.enabled: true\n\n参考：[笔记五十二：常见的集群部署方式](https://learnku.com/articles/40718)\n\n<img src=\"/images/517519-20210427105828307-579430584.png\" width=\"550\" height=\"280\" loading=\"lazy\" />\n\n&nbsp;\n\nElasticSearch node的角色可以分成5种：\n\n1.Master eligible nodes：主节点：\n\n　　负责集群状态（cluster state）的管理，使用低配置的 CPU，RAM 和磁盘\n\n　　从高可用 &amp; 避免脑裂的角色出发：一般在生产环境中配置 3 台；可以有多个master节点，但是同时只能有一个活跃的主节点，在cerebro上活跃的主节点会带有*号；负载分片管理，索引创建，集群管理等操作\n\n　　如果和数据节点或者 Coordinate 节点混合部署：数据节点相对有比较大的内存占用；Coordinate 节点有时候可能会有开销很高的查询，导致 OOM；这些都有可能影响 Master 节点，导致集群的不稳定\n\n2.Data Node：数据节点：\n\n　　负责数据存储及处理客户端请求，使用高配置的 CPU，RAM 和磁盘\n\n3.Coordinating Node：协调节点，也称为client节点：\n\n　　生产环境中，建议为一些大的集群配置 Coordinating Only Nodes，Medium / High CPU; Medium / High RAM; Low Disk\n\n　　扮演 Load Balancers。 降低 Master 和 Data Nodes 的负载\n\n　　负载搜索结果的 Gather / Reduce\n\n　　有时候无法预知客户端会发生怎样的请求\n\n　　大量占用内存的结合操作，一个深度聚合可能引发 OOM\n\n4.Ingest Node：ingest节点：\n\n　　负责数据处理，使用高配置的 CPU ; 中等配置的 RAM; 低配置的磁盘\n\n5.Machine learning：机器学习节点\n\n### 2.node.attr 节点属性\n\nElasticsearch支持给节点打标签，具体方式是在elasticsearch.yml文件中增加\n\nnode.attr.{attribute}: {value}\n\n比如用于进行节点的业务隔离或者冷热分离，business1，business2，hot，warm等等，也可以注释后在es的启动命令添加&nbsp;-Enode.attr.box_type=$xxxx\n\n参考：[Elasticsearch冷热分离原理和实践](https://cloud.tencent.com/developer/article/1544261)\n\n也可以用于机架的隔离&nbsp;node.attr.rack: r1\n\n&nbsp;\n\n## 路径参数\n\n### 1.path.data data路径\n\n比如&nbsp;path.data: /data01/xxx, /data02/xxx\n\n也可以在启动命令中添加 -Epath.data=$datadir\n\n### 2.path.logs log路径\n\n比如path.logs: /data01/logs/xxx\n\n也可以在启动命令中添加&nbsp;-Epath.logs=$logdir\n\n## 内存参数\n\n### 1bootstrap.memory_lock&nbsp;\n\n锁定jvm只用内存，不使用硬盘\n\n## 网络参数\n\n### 1.network.host&nbsp;`绑定监听IP`\n\n默认是注释的\n\n如果不限制主机的访问和节点的交互（client节点需要这么配置），可以修改成0.0.0.0\n\n如果发布给集群中其他节点知道的地址，可以修改成\"_bond1:ipv4_\"\n\n### 2.http.port http端口\n\n也可以在启动命令中添加&nbsp;-Ehttp.port=$http_port；其他还有tcp的端口，比如&nbsp;-Etransport.tcp.port=$tcp_port\n\n### 3.http.enabled&nbsp;是否为 Elasticsearch 服务启用 HTTP\n\n如果是client节点的话，true，并且\n\nhttp.cors.enabled: true<br />http.cors.allow-origin: \"*\"\n\n如果是其他节点的话，false\n\n参考：[Elasticsearch 服务配置属性&nbsp;](https://www.ibm.com/docs/zh/bpm/8.5.6?topic=service-elasticsearch-configuration-properties)\n\n### 3.其他http参数\n\n参考：[ElasticSearch 配置文件](https://coyotey.gitbooks.io/elasticsearch/content/chapter5.html)\n\n&nbsp;\n\n## 发现机制参数\n\n### 1.discovery.zen.ping.unicast.hosts&nbsp;&nbsp;集群中其他节点ip:tcp端口，用来联系集群中其他节点，一般只用配置主节点地址即可\n\n比如&nbsp;discovery.zen.ping.unicast.hosts: [\"host1:9300\",\"host2:9300\",\"host3:9300\",\"host4:9300\",\"host5:9300\"]\n\n### 2.discovery.zen.minimum_master_nodes&nbsp;最小主节点数目，为了防止脑裂，需要设置发现超过半数的主节点候选者才能组成集群\n\n比如节点的主节点数量为5，这里就是3；如果是3，那么就是2\n\n&nbsp;\n\n## 线程池参数\n\n### 1.thread_pool.bulk.queue_size&nbsp;\n\nThe&nbsp;`queue_size`&nbsp;allows to control the size of the queue of pending requests that have no threads to execute them. By default, it is set to&nbsp;`-1`&nbsp;which means its unbounded. When a request comes in and the queue is full, it will abort the request.\n\n比如设置成5000\n\n参考：https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-threadpool.html\n","tags":["ELK"]},{"title":"Java内部类和包装类","url":"/Java内部类和包装类.html","content":"在类Outer的内部再定义一个类Inner，此时类inner就称为**内部类**，而类outer则称为外部类。\n\n内部类的唯一好处就是可以方便的访问外部类中的私有属性\n\n```\nclass outer{\n\tprivate String info = \"Hello World!\";\n\tclass inner{\n\t\tpublic void print(){\n\t\t\tSystem.out.println(info);\n\t\t}\n\t};\n\t\n\tpublic void fun(){\n\t\tnew inner().print();\n\t}\n};\n\npublic class inner_test {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tnew outer().fun();\n\t}\n\n}\n\n```\n\n<!--more-->\n&nbsp;\n\n使用static定义内部类\n\n使用static声明的内部类变成了外部类，但是用static声明的内部类不能方位非static申明的外部类属性。\n\n其中inner()要有static\n\n```\nclass outer{\n\tprivate static String info = \"Hello World!\";\n\tstatic class inner{\n\t\tpublic void print(){\n\t\t\tSystem.out.println(info);\n\t\t}\n\t};\n\t\n\npublic class inner_test {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tnew outer.inner().print();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n在外部访问内部类\n\n一个内部类除了可以通过外部类访问，也可以直接在其他类中进行调用，调用的基本格式：\n\n外部类.内部类 内部类对象 = 外部类实例.new 内部类();\n\n&nbsp;\n\n```\npublic class Outer{\nprivate static int i = 1;\nprivate int j=10;\nprivate int k=20;\npublic static void outer_f1(){\n    //do more something\n}\npublic void out_f2(){\n    //do more something\n}\n\n//成员内部类\nclass Inner{\n//static int inner_i =100; //内部类中不允许定义静态变量\nint j=100;//内部类中外部类的实例变量可以共存\nint inner_i=1;\nvoid inner_f1(){\n    System.out.println(i);//外部类的变量如果和内部类的变量没有同名的，则可以直接用变量名访问外部类的变量\n    System.out.println(j);//在内部类中访问内部类自己的变量直接用变量名\n    System.out.println(this.j);//也可以在内部类中用\"this.变量名\"来访问内部类变量\n    //访问外部类中与内部类同名的实例变量可用\"外部类名.this.变量名\"。\n    System.out.println(k);//外部类的变量如果和内部类的变量没有同名的，则可以直接用变量名访问外部类的变量\n    outer_f1();\n    outer_f2();\n}\n}\n//外部类的非静态方法访问成员内部类\npublic void outer_f3(){\n    Inner inner = new Inner();\n    inner.inner_f1();\n}\n\n//外部类的静态方法访问成员内部类，与在外部类外部访问成员内部类一样\npublic static void outer_f4(){\n    //step1 建立外部类对象\n    Outer out = new Outer();\n    //***step2 根据外部类对象建立内部类对象***\n    Inner inner=out.new Inner();\n    //step3 访问内部类的方法\n    inner.inner_f1();\n}\n\npublic static void main(String[] args){\n    outer_f4();\n}\n}\n\n```\n\n&nbsp;\n\n```\npublic class Outer {\n private int s = 100;\n private int out_i = 1;\n public void f(final int k){\n  final int s = 200;\n  int i = 1;\n  final int j = 10;\n  class Inner{ //定义在方法内部\n   int s = 300;//可以定义与外部类同名的变量\n   //static int m = 20;//不可以定义静态变量\n   Inner(int k){\n    inner_f(k);\n   }\n   int inner_i = 100;\n   void inner_f(int k){\n    System.out.println(out_i);//如果内部类没有与外部类同名的变量，在内部类中可以直接访问外部类的实例变量\n    System.out.println(k);//*****可以访问外部类的局部变量(即方法内的变量)，但是变量必须是final的*****\n//    System.out.println(i);\n    System.out.println(s);//如果内部类中有与外部类同名的变量，直接用变量名访问的是内部类的变量\n    System.out.println(this.s);//用\"this.变量名\" 访问的也是内部类变量\n    System.out.println(Outer.this.s);//用外部\"外部类类名.this.变量名\" 访问的是外部类变量\n   }\n  }\n  new Inner(k);\n }\n\n public static void main(String[] args) {\n //访问局部内部类必须先有外部类对象\n  Outer out = new Outer();\n  out.f(3);\n }\n\n}\n\n```\n\n&nbsp;\n\n```\npublic class Outer {\n private static int i = 1;\n private int j = 10;\n public static void outer_f1(){\n \n }\n public void outer_f2(){\n \n }\n// 静态内部类可以用public,protected,private修饰\n// 静态内部类中可以定义静态或者非静态的成员\n static class Inner{\n  static int inner_i = 100;\n  int inner_j = 200;\n  static void inner_f1(){\n   System.out.println(\"Outer.i\"+i);//静态内部类只能访问外部类的静态成员\n   outer_f1();//包括静态变量和静态方法\n  }\n  void inner_f2(){\n//   System.out.println(\"Outer.i\"+j);//静态内部类不能访问外部类的非静态成员\n//   outer_f2();//包括非静态变量和非静态方法\n  } \n \n }\n \n public void outer_f3(){\n//  外部类访问内部类的静态成员：内部类.静态成员\n  System.out.println(Inner.inner_i);\n  Inner.inner_f1();\n//  外部类访问内部类的非静态成员:实例化内部类即可\n  Inner inner = new Inner();\n  inner.inner_f2();\n \n }\n public static void main(String[] args) {\n  new Outer().outer_f3();\n }\n\n}\n\n```\n\n&nbsp;\n\n```\nclass People\n{\n  run();\n}\nclass Machine{\n   run();\n}\n\n```\n\n```\npublic class Outer {\n private static int i = 1;\n private int j = 10;\n public static void outer_f1(){\n \n }\n public void outer_f2(){\n \n }\n// 静态内部类可以用public,protected,private修饰\n// 静态内部类中可以定义静态或者非静态的成员\n static class Inner{\n  static int inner_i = 100;\n  int inner_j = 200;\n  static void inner_f1(){\n   System.out.println(\"Outer.i\"+i);//静态内部类只能访问外部类的静态成员\n   outer_f1();//包括静态变量和静态方法\n  }\n  void inner_f2(){\n//   System.out.println(\"Outer.i\"+j);//静态内部类不能访问外部类的非静态成员\n//   outer_f2();//包括非静态变量和非静态方法\n  }\n }\n \n public void outer_f3(){\n//  外部类访问内部类的静态成员：内部类.静态成员\n  System.out.println(Inner.inner_i);\n  Inner.inner_f1();\n//  外部类访问内部类的非静态成员:实例化内部类即可\n  Inner inner = new Inner();\n  inner.inner_f2();\n \n }\n public static void main(String[] args) {\n  new Outer().outer_f3();\n }\n\n}\n\n```\n\n&nbsp;\n\n注：一个匿名内部类一定是在new的后面，用其隐含实现一个接口或实现一个类，没有类名，根据多态，我们使用其父类名。因他是局部内部类，那么局部内部类的所有限制都对其生效。匿名内部类是唯一一种无构造方法类。大部分匿名内部类是用于接口回调用的。匿名内部类在编译的时候由系统自动起名Out$1.class。如果一个对象编译时的类型是接口，那么其运行的类型为实现这个接口的类。因匿名内部类无构造方法，所以其使用范围非常的有限。当需要多个对象时使用局部内部类，因此局部内部类的应用相对比较多。匿名内部类中不能定义构造方法。如果一个对象编译时的类型是接口，那么其运行的类型为实现这个接口的类。\n\n&nbsp;\n\n内部类总结：\n\n1.首先，把内部类作为外部类的一个特殊的成员来看待，因此它有类成员的封闭等级：private ,protected,默认(friendly),public；\n\n它有类成员的修饰符:&nbsp;&nbsp; static,final,abstract\n\n&nbsp;\n\n2.非静态内部类nested inner class,内部类隐含有一个外部类的指针this,因此，它可以访问外部类的一切资源（当然包括private） \n\n外部类访问内部类的成员，先要取得内部类的对象,并且取决于内部类成员的封装等级。\n\n非静态内部类不能包含任何static成员\n\n&nbsp;\n\n3.静态内部类：static inner class,不再包含外部类的this指针，并且在外部类装载时初始化.\n\n静态内部类能包含static或非static成员.\n\n静态内部类只能访问外部类static成员.\n\n外部类访问静态内部类的成员，循一般类法规。对于static成员，用类名.成员即可访问，对于非static成员，只能用对象.成员进行访问\n\n&nbsp;\n\n4.对于方法中的内部类或块中内部类只能访问块中或方法中的final变量。\n\n类成员有两种static , non-static，同样内部类也有这两种\n\nnon-static 内部类的实例，必须在外部类的方法中创建或通过外部类的实例来创建(OuterClassInstanceName.new innerClassName(ConstructorParameter)),并且可直接访问外部类的信息,外部类对象可通过OuterClassName.this来引用\n\nstatic 内部类的实例, 直接创建即可，没有对外部类实例的引用。\n\n内部类不管static还是non-static都有对外部类的引用\n\nnon-static 内部类不允许有static成员\n\n方法中的内部类只允许访问方法中的final局部变量和方法的final参数列表，所以说方法中的内部类和内部类没什麽区别。但方法中的内部类不能在方法以外访问，方法中不可以有static内部类\n\n匿名内部类如果继承自接口,必须实现指定接口的方法,且无参数 \n\n匿名内部类如果继承自类,参数必须按父类的构造函数的参数传递\n\n&nbsp;\n\n**自动装箱**：指开发人员可以把一个**基本数据类型**直接赋给**对应的包装类**\n\n**自动拆箱**：指开发人员可以把一个**包装类对象**直接赋给**对应的基本数据类型**\n\n&nbsp;\n\n要把**基本数据类型**称为**对象**的时候，需要**把基本数据类型进行包装，**\n\n**运用：把一个对象赋值给一个基本数据类型（一个由数字组成的字符串赋值给一个int或者float类型的基本数据类型）**\n\n&nbsp;\n\n**比如：**\n\n```\nList list = new ArrayList();\t\t \t//集合List只能添加对象\nlist.add(1);\t\t\t\t\t\t//1是基本数据类型，自动装箱之后才能添加到集合中\n\t\t\nIterator i = list.iterator();\nwhile(i.hasNext()){\n    int m = (Integer)i.next();\t\t//next()方法返回的是Object，需要强转Integer之后，自动拆箱\n}\n\n```\n\n&nbsp;\n\n<img src=\"/images/517519-20160304141256237-13375575.png\" alt=\"\" />\n\n**包装类的应用**，将一个**全由数字组成的字符串**变成一个**int或者float类型的基本数据**\n\n```\npublic class Wrapper_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tString str1 = \"30\";\n\t\tString str2 = \"30.3\";\n\t\tint x = Integer.parseInt(str1);\n\t\tfloat f = Float.parseFloat(str2);\n\t\tSystem.out.println(x);\n\t\tSystem.out.println(f);\n\t}\n\n}\n\n```\n\n&nbsp;\n","tags":["Java"]},{"title":"Java单例设计模式","url":"/Java单例设计模式.html","content":"**单例模式**（Singleton Pattern）是Java 中最简单的设计模式之一。这种类型的设计模式属于创建型模式，**它提供了一种创建对象的最佳方式**。 \n\n这种模式涉及到一个单一的类，**该类负责创建自己的对象，同时确保只有单个对象被创建**。 这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。\n\n<img src=\"/images/517519-20160525221810209-1131872956.png\" alt=\"\" width=\"573\" height=\"665\" />\n\n<img src=\"/images/517519-20160525221847694-157996923.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160525221926538-559361856.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160217230143566-1742313252.png\" alt=\"\" />\n\n## **1.没有使用单例：重复调用会创建多个对象**\n\n```\npackage java_basic;\n\nclass Singleton{\n\tpublic Singleton(){\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t}\n\t\n\tpublic void print(){\n\t\tSystem.out.println(\"Hello Word\");\n\t}\n}\n\npublic class singleton {\n\n\tpublic static void main(String[] args) {\n\t\tSingleton s1 = new Singleton();\n\t\tSingleton s2 = new Singleton();\n\t\tSingleton s3 = new Singleton();\n\t\tSystem.out.println(s1.hashCode());\n\t\tSystem.out.println(s2.hashCode());\n\t\tSystem.out.println(s3.hashCode());\n\t\ts1.print();\n\t\ts2.print();\n\t\ts3.print();\n\t}\n\n}\n\n```\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20170108112746472-363301486.png\" alt=\"\" />\n\n## **2.饿汉式单例：（一定是安全的，只有一份对象）**<br />\n\n\n1.构造器私有化\n\n2.声明私有的静态属性，同时创建该对象\n\n3.对外提供访问属性的静态方法，确保该对象存在。<br />\n\n### **写法1：初始化就加载**\n\n```\npackage java_basic;\n\nclass Singleton{\n\tprivate static Singleton instance = new Singleton();//声明私有的静态属性，同时创建该对象\n\tprivate Singleton(){\t\t\t\t\t//构造函数私有化\n\t}\n\t\n\tpublic static Singleton getInstance(){\t//对外提供访问属性的静态方法，确保该对象存在\n\t\treturn instance;\n\t}\n\t\n\tpublic void print(){\n\t\tSystem.out.println(\"Hello Word\");\n\t}\n}\n\n\npublic class singleton {\n\n\tpublic static void main(String[] args) {\n\t\tSingleton s1 = Singleton.getInstance();\n\t\tSingleton s2 = Singleton.getInstance();\n\t\tSingleton s3 = Singleton.getInstance();\n\t\tSystem.out.println(s1.hashCode());\n\t\tSystem.out.println(s2.hashCode());\n\t\tSystem.out.println(s3.hashCode());\n\t\ts1.print();\n\t\ts2.print();\n\t\ts3.print();\n\t}\n\n}\n\n```\n\n**可以看到输出的hashCode只有一个，证明new出的对象是同一个**\n\n<img src=\"/images/517519-20170108111526050-612402244.png\" alt=\"\" />\n\n### **写法2：类在使用的时候加载，懒加载**\n\n```\npackage java_basic;\n\nclass Singleton{\n\t// 再定义一个Single类，Single类在使用的时候加载，懒加载 \n 　　private static class Single{ \n\t\tprivate static Singleton instance = new Singleton();  //声明私有的静态属性，同时创建该对象\n\t}\n\n\tprivate Singleton(){\t//构造函数私有化\n\t}\n\t\n\tpublic static Singleton getInstance(){\t\t\t//对外提供访问属性的静态方法，确保该对象存在\n\t\treturn Single.instance;\n\t}\n\t\n\tpublic void print(){\n\t\tSystem.out.println(\"Hello Word\");\n\t}\n}\n\npublic class singleton {\n\n\tpublic static void main(String[] args) {\n\t\tSingleton s1 = Singleton.getInstance();\n\t\tSingleton s2 = Singleton.getInstance();\n\t\tSingleton s3 = Singleton.getInstance();\n\t\tSystem.out.println(s1.hashCode());\n\t\tSystem.out.println(s2.hashCode());\n\t\tSystem.out.println(s3.hashCode());\n\t\ts1.print();\n\t\ts2.print();\n\t\ts3.print();\n\t}\n\n}      \n\n```\n\n<img src=\"/images/517519-20170108111526050-612402244.png\" alt=\"\" />\n\n## 3.懒汉式单例：（不一定安全，确保只有一份对象需要synchronized）<br />\n\n\n1.构造器私有化 \n\n2.声明私有的静态属性 \n\n3.对外提供访问属性的静态方法，确保该对象存在。\n\n### **写法1，多线程下不安全**\n\n```\npackage java_basic;\n\nclass Singleton{\t\t//懒汉模式，多线程下不安全\n\tprivate static Singleton instance;\t//声明私有的静态属性，但是不创建该对象\n\tprivate Singleton(){\t\t\t//构造函数私有化\n\t}\n\t\n\tpublic static Singleton getInstance(){\t//对外提供访问属性的静态方法，确保该对象存在\n\t\t if(instance == null){  \n\t            instance = new Singleton();  \n\t        }  \n\t\treturn instance;\n\t}\n\t\n\tpublic void print(){\n\t\tSystem.out.println(\"Hello Word\");\n\t}\n}\n\npublic class singleton {\n\n\tpublic static void main(String[] args) {\n\t\tSingleton s1 = Singleton.getInstance();\n\t\tSingleton s2 = Singleton.getInstance();\n\t\tSingleton s3 = Singleton.getInstance();\n\t\tSystem.out.println(s1.hashCode());\n\t\tSystem.out.println(s2.hashCode());\n\t\tSystem.out.println(s3.hashCode());\n\t\ts1.print();\n\t\ts2.print();\n\t\ts3.print();\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20170108111526050-612402244.png\" alt=\"\" />\n\n### **写法2：加锁**\n\n```\npackage java_basic;\n\nclass Singleton{\t\t//懒汉模式，加锁\n\tprivate static Singleton instance;\t//声明私有的静态属性，但是不创建该对象\n\tprivate Singleton(){\t\t\t//构造函数私有化\n\t}\n\t\n\tpublic static Singleton getInstance(){\t//对外提供访问属性的静态方法，确保该对象存在\n\t\tif(instance == null){  \n\t\t\tsynchronized(Singleton.class){//加锁\n\t\t\t\tif(null == instance){\n\t\t\t\t\tinstance = new Singleton();  \n\t\t\t\t}\n\t\t\t}\n\t\t}  \n\t\treturn instance;\n\t}\n\t\n\tpublic void print(){\n\t\tSystem.out.println(\"Hello Word\");\n\t}\n}\n\npublic class singleton {\n\n\tpublic static void main(String[] args) {\n\t\tSingleton s1 = Singleton.getInstance();\n\t\tSingleton s2 = Singleton.getInstance();\n\t\tSingleton s3 = Singleton.getInstance();\n\t\tSystem.out.println(s1.hashCode());\n\t\tSystem.out.println(s2.hashCode());\n\t\tSystem.out.println(s3.hashCode());\n\t\ts1.print();\n\t\ts2.print();\n\t\ts3.print();\n\t}\n\n}\n\n```\n\n懒汉式参考：[Java学习之线程锁--单例模式写法--synchronized](http://blog.csdn.net/haitaofeiyang/article/details/44124375)\n\n## 4.枚举单例：（线程安全）\n\n```\npublic enum Singleton{\n\n    //定义1个枚举的元素，即为单例类的1个实例\n    INSTANCE;\n\n    // 隐藏了1个空的、私有的 构造方法\n    // private Singleton () {}\n\n}\n// 获取单例的方式：\nSingleton singleton = Singleton.INSTANCE;\n\n```\n\n&nbsp;\n\n《大话设计模式》P232\n\n<img src=\"/images/517519-20160525212339647-238218410.png\" alt=\"\" width=\"682\" height=\"177\" />\n\nP<img src=\"/images/517519-20160525212504975-1758084372.png\" alt=\"\" width=\"684\" height=\"570\" />\n\n<img src=\"/images/517519-20160525212537538-196290610.png\" alt=\"\" width=\"692\" height=\"264\" />\n\n<img src=\"/images/517519-20160525212609975-1420547511.png\" alt=\"\" width=\"689\" height=\"231\" />\n\n<img src=\"/images/517519-20160525215337319-1430065379.png\" alt=\"\" width=\"688\" height=\"595\" />\n\n<img src=\"/images/517519-20160525220324303-269174260.png\" alt=\"\" width=\"689\" height=\"737\" />\n\n<img src=\"/images/517519-20160525223541538-695206012.png\" alt=\"\" width=\"679\" height=\"431\" />\n\n<img src=\"/images/517519-20160525223607694-687063753.png\" alt=\"\" width=\"681\" height=\"354\" />\n\n《剑指Offer》P51\n\n<img src=\"/images/517519-20160525232435866-2081521783.png\" alt=\"\" width=\"613\" height=\"654\" />\n\n<img src=\"/images/517519-20160525232458881-2003000905.png\" alt=\"\" width=\"606\" height=\"227\" />\n\n<img src=\"/images/517519-20160525232521303-8780374.png\" alt=\"\" width=\"612\" height=\"488\" />\n","tags":["设计模式"]},{"title":"面试题目——八股文","url":"/面试题目——八股文.html","content":"## 1 Java基础\n\n**<img src=\"/images/517519-20210426141238477-1588588909.png\" width=\"500\" height=\"230\" loading=\"lazy\" /><!--more-->\n&nbsp;&nbsp; <img src=\"/images/517519-20210426141319187-370198239.png\" width=\"500\" height=\"150\" loading=\"lazy\" />**\n\n### **1、Java为什么能跨平台**\n\n因为**Java**程序编译之后的代码不是**能**被硬件系统直接运行的代码，而是一种&ldquo;中间码&rdquo;&mdash;&mdash;字节码。 因为它有虚拟机（JVM），**JAVA**程序不是直接在电脑上运行的，而是在虚拟机上进行的，每个系统**平台**都是有自己的虚拟机（JVM），所以**JAVA**语言**能跨平台。**\n\n### **2、Java对象在内存当中的结构**\n\n对象在内存中存储的结构由三部分组成：对象头、实例数据、对齐填充。\n\n**<strong><img src=\"/images/517519-20210619184251507-1673789808.png\" width=\"400\" height=\"188\" loading=\"lazy\" />**</strong>\n\n参考：[Java对象在内存的结构](https://juejin.cn/post/6844903832833490957)\n\n### 3、Java代码块\n\n代码块分为普通代码块、构造块、静态代码块、同步代码块4种\n\n**普通代码块**\n\n　　普通代码块是指直接在方法或者是语句中定义的代码块\n\n**构造块**\n\n　　构造块是直接写在类中的代码块\n\n&nbsp;　　构造块优先于构造方法执行，而且每次实例化对象时都会执行构造块中的代码，会执行多次。\n\n**静态代码块**\n\n　　静态代码块是使用static关键字申明的代码块\n\n　　静态代码块优先于主方法执行，而在类中定义的静态代码块会优先于构造块执行，而且不管有多少个对象执行，静态代码块只执行一次。\n\n**执行顺序：**\n\n**　　父类Ｂ静态代码块->子类Ａ静态代码块->父类Ｂ非静态代码块->父类Ｂ构造函数->子类Ａ非静态代码块->子类Ａ构造函数**\n\n[代码块：普通代码块、构造代码块、静态代码块、同步代码块、构造方法、main方法执行顺序](https://blog.csdn.net/SeniorShen/article/details/109842162)\n\n### **4、HashMap实现原理**\n\n[面试必备：HashMap、Hashtable、ConcurrentHashMap的原理与区别](https://www.cnblogs.com/plokmju/p/hash_threshold.html)\n\n[面试八股文之一HashMap源码分析](https://blog.csdn.net/qq_42581175/article/details/108156506)\n\n[ConcurrentHashMap，分段锁，CAS](https://zhuanlan.zhihu.com/p/116748080)\n\nHashTable：线程安全，性能较差；HashMap 允许 null key 和 null value，而 HashTable 不允许；\n\n1.7 HashMap：数组＋链表\n\n1.8 HashMap：数组+链表+红黑树\n\n1.7 ConcurrentHashMap：分段锁\n\n优点\n\n```\n1.其内部将数据分为数个&ldquo;段（Segment）&rdquo;，其数量和并发级别有关系，具体是&ldquo;大于等于并发级别的最小的2的幂次&rdquo;。\n2.每个segment使用单独的ReentrantLock（分段锁）。\n3.如果操作涉及不同segment，则可以并发执行，如果是同一个segment则会进行锁的竞争和等待。\n4.此设计的效率是高于synchronized的。\n\n```\n\n缺点\n\n```\n段Segment继承了重入锁ReentrantLock，有了锁的功能，每个锁控制的是一段，当每个Segment越来越大时，锁的粒度就变得有些大了。\n\n缺点在于分成很多段时会比较浪费内存空间(不连续，碎片化); 操作map时竞争同一个分段锁的概率非常小时，分段锁反而会造成更新等操作的长时间等待; 当某个段很大时，分段锁的性能会下降。\n\n```\n\n为什么不用ReentrantLock而用synchronized ?\n\n```\n1.减少内存开销:如果使用ReentrantLock则需要节点继承AQS来获得同步支持，增加内存开销，而1.8中只有头节点需要进行同步。\n2.内部优化:synchronized则是JVM直接支持的，JVM能够在运行时作出相应的优化措施：锁粗化、锁消除、锁自旋等等。\n\n```\n\n参考：[java8的ConcurrentHashMap为何放弃分段锁](https://cloud.tencent.com/developer/article/1509556)\n\n1.8&nbsp;ConcurrentHashMap：synchronized关键字+CAS（Compare-And-Swap）\n\n[Java进阶（六）从ConcurrentHashMap的演进看Java多线程核心技术](http://www.jasongj.com/java/concurrenthashmap/)\n\n### **5、HashMap扩容机制**\n\ncapacity 即容量，默认16。\n\nloadFactor 加载因子，默认是0.75\n\nthreshold 阈值。阈值=容量*加载因子。默认12。当元素数量超过阈值时便会触发扩容。\n\n1.7和1.8扩容机制参考：[HashMap的扩容机制](https://zhuanlan.zhihu.com/p/114363420)\n\n链表长度达到 8 就转成红黑树，而当长度降到 6 就转换回链表\n\n参考：[为什么 Map 桶中超过 8 个才转为红黑树？](https://www.jianshu.com/p/fdf3d24fe3e8)\n\n### **6、解决Hash冲突的方法**\n\n1.开放地址法：线性探查（线性地查找下一个空白单元，会发生聚集现象）、二次探查（防止聚集产生，探测相隔较远的单元）和再哈希法（可以消除原始聚集和二次聚集）\n\n2.链地址法：参考MyHashMap\n\n参考：[数据结构与算法：hash冲突解决](https://zhuanlan.zhihu.com/p/29520044)\n\n### **7、List**\n\n**ArrayList** : 基于数组实现的非线程安全的集合。查询元素快，插入，删除中间元素慢。 \n\n**LinkedList** : 基于链表实现的非线程安全的集合。查询元素慢，插入，删除中间元素快。 \n\n**Vector** : 基于数组实现的线程安全的集合。线程同步（方法被synchronized修饰），性能比ArrayList差。\n\n **CopyOnWriteArrayList**: 基于数组实现的线程安全的写时复制集合。线程安全（ReentrantLock加锁），性能比Vector高，适合读多写少的场景。\n\n参考：[【你碰到过吗】如果面试官问你ArrayList和LinkedList有什么区别？](https://zhuanlan.zhihu.com/p/47384294)\n\n### 8、Set\n\n**1、散列的存放：HashSet，底层基于HashMap实现**\n\nHashSet是Set接口的一个子类，主要的特点是：里面不能存放重复的元素，而且采用散列的存储方式，所以没有顺序。\n\n**2、有序的存放：TreeSet**\n\n### 9、Java 类的生命周期\n\n类的生命周期包括：加载、链接、初始化、使用和卸载，其中`加载`、`链接`、`初始化`，属于`**类加载**的过程。`\n\n使用是指我们new对象进行使用，卸载指对象被垃圾回收掉了\n\n&nbsp;\n\n在程序执行中JVM通过装载、链接、初始化3个步骤完成\n\n类的装载就是通过类加载器把.class二进制文件装入JVM的方法区，并在堆区创建描述该类的java.lang.Class对象，用来封装数据。\n\n同一个类只会被JVM加载一次。\n\n链接就是把二进制数据组装成可以运行的状态。链接分为校验、准备和解析3个步骤。\n\n校验用来确认此二进制文件是否适合当前的JVM（版本）\n\n准备就是为静态成员分配内存空间，并设置默认值\n\n解析指的是转换常量池的代码引用为直接引用的过程，直到所有的符号引用都可被运行程序使用（建立完整的对应关系）。\n\n完成之后，类型即可初始化，初始化之后类的对象就可以正常地使用，直到一个对象不再使用之后，将被垃圾回收，释放空间。\n\n当没有任何引用指向Class对象时将会被卸载，结束类的生命周期。\n\n<img src=\"/images/517519-20210508153445116-1322256018.png\" width=\"450\" height=\"187\" loading=\"lazy\" />\n\n### 10、Java 类加载的过程&nbsp;\n\n**<img src=\"/images/517519-20210425150442211-1618566686.png\" width=\"500\" height=\"165\" loading=\"lazy\" />**\n\n**第一步：Loading加载**\n\n```\n通过类的全限定名（包名 + 类名），获取到该类的.class文件的二进制字节流\n\n将二进制字节流所代表的静态存储结构，转化为方法区运行时的数据结构\n\n在内存中生成一个代表该类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口\n\n```\n\n总结：`加载二进制数据到内存` &mdash;> `映射成jvm能识别的结构` &mdash;> `在内存中生成class文件`。\n\n**第二步：Linking链接**\n\n链接是指将上面创建好的class类合并至Java虚拟机中，使之能够执行的过程，可分为`验证`、`准备`、`解析`三个阶段。\n\n**① 验证（Verify）**\n\n```\n确保class文件中的字节流包含的信息，符合当前虚拟机的要求，保证这个被加载的class类的正确性，不会危害到虚拟机的安全。\n\n```\n\n**② 准备（Prepare）**\n\n```\n为类中的静态字段分配内存，并设置默认的初始值，比如int类型初始值是0。被final修饰的static字段不会设置，因为final在编译的时候就分配了\n\n```\n\n**③ 解析（Resolve）**\n\n```\n解析阶段的目的，是将常量池内的符号引用转换为直接引用的过程（将常量池内的符号引用解析成为实际引用）。如果符号引用指向一个未被加载的类，或者未被加载类的字段或方法，那么解析将触发这个类的加载（但未必触发这个类的链接以及初始化。）\n\n事实上，解析器操作往往会伴随着 JVM 在执行完初始化之后再执行。 符号引用就是一组符号来描述所引用的目标。符号引用的字面量形式明确定义在《Java 虚拟机规范》的Class文件格式中。直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。\n\n解析动作主要针对类、接口、字段、类方法、接口方法、方法类型等。对应常量池中的 CONSTANT_Class_info、CONSTANT_Fieldref_info、CONSTANT_Methodref_info等。\n\n```\n\n**第三步：initialization初始化**\n\n```\n初始化就是执行类的构造器方法init()的过程。<br />\n这个方法不需要定义，是javac编译器自动收集类中所有类变量的赋值动作和静态代码块中的语句合并来的。<br />\n若该类具有父类，jvm会保证父类的init先执行，然后在执行子类的init。\n\n```\n\n参考：[jvm类加载器，类加载机制详解，看这一篇就够了](https://segmentfault.com/a/1190000037574626)\n\n### 11、Java 类隔离加载\n\n[Java 类隔离加载的正确姿势](https://blog.xiaohansong.com/classloader-isolation.html#more)\n\n### 12、双亲委派模型\n\n双亲委派的意思是如果一个类加载器需要加载类，那么首先它会把这个类请求委派给父类加载器去完成，每一层都是如此。一直递归到顶层，当父加载器无法完成这个请求时，子类才会尝试去加载。\n\n双亲委派的好处：\n\n它使得类有了层次的划分。就拿`java.lang.Object`来说，你加载它经过一层层委托最终是由`Bootstrap ClassLoader`来加载的，也就是最终都是由`Bootstrap ClassLoader`去找`<JAVA_HOME>\\lib`中rt.jar里面的`java.lang.Object`加载到JVM中。\n\n这样如果有不法分子自己造了个`java.lang.Object，`里面嵌了不好的代码，如果我们是按照双亲委派模型来实现的话，最终加载到JVM中的只会是我们rt.jar里面的东西，也就是这些核心的基础类代码得到了保护。因为这个机制使得系统中只会出现一个`java.lang.Object`。不会乱套了。\n\n[参考：面试官：说说双亲委派模型？](https://juejin.cn/post/6844903838927814669)\n\n### 13、JDBC为什么要破坏双亲委派模型\n\n**因为类加载器受到加载范围的限制，在某些情况下父类加载器无法加载到需要的文件，这时候就需要委托子类加载器去加载class文件。**\n\nJDBC的Driver接口定义在JDK中，其实现由各个数据库的服务商来提供，比如MySQL驱动包。DriverManager 类中要加载各个实现了Driver接口的类，然后进行管理，但是DriverManager位于 JAVA_HOME中jre/lib/rt.jar 包，由BootStrap类加载器加载，而其Driver接口的实现类是位于服务商提供的 Jar 包，**根据类加载机制，当被装载的类引用了另外一个类的时候，虚拟机就会使用装载第一个类的类装载器装载被引用的类。**也就是说BootStrap类加载器还要去加载jar包中的Driver接口的实现类。我们知道，BootStrap类加载器默认只负责加载 JAVA_HOME中jre/lib/rt.jar 里所有的class，所以需要由子类加载器去加载Driver实现，这就破坏了双亲委派模型。\n\n参考：[阿里面试题：JDBC、Tomcat为什么要破坏双亲委派模型](https://www.javazhiyin.com/44347.html)\n\n### 14、java异常体系\n\n<img src=\"/images/517519-20210425164205998-1185503485.png\" width=\"500\" height=\"323\" loading=\"lazy\" />\n\njava中的Exception类的子类不仅仅只是像上图所示只包含IOException和RuntimeException这两大类，事实上Exception的子类很多很多，主要可概括为：运行时异常与非运行时异常。\n\n&nbsp;\n\nThorwable类（表示可抛出）是所有异常和错误的超类，两个直接子类为Error和Exception，分别表示错误和异常。其中异常类Exception又分为运行时异常(RuntimeException)和非运行时异常，这两种异常有很大的区别，也称之为不检查异常（Unchecked Exception）和检查异常（Checked Exception）。下面将详细讲述这些异常之间的区别与联系：\n\n1、Error与Exception&nbsp;&nbsp;\n\n　　Error是程序无法处理的错误，它是由JVM产生和抛出的，比如OutOfMemoryError、ThreadDeath等。这些异常发生时，Java虚拟机（JVM）一般会选择线程终止。\n\n　　Exception是程序本身可以处理的异常，这种异常分两大类运行时异常和非运行时异常。程序中应当尽可能去处理这些异常。\n\n2、运行时异常和非运行时异常&nbsp;&nbsp;\n\n　　运行时异常都是RuntimeException类及其子类异常，如NullPointerException、IndexOutOfBoundsException等，这些异常是不检查异常，程序中可以选择捕获处理，也可以不处理。这些异常一般是由程序逻辑错误引起的，程序应该从逻辑角度尽可能避免这类异常的发生。\n\n　　非运行时异常是RuntimeException以外的异常，类型上都属于Exception类及其子类。从程序语法角度讲是必须进行处理的异常，如果不处理，程序就不能编译通过。如IOException、SQLException等以及用户自定义的Exception异常，一般情况下不自定义检查异常。\n\n参考：[Java 异常体系(美团面试)](https://www.cnblogs.com/aspirant/p/10790803.html)\n\n### **15、常见的OOM**\n\n1. java.lang.OutOfMemoryError: Java heap space，表示java堆空间不足。当应用程序申请更多的内存时，若java堆内存已经无法满足应用程序的需要，则抛出该异常。\n\n2. java.lang.OutOfMemoryError: PermGen space，表示java永久代（方法区）的空间不足。永久代用于存放类的字节码和常量池，类的字节码被加载后存放在这个区域。大多数jvm的实现都不会对永久代进行垃圾回收，因此，只要类加载过多就会出现这个问题。\n\n3. java.lang.OutOfMemoryError: unable to create new native thread，本质原因是创建了太多的线程，而系统允许创建的线程数量是有限的。\n\n4. java.lang.OutOfMemoryError: GC overhead limit exceeded，是并行（或者并发）垃圾回收器的GC回收时间长，超过98%的时间用来做GC并且回收了不到2%的对内存时抛出的异常，用来提前预警，避免内存过小导致应用程序不能正常工作。&nbsp;\n\n参考：[教你分析9种 OOM 常见原因及解决方案](https://cloud.tencent.com/developer/article/1480668)\n\n[几种常见的OOM（OutOfMemoryError）错误](https://blog.csdn.net/li_canhui/article/details/91377984)\n\n### **16、<strong>线程池**优势</strong>\n\n线程池优势： \n\n（1）降低系统资源消耗，通过重用已存在的线程，降低线程创建和销毁造成的消耗；\n\n（2）提高系统响应速度，当有任务到达时，通过复用已存在的线程，无需等待新线程的创建便能立即执行；\n\n（3）方便线程并发数的管控\n\n### **17、ThreadPoolExecutor的参数**\n\n1. corePoolSize 核心线程数\n\n2. maxumumPoolSize 线程池所能容纳的最大线程数\n\n3. keepAliveTime <br />\n\n4. unit keepAliveTime的单位\n\n5. workQueue 线程池中的任务队列：\n\nnewSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。\n\nnewFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。\n\nnewCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。\n\nnewScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。 \n\n参考：[JDK提供的线程池有哪些？线程池的使用?](https://blog.csdn.net/zwb_dzw/article/details/115584153)\n\n6. 拒绝策略：ThreadPoolExecutor是一个典型的缓存池化设计的产物，因为池子有大小，当池子体积不够承载时，就涉及到拒绝策略，有4种：\n\nAbortPolicy&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- 当任务添加到线程池中被拒绝时，它将抛出 RejectedExecutionException 异常。\n\nCallerRunsPolicy&nbsp;&nbsp;&nbsp; -- 当任务添加到线程池中被拒绝时，会在线程池当前正在运行的Thread线程池中处理被拒绝的任务。\n\nDiscardOldestPolicy -- 当任务添加到线程池中被拒绝时，线程池会放弃等待队列中最旧的未处理任务，然后将被拒绝的任务添加到等待队列中。\n\nDiscardPolicy&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- 当任务添加到线程池中被拒绝时，线程池将丢弃被拒绝的任务。 <br />\n\n参考：[Java 线程池之 四个拒绝策略](https://blog.csdn.net/qq_27093465/article/details/105248633)\n\n### **18、ThreadPoolExecutor的ctl变量**\n\nThreadPoolExecutor中有一个重要的变量ctl，有2个作用：\n\n```\nprivate final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));\n\n```\n\n1、记录有效的线程数量，即 workerCount\n\n2、记录当线程池的状态，即 runState\n\n如果用二进制表示的话，这5种状态至少需要3位来表示，而一个 int 类型的数占32位，所以，高3位表示线程状态（runState），剩下的29位表示线程数量（workerCount）\n\n### **19、object九大方法**\n\n```\nclone() 创建并返回此对象的一个副本。 \nequals(Object obj) 指示某个其他对象是否与此对象&ldquo;相等&rdquo;。 \nfinalize() 当垃圾回收器确定不存在对该对象的更多引用时，由对象的垃圾回收器调用此方法。 \ngetClass() 返回一个对象的运行时类。 \nhashCode() 返回该对象的哈希码值。 \nnotify() 唤醒在此对象监视器上等待的单个线程。 \nnotifyAll() 唤醒在此对象监视器上等待的所有线程。 \ntoString() 返回该对象的字符串表示。 \nwait() 导致当前的线程等待，直到其他线程调用此对象的 notify() 方法或 notifyAll() 方法。 \n\n```\n\n### **20、java多态**\n\n多态性在面向对象中主要有两种体现：\n\n<1>方法的重载与覆写\n\n重载：\n\n重载(overloading) 是在一个类里面，方法名字相同，而参数不同。返回类型可以相同也可以不同。\n\n每个重载的方法（或者构造函数）都必须有一个独一无二的参数类型列表。\n\n最常用的地方就是构造器的重载。\n\n覆写：\n\n重写是子类对父类的允许访问的方法的实现过程进行重新编写, 返回值和形参都不能改变。**即外壳不变，核心重写！**\n\n重写的好处在于子类可以根据需要，定义特定于自己的行为。 也就是说子类能够根据需要实现父类的方法。\n\n重写方法不能抛出新的检查异常或者比被重写方法申明更加宽泛的异常。例如： 父类的一个方法申明了一个检查异常 IOException，但是在重写这个方法的时候不能抛出 Exception 异常，因为 Exception 是 IOException 的父类，只能抛出 IOException 的子类异常\n\n<2>对象的多态性：向上转型：子类对象-->父类对象，向上转型会自动完成\n\n　　　　　　　　向下转型：父类对象-->子类对象，向下转型时，必须明确地指明转型的子类类型\n\n参考：[Java对象的多态性（转型）](https://www.cnblogs.com/tonglin0325/p/5239903.html)\n\n## 2 JVM\n\n### **1、Java中的常用的内存区域以及内存分配策略**\n\n栈区：存放方法局部变量，基本类型变量区、执行环境上下文、操作指令区，线程不共享;\n\n堆区：只存放类对象，线程共享；\n\n方法区（静态存储区）：又叫静态存储区，存放class文件和static静态方法或者静态变量，线程共享;\n\n```\nclass A {\n    private String a = &ldquo;aa&rdquo;;\n    public boolean methodB() {\n        String b = &ldquo;bb&rdquo;;\n        final String c = &ldquo;cc&rdquo;;\n    }\n}\n\n```\n\na在堆区，b和c在栈区\n\n### **2、JVM内存空间组成**\n\nGithub：[JVM 内存结构](https://github.com/doocs/jvm/blob/main/docs/01-jvm-memory-structure.md)\n\nJava 虚拟机的内存空间分为 5 个部分：程序计数器，Java 虚拟机栈，本地方法栈，堆，方法区\n\n**<img src=\"/images/517519-20210304145732157-1186399695.png\" width=\"450\" height=\"340\" />**\n\n图来自：[【JVM学习】&mdash;&mdash;本地方法栈、堆](https://segmentfault.com/a/1190000037428080)\n\n1.**程序计数器**\n\n程序计数器（Program Counter Register)，在JVM规范中，每个线程都有自己的程序计数器。这是一块比较小的内存空间，存储当前线程正在执行的Java方法的JVM指令地址，即字节码的行号。如果正在执行Native方法，则这个计数器为空。该内存区域是唯一一个在Java虚拟机规范中没有规定任何OOM情况的内存区域。\n\n2.**Java虚拟机栈**\n\nJava虚拟机栈(Java Virtal Machine Stack)，同样也是属于线程私有区域，每个线程在创建的时候都会创建一个虚拟机栈，生命周期与线程一致，线程退出时，线程的虚拟机栈也回收。虚拟机栈内部保持一个个的栈帧，每次方法调用都会进行压栈，JVM对栈帧的操作只有出栈和压栈两种，方法调用结束时会进行出栈操作。\n\nJava虚拟机栈也叫栈内存，是在线程创建时创建，它的**生命期是跟随线程的生命期**，线程结束栈内存也就释放，对于栈来说不存在垃圾回收问题，只要线程一结束，该栈就 Over，所以不存在垃圾回收。\n\n<img src=\"/images/517519-20210609142303268-190698830.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\njava虚拟机栈包括局部变量表等，参考：[JVM系列之栈帧（Stack Frame）结构](https://www.huaweicloud.com/articles/f506d1b5b5c4044d11d46f889069f7b1.html)\n\n3.**本地方法栈**\n\n本地方法栈（Native Method Stack）与虚拟机栈类似，本地方法栈是在调用本地方法时使用的栈，每个线程都有一个本地方法栈。\n\n4.**堆内存**\n\n数组中，数组的名字，即地址存在栈内存中，地址指向的内容存在堆内存中，开辟新的堆内存必须要用关键字new，栈内存中存储的是堆内存的访问地址\n\n当堆空间没有任何栈空间引用的时候，就成为了垃圾空间，等待这垃圾回收机制进行回收\n\n&nbsp;\n\n使用jmap查看JVM的**堆内存**情况\n\n堆（Heap），几乎所有创建的Java对象实例，都是被直接分配到堆上的。堆被所有的线程所共享，在堆上的区域，会被垃圾回收器做进一步划分，例如新生代、老年代的划分。Java虚拟机在启动的时候，可以使用&ldquo;Xmx&rdquo;之类的参数指定堆区域的大小。\n\n**堆内存**分成&nbsp;**Young Generation**（年轻代）和 **Old Generation**（老年代）\n\n此外，**Young Generation**（年轻代）还分成&nbsp;Eden Space ， From Space 和 To Space（也叫 Survivor0空间 和 Survivor1空间）\n\n5.**方法区**\n\nJDK 1.7及之前，被static修饰的变量会存放在方法区，被所有线程所共享。\n\n方法区（Method Area)。方法区与堆一样，也是所有的线程所共享，存储被虚拟机加载的元（Meta）数据，包括类信息、常量、静态变量、即时编译器编译后的代码等数据。这里需要注意的是运行时常量池也在方法区中。\n\n根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。由于早期HotSpot JVM的实现，将CG分代收集拓展到了方法区，因此很多人会将方法区称为永久代。\n\nOracle JDK8中已移除永久代，同时增加了元数据区（Metaspace）。\n\n6.**运行时常量池**\n\n运行时常量池（Run-Time Constant Pool)，这是方法区的一部分，受到方法区内存的限制，当常量池无法再申请到内存时，会抛出OutOfMemoryError异常。\n\n7.**直接内存**\n\n直接内存（Direct Memory），直接内存并不属于Java规范规定的属于Java虚拟机运行时数据区的一部分。Java的NIO可以使用Native方法直接在java堆外分配内存，使用DirectByteBuffer对象作为这个堆外内存的引用。\n\n### **3、JDK 1.8 JVM的变化**\n\n1、移除方法区\n\n　　JDK 1.7及之前方法区存放的数据有**类信息**（类名，修饰符，字段描述，方法描述等），**常量**，**静态变量**，**即时编译后的class文件**。\n\n　　方法区中还包含有常量池：常量池中主要有字面量和符号引用\n\n　　　　字面量：文本字符串，声明为final的常量值；\n\n　　　　符号引用：包括了三种常量，分别是：类和接口的全限定名，字段的名称和描述符，方法的名称和修饰符。\n\n2、MetaSpace元空间 取而代之\n\n　　JDK 1.8将方法区中的**字符串常量移至堆内存，其他内容如类信息、静态变量、其他常量（如整形常量），即时编译后的class文件等都移动到元空间内**。\n\n　　元空间（MetaSpace）不在堆内存上，而是直接占用的本地内存。因此元空间的大小仅受本地内存限制。也可通过参数来设定元空间的大小\n\n　　-XX:MetaSpaceSize　　初始元空间大小，达到该值就会触发垃圾收集器进行类型卸载，同时GC会对该值进行调整：如果释放了大量的空间，就适当降低\n\n该值；如果释放了很少的空间，那么在不超过MaxMetaSpaceSize时，适当提高该值。\n\n　　-XX:MaxMetaSpaceSize　　最大元空间大小，默认没有限制\n\n参考：[JDK 1.8 JVM的变化](https://www.cnblogs.com/yangyongjie/p/10646255.html)\n\n### 4、java对象的生命周期\n\n1）对象优先在Eden分配：major gc时经常会伴随至少一次的minor gc，但并非绝对，比如说 Parallel Scavenge就是直接major gc没有伴随minor gc;<br />\n\n\n一种情况是看对象大小，由-XX:PretenureSizeThreshold启动参数控制，若对象大小大于此值，就会绕过新生代, 直接在老年代分配，PretenureSizeThreshold 参数只对Serial和ParNew两款收集器有效。\n\n这个是在Java9之前是这样的，Java9以后的版本基于Region分区，里面专门开出一部分作为大对象存储区域。<br />\n\n\nYGC时，To Survivor区不足以存放存活的对象，对象会直接进入到老年代。<br />\n\n\n经过多次YGC后，如果存活对象的年龄达到了设定阈值，则会晋升到老年代中。<br />\n\n\n动态年龄判定规则，To Survivor区中相同年龄的对象，如果其大小之和占到了 To Survivor区一半以上的空间，那么大于此年龄的对象会直接进入老年代，而不需要达到默认的分代年龄。<br />\n\n\nParallel Scavenge收集器不需要设置；如果遇到必须使用此参数的场景，可以考虑ParNew+CMS组合。<br />\n\n\n3） 长期存活的对象放入老年代:虚拟机给每个对象定义了一个对象年龄（Age）计数器；gc一次Age+1,当Age大于一定程度（默认为15岁）就会被晋升到老年代；-XX:MaxTenuringThreshold：最大年龄；<br />\n\n\n4） 动态对象年龄判定：如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于改年龄的对象就可以直接进入老年代，无须等到MaxTenuringThreshold中要求的年龄；<br />\n\n\n5）空间分配担保：jdk6 update 24后，只要老年代的连续空间大于新生代对象总大小或者历次晋升的平均大小就会进行major gc，否则Full GC；\n\n### **5、GC触发条件**\n\n对象从新生代区域消失的过程，我们称之为 \"minor GC\"&nbsp;\n\n对象从老年代区域消失的过程，我们称之为 \"major GC\"\n\nMinor GC：清理整个YouGen的过程，eden的清理，S0\\S1的清理都会由于MinorGC&nbsp;Allocation Failure(YoungGen区内存不足），而触发minorGC\n\nMajor GC：OldGen区内存不足，触发Major GC\n\nFull GC：Full GC 是清理整个堆空间&mdash;包括年轻代和永久代\n\nFull GC 触发的场景\n\n1）System.gc\n\n2）promotion failed (年代晋升失败,比如eden区的存活对象晋升到S区放不下，又尝试直接晋升到Old区又放不下，那么Promotion Failed,会触发FullGC)\n\n3）CMS的Concurrent-Mode-Failure \n\n由于CMS回收过程中主要分为四步:&nbsp;1.CMS initial mark 2.CMS Concurrent mark 3.CMS remark 4.CMS Concurrent sweep。在2中gc线程与用户线程同时执行，那么用户线程依旧可能同时产生垃圾，&nbsp;如果这个垃圾较多无法放入预留的空间就会产生CMS-Mode-Failure，&nbsp;切换为SerialOld单线程做mark-sweep-compact。\n\n4）新生代晋升的平均大小大于老年代的剩余空间 （为了避免新生代晋升到老年代失败）\n\n当使用G1,CMS 时，FullGC发生的时候 是 Serial+SerialOld。&nbsp;\n\n当使用ParalOld时，FullGC发生的时候是 ParallNew +ParallOld.\n\n参考：https://note.youdao.com/ynoteshare1/index.html?id=c10ecc5535e673b3f7fa396e57866569&amp;type=note\n\n#### **Young Gc触发**：\n\n大多数情况下，对象直接在年轻代中的Eden区进行分配，如果Eden区域没有足够的空间，那么就会触发YGC（Minor GC）。\n\n当发生 Minor GC时，Eden 区和 from 指向的 Survivor 区中的存活对象会被复制(此处采用标记 - 复制算法)到 to 指向的 Survivor区中，然后交换 from 和 to指针，以保证下一次 Minor GC时，to 指向的 Survivor区还是空的。\n\n#### **Full Gc触发条件：**\n\n老年代的内存使用率达到了一定阈值（可通过参数调整），直接触发FGC。\n\n\nMetaspace（元空间）在空间不足时会进行扩容，当扩容到了-XX:MetaspaceSize 参数的指定值时，也会触发FGC。\n\n\n\n\n\n\n\n\n\n\n\n\n\nSystem.gc() 或者Runtime.gc() 被显式调用时，触发FGC。\n\n#### Survivor区对象晋升位老年代对象的条件：\n\nJava虚拟机会记录 Survivor区中的对象一共被来回复制了几次。\n\n**如果一个对象被复制的次数为 15 (对应虚拟机参数 -XX:+MaxTenuringThreshold),那么该对象将被晋升为至老年代**，(至于为什么是 15次，原因是 HotSpot会在对象头的中的标记字段里记录年龄，分配到的空间只有4位，所以最多只能记录到15)。\n\n另外，**如果单个 Survivor 区已经被占用了 50% (对应虚拟机参数: -XX:TargetSurvivorRatio)，那么较高复制次数的对象也会被晋升至老年代。**\n\n### **6、垃圾收集策略与垃圾回收算法**\n\n参考：[垃圾收集策略与算法](https://github.com/doocs/jvm/blob/main/docs/03-gc-algorithms.md)\n\n<img src=\"/images/517519-20210420164815575-796930570.png\" alt=\"\" />\n\n### **7、JVM垃圾收集器**\n\n**第一阶段，Serial（串行）收集器**\n\n在jdk1.3.1之前，java虚拟机仅仅能使用Serial收集器。 Serial收集器是一个单线程的收集器，但它的&ldquo;单线程&rdquo;的意义并不仅仅是说明它只会使用一个CPU或一条收集线程去完成垃圾收集工作，更重要的是在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。\n\n**第二阶段，Parallel（并行）收集器**\n\nParallel收集器也称吞吐量收集器，相比Serial收集器，Parallel最主要的优势在于使用多线程去完成垃圾清理工作，这样可以充分利用多核的特性，大幅降低gc时间。\n\n**第三阶段，CMS（并发）收集器**\n\nCMS收集器在Minor GC时会暂停所有的应用线程，并以多线程的方式进行垃圾回收。在Full GC时不再暂停应用线程，而是使用若干个后台线程定期的对老年代空间进行扫描，及时回收其中不再使用的对象。\n\n**第四阶段，G1（并发）收集器**\n\nG1收集器（或者垃圾优先收集器）的设计初衷是为了尽量缩短处理超大堆（大于4GB）时产生的停顿。相对于CMS的优势而言是内存碎片的产生率大大降低。\n\n**G1收集器的优势：**\n\n1. 独特的分代垃圾回收器,分代GC: 分代收集器, 同时兼顾年轻代和老年代；\n1. 使用分区算法, 不要求eden, 年轻代或老年代的空间都连续；\n1. 并行性: 回收期间, 可由多个线程同时工作, 有效利用多核cpu资源；\n1. 空间整理: 回收过程中, 会进行适当对象移动, 减少空间碎片；\n1. 可预见性: G1可选取部分区域进行回收, 可以缩小回收范围, 减少全局停顿。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&nbsp;\n\n**新生代垃圾收集器**\n\n**1.Serial串行收集器-复制算法**\n\nSerial收集器是新生代单线程收集器，优点是简单高效，算是最基本、发展历史最悠久的收集器。它在进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集完成。\n\n**2.ParNew收集器-复制算法**\n\nParNew收集器是**新生代并行收集器**，其实就是Serial收集器的多线程版本。\n\n**3.Parallel Scavenge（并行回收）收集器-复制算法**\n\nParallel Scavenge收集器是新生代并行收集器，追求高吞吐量，高效利用 CPU。\n\n&nbsp;\n\n**老年代垃圾收集器**\n\n**1.Serial Old 收集器-标记整理算法**\n\nSerial Old是Serial收集器的老年代版本，它同样是一个单线程(串行)收集器，使用标记整理算法。这个收集器的主要意义也是在于给Client模式下的虚拟机使用。\n\n**2.Parallel Old 收集器-标记整理算法**\n\nParallel Old 是Parallel Scavenge收集器的老年代版本，使用多线程和&ldquo;标记-整理&rdquo;算法。这个收集器在1.6中才开始提供。\n\n**3.CMS收集器-标记整理算法**\n\nCMS(Concurrent Mark Sweep)收集器是一种以获取最短回收停顿时间为目标的收集器。\n\n目前很大一部分的Java应用集中在互联网站或者B/S系统的服务端上，这类应用尤其重视服务器的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。CMS收集器就非常符合这类应用的需求。\n\n**CMS收集器是基于&ldquo;标记-清除&rdquo;算法实现的**\n\n&nbsp;\n\n**新生代和老年代垃圾收集器**\n\n**1.G1收集器-标记整理算法**\n\n**JDK1.7后全新的回收器, 用于取代CMS收集器。**\n\n1.独特的分代垃圾回收器,分代GC: 分代收集器, 同时兼顾年轻代和老年代；\n\n2.使用分区算法, 不要求eden, 年轻代或老年代的空间都连续；\n\n3.并行性: 回收期间, 可由多个线程同时工作, 有效利用多核cpu资源；\n\n4.空间整理: 回收过程中, 会进行适当对象移动, 减少空间碎片；\n\n5.可预见性: G1可选取部分区域进行回收, 可以缩小回收范围, 减少全局停顿。\n\n参考：[7种JVM垃圾收集器特点，优劣势、及使用场景](https://zhuanlan.zhihu.com/p/58896728)<br />\n\n### 8、jvm调优命令\n\n1.查看java进程，jps命令可以列出正在运行的虚拟机进程\n\n```\njps -l\n1005373 sun.tools.jps.Jps\n1000153 org.apache.flume.node.Application\n\n```\n\n2.查看flume进程java虚拟机的统计信息\n\n```\njstat -gcutil 1028479\n  S0     S1     E      O      M     CCS    YGC     YGCT    FGC    FGCT     GCT   \n 71.69   0.00  25.63   0.14  96.56  89.78    116    1.074     0    0.000    1.074\n\n```\n\n某springboot web服务进程java虚拟机的统计信息\n\n```\njstat -gcutil 29\n  S0     S1     E      O      M     CCS    YGC     YGCT    FGC    FGCT     GCT   \n 76.84   0.00  47.11  55.27  96.06  93.04     28   17.787     4    8.589   26.376\n\n```\n\n**对应指标的中文含义**\n\n```\nS0：Survivor0的占用比例\nS1：Survivor1的占用比例\nE：新生代Eden区的占用比例\nO：老年代的占用比例\nM：方法区的占用比例\nCCS：压缩类空间的占用比例\nYGC：年轻代垃圾回收次数\nYGCT：年轻代垃圾回收消耗时间\nFGC：老年代垃圾回收次数\nFGCT：老年代垃圾回收消耗时间\nGCT：垃圾回收消耗总时间\n\n```\n\n把xmx和xms设置一致可以让JVM在启动时就直接向OS申请xmx的commited内存，好处是：\n\n```\n1. 避免JVM在运行过程中向OS申请内存\n2. 延后启动后首次GC的发生时机\n3. 减少启动初期的GC次数\n4. 尽可能避免使用swap space\n\n```\n\n参考：[jvm调优-xmx和xms设置成一样的好处](https://blog.csdn.net/qq646040754/article/details/103305632)&nbsp;\n\n### **9、****JVM调优原则**\n\n参考：[JVM- 技术专题 -GCViewer 调优 GC](https://xie.infoq.cn/article/b37b289ef91bad11ce2d37b86)\n\n#### GC 调优原则\n\nGC 是有代价的，因此我们调优的根本原则是每一次 GC 都回收尽可能多的对象，也就是减少无用功。因此我们在做具体调优的时候，针对 CMS 和 G1 两种垃圾收集器，分别有一些相应的策略。\n\n#### CMS 收集器\n\n对于 CMS 收集器来说，最重要的是合理地设置年轻代和年老代的大小。年轻代太小的话，会导致频繁的 Minor GC，并且很有可能存活期短的对象也不能被回收，GC 的效率就不高。而年老代太小的话，容纳不下从年轻代过来的新对象，会频繁触发单线程 Full GC，导致较长时间的 GC 暂停，影响 Web 应用的响应时间。\n\n#### G1 收集器\n\n对于 G1 收集器来说，我不推荐直接设置年轻代的大小，这一点跟 CMS 收集器不一样，这是因为 G1 收集器会根据算法动态决定年轻代和年老代的大小。因此对于 G1 收集器，我们需要关心的是 Java 堆的总大小（-Xmx）。\n\n此外 G1 还有一个较关键的参数是-XX:MaxGCPauseMillis = n，这个参数是用来限制最大的 GC 暂停时间，目的是尽量不影响请求处理的响应时间。G1 将根据先前收集的信息以及检测到的垃圾量，估计它可以立即收集的最大区域数量，从而尽量保证 GC 时间不会超出这个限制。因此 G1 相对来说更加&ldquo;智能&rdquo;，使用起来更加简单。\n\n### 10、正常系统多久一次YGC / FGC\n\n正常有一定流量的系统如果还有一定的并发量，大概几分钟 ~ 几十分钟一次 YGC，一次 几毫秒到 几十毫秒，\n\n几十分钟 ~ 几个小时 ~ 几天一次 FGC 一次大概是 几百毫秒\n\n如果频繁 YGC 比较一分钟好几次，每次上百 毫秒\n\n频繁 FGC 几分钟就一次 FGC，一次几秒钟 这都是不正常的，就需要优化了。\n\n参考：[关于GC](https://juejin.cn/post/6860752870480805902)\n\n### 11、配置参数\n\n参考：[JVM内存调优总结 -Xms -Xmx -Xmn -Xss 参数设置](https://blog.csdn.net/shadow_zed/article/details/88047808)\n\n```\n堆设置\n-Xms :初始堆大小\n-Xmx :最大堆大小\n-Xss :每个线程的堆栈大小\n-Xmn :年轻代大小\n-XX:NewSize=n :设置年轻代大小\n-XX:NewRatio=n: 设置年轻代和年老代的比值。如:为3，表示年轻代与年老代比值为1：3，年轻代占整个年轻代年老代和的1/4\n-XX:SurvivorRatio=n :年轻代中Eden区与两个Survivor区的比值。注意Survivor区有两个。如：3，表示Eden：Survivor=3：2，一个Survivor区占整个年轻代的1/5\n-XX:MaxPermSize=n :设置持久代大小\n收集器设置\n-XX:+UseSerialGC :设置串行收集器\n-XX:+UseParallelGC :设置并行收集器\n-XX:+UseParalledlOldGC :设置并行年老代收集器\n-XX:+UseConcMarkSweepGC :设置并发收集器\n垃圾回收统计信息\n-XX:+PrintGC\n-XX:+PrintGCDetails\n-XX:+PrintGCTimeStamps\n-Xloggc:filename\n并行收集器设置\n-XX:ParallelGCThreads=n :设置并行收集器收集时使用的CPU数。并行收集线程数。\n-XX:MaxGCPauseMillis=n :设置并行收集最大暂停时间\n-XX:GCTimeRatio=n :设置垃圾回收时间占程序运行时间的百分比。公式为1/(1+n)\n并发收集器设置\n-XX:+CMSIncrementalMode :设置为增量模式。适用于单CPU情况。\n-XX:ParallelGCThreads=n :设置并发收集器年轻代收集方式为并行收集时，使用的CPU数。并行收集线程数。\n\n```\n\n比如springboot web服务的jvm参数\n\n```\njava -jar \\\n-XX:+UseG1GC \\\n-XX:MaxGCPauseMillis=200 \\\n-Xloggc:../gc-%t.log \\\n-XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=32 -XX:GCLogFileSize=64m \\\n-XX:+PrintGCDetails \\\n-XX:+PrintGCDateStamps \\\n-XX:+PrintTenuringDistribution \\\n-XX:+PrintHeapAtGC \\\n-XX:+PrintReferenceGC \\  显示各种Reference的个数和处理时间\n-XX:+PrintGCApplicationStoppedTime \\\n-Xms15g \\\n-Xmx15g \\\n/xxx.jar\n\n```\n\n比如es默认的jvm参数\n\n```\n[2021-04-29T10:14:15,590][INFO ][o.e.n.Node               ] \nJVM arguments [\n-Xms1g, \n-Xmx1g, \n-XX:+UseConcMarkSweepGC, \n-XX:CMSInitiatingOccupancyFraction=75, \n-XX:+UseCMSInitiatingOccupancyOnly, \n-XX:+AlwaysPreTouch, \n-Xss1m, \n-Djava.awt.headless=true, \n-Dfile.encoding=UTF-8, \n-Djna.nosys=true, \n-XX:-OmitStackTraceInFastThrow, \n-Dio.netty.noUnsafe=true, \n-Dio.netty.noKeySetOptimization=true, \n-Dio.netty.recycler.maxCapacityPerThread=0, \n-Dlog4j.shutdownHookEnabled=false, \n-Dlog4j2.disable.jmx=true, \n-Djava.io.tmpdir=/tmp/elasticsearch.P9YagNAJ, \n-XX:+HeapDumpOnOutOfMemoryError, \n-XX:+PrintGCDetails, \n-XX:+PrintGCDateStamps, \n-XX:+PrintTenuringDistribution, \n-XX:+PrintGCApplicationStoppedTime, \n-Xloggc:logs/gc.log, \n-XX:+UseGCLogFileRotation, \n-XX:NumberOfGCLogFiles=32, \n-XX:GCLogFileSize=64m, \n-Des.path.home=/home/lintong/software/apache/elasticsearch-6.2.4, \n-Des.path.conf=/home/lintong/software/apache/elasticsearch-6.2.4/config\n]\n\n```\n\n　　\n\n**JVM参数设置参考：**[JVM系列三:JVM参数设置、分析](https://www.cnblogs.com/redcreen/archive/2011/05/04/2037057.html)\n\n```\njmap -heap 29\nAttaching to process ID 29, please wait...\nDebugger attached successfully.\nServer compiler detected.\nJVM version is 25.121-b13\n\nusing thread-local object allocation.\nParallel GC with 33 thread(s)\n\nHeap Configuration:\n   MinHeapFreeRatio         = 0\n   MaxHeapFreeRatio         = 100\n   MaxHeapSize              = 32210157568 (30718.0MB)\n   NewSize                  = 715653120 (682.5MB)\n   MaxNewSize               = 10736369664 (10239.0MB)\n   OldSize                  = 1431830528 (1365.5MB)\n   NewRatio                 = 2\n   SurvivorRatio            = 8\n   MetaspaceSize            = 21807104 (20.796875MB)\n   CompressedClassSpaceSize = 1073741824 (1024.0MB)\n   MaxMetaspaceSize         = 17592186044415 MB\n   G1HeapRegionSize         = 0 (0.0MB)\n\nHeap Usage:\nPS Young Generation\nEden Space:\n   capacity = 5859442688 (5588.0MB)\n   used     = 2972839024 (2835.1202239990234MB)\n   free     = 2886603664 (2752.8797760009766MB)\n   50.73586657120658% used\nFrom Space:\n   capacity = 505937920 (482.5MB)\n   used     = 388770920 (370.7608413696289MB)\n   free     = 117167000 (111.7391586303711MB)\n   76.84162515432723% used\nTo Space:\n   capacity = 558891008 (533.0MB)\n   used     = 0 (0.0MB)\n   free     = 558891008 (533.0MB)\n   0.0% used\nPS Old Generation\n   capacity = 2802843648 (2673.0MB)\n   used     = 1549069632 (1477.3079223632812MB)\n   free     = 1253774016 (1195.6920776367188MB)\n   55.26778609664352% used\n```\n\n### 12**、**判断Java对象是否存活的方法\n\nHotSpot虚拟机使用的是可达性分析算法\n\n其他的还有引用计数算法\n\n参考：[判断Java对象是否存活的方法](https://blog.csdn.net/TimHeath/article/details/53055193)\n\n### 13**、safepoint和stop the world**\n\n**safepoint**就是一个安全点，所有的线程执行到安全点的时候就会去检查是否需要执行**safepoint**操作，如果需要执行，那么所有的线程都将会等待，直到所有的线程进入**safepoint**。 然后JVM执行相应的操作之后，所有的线程再恢复执行。\n\nGC 一定需要所有线程同时进入 SafePoint，并停留在那里，等待 GC 处理完内存，再让所有线程继续执。像这种 所有线程进入 SafePoint 等待的情况，就是**Stop the world**\n\n&nbsp;\n\n## 3 Java设计模式\n\n所有设计模式请参考：[https://www.runoob.com/design-pattern/design-pattern-tutorial.html](https://www.runoob.com/design-pattern/design-pattern-tutorial.html)\n\n### **1<strong>、**单例模式</strong>\n\n保证整个系统中一个类只有一个对象的实例，实现这种功能的方式就叫单例模式。\n\n优点：\n\n1.单例模式节省公共资源：在实际的项目中，有一些东西是大家共用的，比如：系统配置、redis实例、数据库实例等，我们不希望每个用户在使用这些资源的时候，自己去新建一份实例，这样会造成资源的浪费。\n\n2.单例模式方便控制：\n\n使用场景：获取jdbc连接，参考：&nbsp;[每日一发设计模式 - 单例模式（singleton）](https://www.jianshu.com/p/faa86bb5466d)\n\n解决方案：判断该类是否已经有实例，如果有则返回，没有则创建\n\n关键代码：私有化构造函数，让使用方不能随意的去实例\n\n实现方案：懒汉（有线程安全和不安全2种）、饿汉、双重校验锁、静态内部类、枚举，参考：[单例模式详解](https://zhuanlan.zhihu.com/p/51854665)\n\n代码：[Java单例设计模式](https://www.cnblogs.com/tonglin0325/p/5196818.html)\n\n### **2、工厂模式**\n\n程序在**接口和子类**之间加入一个**过渡类**，通过此**过渡类**端取得**接口的实例化对象**，一般都会称这个过渡端为**工厂类**\n\n《深入浅出mybatis》P130\n\n[Java工厂设计模式](https://www.cnblogs.com/tonglin0325/p/5241284.html)\n\n### **3<strong>、**适配器模式</strong>\n\n适配器设计模式，一个接口首先被一个抽象类先实现（此抽象类通常称为**适配器类**，比如下面的**WindowAdapter**），并在此抽象类中实现若干方法（但是这个抽象类中的方法体是空的），则以后的子类直接继承此抽象类，就可以有选择地覆写所需要的方法。\n\n《深入浅出mybatis》P145\n\n### **4、观察者设计**模式\n\n在java.util包中提供了**Observable类**和**Observer接口**，使用它们即可完成**观察者模式**。\n\n参考：[Java观察者设计模式](https://www.cnblogs.com/tonglin0325/p/5270965.html)\n\n&nbsp;\n\n## 4 Java多线程八股文\n\n### 1、线程和进程区别\n\n**<img src=\"/images/517519-20210427202216837-1942521182.png\" width=\"550\" height=\"245\" loading=\"lazy\" />**\n\n**进程**是系统分配资源的最小单位，是程序的一次动态执行过程，它经历了从代码加载、执行到执行完毕的一个完整过程，这个过程也是进程本身从产生、发展到最终消亡的过程。\n\n**线程**是CPU调度的基本单位。\n\n**多进程**操作系统能同时运行多个进程(程序)，由于CPU具备分时机制，所以每个进程都能循环获得自己的CPU时间片。\n\n**多线程**是指一个进程在执行过程中可以产生多个线程，这些线程可以同时存在、同时运行，**一个进程**可能包含了**多个同时执行的线程**。\n\n比如JVM就是一个操作系统，每当使用java命令执行一个类时，实际上都会启动一个jvm，每一个JVM实际上就是在操作系统中启动一个进程，java本身具备了垃圾回收机制，\n\n所以每个java运行时**至少会启动两个线程**，**一个main线程**，**另外一个是垃圾回收机制**。\n\n参考：[Java多线程](https://www.cnblogs.com/tonglin0325/p/5252044.html)\n\n### 2、线程的生命周期：\n\n<img src=\"/images/517519-20160308162330882-1429306344.png\" alt=\"\" />\n\n已经**废弃**的Thread的suspend()和resume()\n\nObject.wait：使得当前锁持有者的**线程暂停**，如果当前线程不是锁持有者，会抛出IllegalMonitorStateException异常，参考：[Object.wait()与Object.notify()的用法](https://www.cnblogs.com/xwdreamer/archive/2012/05/12/2496843.html)\n\nObject.notifyAll：解除**所有**那些在该对象上调用wait方法的线程的阻塞状态\n\nyeild：**线程的礼让。在线程的操作中，可以使用<strong>yield()方法**将一个线程的操作暂时让给其他线程执行。本线程暂停，让其他进程先执行。</strong>\n\nstop：**停止线程运行。**\n\nsleep：**线程的休眠。在程序中允许一个线程进行暂时的休眠，直接使用<strong>Thread.sleep()方法**即可实现休眠。</strong>\n\njoin：**线程的强制运行。在线程操作中，可以使用<strong>join()方法**让一个线程强制运行，线程强制运行期间，期间线程无法运行，必须等待此线程完成之后才可以继续执行。</strong>\n\nstart：**开启线程运行。**\n\nrun：在线程启动时虽然调用的是start()方法，但是实际上调用的却是run()方法的主体\n\n具体方式使用参考：[Java线程操作方法](https://www.cnblogs.com/tonglin0325/p/5252403.html)\n\n### 3、实现runnable接口和继承Thread类\n\n代码参考：[Java多线程](https://www.cnblogs.com/tonglin0325/p/5252044.html)\n\n### 4、Thread和Runnable区别\n\n**实现Runnable接口相对于继承Thread类来说，有下列优势：**\n\n<1>适合多个相同程序代码的线程去处理同一资源的情况；如果一个类继承了Thread类，则不适合多个线程共享数据；而实现了Runnable接口，就可以实现资源的共享\n\n<2>可以避免由于Java的单继承特性带来的局限\n\n<3>增强了程序的健壮性，代码能够被多个线程共享，代码和数据是独立的\n\n参考：[Java多线程](https://www.cnblogs.com/tonglin0325/p/5252044.html)：\n\n### 5、**资源的同步**\n\n参考：[Java同步synchronized与死锁](https://www.cnblogs.com/tonglin0325/p/5253434.html)\n\n### 6、Object的wait()和notifyAll()\n\n以及已经**废弃**的Thread的suspend()和resume()\n\nwait是使得当前锁持有者的线程暂停，如果当前线程不是锁持有者，会抛出IllegalMonitorStateException异常\n\nnotifyAll解除**所有**那些在该对象上调用wait方法的线程的阻塞状态\n\n参考：[Object.wait()与Object.notify()的用法](https://www.cnblogs.com/xwdreamer/archive/2012/05/12/2496843.html)\n\nwait()会放弃对象锁，sleep()不会释放对象锁\n\n因为：在Java中，每个对象都有两个池，锁(monitor)池和等待池\n\n如果线程调用了对象的 wait()方法，那么线程便会处于该对象的等待池中，等待池中的线程不会去竞争该对象的锁。\n\n当有线程调用了对象的 notifyAll()方法（唤醒所有 wait 线程）或 notify()方法（只随机唤醒一个 wait 线程），被唤醒的的线程便会进入该对象的锁池中，锁池中的线程会去竞争该对象锁。\n\n参考：[一题带你彻底理解 sleep() 和 wait()](https://cloud.tencent.com/developer/article/1372649)\n\n### 7、volatile\n\n一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义：\n\n1. 保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。\n\n2. 禁止进行指令重排序。\n\n参考：[Java关键字&mdash;&mdash;volatile](https://www.cnblogs.com/tonglin0325/p/5251978.html)\n\n<img src=\"/images/517519-20210614145029502-1168550414.png\" width=\"800\" height=\"428\" loading=\"lazy\" />\n\nvolatile原理：\n\n### 8、synchronized\n\nSynchronized能够实现原子性和可见性；在Java内存模型中，synchronized规定，线程在加锁时，先清空工作内存&rarr;在主内存中拷贝最新变量的副本到工作内存&rarr;执行完代码&rarr;将更改后的共享变量的值刷新到主内存中&rarr;释放互斥锁。\n\n参考：[Java同步synchronized与死锁](https://www.cnblogs.com/tonglin0325/p/5253434.html)\n\n### 9、**Synchronized和Volatile的比较**\n\n&nbsp;&nbsp;&nbsp; 1）Synchronized保证内存可见性和操作的原子性<br />&nbsp;&nbsp;&nbsp; 2）Volatile只能保证内存可见性<br />&nbsp;&nbsp;&nbsp; 3）Volatile不需要加锁，比Synchronized更轻量级，并不会阻塞线程（volatile不会造成线程的阻塞；synchronized可能会造成线程的阻塞。）<br />&nbsp;&nbsp;&nbsp; 4）volatile标记的变量不会被编译器优化,而synchronized标记的变量可以被编译器优化（如编译器重排序的优化）.<br />&nbsp;&nbsp;&nbsp; 5）volatile是变量修饰符，仅能用于变量，而synchronized是一个方法或块的修饰符。<br />\n\nvolatile本质是在告诉JVM当前变量在寄存器中的值是不确定的，使用前，需要先从主存中读取，因此可以实现可见性。而对n=n+1,n++等操作时，volatile关键字将失效，不能起到像synchronized一样的线程同步（原子性）的效果。\n\n参考：[内存可见性和原子性：Synchronized和Volatile的比较](https://blog.csdn.net/guyuealian/article/details/52525724)\n\n### 10、各种锁\n\n<img src=\"/images/517519-20210612205317056-1608023324.png\" width=\"800\" height=\"703\" loading=\"lazy\" />\n\n&nbsp;参考：[不可不说的Java&ldquo;锁&rdquo;事](https://tech.meituan.com/2018/11/15/java-lock.html)\n\n### 11、悲观锁和乐观锁\n\n**悲观锁**：\n\n每次去取数据，很悲观，都觉得会被别人修改，所以在拿数据的时候都会上锁。\n\n简言之，共享资源每次都只给一个线程使用，其他线程阻塞，等第一个线程用完后再把资源转让给其他线程。\n\nsynchronized和ReentranLock等都是悲观锁思想的体现。\n\n**乐观锁**：\n\n每次去取数据，都很乐观，觉得不会被被人修改。\n\n因此每次都不上锁，但是在更新的时候，就会看别人有没有在这期间去更新这个数据，如果有更新就重新获取，再进行判断，一直循环，直到拿到没有被修改过的数据。\n\nCAS(Compare and Swap 比较并交换)就是乐观锁的一种实现方式，比如使用version字段或者修改时间字段来判断数据是都被修改\n\nCAS是一种著名的无锁算法，直白来说就是在不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，因此也被称为非阻塞同步算法（non-blocking synchronization）\n\n参考：[多用户同时操作一条Mysql记录问题](https://www.cnblogs.com/tonglin0325/p/11725080.html)\n\n乐观锁适用于读多写少的情况下（多读场景），悲观锁比较适用于写多读少场景 \n\n### 12、**公平锁**和非公平锁的区别\n\n公平锁：多个线程按照申请锁的顺序去获得锁，线程会直接进入队列去排队，永远都是队列的第一位才能得到锁。<br />\n\n　　优点：所有的线程都能得到资源，不会饿死在队列中。\n\n　　缺点：吞吐量会下降很多，队列里面除了第一个线程，其他的线程都会阻塞，cpu唤醒阻塞线程的开销会很大。<br />\n\n非公平锁：多个线程去获取锁的时候，会直接去尝试获取，获取不到，再去进入等待队列，如果能获取到，就直接获取到锁。<br />\n\n　　优点：可以减少CPU唤醒线程的开销，整体的吞吐效率会高点，CPU也不必取唤醒所有线程，会减少唤起线程的数量。<br />\n\n　　缺点：你们可能也发现了，这样可能导致队列中间的线程一直获取不到锁或者长时间获取不到锁，导致饿死。 <br />\n\n参考：[阿里面试官：说一下公平锁和非公平锁的区别？](https://blog.csdn.net/qq_35190492/article/details/104943579)\n\n默认的ReentrantLock是非公平锁\n\nReentrantLock lock = new ReentrantLock(true); // 公平锁是true\n\n### 13、自旋锁\n\n自旋锁（spinlock）：是指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。\n\n获取锁的线程一直处于活跃状态，但是并没有执行任何有效的任务，使用这种锁会造成busy-waiting。\n\n参考：[Java中的自旋锁](https://blog.csdn.net/fuyuwei2015/article/details/83387536)\n\n### 14、无锁 VS 偏向锁 VS 轻量级锁 VS 重量级锁\n\n偏向锁通过对比Mark Word解决加锁问题，避免执行CAS操作。而轻量级锁是通过用CAS操作和自旋来解决加锁问题，避免线程阻塞和唤醒而影响性能。重量级锁是将除了拥有锁的线程以外的线程都阻塞。\n\n参考：[不可不说的Java&ldquo;锁&rdquo;事](https://tech.meituan.com/2018/11/15/java-lock.html)\n\n### 15、**Synchronized**和Lock的区别\n\n**Synchronized**是关键字，内置语言实现，**Lock**是接口。 \n\n**Synchronized**在线程发生异常时会自动释放锁，因此不会发生异常死锁。 \n\n**Lock**异常时不会自动释放锁，所以需要在finally中实现释放锁。 \n\n**Lock**是可以中断锁，**Synchronized**是非中断锁，必须等待线程执行完成释放锁。\n\n<img src=\"/images/517519-20210612214830395-1922687589.png\" width=\"500\" height=\"319\" loading=\"lazy\" />\n\n条件队列：\n\nSynchronized只有一个等待队列。\n\nReentrantLock中一把锁可以对应多个条件队列。通过newCondition表示。\n\n参考：[从ReentrantLock的实现看AQS的原理及应用](https://tech.meituan.com/2019/12/05/aqs-theory-and-apply.html) \n\n### **16、Atomic类如何保证原子性**\n\n**CAS操作**\n\n### 17、java线程间通信的方式\n\n1.共享变量\n\n2.BlockingQueue\n\n参考：[JAVA线程间通信的几种方式](https://blog.csdn.net/u011514810/article/details/77131296)\n\n## 5 Java多线程题目\n\n### 1、实现一个**死锁**\n\n**死锁**就是指两个线程都在等待彼此先完成，造成了程序的停滞,一般程序的死锁都是在程序运行时出现的。\n\n多个线程共享同一资源时需要进行同步，以保证资源操作的完整性，但是过多的同步就有可能产生死锁。\n\n即程序hang住，2个进程都在等待对象释放锁，参考本地代码DeadLock或者文章：[https://blog.csdn.net/qq_35064774/article/details/51793656](https://blog.csdn.net/qq_35064774/article/details/51793656)\n\n**思路**：写一个DeadLock，实现了Runnable接口，再写2个static的成员变量，用于互相加锁\n\n### 2、实现交替打印\n\n实现2个线程，交替打印，第一个线程打印后间隔1秒打印第二个，第二个线程间隔3秒后打印第一个\n\n参考本地代码 PrintAlternately，以及 [面试官：线程交替打印，你能实现几种？看完这篇吊打面试官！](https://zhuanlan.zhihu.com/p/446138152)\n\n**思路**：1.使用ReentrantLock和2个Condition，thread1先lock，打印结束后Condition t2 signal唤醒，Condition t1 await等待，最后unlock；thread2相反，记得要先唤醒另外一个线程\n\n2.使用synchronized，然后锁住一个Object对象，先打印，再lock.notify()，然后lock.wait()\n\n3.使用Semaphore来实现，size为1，fair为true（下次执行的线程会是等待最久的线程，否则是随机）；先acquire()获取访问许可，然后再release()释放访问许可\n\n### 3、实现生产者和消费者\n\n实现2个线程，一个线程生产，一个线程消费\n\n参考本地代码 MyProducerConsumer\n\n**思路**：定义一个MyDeque，有一个成员变量deuqe，和2个synchronized的方法produce和consume，再定义一个Worker，成员变量有MyQueue，以及启动生产者和消费者线程的代码\n\n### 4、实现BlockingQueue\n\n参考本地代码BoundedBlockingQueue，一个锁内部可以有多个Condition，即有多路等待和通知，可以参看jdk1.5提供的Lock与Condition实现的可阻塞队列的应用案例 \n\n**思路**：定义个BoundedBlockingQueue，使用ReentrantLock和2个Condition，入队列的时候，先lock，当队列满，使得enCond while一直等待awit()，不为空入队列，然后触发deCond的signal，最后unlock；出队列的时候相反\n\n### 5、实现一个多线程订票\n\n参考本地代码SaleTickets\n\n**思路**：1.使用synchronized关键字，定义一个SaleWorker类，实现Runnable，在run()方法中while(true)订票，使用同步代码块synchronized(this)，或者调用同步方法sale()\n\n2.使用AtomInteger+CAS来实现，记下pre和next，使用compareAndSet的返回值来判断是否更新成功\n\n### 6、按序打印\n\n实现多个线程从1-N按顺序打印数字\n\n参考本地代码PrintOneToN\n\n**思路**：使用synchronized关键字，定义一个PrintWorker类，和多线程订票类似 \n\n### 7、编写两个线程，一个线程打印1~52，另一个线程打印字母A~Z，打印顺序为12A34B56C&hellip;&hellip;5152Z，要求使用线程间的通信\n\n**思路**：使用synchronized关键字+notify()+wait()，记得先唤醒，再等待\n\n题目地址：[JAVA线程间通信的几种方式](https://www.cnblogs.com/wumz/p/11306502.html)，该文章分别使用了synchronized+notify+wait、Lock+Condition、volatile、AtomicInteger、CyclicBarrier、PipedInputStream、BlockingQueue的方法实现\n\n### 8、实现reentrantlock\n\n使用synchronized实现reentrantlock\n\n参考本地代码MyReentrantLock\n\n**思路**：实现lock()和unlock()方法，lock方法通过比较线程的id，while如果一直是-1，则wait()，然后更新线程id；unlock方法，将线程id更新为-1，然后notify()\n\n### 9、实现ReadWriteLock\n\n实现ReadWriteLock类\n\n参考本地代码MyReadWriteLock\n\n**思路**：有写锁的时候，其他线程不能读也不能写；有读锁的时候，其他线程可以读但是不能写\n\n&nbsp;\n\n[https://leetcode-cn.com/problems/print-in-order/submissions/](https://leetcode-cn.com/problems/print-in-order/submissions/)\n\n使用**信号灯**，参考：[Java多线程&mdash;&mdash;Semaphore信号灯](https://www.cnblogs.com/tonglin0325/p/6264634.html)\n\n使用**倒计时器**，参考：[Java多线程&mdash;&mdash;其他工具类CyclicBarrier、CountDownLatch和Exchange](https://www.cnblogs.com/tonglin0325/p/6265379.html)\n\n&nbsp;\n\n## 2.MySQL\n\nMySQL支持诸多存储引擎，而各种存储引擎对索引的支持也各不相同，因此MySQL数据库支持多种索引类型，如BTree索引，B+Tree索引，哈希索引，全文索引等等\n\n### **1<strong><strong>、Mysql**</strong>索引分类</strong>\n\n数据结构：B+，hash，全文索引，R-Tree\n\n物理存储：聚簇索引、非聚簇索引\n\n逻辑角度：主键、单列索引、多列索引、空间索引\n\n### **2<strong><strong>、**</strong>不同存储引擎的索引类型</strong>\n\n参考：\n\n```\nhttps://dev.mysql.com/doc/refman/5.7/en/create-index.html\n\n```\n\n<img src=\"/images/517519-20210305110438653-757965480.png\" alt=\"\" />\n\n### **3<strong>、**InnoDB索引引擎的特性</strong>\n\n参考：\n\n```\nhttps://dev.mysql.com/doc/refman/5.7/en/innodb-introduction.html\n\n```\n\n**<img src=\"/images/517519-20210305111449826-752557055.png\" width=\"1000\" height=\"613\" />**\n\n### **4****<strong>、**</strong>Innodb存储引擎使用的是B+树\n\n[为什么MySQL索引要用B+树，而不是B树？](https://database.51cto.com/art/201909/603430.htm)\n\n[mysql InnoDB引擎支持hash索引吗](https://blog.csdn.net/doctor_who2004/article/details/77414742)\n\n### **5<strong><strong>、**</strong>Innodb</strong>和**MyISAM**区别\n\n**存储区别**\n\nMyISAM在磁盘上会存储3个文件：frm文件存储表定义，myd存储数据文件，myi存储索引文件\n\nInnodb在磁盘上会存储2个文件：frm文件存储表定义，ibd存储数据和索引文件\n\n**锁区别**\n\nMyISAM存储引擎使用的是表锁，Innodb存储引擎默认使用的是行锁，同时也支持表级锁\n\n参考：[【一分钟系列】一分钟快速了解MyISAM和Innodb的主要区别](https://www.bilibili.com/s/video/BV1Qk4y167pi)\n\n### **6<strong><strong>、**</strong>B树</strong>\n\nBTree是平衡搜索多叉树，设树的度为2d（d>1），高度为h，那么BTree要满足以一下条件：\n\n每个叶子结点的高度一样，等于h；\n\n每个非叶子结点由n-1个key和n个指针point组成，其中d<=n<=2d,key和point相互间隔，结点两端一定是key；\n\n叶子结点指针都为null；\n\n非叶子结点的key都是[key,data]二元组，其中key表示作为索引的键，data为键值所在行的数据；\n\n<img src=\"/images/517519-20210305143151990-257814282.png\" alt=\"\" />\n\n在BTree的结构下，就可以使用二分查找的查找方式，查找复杂度为h*log(n)，一般来说树的高度是很小的，一般为3左右，因此BTree是一个非常高效的查找结构。\n\n### **7<strong><strong>、**</strong>B+树</strong>\n\nB+Tree是BTree的一个变种，设d为树的度数，h为树的高度，B+Tree和BTree的不同主要在于：\n\nB+Tree中的非叶子结点不存储数据，只存储键值；\n\nB+Tree的叶子结点没有指针，所有键值都会出现在叶子结点上，且key存储的键值对应data数据的物理地址；\n\nB+Tree的每个非叶子节点由n个键值key和n个指针point组成；\n\n<img src=\"/images/517519-20210305143310496-2120756542.png\" alt=\"\" />\n\n### **8<strong><strong>、**</strong>B+Tree对比BTree的优点</strong>\n\n1、磁盘读写代价更低\n\n一般来说B+Tree比BTree更适合实现外存的索引结构，因为存储引擎的设计专家巧妙的利用了外存（磁盘）的存储结构，即磁盘的最小存储单位是扇区（sector），而操作系统的块（block）通常是整数倍的sector，操作系统以页（page）为单位管理内存，一页（page）通常默认为4K，数据库的页通常设置为操作系统页的整数倍，因此索引结构的节点被设计为一个页的大小，然后利用外存的&ldquo;预读取&rdquo;原则，每次读取的时候，把整个节点的数据读取到内存中，然后在内存中查找，已知内存的读取速度是外存读取I/O速度的几百倍，那么提升查找速度的关键就在于尽可能少的磁盘I/O，那么可以知道，每个节点中的key个数越多，那么树的高度越小，需要I/O的次数越少，因此一般来说B+Tree比BTree更快，因为B+Tree的非叶节点中不存储data，就可以存储更多的key。\n\n2、查询速度更稳定\n\n由于B+Tree非叶子节点不存储数据（data），因此所有的数据都要查询至叶子节点，而叶子节点的高度都是相同的，因此所有数据的查询速度都是一样的。\n\n### **9<strong><strong>、聚簇索引和非举簇索引**</strong></strong>\n\nMySQL中最常见的两种存储引擎分别是MyISAM和InnoDB，分别实现了非聚簇索引和聚簇索引。\n\n聚簇索引的解释是：聚簇索引的顺序就是数据的物理存储顺序\n\n非聚簇索引的解释是：索引顺序与数据物理排列顺序无关\n\n&nbsp;<img src=\"/images/517519-20210308155731828-860798107.png\" alt=\"\" width=\"547\" height=\"470\" />\n\n具体参考：[深入理解MySQL索引原理和实现&mdash;&mdash;为什么索引可以加速查询？](https://blog.csdn.net/tongdanping/article/details/79878302)\n\n上图中的Secondary Key就是**二级索引**\n\n参考：[Mysql聚簇索引和二级索引到底有何不同](https://zhuanlan.zhihu.com/p/137647823)\n\n### 1**0****<strong>、**</strong>脏读、幻读与不可重复读\n\n脏读：所谓脏读是指一个事务中访问到了另外一个事务未提交的数据\n\n幻读：一个事务读取2次，得到的记录条数不一致\n\n不可重复读：一个事务读取同一条记录2次，得到的结果不一致\n\n参考：[脏读、幻读与不可重复读](https://juejin.cn/post/6844903665367547918)\n\n### 11、MySQL binlog三种模式\n\n1. Row Level：日志中会记录成每一行数据被修改的情况，然后在slave端再对相同的数据进行修改。\n\n**&nbsp; &nbsp;优点：**在row level情况下，bin-log中可以不记录执行的sql语句上下文相关的信息，仅仅只需要记录那一条记录被修改了，修改成说明样子。所以row level的日志内容会非常清楚的记录下每一行数据修改的细节，非常容易理解，而且不会出现某些特点情况下的存储过程或function，以及trigget的电泳和触发无法被正确复制的问题。\n\n**&nbsp; &nbsp;缺点**：row level下，所有的执行语句当记录到日志中的时候，都将以每行记录的修改来记录，这样可能会产生大量的日志内容，比如会产生大量的binlog日志。\n\n2. Statement Level（默认）：每一条被修改的数据的sql语句都会记录到master的binlog中，slave在复制的时候sql进程会解析成和原来master端执行过的相同的sql来再次执行。\n\n**&nbsp;&nbsp; 优点：**解决了行模式的缺点，它不需要记录每一行数据的变化，减少binlog日志量，节约磁盘IO，提高性能，因为他只需要记录在Master上所执行的语句的细节，以及执行语句时候的上下文的信息。\n\n**&nbsp;&nbsp; 缺点：**由于它是记录的执行语句，所以，为了让这些语句在slave端也能正确执行，那么他还必须记录每条语句在执行的时候的一些相关信息，也就是上下文信息，以保证所有语句在slave端被执行的时候能够得到和在master端执行时候相同的结果，另外就是，由于MaSQL现在发展比较快，很多新功能不断的加入，使MySQL的复制遇到了不小的挑战。容易造成主从数据不一致。\n\n3. Mixed：实际上就是钱两种模式的结合，在mixed模式下，MySQL会根据执行的每一条具体的sql语句来区分对待记录的日志形式，也就是在Staterment和row行模式之间选一种，当他认为数据可能会造成不一致的情况时，他就会选择行模式，当他认为这是一百万条记录只需要一条就搞定了，就会悬着选择statement模式\n\n企业如何选择binlog的模式\n\n1. 互联网公司，使用MySQL的功能相对少（存储过程、触发器、函数）。\n\n&nbsp;&nbsp;选择默认的语句模式，statementlevel（默认）。\n\n2. 公司如果用到使用MySQL的特殊功能（存储过程、触发器、函数）。\n\n&nbsp;&nbsp;则选择Mined模式。\n\n3. 公司如果用到使用MySQL特殊功能（存储过程、触发器、函数），又希望数据最大化一致，此时最 好Row level模式。\n\n### 12、MySQL表级锁和行级锁<br />\n\nMySQL的锁机制比较简单，其最显著的特点是不同的存储引擎支持不同的锁机制。比如，MyISAM和MEMORY存储引擎采用的是表级锁（table-level\n locking）；InnoDB存储引擎既支持行级锁（ row-level locking），也支持表级锁，但默认情况下是采用行级锁。\n\nMySQL主要的两种锁的特性可大致归纳如下:<br />\n\n\n表级锁： 开销小，加锁快；不会出现死锁(因为MyISAM会一次性获得SQL所需的全部锁)；锁定粒度大，发生锁冲突的概率最高,并发度最低。<br />\n\n\n行级锁： 开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低,并发度也最高。<br />\n\n\n&nbsp;\n\n考虑上述特点，表级锁使用与并发性不高，以查询为主，少量更新的应用，比如小型的web应用；而行级锁适用于高并发环境下，对事务完整性要求较高的系统，如在线事务处理系统。\n\n### 13、MVCC多版本并发控制\n\nMVCC，全称Multi-Version Concurrency Control，即多版本并发控制。\n\nMVCC是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问，在编程语言中实现事务内存。\n\n多版本的意思就是数据库中同时存在多个版本的数据，并不是整个数据库的多个版本，而是某一条记录的多个版本同时存在，在某个事务对其进行操作的时候，需要查看这一条记录的隐藏列事务版本id，比对事务id并根据事物隔离级别去判断读取哪个版本的数据。\n\nMVCC的作用：\n\n1.MVCC在MySQL InnoDB中的实现主要是为了提高数据库并发性能，用更好的方式去处理读-写冲突，做到即使有读写冲突时，也能做到不加锁，非阻塞并发读<br />2.我们都知道并发访问数据库造成的四种问题（脏写（修改丢失）、脏读、不可重复读、幻读），MVCC就是在尽量减少锁使用的情况下高效避免这些问题\n\nMVCC的实现原理：\n\n[MVCC到底是什么？这一篇博客就够啦](https://blog.csdn.net/flying_hengfei/article/details/106965517)\n\n### 14、查询在什么时候不走（预期中的）索引\n\n### **15****<strong>、**</strong>MySQL数据库隔离级别\n\n在数据库操作中，为了有效保证并发读取数据的正确性，提出的事务隔离级别。我们的数据库锁，也是为了构建这些隔离级别存在的。总共有4种：\n\n1.未提交读(Read Uncommitted)：允许脏读，也就是可能读取到其他会话中未提交事务修改的数据\n\n2.提交读(Read Committed)：只能读取到已经提交的数据。Oracle等多数数据库默认都是该级别 (不重复读)\n\n3.可重复读(Repeated Read)：可重复读。在同一个事务内的查询都是事务开始时刻一致的，InnoDB默认级别。在SQL标准中，该隔离级别消除了不可重复读，但是还存在幻象读\n\n4.串行读(Serializable)：完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞\n\n<img src=\"/images/517519-20210424201124177-1141693302.png\" width=\"1000\" height=\"177\" />\n\n参考美团点评的文章：[Innodb中的事务隔离级别和锁的关系](https://tech.meituan.com/2014/08/20/innodb-lock.html)\n\n&nbsp;\n\n其他一些八股文：[面试卡片](https://iotacecil.github.io/2018/12/25/interview-recite/)\n\n## 3.Redis\n\n### **1、Redis基本数据类型**\n\n**Redis**支持五种**数据类型**：string（字符串），hash（哈希），list（列表），set（集合）及zset(sorted set：有序集合)\n\n### **2<strong><strong>、**</strong>Redis高性能的原因</strong>\n\nredis性能卓越，作为key-value系统最大负载数量级为10W/s， set和get耗时数量级为10ms和5ms\n\n- 内存存储：Redis是使用内存(in-memeroy)存储,没有磁盘IO上的开销\n- 单线程实现：Redis使用单个线程处理请求，避免了多个线程之间线程切换和锁资源争用的开销\n- 非阻塞IO：Redis使用多路复用IO技术，在poll，epoll，kqueue选择最优IO实现 （**select**就是轮询，在Linux上限制个数一般为1024个**；****poll**解决了select的个数限制，但是依然是轮询**；****epoll**解决了个数的限制，同时解决了轮询的方式）\n- 优化的数据结构：Redis有诸多可以直接应用的优化数据结构的实现，应用层可以直接使用原生的数据结构提升性能\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n参考：[Redis为什么这么快？](https://segmentfault.com/a/1190000022088928)\n\n### **3<strong><strong>、**</strong>Redis持久化：RDB和AOF</strong>\n\nredis 持久化的两种机制：RDB，AOF\n\nRDB是一种快照存储持久化方式，具体就是将Redis某一时刻的内存数据保存到硬盘的文件当中，默认保存的文件名为dump.rdb，而在Redis服务器启动时，会重新加载dump.rdb文件的数据到内存当中恢复数据。\n\nAOF(Append-only file)持久化方式会记录客户端对服务器的每一次写操作命令，并将这些写操作以Redis协议追加保存到以后缀为aof文件末尾，在Redis服务器重启时，会加载并运行aof文件的命令，以达到恢复数据的目的。\n\n参考：[009. 图解分析 redis 的 RDB 和 AOF 两种持久化机制的工作原理](https://zq99299.github.io/note-book/cache-pdp/redis/009.html#rdb)\n\n[10分钟彻底理解Redis的持久化机制：RDB和AOF](https://www.cnblogs.com/javazhiyin/p/11425060.html)&nbsp;\n\n知乎：[Redis 持久化之RDB和AOF对比整理](https://zhuanlan.zhihu.com/p/104404202)\n\n### **4<strong><strong>、**</strong>Redis RDB和AOF的优缺点</strong>\n\n参考：[010. redis 的 RDB 和 AOF 两种持久化机制的优劣势对比](https://zq99299.github.io/note-book/cache-pdp/redis/010.html#rdb-%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6%E7%9A%84%E4%BC%98%E7%82%B9)\n\n### **5<strong><strong>、**</strong>Redis主从复制</strong>\n\n参考：[Redis 主从复制详细解读](https://learnku.com/articles/36375)\n\n### **6<strong><strong>、**</strong>Redis事务</strong>\n\nRedis 通过&nbsp;[MULTI](http://redis.readthedocs.org/en/latest/transaction/multi.html#multi)&nbsp;、&nbsp;[DISCARD](http://redis.readthedocs.org/en/latest/transaction/discard.html#discard)&nbsp;、&nbsp;[EXEC](http://redis.readthedocs.org/en/latest/transaction/exec.html#exec)&nbsp;和&nbsp;[WATCH](http://redis.readthedocs.org/en/latest/transaction/watch.html#watch)&nbsp;四个命令来实现事务功能\n\n以&nbsp;[MULTI](http://redis.readthedocs.org/en/latest/transaction/multi.html#multi)&nbsp;开始一个事务， 然后将多个命令入队到事务中， 最后由&nbsp;[EXEC](http://redis.readthedocs.org/en/latest/transaction/exec.html#exec)&nbsp;命令触发事务， 一并执行事务中的所有命令\n\n[DISCARD](http://redis.readthedocs.org/en/latest/transaction/discard.html#discard)&nbsp;命令用于取消一个事务， 它清空客户端的整个事务队列， 然后将客户端从事务状态调整回非事务状态， 最后返回字符串&nbsp;`OK`&nbsp;给客户端， 说明事务已被取消\n\n[WATCH](http://redis.readthedocs.org/en/latest/transaction/watch.html#watch)&nbsp;命令用于在事务开始之前监视任意数量的键： 当调用&nbsp;[EXEC](http://redis.readthedocs.org/en/latest/transaction/exec.html#exec)&nbsp;命令执行事务时， 如果任意一个被监视的键已经被其他客户端修改了， 那么整个事务不再执行， 直接返回失败\n\n参考：https://redisbook.readthedocs.io/en/latest/feature/transaction.html\n\n### **7<strong><strong>、**</strong>Redis分布式部署</strong>\n\nRedis 支持三种集群方案：\n\n- 主从复制模式\n- Sentinel（哨兵）模式\n- Cluster 模式\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n参考：[[Redis] 你了解 Redis 的三种集群模式吗？](https://segmentfault.com/a/1190000022808576)\n\n### **8<strong><strong>、**</strong>Redis数据过期策略</strong>\n\nRedis key过期的方式有三种：\n\n**定时过期：**每个设置过期时间的key都需要创建一个定时器，到过期时间就会立即清除。该策略可以立即清除过期的数据，对内存很友好；但是会占用大量的CPU资源去处理过期的数据，从而影响缓存的响应时间和吞吐量。\n\n**惰性过期：**只有当访问一个key时，才会判断该key是否已过期，过期则清除。该策略可以最大化地节省CPU资源，却对内存非常不友好。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，占用大量内存。\n\n**定期过期：**每隔一定的时间，会扫描一定数量的数据库的expires字典中一定数量的key，并清除其中已过期的key。该策略是前两者的一个折中方案。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得CPU和内存资源达到最优的平衡效果。(expires字典会保存所有设置了过期时间的key的过期时间数据，其中key是指向键空间中的某个键的指针，value是该键的毫秒精度的UNIX时间戳表示的过期时间。键空间是指该Redis集群中保存的所有键。)\n\nRedis同时使用了惰性过期和定期过期两种过期策略。但是Redis定期删除是随机抽取机制，不可能扫描删除掉所有的过期Key。因此需要内存淘汰机制。\n\n### **9<strong><strong>、**</strong>Redis的内存淘汰策略</strong>\n\nRedis的内存淘汰策略是指在Redis的用于缓存的内存不足时，怎么处理需要新写入且需要申请额外空间的数据。\n\n**no-eviction**：当内存不足以容纳新写入数据时，新写入操作会报错。\n\n**allkeys-lru**：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。\n\n**allkeys-random**：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。\n\n**volatile-lru**：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。\n\n**volatile-random**：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。\n\n**volatile-ttl**：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。\n\n**过期键删除策略和内存淘汰机制之间的关系：**\n\n- 过期健删除策略强调的是对过期健的操作，如果有健过期了，而内存还足够，不会使用内存淘汰机制，这时也会使用过期健删除策略删除过期健。\n- 内存淘汰机制强调的是对内存的操作，如果内存不够了，即使有的健没有过期，也要删除一部分，同时也针对没有设置过期时间的健。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&nbsp;\n\n## 大数据\n\n大数据八股文\n\n```\nhttps://www.codenong.com/cs105378376/\n\n```\n\n**&nbsp;**\n\n## **4<strong><strong>、**</strong>Kafka</strong>\n\nkafka源码阅读笔记\n\n```\nhttps://zhuanlan.zhihu.com/p/358918515\n\n```\n\nkafka的group coordinator\n\n```\nhttps://www.jianshu.com/p/833b64e141f8\n\n```\n\nrebalance时机，在如下条件下，partition要在consumer中重新分配：\n\n```\n条件1：有新的consumer加入\n条件2：旧的consumer挂了\n条件3：coordinator挂了，集群选举出新的coordinator\n条件4：topic的partition新加\n条件5：consumer调用unsubscrible()，取消topic的订阅\n```\n\nKafka 无法消费问题\n\n```\nhttps://www.infoq.cn/article/hff8ls447ao03e7bjza4\n\n```\n\n视频\n\n```\nhttps://www.youtube.com/watch?v=AVuXJHXawns\n\n```\n\n### 1、为什么kafka能实现高吞吐？单节点kafka的吞吐也比其他消息队列大，为什么？\n\n读取快：\n\n答：1. 基于sendfile的零拷贝\n\n<img src=\"/images/517519-20210117181909738-1787428184.png\" width=\"500\" height=\"285\" />&nbsp;&nbsp; <img src=\"/images/517519-20210117182025805-1953731722.png\" width=\"500\" height=\"251\" />\n\n```\nhttps://cloud.tencent.com/developer/article/1421266\n\n```\n\n2. 批量压缩，即将多个消息一起压缩而不是单个消息压缩\n\n```\nhttps://zhuanlan.zhihu.com/p/147054382\n\n```\n\n3. 操作系统提供了**预读取**技术，预读就是当顺序读取文件内容时，page cache会提前将当前读取页面之后的几个页面也加载到page cache当中，这样程序相当于直接读取cache中的内容，而不必直接与磁盘交互\n\n4. 分段日志，不是写到一个文件当中，segment=>index+log\n\n写入：\n\n1. 顺序写入日志，磁盘顺序访问速度>随机访问速度，寻址+写入，其中寻址是一个机械动作，最为耗时\n\n2. 内存映射文件（**Memory Mapped Files）**，操作系统提供的**后写入**技术，先写入page cache中，到达一定数量后由操作系统把缓存中的数据写到文件中\n\n3. 批处理，batch，不是一条一条处理\n\n### 2、kafka的偏移量存放在哪里，为什么？\n\n答：1. 0.9之前存放在zk中；\n\n2. 0.9之后存放在kafka cluster的__consumer_offset topic中，默认是50个分区\n\n3. 自定义，上面2种可能会出现数据的重复，可以禁用自动提交offset之后可以自行存储offset到其他存储中\n\n### 3、kafka里面用的是什么方式消费数据，拉的方式还是推的方式？\n\n答：拉的方式，由消费者控制速率\n\n### 4、如何保证数据不会**保证丢失**或者**重复消费**的情况？做过哪些预防措施，怎么解决以上问题的？\n\n答：\n\nA 对于**生产者**，可能发生**数据丢失**的情况：\n\n1. 采用了**发送并忘记**的消息发送方式（参考《kafka权威指南》P34，kafka的**消息发送方式**有3种：发送并忘记、同步发送、异步发送）\n\n如果要避免数据丢失，需要采用同步发送的方式，同时ack=all\n\n（ack=0是生产者在成功写入消息之前不会等待任何来自服务器的响应；ack=1是只要集群的leader节点收到消息，生产者就会收到成功响应，这时候如果发生了选举，一个没有收到消息的节点成为新首领，则会有消息丢失）\n\nack=all的时候，表示只有所有参与复制的节点(ISR列表的副本)全部收到消息时，生产者才会接收到来自服务器的响应. 这种模式是最高级别的，也是最安全的，可以确保不止一个Broker接收到了消息. 该模式的**延迟会很高**\n\n参考：[Kafka生产者ack机制剖析](https://juejin.cn/post/6857514628516315149)\n\n&nbsp;\n\nB 对于**消费者**，**重复消费**和**丢失数据**的问题，参考：\n\n```\nhttps://www.jianshu.com/p/b64379877cb8\n\n```\n\n**1.重复消费**：\n\n```\n底层根本原因：已经消费了数据，但是offset没提交。\n\n原因1：强行kill线程，导致消费后的数据，offset没有提交。\n\n原因2：设置offset为自动提交，关闭kafka时，如果在close之前，调用 consumer.unsubscribe() 则有可能部分offset没提交，下次重启会重复消费。\n\n原因3（重复消费最常见的原因）：消费后的数据，当offset还没有提交时，partition就断开连接。\n比如，通常会遇到消费的数据，处理很耗时，导致超过了Kafka的session timeout时间（0.10.x版本默认是30秒），那么就会re-blance重平衡，此时有一定几率offset没提交，会导致重平衡后重复消费。\n\n原因4：当消费者重新分配partition的时候，可能出现从头开始消费的情况，导致重发问题。\n\n原因5：当消费者消费的速度很慢的时候，可能在一个session周期内还未完成，导致心跳机制检测报告出问题。\n\n```\n\n　　\n\n**2.丢失数据的原因** ：\n\n```\n使用了offset自动提交可能导致数据丢失，也可能导致数据重复\n\n```\n\n&nbsp;\n\nC 对于**kafka集群**\n\n《kafka权威指南》P91，如果发生了&ldquo;不完全的选举&rdquo;，即允许不同步的副本成为首领，会有消息丢失的风险\n\n### 5、kafka的元数据存在哪里？\n\n答：1.zk，（/controller，/cluster，/consumer，/broker，/admin等）\n\n&nbsp;<img src=\"/images/517519-20210219111634018-574980321.png\" alt=\"\" />\n\n&nbsp;结构如下图\n\n<img src=\"/images/517519-20210219112554568-1681687868.png\" width=\"800\" height=\"524\" />\n\n&nbsp;\n\n/controller\n\n```\n[zk: xxxxx:2181(CONNECTED) 7] get /controller\n{\"version\":1,\"brokerid\":150,\"timestamp\":\"16119095XXXXX\"}\ncZxid = 0xXXXXXXbf78d\nctime = Fri Jan 29 16:38:26 CST 2021\nmZxid = 0xXXXXXXbf78d\nmtime = Fri Jan 29 16:38:26 CST 2021\npZxid = 0xXXXXXXbf78d\ncversion = 0\ndataVersion = 0\naclVersion = 0\nephemeralOwner = 0xXXXXXdce12be00d\ndataLength = 56\nnumChildren = 0\n\n```\n\n/controller_epoch\n\n```\n[zk: XXXXXXX:2181(CONNECTED) 8] get /controller_epoch \n122\n\n```\n\n/cluster\n\n```\n[zk: xxxxx:2181(CONNECTED) 12] get /cluster/id  \n{\"version\":\"1\",\"id\":\"HoEDj5VwxxxxxxxxFp1Qlg\"}\n\n```\n\n/brokers\n\n/brokers/topics\n\n```\n[zk: xxxxx:2181(CONNECTED) 20] ls /brokers/topics/test_flink/partitions\n[0, 1, 2]\n[zk: xxxxxx:2181(CONNECTED) 23] get /brokers/topics/test_flink/partitions/0/state\n{\"controller_epoch\":122,\"leader\":150,\"version\":1,\"leader_epoch\":238,\"isr\":[150,151]}\n\n```\n\n/brokers/ids\n\n```\n[zk: xxxxxx:2181(CONNECTED) 26] ls /brokers/ids\n[165, 150, 151, 152]\n[zk: xxxxxx:2181(CONNECTED) 28] get /brokers/ids/165\n{\"listener_security_protocol_map\":{\"PLAINTEXT\":\"PLAINTEXT\",\"EXTERNAL\":\"PLAINTEXT\"},\"endpoints\":[\"PLAINTEXT://xxxxxxx:9092\",\"EXTERNAL://xxxxxx:x9092\"],\"jmx_port\":9393,\"host\":\"xxxxxxx\",\"timestamp\":\"1608786878045\",\"port\":9092,\"version\":4}\n\n```\n\n/config\n\n```\n[zk: xxxxxxx:2181(CONNECTED) 34] ls /config \n[changes, clients, topics]\n[zk: xxxxxxx:2181(CONNECTED) 43] get /config/topics/test\n{\"version\":1,\"config\":{\"retention.ms\":\"604800000\"}}\n\n```\n\n/admin\n\n```\n[zk: xxxxxx:2181(CONNECTED) 45] ls /admin\n[delete_topics]\n\n```\n\n/consumer\n\n老消费者consumer的信息存放在zk中\n\n```\n[zk: xxxxxx:2181(CONNECTED) 52] ls /consumers\n[mygroupid1537449407726, console-consumer-78441, subscribeGroupid_1543820386755]\n```\n\n### 6、kafka如何保证不同的订阅源都收到相同的一份内容\n\n参考：\n\n```\nhttps://juejin.cn/post/6844903960768151566\n\n```\n\n答：HW （High Watermark）俗称高水位，它标识了一个特定的消息偏移量（offset），消费者只能拉取到这个offset之前的消息,\n\n下图表示一个日志文件，这个日志文件中只有9条消息，第一条消息的offset（LogStartOffset）为0，最有一条消息的offset为8，offset为9的消息使用虚线表示的，代表下一条待写入的消息。\n\n日志文件的 HW 为6，表示消费者只能拉取offset在 0 到 5 之间的消息，offset为6的消息对消费者而言是不可见的。\n\n<img src=\"/images/517519-20210117175138105-2099343081.png\" width=\"500\" height=\"216\" />\n\nLEO（Log End Offset），标识当前日志文件（leader副本）中下一条待写入的消息的offset。\n\n上图中offset为9的位置即为当前日志文件的 LEO，LEO 的大小相当于当前日志分区中最后一条消息的offset值加1.\n\n分区 ISR 集合中的每个副本都会维护自身的 LEO ，而 ISR 集合中最小的 LEO 即为分区的 HW，对消费者而言只能消费 HW 之前的消息。\n\n### 7、kafka中leader的选举机制？\n\n答：和zk中/controller有关，3个broker中先创建出/controller临时节点的broker为controller，由controller从ISR中选出leader，ISR一个都没有的话，leader就会为-1，其他的broker给这个/controller临时节点添加watch\n\n### 8、kafka如何删除数据\n\n答：Kakfa提供了两种策略来删除数据：&nbsp;\n\n1、顺序写入一是基于时间\n\n2、顺序写入二是基于partition文件大小\n\n```\nhttp://trumandu.github.io/2019/04/13/Kafka面试题与答案全套整理\nhttps://zhuanlan.zhihu.com/p/73475227\n\n```\n\n参考：\n\n```\nhttps://zhuanlan.zhihu.com/p/344095377\n\n```\n\n[深入理解kafka必知必会（1）](https://www.cnblogs.com/luozhiyun/p/11811835.html)\n\n### 9、Kafka中的ISR、AR又代表什么？\n\n答：简单来说，分区中的所有副本统称为 **AR (Assigned Replicas)**。所有与leader副本保持一定程度同步的副本（包括leader副本在内）组成 **ISR (In Sync Replicas)**。 ISR 集合是 AR 集合的一个子集。\n\n消息会先发送到leader副本，然后follower副本才能从leader中拉取消息进行同步。同步期间，follow副本相对于leader副本而言会有一定程度的滞后。前面所说的 &rdquo;一定程度同步&ldquo; 是指可忍受的滞后范围，这个范围可以通过参数进行配置。于leader副本同步滞后过多的副本（不包括leader副本）将组成 **OSR （Out-of-Sync Replied）**由此可见，AR = ISR + OSR。\n\n正常情况下，所有的follower副本都应该与leader 副本保持 一定程度的同步，即AR=ISR，OSR集合为空。\n\nISR的伸缩又指什么?\n\n答：leader副本负责维护和跟踪 ISR 集合中所有follower副本的滞后状态，当follower副本落后太多或失效时，leader副本会把它从 ISR 集合中剔除。如果 OSR 集合中所有follower副本&ldquo;追上&rdquo;了leader副本，那么leader副本会把它从 OSR 集合转移至 ISR 集合。\n\n默认情况下，当leader副本发生故障时，只有在 ISR 集合中的follower副本才有资格被选举为新的leader，而在OSR 集合中的副本则没有任何机会（不过这个可以通过配置来改变）。\n\n### 10、Kafka中的HW、LEO、LSO、LW等分别代表什么？\n\n答：\n\nLogStartOffset，一般情况下，日志文件的起始偏移量 logStartOffset 等于第一个日志分段的 baseOffset，但这并不是绝对的，logStartOffset 的值可以通过 DeleteRecordsRequest 请求、使用 kafka-delete-records.sh 脚本、日志的清理和截断等操作进行修改。\n\nLW是Low Watermark的缩写，俗称&ldquo;低水位&rdquo;，代表AR集合中最小的logStartOffset值。\n\nLSO，特指LastStableOffset，它与kafka 事务有关。对于未完成的事务而言，LSO的值等于事务中的第一条消息所在的位置（firstUnstableOffset）；对于已经完成的事务而言，它的值等同于HW相同。\n\nLEO是Log End Offset的缩写，它表示了当前日志文件中下一条待写入消息的offset。分区ISR集合中的每个副本都会维护自身的LEO，而ISR集合中最小的LEO即为分区的HW。\n\nHW是 High Watermark 的缩写，俗称高水位，分区ISR集合中的每个副本都会维护自身的LEO，而ISR集合中最小的LEO即为分区的HW。\n\n所以，HW、LW 是分区层面的概念；而LEO、LogStartOffset 是日志层面的概念；LSO 是事务层面的概念\n\n<img src=\"/images/517519-20210125232223398-78917727.png\" width=\"400\" height=\"271\" />\n\n### 11、Kafka中是怎么体现消息顺序性的？\n\nhttps://blog.csdn.net/JacksonKing/article/details/107458383\n\n### 12、Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？\n\n**序列化器**：生产者需要用序列化器（Serializer）把对象转换成字节数组才能通过网络发送给 Kafka。而在对侧，消费者需要用反序列化器（Deserializer）把从 Kafka 中收到的字节数组转换成相应的对象。\n\n**分区器**：分区器的作用就是为消息分配分区。如果消息 ProducerRecord 中没有指定 partition 字段，那么就需要依赖分区器，根据 key 这个字段来计算 partition 的值。\n\nKafka 一共有两种拦截器：**生产者拦截器**和**消费者拦截器**。\n\n　　**生产者拦截器**允许在**发送消息前**以及**消息提交成功后**植入拦截逻辑，既可以用来在消息发送前做一些准备工作，比如按照某个规则过滤不符合要求的消息、修改消息的内容等，也可以用来在发送回调逻辑前做一些定制化的需求，比如统计类工作。\n\n　　**消费者拦截器**允许在**消费消息前**以及**提交位移后**植入拦截逻辑**，**主要在消费消息或在提交消费位移时进行一些定制化的操作。\n\n**生产者拦截器：**要实现 : org.apache.kafka.clients.producer.ProducerInterceptor 接口\n\n```\n1.onSend：该方法会在消息发送之前被调用。\n2.onAcknowledgement：该方法会在消息成功提交或发送失败之后被调用。onAcknowledgement 的调用要早于 callback 的调用。<br />值得注意的是，这个方法和 onSend 不是在同一个线程中被调用的，因此如果你在这两个方法中调用了某个共享可变对象，一定要保证线程安全哦。<br />还有一点很重要，这个方法处在 Producer 发送的主路径中，所以最好别放一些太重的逻辑进去，否则你会发现你的 Producer TPS 直线下降。\n\n```\n\n**消费者拦截器**：要实现org.apache.kafka.clients.consumer.ConsumerInterceptor 接口\n\n```\n1. onConsume：该方法在消息返回给 Consumer 程序之前调用。\n2. onCommit：Consumer 在提交位移之后调用该方法。通常你可以在该方法中做一些记账类的动作，比如打日志等。\n```\n\n### 13、Kafka生产者客户端的整体结构是什么样子的？\n\n<img src=\"/images/517519-20210126225610393-2134644505.png\" width=\"600\" height=\"498\" />\n\n整个生产者客户端由两个线程协调运行，这两个线程分别为主线程和 Sender 线程（发送线程）。\n\n在主线程中由 KafkaProducer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（RecordAccumulator，也称为消息收集器）中。\n\nSender 线程负责从 RecordAccumulator 中获取消息并将其发送到 Kafka 中。\n\nRecordAccumulator 主要用来缓存消息以便 Sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性能。\n\n### 14、Kafka生产者客户端中使用了几个线程来处理？分别是什么？\n\n答：整个生产者客户端由两个线程协调运行，这两个线程分别为主线程和 Sender 线程（发送线程）。\n\n在主线程中由 KafkaProducer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（RecordAccumulator，也称为消息收集器）中。\n\nSender 线程负责从 RecordAccumulator 中获取消息并将其发送到 Kafka 中\n\n### 15、Kafka的旧版Scala的消费者客户端的设计有什么缺陷？\n\n答：老版本的 Consumer Group 把位移保存在 ZooKeeper 中。Apache ZooKeeper 是一个分布式的协调服务框架，Kafka 重度依赖它实现各种各样的协调管理。\n\n将位移保存在 ZooKeeper 外部系统的做法，最显而易见的好处就是减少了 Kafka Broker 端的状态保存开销。\n\nZooKeeper 这类元框架其实并不适合进行频繁的写更新，而 Consumer Group 的位移更新却是一个非常频繁的操作。这种大吞吐量的写操作会极大地拖慢 ZooKeeper 集群的性能。\n\n### 16、&ldquo;消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据&rdquo;这句话是否正确？如果正确，那么有没有什么hack的手段？\n\n答：一般来说如果消费者过多，出现了消费者的个数大于分区个数的情况，就会有消费者分配不到任何分区。\n\n开发者可以继承AbstractPartitionAssignor实现自定义消费策略，从而实现同一消费组内的任意消费者都可以消费订阅主题的所有分区：\n\n注意组内广播的这种实现方式会有一个严重的问题&mdash;默认的消费位移的提交会失效。\n\n### 17、有哪些情形会造成重复消费？\n\n1.Rebalance&nbsp;&nbsp;\n\n一个consumer正在消费一个分区的一条消息，还没有消费完，发生了rebalance(加入了一个consumer)，从而导致这条消息没有消费成功，rebalance后，另一个consumer又把这条消息消费一遍。\n\n2.消费者端手动提交\n\n如果先消费消息，再更新offset位置，这时候强行kill，或者宕机了，会导致消息重复消费。\n\n3.消费者端自动提交\n\n设置offset为自动提交，关闭kafka时，如果在close之前，调用 consumer.unsubscribe() 则有可能部分offset没提交，下次重启会重复消费。\n\n4.生产者端\n\n生产者因为业务问题导致的宕机，在重启之后可能数据会重发\n\n### 18、那些情景下会造成消息漏消费？\n\n1.自动提交\n\n设置offset为自动定时提交，当offset被自动定时提交时，数据还在内存中未处理，此时刚好把线程kill掉，那么offset已经提交，但是数据未处理，导致这部分内存中的数据丢失。\n\n2.生产者发送消息\n\n发送消息设置的是fire-and-forget（发后即忘），它只管往 Kafka 中发送消息而并不关心消息是否正确到达。不过在某些时候（比如发生不可重试异常时）会造成消息的丢失。这种发送方式的性能最高，可靠性也最差。\n\n3.消费者端&nbsp;&nbsp;\n\n先提交位移，但是消息还没消费完就宕机了，造成了消息没有被消费。自动位移提交同理\n\n4.acks没有设置为all&nbsp;&nbsp;\n\n如果在broker还没把消息同步到其他broker的时候宕机了，那么消息将会丢失\n\n### 19、KafkaConsumer是非线程安全的，那么怎么样实现多线程消费？\n\n参考：[深入理解Kafka必知必会（1）](https://www.cnblogs.com/luozhiyun/p/11811835.html)\n\n### 20、简述消费者与消费组之间的关系\n\nConsumer Group 下可以有一个或多个 Consumer 实例。这里的实例可以是一个单独的进程，也可以是同一进程下的线程。在实际场景中，使用进程更为常见一些。\n\nGroup ID 是一个字符串，在一个 Kafka 集群中，它标识唯一的一个 Consumer Group。\n\nConsumer Group 下所有实例订阅的主题的单个分区，只能分配给组内的某个 Consumer 实例消费。这个分区当然也可以被其他的 Group 消费。\n\n### 21、当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？\n\n在执行完脚本之后，Kafka 会在 log.dir 或 log.dirs 参数所配置的目录下创建相应的主题分区，默认情况下这个目录为/tmp/kafka-logs/。\n\n在 ZooKeeper 的/brokers/topics/目录下创建一个同名的实节点，该节点中记录了该主题的分区副本分配方案。示例如下：\n\n```\n[zk: localhost:2181/kafka(CONNECTED) 2] get /brokers/topics/topic-create\n{\"version\":1,\"partitions\":{\"2\":[1,2],\"1\":[0,1],\"3\":[2,1],\"0\":[2,0]}}\n\n```\n\n　　\n\n### 22、topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？\n\n可以增加，使用 kafka-topics 脚本，结合 --alter 参数来增加某个主题的分区数，命令如下：\n\n```\nbin/kafka-topics.sh --bootstrap-server broker_host:port --alter --topic <topic_name> --partitions <新分区数>\n\n```\n\n当分区数增加时，就会触发订阅该主题的所有 Group 开启 Rebalance。\n\n首先，Rebalance 过程对 Consumer Group 消费过程有极大的影响。在 Rebalance 过程中，所有 Consumer 实例都会停止消费，等待 Rebalance 完成。这是 Rebalance 为人诟病的一个方面。\n\n其次，目前 Rebalance 的设计是所有 Consumer 实例共同参与，全部重新分配所有分区。其实更高效的做法是尽量减少分配方案的变动。\n\n最后，Rebalance 实在是太慢了。\n\n### 23、topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？\n\n不支持，因为删除的分区中的消息不好处理。如果直接存储到现有分区的尾部，消息的时间戳就不会递增，如此对于 Spark、Flink 这类需要消息时间戳（事件时间）的组件将会受到影响；如果分散插入现有的分区，那么在消息量很大的时候，内部的数据复制会占用很大的资源，而且在复制期间，此主题的可用性又如何得到保障？与此同时，顺序性问题、事务性问题，以及分区和副本的状态机切换问题都是不得不面对的。\n\n### 24、创建topic时如何选择合适的分区数？\n\n在 Kafka 中，性能与分区数有着必然的关系，在设定分区数时一般也需要考虑性能的因素。对不同的硬件而言，其对应的性能也会不太一样。\n\n可以使用Kafka 本身提供的用于生产者性能测试的 kafka-producer- perf-test.sh 和用于消费者性能测试的 kafka-consumer-perf-test.sh来进行测试。\n\n增加合适的分区数可以在一定程度上提升整体吞吐量，但超过对应的阈值之后吞吐量不升反降。如果应用对吞吐量有一定程度上的要求，则建议在投入生产环境之前对同款硬件资源做一个完备的吞吐量相关的测试，以找到合适的分区数阈值区间。\n\n分区数的多少还会影响系统的可用性。如果分区数非常多，如果集群中的某个 broker 节点宕机，那么就会有大量的分区需要同时进行 leader 角色切换，这个切换的过程会耗费一笔可观的时间，并且在这个时间窗口内这些分区也会变得不可用。\n\n分区数越多也会让 Kafka 的正常启动和关闭的耗时变得越长，与此同时，主题的分区数越多不仅会增加日志清理的耗时，而且在被删除时也会耗费更多的时间&nbsp;\n\n参考：[深入理解Kafka必知必会（2）](https://www.cnblogs.com/luozhiyun/p/11909315.html)\n\n### 25、Kafka目前有哪些内部topic，它们都有什么特征？各自的作用又是什么？\n\n&nbsp;\n\n## **5、kv存储设计原理**\n\n[从零开始写KV数据库：基于哈希索引](https://zhuanlan.zhihu.com/p/351897096)\n\n&nbsp;\n\n## 6、Zookeeper\n\n### 1、Paxos、Raft和ZAB\n\n参考：[分布式协议&mdash;&mdash;Paxos、Raft和ZAB](https://www.cnblogs.com/tonglin0325/p/13331440.html)\n\n&nbsp;\n\n## 7、Flink\n\n### 1、flink checkpoint的原理\n\nCheckpoint 由 CheckpointCoordinator 发起、确认，通过Rpc 通知 Taskmanager 的具体算子完成 Checkpoint 操作，参考：[Flink Checkpoint 流程](https://www.cnblogs.com/Springmoon-venn/p/13565208.html)，具体步骤如下\n\n```\n1、CheckpointCoordicator tirgger checkpoint 到 source\n2、Source\n　　1、生成并广播 CheckpointBarrier\n　　2、Snapshot state（完成后 ack Checkpoint 到 CheckpointCoordicator）\n3、Map  \n　　1、接收到 CheckpointBarrier  \n　　2、广播 CheckpointBarrier   \n　　3、Snapshot state（完成后 ack Checkpoint 到 CheckpointCoordicator）\n4、Sink \n　　1、接收到 CheckpointBarrier  \n　　2、Snapshot state（完成后 ack Checkpoint 到 CheckpointCoordicator）\n5、CheckpointCoordicator 接收到 所有 ack \n　　1、给所有算子发 notifyCheckpointComplete\n6、Source、Map、Sink 收到 notifyCheckpointComplete\n\n```\n\n详细步骤参考：[Checkpoint 原理剖析与应用实践](https://blog.csdn.net/hhhhhhfq/article/details/123852724)\n\n### 2、flink内存模型\n\ntask manager内存模型\n\n<img src=\"/images/517519-20230829001748340-650625699.png\" width=\"300\" height=\"606\" loading=\"lazy\" />\n\n<th style=\"text-align: left;\">&nbsp;&nbsp;**组成部分**&nbsp;&nbsp;</th><th style=\"text-align: left;\">&nbsp;&nbsp;**配置参数**&nbsp;&nbsp;</th><th style=\"text-align: left;\">&nbsp;&nbsp;**描述**&nbsp;&nbsp;</th>\n|------\n<td style=\"text-align: left;\">[框架堆内存（Framework Heap Memory）](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/memory/mem_setup_tm/#framework-memory)</td><td style=\"text-align: left;\">[`taskmanager.memory.framework.heap.size`](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/config/#taskmanager-memory-framework-heap-size)</td><td style=\"text-align: left;\">用于 Flink 框架的 JVM 堆内存（进阶配置）。</td>\n<td style=\"text-align: left;\">[任务堆内存（Task Heap Memory）](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/memory/mem_setup_tm/#task-operator-heap-memory)</td><td style=\"text-align: left;\">[`taskmanager.memory.task.heap.size`](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/config/#taskmanager-memory-task-heap-size)</td><td style=\"text-align: left;\">用于 Flink 应用的算子及用户代码的 JVM 堆内存。</td>\n<td style=\"text-align: left;\">[托管内存（Managed memory）](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/memory/mem_setup_tm/#managed-memory)</td><td style=\"text-align: left;\">[`taskmanager.memory.managed.size`](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/config/#taskmanager-memory-managed-size) <br /> [`taskmanager.memory.managed.fraction`](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/config/#taskmanager-memory-managed-fraction)</td><td style=\"text-align: left;\">由 Flink 管理的用于排序、哈希表、缓存中间结果及 RocksDB State Backend 的本地内存。</td>\n<td style=\"text-align: left;\">[框架堆外内存（Framework Off-heap Memory）](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/memory/mem_setup_tm/#framework-memory)</td><td style=\"text-align: left;\">[`taskmanager.memory.framework.off-heap.size`](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/config/#taskmanager-memory-framework-off-heap-size)</td><td style=\"text-align: left;\">用于 Flink 框架的[堆外内存（直接内存或本地内存）](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/memory/mem_setup_tm/#configure-off-heap-memory-direct-or-native)（进阶配置）。</td>\n<td style=\"text-align: left;\">[任务堆外内存（Task Off-heap Memory）](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/memory/mem_setup_tm/#configure-off-heap-memory-direct-or-native)</td><td style=\"text-align: left;\">[`taskmanager.memory.task.off-heap.size`](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/config/#taskmanager-memory-task-off-heap-size)</td><td style=\"text-align: left;\">用于 Flink 应用的算子及用户代码的[堆外内存（直接内存或本地内存）](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/memory/mem_setup_tm/#configure-off-heap-memory-direct-or-native)。</td>\n<td style=\"text-align: left;\">网络内存（Network Memory）</td><td style=\"text-align: left;\">[`taskmanager.memory.network.min`](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/config/#taskmanager-memory-network-min) <br /> [`taskmanager.memory.network.max`](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/config/#taskmanager-memory-network-max) <br /> [`taskmanager.memory.network.fraction`](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/config/#taskmanager-memory-network-fraction)</td><td style=\"text-align: left;\">用于任务之间数据传输的直接内存（例如网络传输缓冲）。该内存部分为基于 [Flink 总内存](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/memory/mem_setup/#configure-total-memory)的[受限的等比内存部分](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/memory/mem_setup/#capped-fractionated-components)。</td>\n<td style=\"text-align: left;\">[JVM Metaspace](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/memory/mem_setup/#jvm-parameters)</td><td style=\"text-align: left;\">[`taskmanager.memory.jvm-metaspace.size`](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/config/#taskmanager-memory-jvm-metaspace-size)</td><td style=\"text-align: left;\">Flink JVM 进程的 Metaspace。</td>\n<td style=\"text-align: left;\">JVM 开销</td><td style=\"text-align: left;\">[`taskmanager.memory.jvm-overhead.min`](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/config/#taskmanager-memory-jvm-overhead-min) <br /> [`taskmanager.memory.jvm-overhead.max`](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/config/#taskmanager-memory-jvm-overhead-max) <br /> [`taskmanager.memory.jvm-overhead.fraction`](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/config/#taskmanager-memory-jvm-overhead-fraction)</td><td style=\"text-align: left;\">用于其他 JVM 开销的本地内存，例如栈空间、垃圾回收空间等。该内存部分为基于[进程总内存](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/memory/mem_setup/#configure-total-memory)的[受限的等比内存部分](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/memory/mem_setup/#capped-fractionated-components)。</td>\n\n&nbsp;参考：[配置 TaskManager 内存](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/memory/mem_setup_tm/)\n\n&nbsp;\n\n### 3、flink算子链\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n\n## 8、Spark\n\n### 1、spark的架构/spark任务提交的流程\n\n总结 ：整个Spark 应用程序的执行，就是Stage分批次作为taskset提交到executor执行 针对RDD的一个partition,执行我们定义的算子和函数，以此类推直到所有操作执行完为止。参考：[Spark 执行流程](https://blog.csdn.net/W316548854/article/details/96873808) \n\n<img src=\"/images/517519-20231010155302187-843186540.png\" width=\"600\" height=\"477\" loading=\"lazy\" />\n\n- 1、我们提交Spark程序通过 spark-submit (shell）提交到Spark集群中 我们通过client向ResourceManager提交程序请求启动一个Application，同时检查是否有足够的资源满足Application的需求，如果资源条件满足，则准备ApplicationMaster的启动上下文，交给ResourceManager，并循环监控Application状态。 \n\n\n\n\n\n\n\n\n- 2、当提交的资源队列中有资源时，ResourceManager会在某个NodeManager上启动ApplicationMaster进程，ApplicationMaster会单独启动Driver后台线程，当Driver启动后，ApplicationMaster会通过本地的RPC连接Driver，并开始向ResourceManager申请Container资源运行Executor进程（一个Executor对应与一个Container），当ResourceManager返回Container资源，则在对应的Container上启动Executor。\n<li>3、Driver  构造SparkConf 初始化SparkContext ，SparkContext 构建 DAGScheduler和TaskScheduler。Driver初始化SparkContext过程中，会分别初始化DAGScheduler、TaskScheduler、SchedulerBackend以及HeartbeatReceiver，并启动SchedulerBackend以及HeartbeatReceiver。SchedulerBackend通过ApplicationMaster申请资源，并不断从TaskScheduler中拿到合适的Task分发到Executor执行。HeartbeatReceiver负责接收Executor的心跳信息，监控Executor的存活状况，并通知到TaskScheduler。\n</li>\n\n\n\n\n\n\n\n\n<li>4、TaskScheduler 初始化后会调用会去连接Master，向Master注册Application\n</li>\n<li>5、Master 接受TaskScheduler 注册的请求之后会调用自己的资源调度算法，在spark集群的Worker上，为这个Application分配多个Executor\n</li>\n<li>6、Master 并通知Worker启动Executor进程，Executor启动后会反向注册到 TaskScheduler\n</li>\n<li>7、Driver继续执行 我们提交的（JAR）代码，读取文件生成RDD,\n</li>\n<li>8、Driver每执行一个Action都会创建一个Job提交给DAGScheduler\n</li>\n<li>9、DAGScheduler Job会为每个Job Stage划分算法，划分多个Stage,然后每个Stage 会创建一个TaskSet\n</li>\n<li>10、DAGScheduler 会将TaskSet提交到TaskScheduler,TaskScheduler会将每个TaskSet里面的Task,提交到Executor\n</li>\n<li>11、Executor每接受到一个Task后都会用TaskRunner封装，然后从线程池中获取一个线程来执行TaskRunner\n</li>\n- 12、TaskRunner 将我们编写的代码，也就是要执行的算子以及函数，拷贝、反序列化，然后执行Task (Task有两种 ShuffleMapTask和ResultTask，只有最后一个Stage是ResultTask,其他都是ShuffleMapTask)\n\n\n\n\n\n\n\n\n参考：[Spark学习笔记&mdash;&mdash;Spark on YARN](https://www.cnblogs.com/tonglin0325/p/6688720.html)\n\n### 2、DAGScheduler和TaskScheduler\n\nDAGScheduler：Spark的任务调度是从DAG切割开始，主要是由DAGScheduler来完成。Job由最终的RDD和Action方法封装而成，SparkContext将Job交给DAGScheduler提交，它会根据RDD的血缘关系构成的DAG进行切分，将一个Job划分为若干Stages，具体划分策略是，由最终的RDD不断通过依赖回溯判断父依赖是否是款依赖，即以Shuffle为界，划分Stage，窄依赖的RDD之间被划分到同一个Stage中，可以进行pipeline式的计算。\n\nTaskScheduler：Spark Task的调度是由TaskScheduler来完成，由前文可知，DAGScheduler将Stage打包到TaskSet交给TaskScheduler，TaskScheduler会将其封装为TaskSetManager加入到调度队列中，TaskSetManager负责监控管理同一个Stage中的Tasks，TaskScheduler就是以TaskSetManager为单元来调度任务。\n\n参考：[Spark Scheduler内部原理剖析](https://www.cnblogs.com/xia520pi/p/8695141.html)\n\n### 3、spark Application，Driver，Job，Stage，Task概念\n\nJob：Spark中的Job和MR中Job不一样不一样。MR中Job主要是Map或者Reduce Job。而Spark的Job其实很好区别，一个action算子就算一个Job，比方说count，first等。\n\nStage：Stage概念是spark中独有的。一般而言一个Job会切换成一定数量的stage。各个stage之间按照顺序执行。至于stage是怎么切分的，首选得知道spark论文中提到的narrow dependency(窄依赖)和wide dependency（ 宽依赖）的概念。其实很好区分，看一下父RDD中的数据是否进入不同的子RDD，如果只进入到一个子RDD则是窄依赖，否则就是宽依赖。宽依赖和窄依赖的边界就是stage的划分点。\n\nTask：Task是Spark中最小的执行单元。RDD一般是带有partitions的，在执行 Stage 的时候，会按此 Stage 对应的 RDD 的分区数量，对应每一个分区创建一个 Task。如果是 ShuffleMapStage 则创建 ShuffleMapTask，如果是 ResultStage 则创建 ResultTask。在对每个要处理的分区创建出各个 Task 之后，DAGScheduler 会将同一个 Stage 的各个 Task 合并成一个 TaskSet，并将其提交给 TaskScheduler。这些 Task 在后面将会被序列化后发到其他的 executor 上面去运行。 \n\n参考：[Spark中Task，Partition，RDD、节点数、Executor数、core数目的关系和Application，Driver，Job，Task，Stage理解](https://blog.csdn.net/ASN_forever/article/details/112801087)\n\n[从源码剖析一个 Spark WordCount Job 执行的全过程](https://juejin.cn/post/6844903829389967367)\n\n&nbsp;\n\n### 4、coalesce和repartition的区别\n\n减少分区：使用 coalesce（合并）shuffle=false，不会触发shuffle；使用repartition会触发shuffle\n\n增加分区：使用 repartition（重分区）会触发shuffle，可能增加和减少分区数量，相当于coalesce的shuffle=true\n\n参考：[spark重分区算子repartition和coalesce解析&nbsp;](https://blog.csdn.net/u010199356/article/details/89008480)\n\n### 5、Spark的容错机制\n\n1.数据检查点（checkpoint机制）\n\n2.记录数据的更新（spark中的Lineage血统机制），RDD部分分区数据丢失，可以通过Lineage获取足够的信息来重新计算和恢复丢失的数据\n\n### 6、宽依赖和窄依赖\n\nRDD在Lineage容错方面采用2种依赖来保证容错性能：\n\n1.窄依赖：父RDD的每一个分区最多被一个子RDD的分区使用，比如map,filter\n\n2.宽依赖：父RDD分区对应多个子RDD分区\n\n依赖关系在Lineage容错中的应用：\n\n1.窄依赖数据丢失的时候，只需要重新计算丢失的父RDD分区\n\n2.宽依赖要将祖先RDD中的所有数据块全部重新计算，所以通常在Lineage血统链比较长，并且含有宽依赖关系的容错中使用checkpoint机制设置检查点\n\n### 7、spark的优化\n\n[Spark性能优化指南&mdash;&mdash;基础篇](https://tech.meituan.com/2016/04/29/spark-tuning-basic.html)\n\n[Spark性能优化指南&mdash;&mdash;高级篇](https://tech.meituan.com/2016/05/12/spark-tuning-pro.html)\n\n### 8、spark内存管理\n\n1.堆内内存\n\n默认情况下，Spark 仅仅使用了堆内内存。Executor 端的堆内内存区域大致可以分为以下四大块：\n\n**Execution 内存**：主要用于存放 Shuffle、Join、Sort、Aggregation 等计算过程中的临时数据；\n\n**Storage 内存**：主要用于存储 spark 的 cache 数据，例如RDD的缓存、unroll数据；\n\n**用户内存（User Memory）**：主要用于存储 RDD 转换操作所需要的数据，例如 RDD 依赖等信息；\n\n**预留内存（Reserved Memory）**：系统预留内存，会用来存储Spark内部对象。\n\n<img src=\"/images/517519-20230828171732890-1013074942.png\" width=\"400\" height=\"340\" loading=\"lazy\" />\n\n2.堆外内存\n\n如果堆外内存被启用，那么 Executor 内将同时存在堆内和堆外内存，两者的使用互补影响，这个时候 Executor 中的 Execution 内存是堆内的 Execution 内存和堆外的 Execution 内存之和，同理，Storage 内存也一样。\n\n相比堆内内存，堆外内存只区分 Execution 内存和 Storage 内存。\n\n<img src=\"/images/517519-20230828172215535-226911950.png\" width=\"350\" height=\"210\" loading=\"lazy\" />\n\n参考：[Apache Spark 统一内存管理模型详解](https://toutiao.io/posts/9x4hj5/preview)\n\n[Spark 内存管理及调优](https://qiankunli.github.io/2022/05/05/spark_detail.html)\n\n### 9、spark join实现原理\n\n当前SparkSQL支持三种Join算法－**shuffle hash join、broadcast hash join以及sort merge join**。\n\nHash Join适合至少有一个小表的情况，其中前两者归根到底都属于hash join，只不过在hash join之前需要先shuffle还是先broadcast。\n\n大表和大表join的情况，使用sort merge join\n\n<img src=\"/images/517519-20230828173356476-458700229.png\" width=\"700\" height=\"504\" loading=\"lazy\" />\n\n1.Shuffle Hash Join\n\n当要JOIN的表数据量比较大时，可以选择Shuffle Hash Join。这样可以将大表进行**按照JOIN的key进行重分区**，保证每个相同的JOIN key都发送到同一个分区中。\n\n2.Broadcast Hash Join\n\n也称之为**Map端JOIN**。当有一张表较小时，我们通常选择Broadcast Hash Join，这样可以避免Shuffle带来的开销，从而提高性能。\n\n3.Sort Merge Join\n\n该JOIN机制是Spark默认的，可以通过参数**spark.sql.join.preferSortMergeJoin**进行配置，默认是true，即优先使用Sort Merge Join。一般在两张大表进行JOIN时，使用该方式。\n\nSort Merge Join可以减少集群中的数据传输，该方式不会先加载所有数据的到内存，然后进行hashjoin，但是在JOIN之前需要对join key进行排序。\n\n1. shuffle阶段：将两张大表根据join key进行重新分区，两张表数据会分布到整个集群，以便分布式并行处理\n\n2. sort阶段：对单个分区节点的两表数据，分别进行排序\n\n3. merge阶段：对排好序的两张分区表数据执行join操作。join操作很简单，分别遍历两个有序序列，碰到相同join key就merge输出，否则继续取更小一边的key。\n\n### 10、spark的shuffle\n\nSpark中有两种Shuffle管理类型，HashShuffleManager和SortShuffleManager，Spark1.2之前是HashShuffleManager，Spark1.2引入SortShuffleManager，在Spark2.0+版本中已经将HashShuffleManager丢弃\n\n参考：[spark shuffle 的两种实现](https://www.jianshu.com/p/93c2462ed9e4)\n\n### 11、spark数据倾斜优化\n\n1. 如果倾斜的key数量比较少，那么过滤出来，对其进行单独处理\n\n2. 如果倾斜的key数量比较多，那么将key值 map成 key_01到key_100，join的另一边map成key_1~100的随机数，那么这样join之后，原本同样的key值会落到同一个partition，现在变成随机落到了100个节点上，使得数据分布更加平均。\n\n### 12、spark Accumulators(累加器) 原理\n\n### 13、spark broadcast 原理\n\n参考：[Spark---Broadcast变量&amp;Accumulators](https://juejin.cn/post/6976463161993986061)\n\n### 14、yarn-cluster和yarn-client的区别\n\nYarn-Cluster 运行模式\n\n```\n1、spark-submit提交 到ResourceManager\n2、ResourceManager 分配一个Container 在某个NodeManager上，启动ApplicationMaster\n3、ApplicationMaster（相当于是Driver）找到ResourceManager请求Container,启动Executor\n4、ResourceManager 分配一批Container,用于启动Executor\n5、ApplicationMaster 连接分配的NodeManager 启动 Executor\n6、Executor 反向向ApplicationMaster注册\n7、ApplicationMaster 开始执行我们编写的代码，每执行一个Action 操作都会创建一个Job 提交给DAGScheduler\n8、DAGScheduler Job会根据Stage划分算法，划分多个Stage,每个Stage都会创建一个TaskSet\n9、DAGScheduler会将TaskSet提交给TaskScheduler\n10、TaskScheduler会将TaskSet里面的每个Task提交到Executor上执行\n11、Executor接受到Task后都会用一个TaskRunner来进行封装，然后从线程池中获取一个线程来执行这个Task\n12、Task分为两种 一个是ShuffeMapTask和ResultTask,只有最后一个Task是ResultTask其他都是ShuffMapTask\n\n```\n\n基于Yarn-Client <br />\n\n```\n1、spark-submit提交 到ResourceManager 会在本地启动Driver\n2、ResourceManager 分配一个Container 在某个NodeManager上，启动ApplicationMaster（这里的AM其实只是ExecutorLanucher）\n3、ApplicationMaster向ResourceManager申请container 启动Executor\n4、ResourceManager 分配一批Container，\n5、ApplicationMaster 连接其他NodeManager 启动Executor\n6、Executor 启动后反向注册 到本地提交的Driver进程上 而不是注册到ApplicationMaster上\n\n```\n\n## 10、Hadoop\n\n### 1.namenode的fsimage和editlog\n\nfsimage保存了最新的元数据检查点，包含了整个HDFS文件系统的所有目录和文件的信息。对于文件来说包括了数据块描述信息、修改时间、访问时间等；对于目录来说包括修改时间、访问权限控制信息(目录所属用户，所在组)等。\n\neditlog主要是在NameNode已经启动情况下对HDFS进行的各种更新操作进行记录，HDFS客户端执行所有的写操作都会被记录到editlog中。\n\n参考：[（3）hadoop学习&mdash;&mdash;namenode的fsimage与editlog详解](https://blog.csdn.net/chenKFKevin/article/details/61196409)\n\n### 2.namenode的HA\n\nHadoop2.0的HA 机制有两个NameNode，一个是Active状态，另一个是Standby状态。两者的状态可以切换，但同时最多只有1个是Active状态。只有Active Namenode提供对外的服务。Active NameNode和Standby NameNode之间通过NFS或者JN（JournalNode，QJM方式）来同步数据。\n\n### 3.hadoop读写文件的过程\n\n<img src=\"/images/517519-20210703143436007-1484285926.png\" width=\"600\" height=\"357\" loading=\"lazy\" />\n\n&nbsp;\n\n参考：[【Hadoop】HDFS文件写入与文件读取过程](https://blog.csdn.net/ARPOSPF/article/details/107329042)\n\n[HDFS写文件流程（详细必看）](https://zhuanlan.zhihu.com/p/66051354)\n\n## 11、Hive\n\n### 1.hive join的原理\n\nHive中的Join可分为Common Join（Reduce阶段完成join）和Map Join（Map阶段完成join）\n\n参考：[[一起学Hive]之十-Hive中Join的原理和机制](http://lxw1234.com/archives/2015/06/313.htm)\n\n### 2.hive HQL执行的过程\n\nHive在执行一条HQL的时候，会经过以下步骤：\n\n<li>\n语法解析：Antlr定义SQL的语法规则，完成SQL词法，语法解析，将SQL转化为抽象 语法树AST Tree；\n</li>\n<li>\n语义解析：遍历AST Tree，抽象出查询的基本组成单元QueryBlock；\n</li>\n<li>\n生成逻辑执行计划：遍历QueryBlock，翻译为执行操作树OperatorTree；\n</li>\n<li>\n优化逻辑执行计划：逻辑层优化器进行OperatorTree变换，合并不必要的ReduceSinkOperator，减少shuffle数据量；\n</li>\n\n<li>\n生成物理执行计划：遍历OperatorTree，翻译为MapReduce任务；\n</li>\n<li>\n优化物理执行计划：物理层优化器进行MapReduce任务的变换，生成最终的执行计划；\n</li>\n\n参考：[[一起学Hive]之十九-使用Hive API分析HQL的执行计划、Job数量和表的血](http://lxw1234.com/archives/2015/09/476.htm)\n\n## 12、Spring\n\n### **1.spring中的设计模式**\n\n[Spring 中经典的 9 种设计模式，打死也要记住啊！](https://zhuanlan.zhihu.com/p/114244039)\n\n### **<strong>2.面向切分编程&nbsp;**Aop (Aspect-Oriented Programming)</strong>\n\n**<strong>它是一种在运行时，动态地将代码切入到类的指定方法、指定位置上的编程思想。**</strong>**用于<strong>切入到指定类指定方法的代码片段叫做切面，**而**切入到哪些类中的哪些方法叫做切入点**</strong>\n\n**<strong>AOP编程允许把遍布应用各处的功能分离出来形成可重用的组件**</strong>\n\n[SpringBoot使用AOP+注解实现简单的权限验证](https://segmentfault.com/a/1190000012845239)\n\n### **<strong>3.控制反转&nbsp;**IoC**&nbsp;**(Inversion of Control)</strong>\n\n主要实现方法有两种：`依赖注入 DI （Dependency injection）`与`依赖查找`\n\n**依赖注入 :**&nbsp;应用程序被动的接收对象，`IoC`容器通过类型或名称等信息来判断将不同的对象注入到不同的属性中\n\n通过依赖注入，对象的依赖关系将由负责协调系统中各个对象的第三方组件在创建对象的时候设定，依赖注入让互相协作的软件组件保持松散耦合\n\n依赖注入主要有以下的方式：\n\n　　1.基于`set`方法 : 实现特定属性的public set()方法，来让`IoC`容器调用注入所依赖类型的对象（《spring实战》P41）\n\n　　2.基于接口 : 实现特定接口以供`IoC`容器注入所依赖类型的对象\n\n　　3.基于构造函数 : 实现特定参数的构造函数，在创建对象时来让`IoC`容器注入所依赖类型的对象（《spring实战》P7）\n\n　　4.基于注解 : 通过`Java`的注解机制来让`IoC`容器注入所依赖类型的对象，例如`Spring`框架中的`@Autowired，[@Autowire和@Resource注解使用的正确姿势，别再用错的了！](https://zhuanlan.zhihu.com/p/348134947)`\n\n**依赖查找 :**&nbsp;它相对于`依赖注入`而言是一种更为主动的方法，它会在需要的时候通过调用框架提供的方法来获取对象，获取时需要提供相关的配置文件路径、key等信息来确定获取对象的状态\n\n参考：[IoC与AOP的那点事儿](https://blog.didispace.com/spring-ioc-aop/)\n\n### **4、Spring 装配 Bean 的三种方式**\n\n隐式的bean扫描发现机制和自动装配\n\n在java中进行显示配置\n\n在XML中进行显示配置\n\n参考：[记下来 Spring 装配 Bean 的三种方式](https://zhuanlan.zhihu.com/p/61337718)<br />\n\n### 5、Spring容器\n\nspring容器使用依赖注入管理构成应用的组件，它会创建相互协作的组件之间的关联。\n\nBeanFactory（bean工厂）\n\nBeanFactory是spring的原始接口，针对原始结构的实现类功能比较单一，BeanFactory接口实现的容器，特点是在每次获取对象时才会创建对象\n\nApplicationContext（应用上下文）\n\n继承了BeanFactory接口，拥有BeanFactory的全部功能，并且扩展了很多高级特性，每次容器启动时就会创建所有的对象。\n\nspring通用应用上下文装载bean的定义并把他们组装起来\n\n### 6**、spring容器初始化的过程**\n\n分为容器启动阶段、Bean实例化阶段\n\n1.容器启动阶段：读取解析配置文件，保存成BeanDefinition并注册。spring通过应用上下文（ApplicationContext）装载Bean的定义并把他们组装起来；而BeanFactory是延迟初始化。\n\n2.Bean实例化阶段：根据BeanDefinition实例化对象，并注入依赖。\n\n### **<strong>7、spring请求的过程**<br /></strong>\n\n<img src=\"/images/517519-20210426102309571-1962123395.png\" width=\"600\" height=\"412\" loading=\"lazy\" />\n\n### **<strong>8、spring如何解决循环依赖**</strong>\n\n**<strong>三级缓存**</strong>\n\n### **<strong>9、spring的@Transactional如何实现**</strong>\n\n&nbsp;\n\n&nbsp;\n\n### **<strong>10、动态代理的实现方法**</strong>\n\n&nbsp;\n\n&nbsp;\n\n## 13. MyBatis\n\n### **1.MyBatis一级缓存和二级缓存**\n\nMyBatis没有配置的默认情况下，只开启一级缓存，以及缓存只是相对于同一个SqlSession而言，即\n\n在参数和SQL完全一样的情况下，我们使用**同一个SqlSession对象**调用一个Mapper的方法，往往只执行一次SQL\n\n如果使用**不同的SqlSession对象**，会再次发送SQL到数据库中去执行\n\n参考：《深入浅出MyBatis技术原理与实战》P113\n\n### **2.SQL注入**\n\nmybatis中的#和$的区别：\n\n1、#将传入的数据都当成一个字符串，会对自动传入的数据加一个双引号。\n\n如：where username=#{username}，如果传入的值是111,那么解析成sql时的值为where username=\"111\", 如果传入的值是id，则解析成的sql为where username=\"id\".　\n\n2、$将传入的数据直接显示生成在sql中。\n\n如：where username=${username}，如果传入的值是111,那么解析成sql时的值为where username=111；\n\n3、#方式能够很大程度防止sql注入，$方式无法防止Sql注入。\n\n4、$方式一般用于传入数据库对象，例如传入表名\n\n5、一般能用#的就别用$，若不得不使用&ldquo;${xxx}&rdquo;这样的参数，要手工地做好过滤工作，来防止sql注入攻击。\n\n6、在MyBatis中，&ldquo;${xxx}&rdquo;这样格式的参数会直接参与SQL编译，从而不能避免注入攻击。但涉及到动态表名和列名时，只能使用&ldquo;${xxx}&rdquo;这样的参数格式。所以，这样的参数需要我们在代码中手工进行处理来防止注入。\n\n【结论】在编写MyBatis的映射语句时，尽量采用&ldquo;#{xxx}&rdquo;这样的格式。若不得不使用&ldquo;${xxx}&rdquo;这样的参数，要手工地做好过滤工作，来防止SQL注入攻击。\n\n### 3.MyBatis是如何做到**SQL预编译**的呢？\n\n其实在框架底层，是JDBC中的**PreparedStatement类**在起作用，PreparedStatement是我们很熟悉的Statement的子类，它的对象包含了编译好的SQL语句。\n\n这种&ldquo;准备好&rdquo;的方式不仅能提高安全性，而且在多次执行同一个SQL时，能够提高效率。原因是SQL已编译好，再次执行时无需再编译。\n\n参考：[mybatis是如何防止SQL注入的](https://zhuanlan.zhihu.com/p/39408398)\n\n&nbsp;\n\n## 14. 计算机网络\n\n### 1.分层\n\n7&nbsp;[应用层](https://baike.baidu.com/item/%E5%BA%94%E7%94%A8%E5%B1%82/4329788)&nbsp;6&nbsp;[表示层](https://baike.baidu.com/item/%E8%A1%A8%E7%A4%BA%E5%B1%82/4329716)&nbsp;5 会话层 4&nbsp;[传输层](https://baike.baidu.com/item/%E4%BC%A0%E8%BE%93%E5%B1%82/4329536) (TCP, UDP) 3 网络层 (IP, ICMP, IGMP) 2&nbsp;[数据链路层](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/4329290) (ARP) 1&nbsp;[物理层](https://baike.baidu.com/item/%E7%89%A9%E7%90%86%E5%B1%82/4329158)\n\n参考：[网络七层协议](https://baike.baidu.com/item/%E7%BD%91%E7%BB%9C%E4%B8%83%E5%B1%82%E5%8D%8F%E8%AE%AE)\n\n通用明码传输的协议：telnet, ftp, http\n\n通用加密传输的协议：SSH, SSL, TSL, https\n\nSMTP：简单邮件传输协议\n\nPOP3：邮局协议版本3\n\n### 2.Cookie与Session的区别\n\ncookie数据存放在客户的浏览器（客户端）上，session数据放在服务器上，但是服务端的session的实现对客户端的cookie有依赖关系的；\n\ncookie不是很安全，别人可以分析存放在本地的COOKIE并进行COOKIE欺骗，考虑到安全应当使用session；\n\nsession会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能。考虑到减轻服务器性能方面，应当使用COOKIE；\n\n单个cookie在客户端的限制是3K，就是说一个站点在客户端存放的COOKIE不能超过3K；\n\n## 15. 逻辑推理\n\n### 1. 甲、乙、丙、丁四个人要过河，只能走一座很古老的桥。\n\n> 桥一次最多只能通过两人。而且天色已晚，过河必须有手电筒才能保证不会掉下去，但他们四个只找到一只手电筒。所以，两个一快一慢的人一起过河的话，走得快的那个人必须放慢脚步，陪慢的人一起走才行。甲走得最慢，需要十分钟；乙需要5分钟；丙需要2分钟；丁1分钟就跑过去了。\n\n那么什么策略才能让这四个人在最短的时间内都安全到河对岸去哪？\n\n答：17分钟，2+1+10+2+2=17\n\n参考：[过河问题 River Crossing](https://zhuanlan.zhihu.com/p/33193458)\n\n### 2. 你和你的同事知道你的老板A的生日是在下面10天中的一天\n\n3月4日，3月5日，3月8日\n\n6月4日，6月7日\n\n9月1日，9月5日\n\n12月1日，12月2日，12月8日\n\nA只告诉你他生日的月份，只告诉你同事C生日日期中的日。所以，你说：&ldquo;我不知道他具体的生日，C也不知道。&rdquo;听了之后，C回答：&ldquo;我之前确实不知道，但现在知道了&rdquo;。你笑了，并回答：&ldquo;现在我也知道了&rdquo;。看了上面10个生日日期，并听了你和C的对话，你的行政助理没有问任何问题就写下了A的生日。\n\n请问: 助理写下的是哪一天？\n\n9月1日\n\n参考：[量化岗测试问题与解答系列|第四期-生日问题](https://zhuanlan.zhihu.com/p/393162277)\n\n### 3. **一个家庭中有两个小孩，已知其中有一个是女孩，则这时另一个小孩是男孩的概率是多少？（假定生男孩和生女孩的概率是一样的）**\n\n解答：一个家庭中有两个小孩只有4种可能：{男，男}、{男，女}、{女，男}、{女，女}。记事件A为&ldquo;其中一个是女孩&rdquo;，事件B为&ldquo;另一个是男孩&rdquo;，则: A={(男，女)，(女，男)，(女，女)}， B={(男，女)，(女，男)，(男，男)}，AB={(男，女)，(女，男)}。 可知，P(A)=3/4，P(AB)=2/4 由条件概率公式：P(BA)=P(AB)/P(A)=(2/4)/(3/4)=2/3 \n\n通俗一点讲两个小孩的4种可能中，由于一个孩子已经是女孩了，所以排除{男，男}这种可能。也就是总过有3种可能，而带有男孩的有2种，所以概率是2/3。这里很容易答成1/2，如果题目修改为&ldquo;一个家庭中生了一个孩子是女孩，那么再生一个是男孩的概率是多少？&rdquo;，这种情况概率就是1/2了，两者的不同是原题是一个条件概率事件，而修改后的题目是两个独立事件。\n\n### **4. 假设一个班有50个同学，那么他们中有人生日相同的概率是多少？（假设一年有365天，即不考虑闰年的情况）**\n\n**解答：直接上答案，约等于97%！！ **\n\n**我们先考虑简单的情况，如果房子里有1个人，那么其他人与他生日相同的概率，很显然是0，因为就没有其他人。 另一个极端情况，如果房子里有366个人，由于一年只有365天，那么至少有1人会跟其他人生日一样，所以有人生日相同的概率是1。 慢慢推到，如果房子里有2个人，两者生日各不相同的概率很显然是364/365，那么两者有生日相同的概率就是1-364/365。 我们再推广到三个人，第三个人与前两个人生日不相同的概率是363/365，那么三个人生日都不相同的概率是(364/365)*(363/365)，此时三者有人生日相同的概率就是1-(364/365)*(363/365)。 貌似你已经发现规律了，如果有n（1~365之间）个人，那么他们生日都不相同的概率是(364/365)*(363/365)*(362/365)&hellip;*((365-n)/365)，此时n个人有生日相同的概率就是1-(364/365)*(363/365)*(362/365)&hellip;*((365-n)/365)。**\n\n### **5. 纸牌游戏<br />**\n\n**一家赌场提供一种纸牌游戏，纸牌为一副普通的52张牌。规则是你每次交出两张牌。对于每一对，如果都是黑色的，它们归庄家那边；如果两个都是红色的，它们就归你这边；如果一个黑色和一个红色，它们将被丢弃。重复这个过程，直到你们两个看完所有的52张牌。如果你的牌堆里有更多的牌，你就赢了100美元；否则你什么也得不到。赌场允许你协商你想为游戏支付的价格。**\n\n**你愿意花多少钱玩这个游戏？**\n\n**解决方案**：这肯定是一个阴险的赌场。不管牌是怎么排列的，你和庄家总是有相同数量的牌。\n\n为什么？因为每对被丢弃的牌都有一张黑牌和一张红牌，所以被丢弃的红牌和黑牌数量相等。\n\n因此，留给你的红牌数量和留给经销商的黑牌数量总是一样的。庄家总是赢！所以我们不应该付任何钱来玩这个游戏。\n\n### **6. 老虎吃羊<br />**\n\n题目：100只老虎和一只羊共同生活在一个只有草的魔法岛上。老虎可以吃草，但它们更喜欢吃羊。\n\n假设：A、每次一只老虎只能吃一只羊，并且吃完后老虎会变成羊；B、所有的老虎都很精明，并且都想活下去。\n\n问：羊会被吃掉吗？\n\n答案：这里100是个大数目，所以让我们再次把问题简化。如果只有一只老虎（m=1），它肯定会吃掉羊，因为它不用担心变成羊后会被吃掉。\n\n如果有2只老虎呢？因为两只老虎都很精明，都清楚如果自己吃掉羊后变成羊，就会被另一只老虎吃掉，所以结果是，谁也不去吃羊。\n\n如果有3只老虎呢？如果其中一只老虎吃掉一只羊后变身，剩下的两只老虎不会再继续吃羊，所以第一只老虎把羊吃掉。 \n\n如果有四只老虎，每只老虎都知道如果它吃了羊，它就会变成羊。还剩下3只老虎，它还是会被吃掉的。所以为了保证最大的生存可能性，没有老虎会吃羊肉。\n\n同样的逻辑，我们自然可以证明，如果老虎的数量是偶数，羊就不会被吃掉。如果数字是奇数，羊就会被吃掉。对于100只老虎的情况，羊不会被吃掉。\n\n### 7. 燃烧的绳索\n\n题目：\n\n你有两根绳子，每根都需要一个小时才能烧完。但是两条绳子在不同的点有不同的密度，所以不能保证绳子内不同部分燃烧的时间一致。\n\n**你怎么用这两条绳子测量45分钟？**\n\n**解决方案：**\n\n这是一个经典的脑筋急转弯问题。对于一根需要x分钟才能燃烧的绳子，如果你同时点燃绳子的两端，它需要x/2分钟才能燃烧。所以我们应该点燃第一根绳子的两端，点燃第二根绳子的一端。30分钟后，第一根绳子将完全燃烧，而第二根绳子现在变成30分钟的绳子。这时，我们可以点燃第二根绳子的另一端（第二根绳子还在燃烧）。当它被烧毁时，总时间正好是45分钟。\n\n### 8. 残次小球\n\n你有 12 个相同的球。 其中一个球比其他球重或轻（您不知道是哪个）。 使用只能显示托盘哪一侧较重的天平，如何通过 3 次测量确定哪个球是有缺陷的？\n\n将12个球分成3组，每组4个球\n\n参考：[Defective ball](https://zhuanlan.zhihu.com/p/622230422)\n\n### 9. 25匹马，5个跑道，每个跑道最多能有 1 匹马进行比赛，最少比多少次能比出前 3 名？前 5名？\n\n都是7次\n\n参考：[BAT 面试题：25匹马，5个跑道，每个跑道最多能有1匹马进行比赛，最少比多少次能比出前3名？前5名](https://blog.csdn.net/Xu_JL1997/article/details/89021916)？\n\n### 10. 猜数字具体，在1-1000里猜数字，若猜的数字比该数大，损失a元，比该数小，损失b元。不同a，b下，至少要准备多少钱才能保证猜对。\n\n(1)a=1,b=1;\n\n(2)a=1,b=2;\n\n(3)a=1,b=1.5;\n\nf(x)=f(x-a)+f(x-b)+1\n\n参考：[量化面试&mdash;&mdash;猜数字（原题来自某知乎量化第一梯队）](https://zhuanlan.zhihu.com/p/89352581)\n\n### 11. 25桶水其中一桶有毒，猪喝水后会在15分钟内死去，想用一个小时找到这桶毒水，至少需要几头猪？\n\n2只猪，5&times;5矩阵，一只行，一只列\n\n参考：[猪喝毒水 信息熵](https://www.jianshu.com/p/41309e038e6b)\n\n### 12. 100人坐飞机，第一个乘客在座位中随便选一个坐下，第100人正确坐到自己坐位的概率是？\n\n1/2\n\n参考：[https://www.zhihu.com/question/35950050](https://www.zhihu.com/question/35950050)\n\n### 13. 5个海盗抢到100颗宝石\n\n<img src=\"/images/517519-20210426171517718-1659181804.png\" width=\"800\" height=\"515\" loading=\"lazy\" />\n\n### 14. 如果x^x^x^x^x...=2，其中x^y= xy。x是多少？\n\n<img src=\"/images/517519-20230819201405670-1609390201.png\" width=\"500\" height=\"250\" loading=\"lazy\" />\n\n### 15. 三门问题\n\n应该要换选项\n\n[决胜21点中的&ldquo;三门问题&rdquo;是怎么回事？应该如何提高中奖的概率？李永乐老师讲解蒙提霍尔问题](https://www.youtube.com/watch?v=eYmSDnVFxT4&amp;ab_channel=%E6%9D%8E%E6%B0%B8%E4%B9%90%E8%80%81%E5%B8%88)\n\n### 16. **俄罗斯轮盘问题**\n\n**1）两个人玩经典的俄罗斯轮盘，转轮有6个槽位，且枪中只有一发子弹。每次射击后不重置转轮（即，不是memoryless），请问先发和后发者，谁的赢面更大？**\n\n若子弹在奇数位，先发者死，偶数位，后发者死，概率相同\n\n**2）假如每次射击后重置转轮，即，每次射击都有1/6的概率暴毙（memoryless），请问先发者的胜率是？**\n\n设先发者的胜率为p，假设A先开枪，有1/6几率暴毙，5/6变为B先开枪，故p=5/6*(1-p)，p=5/11\n\n**3）假设弹夹中有两颗随机放置的子弹。你的对手对自己开枪并活了下来，现在轮到你，假如你有机会重置转轮，你应该重置吗？**\n\n重置的话，死亡率为2/6，不重置的话，死亡率为2/5，故应当重置\n\n**4）假设弹夹中有两颗连续放置的子弹。你的对手对自己开枪并活了下来，现在轮到你，假如你有机会重置转轮，你应该重置吗？**\n\n重置的话，死亡率为2/6\n\n不重置的话，由于子弹连续放置，把对手用掉的位置命名为1号位，那么子弹只有2-3，3-4，4-5，5-6四种可能，你只有在2-3情况下才会被击中，死亡率为1/4，故不应该重置\n\n参考：[https://zhuanlan.zhihu.com/p/547461248](https://zhuanlan.zhihu.com/p/547461248)\n\n### 17. 概率题\n\n52张牌，抽出3带2的概率\n\n(A（13,2）* C（4,3）* C（4,2）) / C（52,5），其中：**C(n,k)=**n！/ [k！（n-k）！]，**A(n,k)**=n！/（n-k）！\n\n[求概率。 一副扑克牌（52张）。随机抽5张牌。抽到3张相同和另外两张相同的牌（葫芦）的概率是多少？ ](https://zhidao.baidu.com/question/595981208.html)\n\n有52个球，26个黑，26个红。不放回抽两次，获得不同颜色的概率为多少？\n\n52 * 26 / 52 * 51\n\n七局四胜制比赛，2个人赢的概率是一样的，求A在第4局、第5局、第6局、第7局的胜利的概率是多少？\n\n4局胜 1/8\n\n5局胜 C (4,1) * (1/2) * (1/2) * (1/2) * (1/2) * (1/2) = 4/32 = 1/8\n\n6局胜 C (5,2) * (1/2) * (1/2) * (1/2) * (1/2) * (1/2)&nbsp;* (1/2) = 5*2/64 = 5/32\n\n7局胜 C (6,3) * (1/2) * (1/2) * (1/2) * (1/2) * (1/2)&nbsp;* (1/2)&nbsp;* (1/2) = 20/128 = 5/32\n\n### 18.求证：对于任意有限长的二进制序列，可以通过有限次操作将所有位翻转为 0\n\n对于一个初始化为 01....的二进制序列，定义在其上的 X 操作：翻转序列中第 x 位（0翻转为1，1翻转为0），x为序列中 1 的个数。如序列 1011 有 3 个 1，翻转第三位后得到 1001，继续反复进行 X 操作依次得到 1101, 1111, 1110, 1100, 1000, 0000。求证：对于任意有限长的二进制序列，可以通过有限次操作将所有位翻转为 0\n\nchatgpt答案：\n\n让我们通过数学归纳法来证明这个结论。\n\n**归纳假设：** 假设对于长度为 n 的二进制序列，可以通过有限次 X 操作将所有位翻转为 0。\n\n**基础情况：** 对于长度为 1 的二进制序列，如果初始位是 0，则不需要进行任何操作；如果初始位是 1，则需要进行一次 X 操作，将其翻转为 0。无论初始位是什么，都可以通过有限次操作将其翻转为 0。\n\n**归纳步骤：** 现在假设对于长度为 n 的序列，通过有限次操作可以将所有位翻转为 0。我们来考虑长度为 n+1 的序列。\n\n考虑一个长度为 n+1 的二进制序列，其中最左边的位是 b，剩余的 n 位构成序列 S。根据归纳假设，我们可以通过有限次操作将序列 S 中的所有位翻转为 0。\n\n现在分两种情况讨论：\n\n<li>\n如果 b = 0：在这种情况下，最左边的位已经是 0，不需要进行任何操作，因此整个序列都可以通过有限次操作变为 0。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n</li>\n<li>\n如果 b = 1：在这种情况下，我们需要进行一次 X 操作来翻转最左边的位。这会使得序列 S 中的位数（即 n）增加 1，同时会增加一次操作次数。根据归纳假设，我们可以通过有限次操作将序列 S 中的所有位翻转为 0。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n</li>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n综上所述，对于长度为 n+1 的序列，无论最左边的位是 0 还是 1，都可以通过有限次操作将所有位翻转为 0。\n\n根据数学归纳法，我们可以得出结论：对于任意有限长的二进制序列，可以通过有限次操作将所有位翻转为 0。\n\n&nbsp;\n\n## 大数据/小内存问题\n\n### 1. 100亿个整数如何找到中位数？内存足够和内存不足两个场景\n\n#### （1）当内存足够时：\n\n采用快排，找到第n大的数。\n\n&bull; 随机选取一个数，将比它小的元素放在它左边，比它大的元素放在右边<br />&bull; 如果它恰好在中位数的位置，那么它就是中位数，直接返回<br />&bull; 如果小于它的数超过一半，那么中位数一定在左半边，递归到左边处理（还是第几大）<br />&bull; 否则中位数一定在右半边，根据左半边的元素个数计算出中位数是右半边的第几大（重新算第几大），然后递归到右半边处理\n\n#### （2）当内存不足时：\n\n分桶法\n\n把所有数划分到各个小区间，把每个数映射到对应的区间里，对每个区间中数的个数进行计数，数一遍各个区间，看看中位数落在哪个区间，若够小，使用基于内存的算法，否则继续划分。\n\n比如数是32位的，根据每个整数的二进制前5位，划分为32个桶，把数放进对应桶中。如果该桶放不下，继续划分，直至内存可以放心为止。统计每个桶中元素个数，算出中位数一定出现在哪个桶中，而且计算出是该桶中的第几大。 \n\n### 2. 40亿非负整数中找到没出现的数\n\n参考：左神P309\n\n由于1K=1千，1M=1百万，1G=10亿<br />\n\n所以40亿整数转换成bitArr=5亿=0.5G=500M\n\n1.使用1GB内存，找全部\n\n2.使用10MB内存，将40亿分成64份，统计落入每个区间的数的数量，肯定有一个少了，然后再使用bitArr\n\n### 3. 找到100亿个URL中重复的URL\n\n假设每个URL为64B，由于10亿=1G\n\n100亿*64B=6400亿=640G\n\n1.将大文件通过哈希函数分配到100台机器，每台机器分别统计\n\n2.在单机上将大文件通过哈希函数拆分成1000个小文件，对每个小文件分别统计\n\n### 4. 搜索公司一天的用户搜索是（百亿数量级），求top100词汇\n\n使用哈希函数分流，然后再每台机器上使用小根堆和外排序\n\n### 5. 40亿非负整数中找到出现两次的数\n\n找到所有，1G内存\n\n40亿=5亿Byte=0.5G\n\n8bit=1Byte\n\n用bitArr的两位来表示数量，内存占用1G\n\n没出现 00\n\n出现1次 01\n\n出现2次 10\n\n出现2次以上 11\n\n### 6. 使用10MB内存，找40亿个整数的中位数\n\n只有10MB内存，又因为一个int整数占4Byte\n\n每个分区长度为10MB/4Byte=10000/4=2500\n\n统计落入每个区间的数量，设计0~2M-1有19.98亿，只需求2M~4M-1中的0.02亿个数\n\n&nbsp;\n","tags":["刷题"]},{"title":"RocksDB原理","url":"/RocksDB原理.html","content":"[RocksDB](https://github.com/facebook/rocksdb)<!--more-->\n&nbsp;是由 Facebook 基于 LevelDB 开发的一款提供键值存储与读写功能的 LSM-tree 架构引擎。而LevelDB是一个可持久化的KV数据库引擎，由Google传奇工程师Jeff Dean和Sanjay Ghemawat开发并开源\n\n用户写入的键值对会先写入磁盘上的 WAL (Write Ahead Log)，然后再写入内存中的跳表（SkipList，这部分结构又被称作 MemTable）。LSM-tree 引擎由于将用户的随机修改（插入）转化为了对 WAL 文件的顺序写，因此具有比 B 树类存储引擎更高的写吞吐。\n\n内存中的数据达到一定阈值后，会刷到磁盘上生成 SST 文件 (Sorted String Table)，SST 又分为多层（默认至多 6 层），每一层的数据达到一定阈值后会挑选一部分 SST 合并到下一层，每一层的数据是上一层的 10 倍（因此 90% 的数据存储在最后一层）。\n\nRocksDB 允许用户创建多个 ColumnFamily ，这些 ColumnFamily 各自拥有独立的内存跳表以及 SST 文件，但是共享同一个 WAL 文件，这样的好处是可以根据应用特点为不同的 ColumnFamily 选择不同的配置，但是又没有增加对 WAL 的写次数。\n\nTiKV使用RocksDB，参考\n\n```\nhttps://docs.pingcap.com/zh/tidb/stable/rocksdb-overview\n\n```\n\n　　\n\nSSTable\n\n参考：[浅析RocksDB的SSTable格式](https://zhuanlan.zhihu.com/p/37633790)\n","tags":["RocksDB"]},{"title":"python中args，*args，**kwargs的区别","url":"/python中args，*args，**kwargs的区别.html","content":"args 表示参数是一个变量\n\n*args 表示参数是一个tuple\n\n**kwargs 表示参数是一个dict\n\n<!--more-->\n&nbsp;\n\n比如\n\n```\ndef function(arg,*args,**kwargs):\n      print(arg,args,kwargs)\nfunction(6,7,8,9,a=1, b=2, c=3)\n```\n\n结果为&nbsp;6 (7, 8, 9) {'a': 1, 'b': 2, 'c': 3}\n\n&nbsp;\n","tags":["Python"]},{"title":"机器学习——FM","url":"/机器学习——FM.html","tags":["ML"]},{"title":"logback，log4j和log4j2的区别","url":"/logback，log4j和log4j2的区别.html","content":"1.logback的使用，参考：[logback的使用和logback.xml详解](https://www.cnblogs.com/warking/p/5710303.html)\n\npom文件\n\n```\n<properties>\n  <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n  <logback.version>1.1.7</logback.version>\n  <slf4j.version>1.7.21</slf4j.version>\n</properties>\n\n<dependencies>\n  <dependency>\n    <groupId>org.slf4j</groupId>\n    <artifactId>slf4j-api</artifactId>\n    <version>${slf4j.version}</version>\n    <scope>compile</scope>\n  </dependency>\n  <dependency>\n    <groupId>ch.qos.logback</groupId>\n    <artifactId>logback-core</artifactId>\n    <version>${logback.version}</version>\n  </dependency>\n  <dependency>\n    <groupId>ch.qos.logback</groupId>\n    <artifactId>logback-classic</artifactId>\n    <version>${logback.version}</version>\n    </dependency>\n</dependencies>\n\n```\n\n代码\n\n```\nimport org.slf4j.Logger\nimport org.slf4j.LoggerFactory\n\nprivate static final Logger log = LoggerFactory.getLogger(LoggingDemo.class);\n\n```\n\n　　　　\n\n2.log4j的使用\n\npom文件\n\n```\n<dependency>\n    <groupId>log4j</groupId>\n    <artifactId>log4j</artifactId>\n    <version>1.2.17</version>\n</dependency>\n\n```\n\n代码\n\n```\nimport org.apache.log4j.Logger;\n\nprivate final Logger LOGGER = Logger.getLogger(LoggingDemo.class.getName());\n\n```\n\n　　<!--more-->\n&nbsp;\n\n3.log4j2的使用，参考：[浅谈Log4j和Log4j2的区别](https://blog.csdn.net/FANGAOHUA200/article/details/53561718)\n\npom文件\n\n```\n<dependency>\n    <groupId>org.apache.logging.log4j</groupId>\n    <artifactId>log4j-core</artifactId>\n    <version>2.5</version>\n</dependency>\n<dependency>\n    <groupId>org.apache.logging.log4j</groupId>\n    <artifactId>log4j-api</artifactId>\n    <version>2.5</version>\n</dependency>\n\n```\n\n代码\n\n```\nimport org.apache.logging.log4j.Level;\nimport org.apache.logging.log4j.LogManager;\nimport org.apache.logging.log4j.Logger;\n\nprivate static Logger logger = LogManager.getLogger(LoggingDemo.class.getName());\n```\n\n　　\n\n　　\n\n&nbsp;\n","tags":["开发工具"]},{"title":"YARN学习笔记——动态资源池","url":"/YARN学习笔记——动态资源池.html","content":"在YARN上可以配置动态资源池，来对集群资源进行分配和隔离\n\n同时，支持计划模式，可以通过时间对资源池来进行动态调整\n\n<!--more-->\n&nbsp;\n","tags":["YARN"]},{"title":"ubuntu亮度调节失效","url":"/ubuntu亮度调节失效.html","content":"<li class=\"exp-content-list list-item-1\">\n\nctrl+alt+T\n打开终端\n输入下面的指令\nsudo touch /usr/share/X11/xorg.conf.d/20-intel.conf\n\n</li>\n<li class=\"exp-content-list list-item-2\">\n2\n\n再输入下面的指令：\nsudo gedit<!--more-->\n&nbsp;/usr/share/X11/xorg.conf.d/20-intel.conf\n\n</li>\n<li class=\"exp-content-list list-item-3\">\n3\n\n在跳出的gedit文件编辑器中输出以下命令：\nSection \"Device\" Identifier \"card0\" Driver \"intel\" Option \"Backlight\" \"intel_backlight\" BusID \"PCI:0:2:0\" EndSection\n保存，重启。\n&nbsp;\n&nbsp;\n这片博文的方法无效，但是保留着：http://blog.163.com/tym190@126/blog/static/8776005920143192412477/\n\n</li>\n","tags":["Linux"]},{"title":"安装ubuntu和安装ubuntu后要安装的软件列表","url":"/安装ubuntu和安装ubuntu后要安装的软件列表.html","content":"#### 重装ubuntu并做些笔记以及ubuntu下使用的一些开发环境和软件\n\n### <font color=#ff0000>我现在的ubuntu桌面</font>\n![ubuntu_desktop](http://o6g92sjqd.bkt.clouddn.com/ubuntu_desktop.png)\n\n\n### <font color=#ff0000><1>安装ubuntu</font>\n\n<font face=\"微软雅黑\">\n使用的电脑是宏碁的4750G，机械硬盘500G+SSG120G，进行win7+ubuntu 14.04 32位双系统的安装。如果要在双硬盘下安装双系统的话，建议先把win7安装在/dev/hda的那块硬盘下，不然会出现不能进入系统的问题。\n前期准备：一个u盘和两台电脑，方便查资料和制作启动盘。\n这里采用的引导方式为通过win7引导选择进入ubuntu，所以先安装win7的32位版本。\n\n<!--more-->\n&nbsp;\n\n<1>这里默认你已经安装好了win7，以及你手上已经有了一个win PE的U盘启动盘，如果没有就通过老毛桃一键制作一个U盘启动盘。\n\n<2>接着要制作的是ubuntu的u盘启动盘，首先用winpe的启动盘进入PE系统，在PE系统的工具里面找到ultraISO这个软件，在老毛桃制作的u盘启动盘里默认有这个工具，通过下载ultraISO这个软件来制作似乎有问题。或者可以通过先在WIN7下安装EasyBCD这个软件的方法来安装ubuntu，不过我没试过，应该是可以的。\n\n参考步骤：\nwin7+ubuntu 13.04双系统安装方法: [ubuntu安装](http://jingyan.baidu.com/article/60ccbceb18624464cab197ea.html)\n先安装EasyBCD的方法: [先EasyBCD](http://www.360doc.com/content/11/0506/22/6110614_114908124.shtml)\n\n需要注意的是在安装ubuntu的时候要注意所安装的位置，不要安装误删除了有用的资料\n\n<3>如果你安装完了ubuntu不能进入win7或者两个都不能进入的话，这时再制作一个win pe的u盘启动盘，进入pe系统，修复一下win7的引导grub，这时候应该能进入win7了，但是可能还是不能选择进入ubuntu或者是win7,这时候在win7下面安装EasyBCD这个软件，用这个软件制作选择进入win7还是ubuntu的引导。</br>\n</font>\n\n### <font color=#ff0000><2>ubuntu下用的软件,解压安装的时候不要用root用户，不然普通用户必须获得root权限才能打开这些软件</font>\n\n<font face=\"微软雅黑\">\n<1>安装输入法，默认的输入法不好用，可以选择安装sunpinyin和googlepinyin\n\n``` bash\napt-get install ibus-sunpinyin\napt-get install ibus-googlepinyin\n```\n\n安装之后在桌面右上角的文本输入设置中选择输入法，切记要把googlepinyin或者sunpinyin打进去才能搜到。\n\n<2>ubuntu热点\n参考的百度经验: [ubuntu创建热点](http://jingyan.baidu.com/article/363872ecd8f35d6e4ba16f97.html)\n\n<3>安装gnome-tweak-tool和Mac主题，不用安装unity-tweak-tool和ubuntu-tweak了，要重启后才能看到效果。\n参考: [Ubuntu 14.04 下的MAC OS X 主题安装](http://ourjs.com/detail/53d76d072ee1090907000009)\n\n<4>安装docky,好看的底层显示栏，ubuntu原来的侧边栏就可以设置为自动隐藏了\n``` bash\nsudo apt-get install docky\n```\n\n<5>启用Ubuntu额外软件库并更新系统,这一步是参考 《安装完Ubuntu 15.04桌面后要做的15件事》的第一步\n参考: [安装完Ubuntu 15.04桌面后要做的15件事](https://linux.cn/article-5573-1.html)\n\n<6>在ubuntu软件中心安装 新立得软件包管理器 和 经典菜单指示器\n\n<7>安装解压缩的软件\n``` bash\nsudo apt-get install unace unrar zip unzip p7zip-full p7zip-rar sharutils rar uudeview mpack arj cabextract file-roller\n```\n\n<8>安装种子软件Transmission\n``` bash\nsudo apt-get install Transmission \n```\n\n<9>安装SMPplayer\n\n<10>安装google-chrome-stable_27.0.1453.110-r202711_i386.deb\n\n<11>安装youdao-dict和teamviewer\n\n<12>安装JDK和eclipse，JDK安装参考，也会同时安装JRE，其他的安装方法好像有问题。\n\n参考: [ubuntu14.04，安装JDK1.8（JAVA程序需要的开发、运行环境）](地址 http://blog.csdn.net/sunylat/article/details/49882827)\n环境变量配置\n参考: [Ubuntu java 环境变量](http://www.cnblogs.com/BigIdiot/archive/2012/03/26/2417547.html)\neclipse安装的是eclipse-jee-mars-1-linux-gtk.tar.gz，直接解压安装\n然后eclipse的汉化，参考本博客的eclipse安装那篇，里面有汉化的方法。\n\n<13>安装htop，一个内存使用和进程查看的软件，可以用来杀进程\n``` bash\nsudo apt-get install htop \n```\n\n<14>安装g++,编译用的，以后安装hexo也要用\n``` bash\nsudo apt-get install g++ \n```\n\n<15>安装nodejs、Git和npm，为了安装Hexo\n\n<16>安装Lantern,FQ神器，最新的版本在Github可以下到\n\n<17>安装刚出的网易云音乐Linux版本和有道词典，有道词典体验一般，正在考虑换其他的\n\n<18>安装TextLive2015，写Paper用\n\n<19>安装WineQQ、阿里旺旺和微信，这写需要Wine的支持，教程资料博客园中有\n\n<20>安装TextLive2015，写Paper用\n\n<21>安装SublimeText，代码编辑器</br>\n\n</font>","tags":["Linux"]},{"title":"数据治理基本概念","url":"/数据治理基本概念.html","content":"## 1.数据治理解决的问题\n\n1.数据易用性（取数复杂度&amp;速度，需要数据建模，不能都从原始表来查，需要数据仓库设计）\n\n2.数据质量（日志定义口径，指标定义，数据波动报警，和钱相关的一般使用阻塞式）\n\n3.研发成本（研发复杂度&amp;周期，历史负担，数据地图）\n\n4.数据的安全性（加密&amp;脱敏&amp;审计）\n","tags":["Hive"]},{"title":"Java基本语法笔记","url":"/Java基本语法笔记.html","content":"## 1.**基本格式**\n\n```\npublic class HelloWprdApp {\n\n    public static void main(String[] args) {\n        // TODO Auto-generated method stub\n        int num = 10;\n        num = 30;\n        System.out.println(\"num=\"+num);\n        System.out.println(\"num*num=\"+num*num);\n    }\n\n}\n\n```\n\nString args[]：传统的写法，c、c++都用该种写法；\n\nString[] args ：Java的写法，但Java也能用上面的方法定义。\n\n实际上String[] args 是定义一字符串数组变量。\n\n在java中，String[] args和String args[] 都可以定义数组。二者没有什么区别。为规范编码，推荐使用String[] args。另外args可以随意改，和其它变量一样，它只不过是一变量名，只要符合变量名规则即可。\n\n<!--more-->\n&nbsp;\n\n**打印语句**：\n\n```\nSystem.Out.println(\"\");　　带有换行　\nSystem.Out.print(\"\");　　不带有换行\n\n```\n\njava**申明一个类的方法有两种**：\n\n<i>public class　　类名称要和文件的名称一样，否则不能编译\n\n<ii>class　　　　类名称可以和文件的名称不一样，编译时候生成的是 类名称.class\n\n## 2.**java基本数据类型**\n\n\n\n|**数据类型**|**大小/位**|**可表示的数据范围**|**默认值**\n| ---- | ---- | ---- | ---- \n|long(长整数)|64|-2^63～2^63-1|0L\n|int(整数)|32|-2^31～2^31-1-0x80000000~0x7FFFFFFF-2147483648~2147483647|0\n\n-0x80000000~0x7FFFFFFF\n|short(短整数)|16|-32768～32767|0\n|byte(位)|8|-128~127|0\n|char(字符)|2|0~255|\\u0000\n|float(单精度)|32|-3.4E38～3.4E38|0.0f&nbsp; 　定义为float型的时候，数值的后面要加上f\n|double(双精度)|64|-1.7E308～1.7E308|0.0d&nbsp;&nbsp;&nbsp; 注意使用浮点型数值的时候，默认使用的是double型，后面加上f的时候才使用float型\n|boolean|1|flase true|flase\n\n当数值发生**溢出**的时候，会形成循环，即最大值加上1后会变成最小值\n\n强制转换类型的两种方法：\n\nInteger.MAX_VALUE+2L　　　　在加的数字后面加上L表示强制转换成长整形\n\n((long)Integer.MAX_VALUE+2)　　在加的数字前面加上long实现强制转换\n\n&nbsp;\n\n如果需要精确的计算结果，则必须使用**BigDecimal类**，而且使用BigDecimal类也可以进行大数的操作。\n\n<img src=\"/images/517519-20160312200605600-1905035482.png\" alt=\"\" />\n\n```\nimport java.math.BigDecimal;\n\nclass MyMath{\n\tpublic static double add(double d1, double d2){\t\t//进行加法运算\n\t\tBigDecimal b1 = new BigDecimal(d1);\n\t\tBigDecimal b2 = new BigDecimal(d2);\n\t\treturn b1.add(b2).doubleValue();\n\t}\n\t\n\tpublic static double sub(double d1, double d2){\t\t//进行减法运算\n\t\tBigDecimal b1 = new BigDecimal(d1);\n\t\tBigDecimal b2 = new BigDecimal(d2);\n\t\treturn b1.subtract(b2).doubleValue();\n\t}\n\t\n\tpublic static double mul(double d1, double d2){\t\t//进行乘法运算\n\t\tBigDecimal b1 = new BigDecimal(d1);\n\t\tBigDecimal b2 = new BigDecimal(d2);\n\t\treturn b1.multiply(b2).doubleValue();\n\t}\n\t\n\tpublic static double div(double d1, double d2,int len){\t\t//进行除法运算\n\t\tBigDecimal b1 = new BigDecimal(d1);\n\t\tBigDecimal b2 = new BigDecimal(d2);\n\t\treturn b1.divide(b2,len,BigDecimal.ROUND_HALF_UP).doubleValue();\t//表示四舍五入\n\t}\n\t\n\tpublic static double round(double d,int len){\t\t//进行四舍五入\n\t\tBigDecimal b1 = new BigDecimal(d);\n\t\tBigDecimal b2 = new BigDecimal(1);\t\t\t\t\t\t//任何一个数字除以1都是原数字\n\t\treturn b1.divide(b2,len,BigDecimal.ROUND_HALF_UP).doubleValue();\t//表示四舍五入\n\t}\n}\n\npublic class BigDecimal_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tSystem.out.println(\"加法运算：\"+MyMath.round(MyMath.add(10.345, 3.333), 1));\n\t\tSystem.out.println(\"加法运算：\"+MyMath.round(MyMath.sub(10.345, 3.333), 1));\n\t\tSystem.out.println(\"加法运算：\"+MyMath.div(10.345, 3.333, 1));\n\t\tSystem.out.println(\"加法运算：\"+MyMath.round(MyMath.mul(10.345, 3.333), 1));\n\t}\n\n}\n\n```\n\n## 3.**常用的转义字符**\n\n\n\n|**转义字符**|**描述**\n| ---- | ---- \n|**\\f**|**换页**\n|**\\\\**|**反斜线**\n|**\\b**|**倒退一格**\n|**\\'**|**单引号**\n|**\\r**|**归位**\n|**\\\"**|**双引号**\n|**\\t**|**制表符tab**\n|**\\n**|**换行**\n\n## **4.类型的转换**\n\n**　　　当表示范围小的数值类型加上大的数值类型的时候，会自动转换成大的数值类型，比如short+int，会自动把结果转换成int型**\n\n**　　　当数值类型加上字符串的时候，都转换成字符串类型**\n\n## **5.数值格式化**\n\n1.NumberFormat表示数字的格式化类，即可以按照本地的风格习惯进行数字的显示。\n\nNumberFormat是一个抽象类，和MessageFormat类一样，都是Format类的子类，本类在使用时可以直接使用NumberFormat类中提供的静态方法为其实例化。\n\n<img src=\"/images/517519-20160311231126491-1825126228.png\" alt=\"\" />\n\n```\nimport java.text.NumberFormat;\n\npublic class NumberFormat_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tNumberFormat nf = null;\n\t\tnf = NumberFormat.getInstance();\t\t\t//返回当前默认语言环境的数字格式\n\t\tSystem.out.println(\"格式化之后的数字：\"+nf.format(1000000));\n\t\tSystem.out.println(\"格式化之后的数字：\"+nf.format(1000.345));\n\t}\n\n}\n\n```\n\n2.DecimalFormat类也是Format的一个子类，主要作用是格式化数字。\n\n在格式化数字的时候比直接使用NumberFormat更加方便，因为可以直接指定按用户自定义的方式进行格式化操作，与SimpleDateFormat类似，如果要进行自定义格式化操作，则必须指定格式化操作的模板。\n\n<img src=\"/images/517519-20160311233057554-789022740.png\" alt=\"\" />\n\n```\nimport java.text.DecimalFormat;\n\nclass FormatDemo{\n\tpublic void format1(String pattern,double value){\n\t\tDecimalFormat df = null;\t\t\t\t\t//声明一个DecimalFormat对象\n\t\tdf = new DecimalFormat(pattern);\t//实例化对象\n\t\tString str = df.format(value);\t\t\t\t//格式化数字\n\t\tSystem.out.println(\"使用\"+pattern+\"格式化数字\"+value+\":\"+str);\n\t}\n}\n\npublic class DecimalFormat_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tFormatDemo demo = new FormatDemo();\n\t\tdemo.format1(\"###,###.###\", 111222.34567);\n\t\tdemo.format1(\"000,000.000\", 11222.34567);\n\t\tdemo.format1(\"###,###.###￥\", 111222.34567);\n\t\tdemo.format1(\"##.###%\", 0.34567);\t\t\t//使用百分数形式\n\t\tdemo.format1(\"00.###%\", 0.034567);\t\t\t//使用百分数形式\n\t\tdemo.format1(\"###.###\\u2030\", 0.34567);\t//使用千分数形式\n\t}\n\n}\n\n```\n\n## 6.java枚举类型Enum\n\n在JDK5中引入了一个新的**关键字**&mdash;&mdash;**enum**，可以直接定义**枚举类型**\n\n在申明枚举类的时候，也可以申明属性、方法和构造函数，但**枚举类的构造函数必须为私有的，不然就能new出枚举类** \n\n**取出一个枚举内容**\n\n```\nenum Color{\n\tRED,GREEN,BLUE;\n}\n\npublic class Enum_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tColor c = Color.BLUE;\n\t\tSystem.out.println(c);\n\t}\n\n}\n\n```\n\n**枚举类型的数据也可以使用&ldquo;枚举.values()&rdquo;的形式，将全部的枚举类型变为对喜爱数组的形式，之后再直接使用foreach进行输出**\n\n```\nenum Color{\n\tRED,GREEN,BLUE;\n}\n\npublic class Enum_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tfor(Color c:Color.values()){\n\t\t\tSystem.out.println(c);\n\t\t}\n\t}\n\n}\n\n```\n\n**使用switch进行判断**\n\n```\nenum Color{\n\tRED,GREEN,BLUE;\n}\n\npublic class Enum_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tfor(Color c:Color.values()){\n\t\t\tprint(c);\n\t\t}\n\t}\n\t\n\tpublic static void print(Color color){\n\t\tswitch(color){\n\t\t\tcase RED:{\n\t\t\t\tSystem.out.println(\"红色\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase GREEN:{\n\t\t\t\tSystem.out.println(\"绿色\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase BLUE:{\n\t\t\t\tSystem.out.println(\"蓝色\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdefault:{\n\t\t\t\tSystem.out.println(\"其他颜色\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n}\n\n```\n\n<img src=\"/images/517519-20160318095128303-17570331.png\" alt=\"\" />\n\n**使用name()和ordinal()方法取得名称和编号**\n\n```\nenum Color{\n\tRED,GREEN,BLUE;\n}\n\npublic class Enum_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tfor(Color c:Color.values()){\n\t\t\tSystem.out.println(c.ordinal()+\"-->\"+c.name());\n\t\t}\n\t}\n\n}\n\n```\n\n** 通过构造方法为属性赋值**\n\n**通过把构造方法私有，使得不能new 一个新的Color对象，只能使用RED(\"红色\"),GREEN(\"绿色\"),BLUE(\"蓝色\")三个类型**\n\n```\nenum Color{\n\tRED(\"红色\"),GREEN(\"绿色\"),BLUE(\"蓝色\");\t\t//定义枚举的3个类型，&ldquo;红色&rdquo;对应String name\n\t\n\tprivate Color(String name){\t\t\t//定义私有构造方法\n\t\tthis.setName(name);\n\t}\n\t\n\tprivate String name;\n\n\tpublic String getName() {\n\t\treturn name;\n\t}\n\n\tpublic void setName(String name) {\n\t\tthis.name = name;\n\t}\n\t\n}\n\npublic class Enum_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tfor(Color c:Color.values()){\n\t\t\tSystem.out.println(c.ordinal()+\"-->\"+c.name()+c.getName());\n\t\t}\n\t}\n\n}\n\n```\n\n**通过setter()方法为属性赋值**\n\n**<img src=\"/images/517519-20160318110139553-1803631513.png\" alt=\"\" />**\n\n**<img src=\"/images/517519-20160318110209490-1533335629.png\" alt=\"\" />**\n\n&nbsp;\n\n<img src=\"/images/517519-20160318110621959-42897508.png\" alt=\"\" />\n\n&nbsp;\n\n**枚举比较器**\n\n```\nimport java.util.Iterator;\nimport java.util.Set;\nimport java.util.TreeSet;\n\nenum Color{\n\tRED(\"红色\"),GREEN(\"绿色\"),BLUE(\"蓝色\");\t\t//定义枚举的3个类型\n\t\n\tprivate Color(String name){\t\t\t//定义构造方法\n\t\tthis.setName(name);\n\t}\n\t\n\tprivate String name;\n\n\tpublic String getName() {\n\t\treturn name;\n\t}\n\n\tpublic void setName(String name) {\n\t\tthis.name = name;\n\t}\n\t\n}\n\npublic class Enum_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tSet<Color> t = new TreeSet<Color>();\n\t\tt.add(Color.BLUE);\n\t\tt.add(Color.GREEN);\n\t\tt.add(Color.RED);\n\t\tIterator<Color> iter = t.iterator();\n\t\twhile(iter.hasNext()){\n\t\t\tSystem.out.println(iter.next()+\"、\");\n\t\t}\n\t}\n\n\n}\n\n```\n\n&nbsp;\n\n**EnumMap**\n\nEnumMap是Map接口的子类，所以本身还是以Map的形式进行操作，即Key--Value，\n\n如果要使用EnumMap，则首先要创建EnumMap的对象，在创建对象的时候必须指定要操作的枚举类型\n\n```\nimport java.util.EnumMap;\nimport java.util.Map;\n\nenum color{\n\tRED,GREEN,BLUE;\t\t//定义枚举的3个类型\n}\t\n\npublic class EnumMap_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tMap<Color,String> desc = null;\t\t\t\t\t\t\t\t\t//定义一个Map对象\n\t\tdesc = new EnumMap<Color,String>(Color.class);\t//实例化EnumMap\n\t\tdesc.put(Color.BLUE, \"蓝色\");\n\t\tdesc.put(Color.RED, \"红色\");\n\t\tdesc.put(Color.GREEN,\"绿色\");\n\t\t\n\t\tfor(Color c:Color.values()){\t\t\t\t//取得全部的枚举\n\t\t\tSystem.out.println(c.name()+\"-->\"+c.getName());\n\t\t}\n\t\t\n\t\tfor(Color c:desc.keySet()){\t\t\t\t//取得全部的Key\n\t\t\tSystem.out.println(c.name()+\"、\");\n\t\t}\n\t\t\n\t\tfor(String c:desc.values()){\t\t\t\t//取得全部的值\n\t\t\tSystem.out.println(c+\"、\");\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**EnumSet**\n\nEnumSet是Set接口的子类，所以里面的内容是无法重复的。\n\n使用EnumSet时不能直接使用关键字new为其进行实例化，所以在此类中提供了很多的静态方法\n\n<img src=\"/images/517519-20160318115523537-1180324051.png\" alt=\"\" />\n\n&nbsp;\n\n**EnumSet---将全部的集合设置到EnumSet集合中**\n\n```\nimport java.util.EnumSet;\n\nenum color_{\n\tRED,GREEN,BLUE;\t\t//定义枚举的3个类型\n}\t\n\npublic class EnumSet_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tEnumSet<Color> es = null;\t\t\t\t//声明一个EnumSet对象\n\t\tes = EnumSet.allOf(Color.class);\t\t//将枚举的全部类型设置到EnumSet对象中\n\t\t\n\t\tprint(es);\n\t}\n\t\n\tpublic static void print(EnumSet<Color> temp){\t\t//专门的输出操作\n\t\tfor(Color c:temp){\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//循环输出EnumSet中的内容\n\t\t\tSystem.out.println(c+\"、\");\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**只设置一个枚举的类型到集合中**\n\n**使用EnumSet提供的static方法of()，将一个枚举中的一个内容设置到EnumSet集合中**\n\n```\nimport java.util.EnumSet;\n\nenum color_{\n\tRED,GREEN,BLUE;\t\t//定义枚举的3个类型\n}\t\n\npublic class EnumSet_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tEnumSet<Color> es = null;\t\t\t\t//声明一个EnumSet对象\n\t\tes = EnumSet.of(Color.BLUE);\t\t\t//设置一个枚举的内容\n\t\t\n\t\tprint(es);\n\t}\n\t\n\tpublic static void print(EnumSet<Color> temp){\t\t//专门的输出操作\n\t\tfor(Color c:temp){\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//循环输出EnumSet中的内容\n\t\t\tSystem.out.println(c+\"、\");\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**创建只能放入指定枚举类型的集合**\n\n**使用EnumSet提供的static方法noneOf()，将集合设置成只能增加Color类型的集合，但是并不设置任何的内容到集合中**\n\n```\nimport java.util.EnumSet;\n\nenum color_{\n\tRED,GREEN,BLUE;\t\t//定义枚举的3个类型\n}\t\n\npublic class EnumSet_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tEnumSet<Color> es = null;\t\t\t\t//声明一个EnumSet对象\n\t\t\n\t\tes = EnumSet.noneOf(Color.class);\t\t\t//创建一个可以加入Color类型的对象\n\t\tes.add(Color.BLUE);\n\t\tes.add(Color.RED);\n\t\t\n\t\tprint(es);\n\t}\n\t\n\tpublic static void print(EnumSet<Color> temp){\t\t//专门的输出操作\n\t\tfor(Color c:temp){\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//循环输出EnumSet中的内容\n\t\t\tSystem.out.println(c+\"、\");\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**创建不包含指定元素的集合**\n\n```\nimport java.util.EnumSet;\n\nenum color_{\n\tRED,GREEN,BLUE;\t\t//定义枚举的3个类型\n}\t\n\npublic class EnumSet_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tEnumSet<Color> esOld = null;\t\t\t\t//声明一个EnumSet对象\n\t\tEnumSet<Color> esNew = null;\t\t\t\t//声明一个EnumSet对象\n\t\tesOld = EnumSet.noneOf(Color.class);\t\t\t//创建一个可以加入Color类型的对象\n\t\tesOld.add(Color.BLUE);\n\t\tesOld.add(Color.RED);\n\t\tesNew = EnumSet.complementOf(esOld);\t\t//创建一个不包含指定元素的集合\n\t\tprint(esNew);\n\t}\n\t\n\tpublic static void print(EnumSet<Color> temp){\t\t//专门的输出操作\n\t\tfor(Color c:temp){\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//循环输出EnumSet中的内容\n\t\t\tSystem.out.println(c+\"、\");\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**复制已有的内容**\n\n```\nimport java.util.EnumSet;\n\nenum color_{\n\tRED,GREEN,BLUE;\t\t//定义枚举的3个类型\n}\t\n\npublic class EnumSet_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tEnumSet<Color> esOld = null;\t\t\t\t//声明一个EnumSet对象\n\t\tEnumSet<Color> esNew = null;\t\t\t\t//声明一个EnumSet对象\n\t\tesOld = EnumSet.noneOf(Color.class);\t\t\t//创建一个可以加入Color类型的对象\n\t\tesOld.add(Color.BLUE);\n\t\tesOld.add(Color.RED);\n\n\t\tesNew = EnumSet.copyOf(esOld);\t\t\t\t\t//从已有的集合中复制出内容\n\t\tprint(esNew);\n\t}\n\t\n\tpublic static void print(EnumSet<Color> temp){\t\t//专门的输出操作\n\t\tfor(Color c:temp){\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//循环输出EnumSet中的内容\n\t\t\tSystem.out.println(c+\"、\");\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**让枚举类实现一个接口**\n\n在接口中定义了一个getColor()方法，枚举类在实现此接口之后，就必须对枚举类中的每个对象分别实现接口中的getColor()方法\n\n```\ninterface Print_{\t//定义Print方法\n\tpublic String getColor();\t//定义抽象方法\n}\n\nenum color_1 implements Print_{\t//枚举类实现接口\n\tRED{\t//枚举对象实现抽象方法\n\t\t@Override\n\t\tpublic String getColor() {\n\t\t\t// TODO 自动生成的方法存根\n\t\t\treturn \"红色\";\n\t\t}\n\t},\n\tBLUE{\n\t\t@Override\n\t\tpublic String getColor() {\n\t\t\t// TODO 自动生成的方法存根\n\t\t\treturn \"蓝色\";\n\t\t}\n\t},\n\tGREEN{\n\t\t@Override\n\t\tpublic String getColor() {\n\t\t\t// TODO 自动生成的方法存根\n\t\t\treturn \"绿色\";\n\t\t}\n\t};\n}\t\n\npublic class InterfaceEnum_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tfor(color_1 c:color_1.values()){\t//循环输出EnumSet中的内容\n\t\t\tSystem.out.println(c.getColor()+\"、\");\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**在枚举类中定义抽象方法**\n\n```\ninterface Print_{\t//定义Print方法\n\tpublic String getColor();\t//定义抽象方法\n}\n\n//enum color_1 implements Print_{\t//枚举类实现接口\n//\tRED{\t//枚举对象实现抽象方法\n//\t\t@Override\n//\t\tpublic String getColor() {\n//\t\t\t// TODO 自动生成的方法存根\n//\t\t\treturn \"红色\";\n//\t\t}\n//\t},\n//\tBLUE{\n//\t\t@Override\n//\t\tpublic String getColor() {\n//\t\t\t// TODO 自动生成的方法存根\n//\t\t\treturn \"蓝色\";\n//\t\t}\n//\t},\n//\tGREEN{\n//\t\t@Override\n//\t\tpublic String getColor() {\n//\t\t\t// TODO 自动生成的方法存根\n//\t\t\treturn \"绿色\";\n//\t\t}\n//\t};\n//}\t\n\nenum color_1{\t//枚举类实现接口\n\tRED{\t//枚举对象实现抽象方法\n\t\t@Override\n\t\tpublic String getColor() {\n\t\t\t// TODO 自动生成的方法存根\n\t\t\treturn \"红色\";\n\t\t}\n\t},\n\tBLUE{\n\t\t@Override\n\t\tpublic String getColor() {\n\t\t\t// TODO 自动生成的方法存根\n\t\t\treturn \"蓝色\";\n\t\t}\n\t},\n\tGREEN{\n\t\t@Override\n\t\tpublic String getColor() {\n\t\t\t// TODO 自动生成的方法存根\n\t\t\treturn \"绿色\";\n\t\t}\n\t};\n\tpublic abstract String getColor();\t//定义抽象方法\n}\t\n\n\n//主类\n//Function        : \tInterfaceEnum_demo;\npublic class InterfaceEnum_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tfor(color_1 c:color_1.values()){\t//循环输出EnumSet中的内容\n\t\t\tSystem.out.println(c.getColor()+\"、\");\n\t\t}\n\t}\n\n}\n\n```\n\n## **7.位运算符**\n|**位运算符**|**描述**\n|**&amp;**|**按位与**\n|||**按位或**\n|**^**|**异或（相同为0，不同为1）**\n|**～**|**取反**\n|**<<**|**左移位**\n|**>>**|**右移位　　正数右移的时候左边补零，负数左边补一**\n|**>>>**|**无符号移位　　<strong>正数负数右移的时候左边都补零**</strong>\n\n**<strong>数组中，数组的名字，即地址存在栈内存中，地址指向的内容存在堆内存中，开辟新的堆内存必须要用关键字new，栈内存中存储的是堆内存的访问地址**</strong>\n\n**<strong>当堆空间没有任何栈空间引用的时候，就成为了垃圾空间，等待这垃圾回收机制进行回收**</strong>\n\n**<strong>数组的复制：System.arraycopy（i1,3,i2,1,3）源数组名称　　源数组开始点　　目标数组名称　　目标数组开始点　　复制长度**</strong>\n\n## **8.逻辑运算符优先级**\n\n**<img src=\"/images/517519-20160908094156988-1658817175.png\" alt=\"\" />**\n\n## **<strong>9.方法的定义方式**</strong>\n\n```\npublic static void 方法名字（类型 参数，类型 参数。。。）{\n　　程序语句\n}\n\n```\n\n## **<strong>10.可变参数**</strong>\n\n```\n　　fun();\n　　fun(1);\n　　fun(1,2,3,4,5);\n\n　　public static void fun(int... arg){\n　　　　for(int i=0;i<arg.lenght;i++){\n　　　　　　system.out.print(arg[i]+\"、\");\n　　}\n\n```\n\n## **11.类的定义**\n\n```\nclass Person{\n　　　　private string name;//把属性封装，讲不能通过XX.name访问，必须通过setter和getter方法设置和取得\n　　　　private int age;//private也可以用-表示\n \n　　　　public Person(String name,int age){　//构造函数，和类名同名\n　　　　　　this.setName(name);\n　　　　　　this.setAge(age);\n　　　　}\n\n　　　　public void tell(){\n　　　　　　system.out.println();\n　　　　}\n}\n\n```\n\n**对象的创建和使用**\n\n**　　类名 对象名称 = null;<br />**\n\n**　　对象名称 = new 类名();**\n\n**或者<br />**\n\n**　　类名 对象名称 = new 类名();<br />**\n\n**匿名对象**\n\n**　　new Person(\"张三\",30).tell();//匿名对象，匿名对象一般是作为其他类实例化对象的参数传递，匿名对象是一个堆内存空间，没有栈空间<br />**\n\n&nbsp;\n\n**类设计分析**\n\n**<1>根据要求写出类所包含的属性**\n\n**<2>所有的属性都必须进行封装(private)**\n\n**<3>封装之后的属性通过setter和getter设置和取得**\n\n**<4>如果需要可以加入若干构造方法**\n\n**<5>再根据其他要求添加相应的方法**\n\n**<6>类中的所有方法都不要直接输出，而是交给被调用处输出**\n\n**<7> 永远不要继承一个已经实现好的类，只能继承抽象类或者实现接口，因为一旦发生对象的向上转型关系后，所调用的方法一定是被子类所覆写的方法**\n\n## **12.string对象**\n\n**使用==比较string对象的时候比较的是string的地址**\n\n**使用.equals方法比较的是string对象的内容<br />**\n\n&nbsp;\n\n**<1>使用string str = new string(\"hello\");创建一个新的string对象，\"hello\"创建一个对象，new关键字又创建一个对象，所以建议使用第二种方法创建对象<br />**\n\n**<2>string str = \"hello\";<br />**\n\n&nbsp;\n\n**注意：字符串的内容不可以修改，所以要避免对字符串的内容进行连续的修改，因为修改一次就要开辟两个新的内存空间，通过断开和连接进行字符串的修改。**\n\n**要通过StringBuffer类进行修改。**\n\n## **13.引用**\n\n所谓**引用传递**就是指将**堆内存空间**的使用权交给多个**栈内存空间**。\n\n例子<1>\n\n```\npublic class Aliasing {\n\tint temp = 30;\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tAliasing d1 = new Aliasing();\n\t\td1.temp = 50;\n\t\tSystem.out.println(d1.temp);\n\t\tfun(d1);\n\t\tSystem.out.println(d1.temp);\n\t}\n\t\n\tpublic static void fun (Aliasing d2){\n\t\td2.temp = 1000;\n\t}\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160211125540433-1857859840.png\" alt=\"\" />\n\n&nbsp;例子<2> 其中传递的是string对象，由于string的内容是不可以修改，所以str1的值还是hello，如果传递的是对象的string属性，那是可以修改的\n\n```\npublic class Aliasing {\n\tint temp = 30;\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tString str1 = \"hello\";\n\t\tSystem.out.println(str1);\n\t\tfun(str1);\n\t\tSystem.out.println(str1);\n\t}\n\t\n\tpublic static void fun (String str2){\n\t\tstr2 = \"hello2\";\n\t}\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160211130853870-768177344.png\" alt=\"\" />\n\n&nbsp;\n\n例子<3>传递的是对象的string属性\n\n```\npublic class Aliasing {\n\tString temp = \"hello\";\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tAliasing d1 = new Aliasing();\n\t\td1.temp = \"world\";\n\t\tSystem.out.println(d1.temp);\n\t\tfun(d1);\n\t\tSystem.out.println(d1.temp);\n\t}\n\t\n\tpublic static void fun (Aliasing d2){\n\t\td2.temp=\"HELLO\";\n\t}\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160211132053902-1836086300.png\" alt=\"\" />\n\n&nbsp;\n\n一对一关系&nbsp;&nbsp; 例子\n\n一个人对应一本书，一本书对应一个人\n\n```\nclass Person{\n\t\tprivate String name;\n\t\tprivate int age;\n\t\tprivate Book book;\n\t\t\n\t\tpublic Person(String name,int age){\n\t\t\tthis.setName(name);\n\t\t\tthis.setAge(age);\n\t\t}\n\t\t\n\t\tpublic  String getName(){\n\t\t\treturn name;\n\t\t}\n\n\t\tpublic void setName(String n){\n\t\t\tname = n;\n\t\t}\n\t\t\n\t\tpublic int getAge(){\n\t\t\treturn age;\n\t\t}\n\t\t\n\t\tpublic void setAge(int a){\n\t\t\tage = a;\n\t\t}\n\t\t\n\t\tpublic Book getBook(){\n\t\t\treturn book;\n\t\t}\n\t\t\n\t\tpublic void setBook(Book b){\n\t\t\tbook = b;\n\t\t}\n}\n\nclass Book{\n\tprivate String title;\n\tprivate float price;\n\tprivate Person person;\n\t\n\tpublic Book(String title,float price){\n\t\tthis.setTitle(title);\n\t\tthis.setPrice(price);\n\t}\n\t\n\tpublic  String getTitle(){\n\t\treturn title;\n\t}\n\n\tpublic void setTitle(String t){\n\t\ttitle = t;\n\t}\n\t\n\tpublic float getPrice(){\n\t\treturn price;\n\t}\n\t\n\tpublic void setPrice(float p){\n\t\tprice = p;\n\t}\n\t\n\tpublic Person getPerson(){\n\t\treturn person;\n\t}\n\t\n\tpublic void setPerson(Person person){\n\t\tthis.person = person;\n\t}\n\t\n}\n\n\npublic class reference {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tPerson per = new Person(\"zhangsan\",30);\n\t\tBook bk = new Book(\"JAVA SE kaifa\",90.0f);\n\t\tper.setBook(bk);\n\t\tbk.setPerson(per);\n\t\tSystem.out.println(\" name \"+per.getName()+\" age \"+per.getAge()+\" book \"+per.getBook().getTitle()+\" price \"+per.getBook().getPrice());\n\t\tSystem.out.println(\" title \"+bk.getTitle()+\" price \"+bk.getPrice()+\" person \"+bk.getPerson().getName()+\" age \"+bk.getPerson().getAge());\n\t}\n\n}\n\n```\n\n&nbsp;一个人对应一本书，一本书对应一个人，一个孩子对应一本书，一本书对应一个孩子，一个人对应一个孩子\n\n```\nclass Person{\n\t\tprivate String name;\n\t\tprivate int age;\n\t\tprivate Book book;\n\t\tprivate Person child;\n\t\t\n\t\tpublic Person(String name,int age){\n\t\t\tthis.setName(name);\n\t\t\tthis.setAge(age);\n\t\t}\n\t\t\n\t\tpublic  String getName(){\n\t\t\treturn name;\n\t\t}\n\n\t\tpublic void setName(String n){\n\t\t\tname = n;\n\t\t}\n\t\t\n\t\tpublic int getAge(){\n\t\t\treturn age;\n\t\t}\n\t\t\n\t\tpublic void setAge(int a){\n\t\t\tage = a;\n\t\t}\n\t\t\n\t\tpublic Book getBook(){\n\t\t\treturn book;\n\t\t}\n\t\t\n\t\tpublic void setBook(Book b){\n\t\t\tbook = b;\n\t\t}\n\t\t\n\t\tpublic Person getChild(){\n\t\t\treturn child;\n\t\t}\n\t\t\n\t\tpublic void setChild(Person child){\n\t\t\tthis.child = child;\n\t\t}\n}\n\nclass Book{\n\tprivate String title;\n\tprivate float price;\n\tprivate Person person;\n\t\n\tpublic Book(String title,float price){\n\t\tthis.setTitle(title);\n\t\tthis.setPrice(price);\n\t}\n\t\n\tpublic  String getTitle(){\n\t\treturn title;\n\t}\n\n\tpublic void setTitle(String t){\n\t\ttitle = t;\n\t}\n\t\n\tpublic float getPrice(){\n\t\treturn price;\n\t}\n\t\n\tpublic void setPrice(float p){\n\t\tprice = p;\n\t}\n\t\n\tpublic Person getPerson(){\n\t\treturn person;\n\t}\n\t\n\tpublic void setPerson(Person person){\n\t\tthis.person = person;\n\t}\n\t\n}\n\npublic class reference {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tPerson per = new Person(\"zhangsan\",30);\n\t\tPerson cld = new Person(\"zhangcao\",10);\n\t\tBook bk = new Book(\"JAVA SE kaifa\",90.0f);\n\t\tBook b = new Book(\"11111\",30.0f);\n\t\tper.setBook(bk);\n\t\tbk.setPerson(per);\n\t\tcld.setBook(b);\n\t\tb.setPerson(cld);\n\t\tper.setChild(cld);\n\t\tSystem.out.println(\" name \"+per.getName()+\" age \"+per.getAge()+\" book \"+per.getBook().getTitle()+\" price \"+per.getBook().getPrice());\n\t\tSystem.out.println(\" title \"+bk.getTitle()+\" price \"+bk.getPrice()+\" person \"+bk.getPerson().getName()+\" age \"+bk.getPerson().getAge());\n\t\tSystem.out.println(\" cldname \"+per.getChild().getName()+\" age \"+per.getChild().getAge()+\" book \"+per.getChild().getBook().getTitle()+\" price \"+per.getChild().getBook().getPrice());\n\t}\n\n}\n\n```\n\n## **14.构造方法**\n\n识别合法的构造方法：\n\n1. 构造方法可以被重载，一个构造方法可以通过this关键字调用另一个构造方法，this语句必须位于构造方法的第一行;\n\n重载：方法的重载(overload)；重载构成的条件：方法的名称相同，但参数类型或参数个数不同，才能构成方法的重载.\n\n2. 当一个类中没有定义任何构造方法，Java将自动提供一个缺省的构造方法\n\n3. 子类通过super关键字调用父类的一个构造方法\n\n4. 当子类的某个构造方法没有通过super关键字调用父类的构造方法，通过这个构造方法创建子类对象时，会自动先调用父类的缺省构造方法\n\n5. 构造方法不能被static、final、synchronized、abstract、native修饰，但可以被public、private、protected修饰;\n\n6. 构造方法不是类的成员方法\n\n7. 构造方法不能被继承\n\n## 15.Collections接口\n\n**<1>实例操作一：返回不可变的集合**\n\nCollections类中可以返回空的List、Set、Map集合，但是通过这种方式返回的对象是无法进行增加数据的，因为在这些操作中并没有实现add()方法\n\n```\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Set;\n\npublic class Collection_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tList<String> allList = Collections.emptyList();\t\t\t//返回不可变的空List集合\n\t\tSet<String> allSet = Collections.emptySet();\t\t\t//返回不可变的空List集合\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**<2>实例操作二：为集合增加内容**\n\n```\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\n\npublic class Collection_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tList<String> all = new ArrayList<String>();\t\t\t//实例化List\n\t\tCollections.addAll(all, \"zhangsan\",\"lisi\",\"wangwu\");\t\t//增加内容\n\t\tIterator<String> iter = all.iterator();\t\t\t\t\t//实例化iterator对象\n\t\twhile(iter.hasNext()){\n\t\tSystem.out.println(iter.next()+\"、\");\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**<3>实例操作三：反转集合中的内容**\n\n```\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\n\npublic class Collection_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tList<String> all = new ArrayList<String>();\t\t\t//实例化List\n\t\tCollections.addAll(all, \"zhangsan\",\"lisi\",\"wangwu\");\t\t//增加内容\n\t\tCollections.reverse(all);\t\t\t\t\t\t\t\t\t\t\t//内容反转保存\n\t\tIterator<String> iter = all.iterator();\t\t\t\t\t//实例化iterator对象\n\t\twhile(iter.hasNext()){\n\t\tSystem.out.println(iter.next()+\"、\");\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**<4>实例操作四：检索内容**\n\n```\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\n\npublic class Collection_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tList<String> all = new ArrayList<String>();\t\t\t//实例化List\n\t\tCollections.addAll(all, \"zhangsan\",\"lisi\",\"wangwu\");\t\t//增加内容\n\t\tCollections.reverse(all);\t\t\t\t\t\t\t\t\t\t\t//内容反转保存\n\t\tIterator<String> iter = all.iterator();\t\t\t\t\t//实例化iterator对象\n\t\twhile(iter.hasNext()){\n\t\tSystem.out.println(iter.next()+\"、\");\n\t\t}\n\t\t\n\t\tint point = Collections.binarySearch(all,\"zhangsan\");\n\t\tSystem.out.println(\"检索结果：\"+point); \t\t\t//输出位置\n\t\t\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**<5>实例操作五：替换集合中的内容**\n\n```\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\n\npublic class Collection_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n//\t\tList<String> allList = Collections.emptyList();\t\t\t//返回不可变的空List集合\n//\t\tSet<String> allSet = Collections.emptySet();\t\t\t//返回不可变的空List集合\n\t\t\n\t\tList<String> all = new ArrayList<String>();\t\t\t//实例化List\n\t\tCollections.addAll(all, \"zhangsan\",\"lisi\",\"wangwu\");\t\t//增加内容\n\t\t\n\t\tCollections.replaceAll(all, \"wangwu\", \"lisi\");\t\t\t//替换内容\n\t\t\n\t\tCollections.reverse(all);\t\t\t\t\t\t\t\t\t\t\t//内容反转保存\n\t\tIterator<String> iter = all.iterator();\t\t\t\t\t//实例化iterator对象\n\t\twhile(iter.hasNext()){\n\t\tSystem.out.println(iter.next()+\"、\");\n\t\t}\n\t\t\n\t\tint point = Collections.binarySearch(all,\"zhangsan\");\n\t\tSystem.out.println(\"检索结果：\"+point); \t\t\t//输出位置\n\t\t\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**<6>实例操作六：集合排序**\n\n```\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\n\npublic class Collection_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n//\t\tList<String> allList = Collections.emptyList();\t\t\t//返回不可变的空List集合\n//\t\tSet<String> allSet = Collections.emptySet();\t\t\t//返回不可变的空List集合\n\t\t\n\t\tList<String> all = new ArrayList<String>();\t\t\t//实例化List\n\t\tCollections.addAll(all, \"zhangsan\",\"lisi\",\"wangwu\");\t\t//增加内容\n\t\t\n//\t\tCollections.replaceAll(all, \"wangwu\", \"lisi\");\t\t\t//替换内容\n\t\t\n//\t\tCollections.reverse(all);\t\t\t\t\t\t\t\t\t\t\t//内容反转保存\n\t\tIterator<String> iter = all.iterator();\t\t\t\t\t//实例化iterator对象\n\t\twhile(iter.hasNext()){\n\t\tSystem.out.println(iter.next()+\"、\");\n\t\t}\n\t\t\n\t\tCollections.sort(all);\n\t\tSystem.out.println(\"排序之后的集合\");\n\t\titer = all.iterator();\n\t\twhile(iter.hasNext()){\n\t\tSystem.out.println(iter.next()+\"、\");\n\t\t}\n\t\t\n\t\tint point = Collections.binarySearch(all,\"zhangsan\");\n\t\tSystem.out.println(\"检索结果：\"+point); \t\t\t//输出位置\n\t\t\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**<7>实例操作七：交换指定位置的内容**\n\n**直接使用swap()方法可以把集合中两个位置的内容进行交换**\n\n```\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\n\npublic class Collection_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n//\t\tList<String> allList = Collections.emptyList();\t\t\t//返回不可变的空List集合\n//\t\tSet<String> allSet = Collections.emptySet();\t\t\t//返回不可变的空List集合\n\t\t\n\t\tList<String> all = new ArrayList<String>();\t\t\t//实例化List\n\t\tCollections.addAll(all, \"zhangsan\",\"lisi\",\"wangwu\");\t\t//增加内容\n\t\t\n//\t\tCollections.replaceAll(all, \"wangwu\", \"lisi\");\t\t\t//替换内容\n\t\t\n//\t\tCollections.reverse(all);\t\t\t\t\t\t\t\t\t\t\t//内容反转保存\n\t\tIterator<String> iter = all.iterator();\t\t\t\t\t//实例化iterator对象\n\t\twhile(iter.hasNext()){\n\t\tSystem.out.println(iter.next()+\"、\");\n\t\t}\n\t\t\n//\t\tCollections.sort(all);\n\t\tCollections.swap(all,0,2);\t\t//交换指定位置的内容\n\t\tSystem.out.println(\"排序之后的集合\");\n\t\titer = all.iterator();\n\t\twhile(iter.hasNext()){\n\t\tSystem.out.println(iter.next()+\"、\");\n\t\t}\n\t\t\n\t\tint point = Collections.binarySearch(all,\"zhangsan\");\n\t\tSystem.out.println(\"检索结果：\"+point); \t\t\t//输出位置\n\t\t\n\t}\n\n}\n```\n\n## 16.List接口\n\n**List是Collection的子接口**，其中可以**保存各个重复的内容**。\n\n<img src=\"/images/517519-20160316171013771-1216509457.png\" alt=\"\" />\n\n&nbsp;\n\nList接口的常用子类\n\n**1.ArrayList**\n\n**<1>实例操作一：向集合中增加元素**\n\n**<2>实例操作二：删除元素**\n\n**<3>实例操作三：输出List中的内容**\n\n**<4>实例操作四：将集合变为对象数组**\n\n```\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.List;\n\npublic class List_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tList<String> allList = null;\t\t\t//定义List对象\n\t\tCollection<String> allCollection = null;\t\t//定义Collection对象\n\t\tallList = new ArrayList<String>();\t\t\t\t\t\t//实例化List对象，只能是String类型\n\t\tallCollection = new ArrayList<String>();\t\t\t//实例化Collection，只能是String类型\n\t\t\n\t\tallList.add(\"Hello\");\t\t\t\t//是从Collection继承的方法\n\t\tallList.add(0,\"Word\");\t\t\t//此方法为List扩充的方法\n\t\tSystem.out.println(allList); \t//输出集合中的内容\n\t\t\n\t\tallCollection.add(\"zhangsan\");\t\t\t\t//增加数据\n\t\tallCollection.add(\"www.baidu.com\");\t\t\t//增加数据\n\t\t\n\t\tallList.addAll(allCollection);\t\t\t\t//是从Collection继承的方法，增加一组对象\n\t\tallList.addAll(0,allCollection);\t\t\t\t//是从Collection继承的方法，增加一组对象\n\t\tSystem.out.println(allList); \t//输出集合中的内容\n\t\t\n\t\tallList.remove(0);\t\t//删除指定位置的元素\n\t\tallList.remove(\"Hello\");\t\t//删除指定内容的元素\n\t\tSystem.out.println(allList); \t//输出集合中的内容\n\t\t\n\t\tSystem.out.println(\"从前向后输出：\");\n\t\tfor(int i=0;i<allList.size();i++){\n\t\t\tSystem.out.println(allList.get(i)+\"、\");\n\t\t}\n\t\t\n\t\tSystem.out.println(\"从后向前输出：\");\n\t\tfor(int i=allList.size()-1;i>=0;i--){\n\t\t\tSystem.out.println(allList.get(i)+\"、\");\n\t\t}\n\t\t\n\t\tString str[]  =allList.toArray(new String[] {});\t\t//指定的泛型类型\n\t\tSystem.out.println(\"转换为数组类型\");\n\t\tfor(int i =0;i<str.length;i++){\n\t\t\tSystem.out.println(str[i]+\"、\");\n\t\t}\n\t\t\n\t\tSystem.out.println(\"返回对象数组\");\n\t\tObject obj[] = allList.toArray();\n\t\tfor(int i =0;i<obj.length;i++){\n\t\t\tString temp = (String) obj[i];\n\t\t\tSystem.out.println(temp+\"、\");\n\t\t}\n\t\t\n\t\tSystem.out.println(allList.contains(\"zhangsan\")?\"字符串存在\":\"字符串不存在\");\n\t\tList<String> allSub = allList.subList(0, 2);\t\t//取出里面的部分集合,前两个\n\t\tSystem.out.println(allSub); \t//输出集合中的内容\n\t\tSystem.out.println(\"字符串的位置\"+allList.indexOf(\"zhangsan\"));\t//查询字符串的位置\n\t\tSystem.out.println(\"集合操作后是否为空？\"+allList.isEmpty());\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**2.LinkedList子类与Queue接口**\n\n**<strong>LinkedList**表示的是一个链表的操作类，即Java中已经为开发者提供好了一个链表程序，开发者直接使用即可，无需再重新开发。</strong>\n\n**<img src=\"/images/517519-20160316200550240-1155319072.png\" alt=\"\" />**\n\n&nbsp;\n\n<1>实例操作一：在链表的开头和结尾增加数据\n\n<2>实例操作二：找到链表头\n\n<3>实例操作三：以先进先出的方式取出全部的数据\n\n```\nimport java.util.LinkedList;\n\npublic class LinkedList_demo {\n\n\tpublic static void main(String[] args) {\n\t\t\t// TODO 自动生成的方法存根\n\t\t\tLinkedList<String> link = new LinkedList<String>();\n\t\t\tlink.add(\"A\");\n\t\t\tlink.add(\"B\");\n\t\t\tlink.add(\"C\");\n\t\t\tSystem.out.println(\"初始化链表：\"+link);  //输出链表内容，调用toString\n\t\t\t\n\t\t\tlink.addFirst(\"X\");\n\t\t\tlink.addLast(\"Y\");\n\t\t\tSystem.out.println(\"添加头尾之后的链表：\"+link);  //输出链表内容，调用toString\n\t\t\t\n\t\t\tSystem.out.println(\"使用element()方法找到表头：\"+link.element());\n\t\t\tSystem.out.println(\"找到之后的link内容\"+link);\n\t\t\t\n\t\t\tSystem.out.println(\"使用peek()方法找到表头：\"+link.peek());\n\t\t\tSystem.out.println(\"找到之后的link内容\"+link);\n\t\t\t\n\t\t\tSystem.out.println(\"使用poll()方法找到表头：\"+link.poll());\n\t\t\tSystem.out.println(\"找到之后的link内容\"+link);\n\t\t\t\n\t\t\tSystem.out.println(\"以先进先出的方式输出：\");\n\t\t\tfor(int i=0;i<link.size()+1;i++){\n\t\t\t\tSystem.out.println(link.poll()+\"、\");\n\t\t\t}\n\t}\n\t\n\n}\n\n```\n\n&nbsp;\n\n在类集中提供了以下4种常见的输出方式。\n\n**Iterator:**迭代输出，是使用最多的输出方式\n\n**ListIterator:**是Iterator的子接口，专门用于输出List的内容\n\n**Enumeration:**是一个旧的接口，功能与Iterator类似\n\n**foreach:**JDK1.5之后提供的新功能，可以输出数组或者集合\n\n&nbsp;\n\n**Iterator:迭代输出**\n\n**碰到集合输出的操作，就一定使用<strong>Iterator**接口</strong>\n\n**所谓的迭代输出接口就是将元素一个个进行判断，判断其是否有内容，如果有内容则把内容取出。**\n\n**<1>实例操作一：输出Collection中的全部内容**\n\n```\nimport java.util.ArrayList;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.SortedSet;\nimport java.util.TreeSet;\n\npublic class Iterator_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tList<String> all = new ArrayList<String>();\t\t//实例化List接口\n\t\tall.add(\"A\");\n\t\tall.add(\"C\");\n\t\tall.add(\"D\");\n\t\tall.add(\"E\");\n\t\tall.add(\"B\");\n\t\tIterator<String> iter = all.iterator();\t\t\t\t//直接实例化Iterator接口\n\t\twhile(iter.hasNext()){\t\t\t\t\t\t\t\t\t\t//依次判断\n\t\t\tSystem.out.println(iter.next()+\"、\"); \t\t\t//输出内容\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;**<2>实例操作二：使用Iterator删除指定内容**\n\n```\nimport java.util.ArrayList;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.SortedSet;\nimport java.util.TreeSet;\n\npublic class Iterator_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tList<String> all = new ArrayList<String>();\t\t//实例化List接口\n\t\tall.add(\"A\");\n\t\tall.add(\"C\");\n\t\tall.add(\"D\");\n\t\tall.add(\"E\");\n\t\tall.add(\"B\");\n\t\tIterator<String> iter = all.iterator();\t\t\t\t//直接实例化Iterator接口\n\t\twhile(iter.hasNext()){\t\t\t\t\t\t\t\t\t\t//依次判断\n\t\t\tString str = iter.next();\t\t\t\t\t\t\t\t\t//取出内容\n\t\t\tif(\"A\".equals(str)){\n\t\t\t\titer.remove();\n\t\t\t}else{\n\t\t\tSystem.out.println(str+\"、\"); \t\t\t//输出内容\n\t\t\t}\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**ListIterator:双向迭代输出**&nbsp;\n\nListIterator接口的主要功能是由前向后单向输出，而此时如果想实现有后向前或是由前向后的双向输出，则必须使用Iterator接口的子接口&mdash;&mdash;ListIterator。\n\n<img src=\"/images/517519-20160316230201209-1416610293.png\" alt=\"\" />\n\n**<1>进行双向迭代**\n\n**<2>增加及代替元素**\n\n```\nimport java.util.ArrayList;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.ListIterator;\n\npublic class ListIterator_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tList<String> all = new ArrayList<String>();\t\t//实例化List接口\n\t\t\n\t\tall.add(\"C\");\n\t\tall.add(\"D\");\n\t\tall.add(\"E\");\n\t\tall.add(\"A\");\n\t\tall.add(\"B\");\n\t\tListIterator<String> iter = all.listIterator();\t\t\t\t//实例化ListIterator接口\n\t\tSystem.out.println(\"由前向后输出：\"); \t\t\t\t\t//信息输出\n\t\twhile(iter.hasNext()){\t\t\t\t\t\t\t\t\t\t//依次判断\n\t\t\tString str = iter.next();\t\t\t\t\t\t\t\t\t//取出内容\n\t\t\tSystem.out.println(str+\"、\"); \t\t\t//输出内容\n\t\t\titer.set(\"LIN-\"+str);\t\t\t\t\t\t\t\t\t//替换元素\n\t\t}\n\t\t\n\t\tSystem.out.println(\"由后向前输出：\"); \t\t\t\t\t//信息输出\n\t\titer.add(\"TONY\"); \t\t\t\t\t\t\t\t\t\t\t\t\t\t//增加元素\n\t\twhile(iter.hasPrevious()){\t\t\t\t\t\t\t\t\t\t//依次判断\n\t\t\tString str = iter.previous();\t\t\t\t\t\t\t\t\t//取出内容\n\t\t\tSystem.out.println(str+\"、\"); \t\t\t//输出内容\n\t\t}\n\t}\n\n}&nbsp;\n```\n\n## 17.Set接口\n\nSet接口也是Collection接口的子接口，Set接口中不能加入重复的元素\n\nSet接口的常用子类\n\n**1、散列的存放：HashSet**\n\nHashSet是Set接口的一个子类，主要的特点是：里面不能存放重复的元素，而且采用散列的存储方式，所以没有顺序。\n\n**2、有序的存放：TreeSet**\n\n```\nimport java.util.HashSet;\nimport java.util.LinkedList;\nimport java.util.Set;\nimport java.util.TreeSet;\n\npublic class Set_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n//\t\tSet<String> allSet = new HashSet<String>();\t\t//无序\n\t\tSet<String> allSet = new TreeSet<String>();\t\t//有序\n\t\tallSet.add(\"A\");\n\t\tallSet.add(\"C\");\n\t\tallSet.add(\"D\");\n\t\tallSet.add(\"E\");\n\t\tallSet.add(\"B\");\n\n\t\tSystem.out.println(allSet);  //输出集合内容，调用toString\n\t}\n\n}\n\n```\n\n&nbsp;\n\nTreeSet中实现了SortedSet接口，此接口主要用于排序操作，即实现此接口的子类都属于排序的子类。\n\n<img src=\"/images/517519-20160316214852443-74834303.png\" alt=\"\" />\n\n```\nimport java.util.Set;\nimport java.util.SortedSet;\nimport java.util.TreeSet;\n\npublic class SortedSet_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tSortedSet<String> allSet = new TreeSet<String>();\t\t//有序\n\t\tallSet.add(\"A\");\n\t\tallSet.add(\"C\");\n\t\tallSet.add(\"D\");\n\t\tallSet.add(\"E\");\n\t\tallSet.add(\"B\");\n\n\t\tSystem.out.println(\"第一个元素：\"+allSet.first());\n\t\tSystem.out.println(\"最后一个元素：\"+allSet.last()); \t\t\t\n\t\tSystem.out.println(\"headSet元素：\"+allSet.headSet(\"c\")); \t\t\t\n\t\tSystem.out.println(\"tailSet元素：\"+allSet.tailSet(\"C\")); \n\t\tSystem.out.println(\"subSet元素：\"+allSet.subSet(\"B\",\"D\"));\n\t}\n\n}\n```\n\n## 18.Map接口\n\n**Collection、Set、List接口**都属于单值的操作，即每次只能操作一个对象，\n\n而**Map**与他们不同的是，每次操作的是一对对象，即**二元偶对象**，Map中的每个元素都使用**key->value**的形式存储在集合中。\n\n<img src=\"/images/517519-20160316232047959-1484987782.png\" alt=\"\" />\n\n&nbsp;\n\n**<1>实例操作一：向集合中增加和取出内容**\n\n在Map接口中使用put(Object key,Object value)方法可以向集合中增加内容，之后通过get(E key)方法根据key找到其对应的value\n\n```\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic class Map_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tMap<String,String> map = null;\t　　　　　//声明Map对象\n\t\tmap = new HashMap<String,String>();\t//key和value是String类\n\t\tmap.put(\"zhangsan\", \"www.baidu.com\");\t//增加内容\n\t\tmap.put(\"lisi\", \"www.alibaba.com\");　　　//增加内容\n\t\tmap.put(\"wangwu\", \"www.google.com\");\t//增加内容\n\t\tString val = map.get(\"zhangsan\");\n\t\t\n\t\tSystem.out.println(\"取出内容：\"+val);\n\t}\t\n\n}\n\n```\n\n&nbsp;\n\n**<2>实例操作二：判断指定的key或者value是否存在**\n\n```\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic class Map_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tMap<String,String> map = null;\t\t//声明Map对象\n\t\tmap = new HashMap<String,String>();\t//key和value是String类\n\t\tmap.put(\"zhangsan\", \"www.baidu.com\");\t//增加内容\n\t\tmap.put(\"lisi\", \"www.alibaba.com\");\t//增加内容\n\t\tmap.put(\"wangwu\", \"www.google.com\");\t//增加内容\n\t\tString val = map.get(\"zhangsan\");\n\t\t\n\t\tSystem.out.println(\"取出内容：\"+val);\n\t\t\n\t\tif(map.containsKey(\"zhangsan\")){\t\t//查找指定的key\n\t\t\tSystem.out.println(\"搜索的key存在\");\n\t\t}else{\n\t\t\tSystem.out.println(\"搜索的key不存在\");\n\t\t}\n\t\tif(map.containsValue(\"www.baidu.com\")){\t\t//查找指定的value\n\t\t\tSystem.out.println(\"搜索的value存在\");\n\t\t}else{\n\t\t\tSystem.out.println(\"搜索的value不存在\");\n\t\t}\n\t}\t\n\n}\n\n```\n\n&nbsp;\n\n**<3>实例操作三：输出全部的key**\n\n```\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.Map;\nimport java.util.Set;\n\npublic class Map_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tMap<String,String> map = null;\t\t//声明Map对象\n\t\tmap = new HashMap<String,String>();\t//key和value是String类\n\t\tmap.put(\"zhangsan\", \"www.baidu.com\");\t//增加内容\n\t\tmap.put(\"lisi\", \"www.alibaba.com\");\t//增加内容\n\t\tmap.put(\"wangwu\", \"www.google.com\");\t//增加内容\n\t\t\n\t\tSet<String> keys = map.keySet();\t\t//取得所有的key\n\t\tIterator<String> iter = keys.iterator();\t//实例化Iterator\n\t\t\n\t\tSystem.out.println(\"全部的key：\");\n\t\twhile(iter.hasNext()){\n\t\t\tString str = iter.next();\t\t//取出集合的key\n\t\t\tSystem.out.println(str+\"、\");\n\t\t}\n\t}\t\n\n}\n\n```\n\n&nbsp;\n\n<4>实例操作三：输出全部的value\n\n```\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.Map;\nimport java.util.Set;\n\npublic class Map_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tMap<String,String> map = null;\t\t//声明Map对象\n\t\tmap = new HashMap<String,String>();\t//key和value是String类\n\t\tmap.put(\"zhangsan\", \"www.baidu.com\");\t//增加内容\n\t\tmap.put(\"lisi\", \"www.alibaba.com\");\t//增加内容\n\t\tmap.put(\"wangwu\", \"www.google.com\");\t//增加内容\n\t\t\t\n\t\tCollection<String> values = map.values();\t\t//取得所有的value\n\t\tIterator<String> iter = values.iterator();\t\t//实例化Iterator\n\t\tSystem.out.println(\"全部的values：\");\n\t\twhile(iter.hasNext()){\n\t\t\tString str = iter.next();\t\t\t//取出集合的key\n\t\t\tSystem.out.println(str+\"、\");\n\t\t}\n\t}\t\n\n}\n\n```\n\n&nbsp;\n\n**旧的子类：Hashtable**\n\nHashtable也是Map中的一个子类，属于旧的操作类\n\n```\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Hashtable;\nimport java.util.Iterator;\nimport java.util.Map;\nimport java.util.Set;\n\npublic class Map_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tMap<String,String> map = null;\t\t//声明Map对象\n\t\tmap = new Hashtable<String,String>();\t//key和value是String类\n\t\tmap.put(\"zhangsan\", \"www.baidu.com\");\t//增加内容\n\t\tmap.put(\"lisi\", \"www.alibaba.com\");\t//增加内容\n\t\tmap.put(\"wangwu\", \"www.google.com\");\t//增加内容\n\t\t\n\t\tSet<String> keys = map.keySet();\t\t//取得所有的key\n\t\tIterator<String> iter = keys.iterator();\t//实例化Iterator\n\t\tSystem.out.println(\"全部的key：\");\n\t\twhile(iter.hasNext()){\n\t\t\tString str = iter.next();\t\t//取出集合的key\n\t\t\tSystem.out.println(str+\"、\");\n\t\t}\n\t\tCollection<String> values = map.values();\t//取得所有的value\n\t\tIterator<String> iter2 = values.iterator();\t//实例化Iterator\n\t\tSystem.out.println(\"全部的values：\");\n\t\twhile(iter2.hasNext()){\n\t\t\tString str = iter2.next();\t\t//取出集合的value\n\t\t\tSystem.out.println(str+\"、\");\n\t\t}\n\t}\t\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160317103354568-1247723695.png\" alt=\"\" />\n\n&nbsp;\n\n**排序的子类：TreeMap**\n\n**排序后输出**\n\n```\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Hashtable;\nimport java.util.Iterator;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.TreeMap;\n\npublic class Map_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tMap<String,String> map = null;\t\t//声明Map对象\n\t\tmap = new TreeMap<String,String>();\t//实例化Map对象\n\t\tmap.put(\"B\", \"www.baidu.com\");\t　　　　//增加内容\n\t\tmap.put(\"A\", \"www.alibaba.com\");\t//增加内容\n\t\tmap.put(\"C\", \"www.google.com\");\t　　　　//增加内容\n\t\tSet<String> keys = map.keySet();\t\t//取得所有的key\n\t\tIterator<String> iter = keys.iterator();\t//实例化Iterator\n\t\tSystem.out.println(\"全部的key：\");\n\t\twhile(iter.hasNext()){\n\t\t\tString str = iter.next();\t\t//取出集合的key，排序后输出\n\t\t\tSystem.out.println(str+\"、\");\n\t\t}\n\t}\t\n\n}\n\n```\n\n&nbsp;\n\n**&nbsp;弱引用类：WeakHashMap**\n\n**之前的Map子类中的数据都是使用强引用保存的，即里面的内容不管是否使用都始终在集合中保留，如果希望集合自动清理暂时不用的数据就使用WeakHashMap类。这样，当进行垃圾收集时会释放掉集合中的垃圾信息。**\n\n```\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Hashtable;\nimport java.util.Iterator;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.TreeMap;\nimport java.util.WeakHashMap;\n\npublic class Map_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tMap<String,String> map = null;\t\t\t//声明Map对象\n\t\tmap = new WeakHashMap<String,String>();\t\t//实例化Map对象\n\t\tmap.put(new String(\"B\"), new String(\"www.baidu.com\"));\t//增加内容\n\t\tmap.put(\"A\", \"www.alibaba.com\");\t\t\t//增加内容\n\t\tSystem.gc();\n\t\tmap.put(\"C\", \"www.google.com\");\t　　　　　　　　//增加内容\n\t\tSystem.out.println(\"内容：\"+map);\n\t}\t\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160317110410068-1764633811.png\" alt=\"\" />\n\n&nbsp;\n\n<img src=\"/images/517519-20170105215637628-719408969.png\" alt=\"\" />\n\n&nbsp;\n\n**Map输出方式一：Iterator输出Map（使用 allSet = map.entrySet();，<strong>重要：开发中经常使用**）</strong>\n\n```\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Hashtable;\nimport java.util.Iterator;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.TreeMap;\nimport java.util.WeakHashMap;\n\npublic class Map_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tMap<String,String> map = null;\t\t\t//声明Map对象\n\t\tmap = new WeakHashMap<String,String>();\t\t//实例化Map对象\n\t\tmap.put(\"B\", \"www.baidu.com\");\t\t\t//增加内容\n\t\tmap.put(\"A\", \"www.alibaba.com\");\t\t//增加内容\n\t\tmap.put(\"C\", \"www.google.com\");\t\t\t//增加内容\n\t\tSet<Map.Entry<String, String>> allSet = null;\t//声明一个Set集合，指定泛型\n\t\tallSet = map.entrySet();\t\t\t\t\t//将Map接口实例变为Set接口实例\n\t\tIterator<Map.Entry<String, String>> iter = null;\t\t//声明Iterator对象\n\t\titer = allSet.iterator();\t\t\t\t\t//实例化Iterator对象\n\t\twhile(iter.hasNext()){\n\t\t　　Map.Entry<String, String> me = iter.next();\t\t\t//找到Map.Entry实例\n\t\t　　System.out.println(me.getKey()+\"-->\"+me.getValue());\n\t}\n\t}\t\n\n}\n\n```\n\n&nbsp;\n\n**Map输出方式二：foreach输出Map**\n\n```\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Hashtable;\nimport java.util.Iterator;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.TreeMap;\nimport java.util.WeakHashMap;\n\npublic class Map_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tMap<String,String> map = null;\t\t\t//声明Map对象\n\t\tmap = new WeakHashMap<String,String>();\t\t//实例化Map对象\n\t\tmap.put(\"B\", \"www.baidu.com\");\t\t\t//增加内容\n\t\tmap.put(\"A\", \"www.alibaba.com\");\t\t//增加内容\n\t\tmap.put(\"C\", \"www.google.com\");\t\t\t//增加内容\n\t\t\n\t\tfor(Map.Entry<String, String> me: map.entrySet()){\n\t\t\tSystem.out.println(me.getKey()+\"-->\"+me.getValue());\n\t\t}\n\t\t\n\t}\t\n\n}\n\n```\n\n&nbsp;\n\n**直接使用非系统类作为Key**\n\n```\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Hashtable;\nimport java.util.Iterator;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.TreeMap;\nimport java.util.WeakHashMap;\n\npublic class Map_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tMap<String,person> map = null;\t\t\t\t//声明Map对象，指定泛型类型\n\t\tmap = new HashMap<String,person>();\t\t\t//实例化Map对象\n\t\tmap.put(\"zhangsan\", new person(\"张三\",30));\n\t\tSystem.out.println(map.get(\"zhangsan\"));\n\t}\t\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160317134041240-243071476.png\" alt=\"\" />\n\n&nbsp;\n\n**key可以重复的Map集合：IdentityHashMap**\n\n```\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Hashtable;\nimport java.util.IdentityHashMap;\nimport java.util.Iterator;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.TreeMap;\nimport java.util.WeakHashMap;\n\npublic class Map_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tMap<person,String> map = null;\t\t\t\t//声明Map对象，指定泛型类型\n\t\tmap = new IdentityHashMap<person,String>();\t\t//实例化Map对象\n\t\tmap.put(new person(\"张三\",30),\"zhangsan_1\");\n\t\tmap.put(new person(\"张三\",30),\"zhangsan_1\");\n\t\t\n\t\tSet<Map.Entry<person, String>> allSet = null;\n\t\tallSet = map.entrySet();\n\t\tIterator<Map.Entry<person, String>> iter = null;\n\t\titer = allSet.iterator();\n\t\twhile(iter.hasNext()){\n\t\t　　Map.Entry<person, String> me = iter.next();\t\t//找到Map.Entry实例\n\t\t　　System.out.println(me.getKey()+\"-->\"+me.getValue());\n\t\t}\n\t}\t\n\n}\n\n```\n\n&nbsp;\n\nSortedMap接口是排序接口，只要是实现了此接口的子类，都属于排序的子类，TreeMap也是此接口的一个子类。\n\n<img src=\"/images/517519-20160317142343709-1394283262.png\" alt=\"\" />\n\n```\nimport java.util.Map;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\n\npublic class SortedMap_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tSortedMap<String,String> map = null;\t\t\t\t//声明Map对象\n\t\tmap = new TreeMap<String,String>();\t//key和value是String类\n\t\tmap.put(\"zhangsan\", \"www.baidu.com\");\t//增加内容\n\t\tmap.put(\"lisi\", \"www.alibaba.com\");\t\t\t//增加内容\n\t\tmap.put(\"wangwu\", \"www.google.com\");\t//增加内容\n\t\tSystem.out.println(\"第一个元素的内容的key:\"+map.firstKey());\n\t\tSystem.out.println(\"对应的值:\"+map.get(map.firstKey()));\n\t\tSystem.out.println(\"最后一个元素的内容的key:\"+map.lastKey());\n\t\tSystem.out.println(\"对应的值:\"+map.get(map.lastKey()));\n\t\t\n\t\tSystem.out.println(\"输出小于指定范围的\");\n\t\tfor(Map.Entry<String, String> me: map.headMap(\"wangwu\").entrySet()){\n\t\tSystem.out.println(me.getKey()+\"-->\"+me.getValue());\n\t\t}\n\t\t\n\t\tSystem.out.println(\"输出大于等于指定范围的\");\n\t\tfor(Map.Entry<String, String> me: map.tailMap(\"wangwu\").entrySet()){\n\t\tSystem.out.println(me.getKey()+\"-->\"+me.getValue());\n\t\t}\n\t\t\n\t\tSystem.out.println(\"输出在指定范围的\");\n\t\tfor(Map.Entry<String, String> me: map.subMap(\"lisi\",\"zhangsan\").entrySet()){\n\t\tSystem.out.println(me.getKey()+\"-->\"+me.getValue());\n\t\t}\n\t}\n\n}\n\n```\n\n## 19.输入数据类\n\n### 1.Scanner类\n\n专门的输入数据类，可以完成输入数据操作，也可以方便地对输入数据进行验证。\n\n此类存放在java.util包中\n\n<img src=\"/images/517519-20160315105712146-542540192.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160315105733349-386571224.png\" alt=\"\" />\n\n#### **<1>实例操作一：实现基本的数据输入**\n\n使用Scanner类的next()方法\n\n```\nimport java.util.Scanner;\n\npublic class Scanner_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tScanner scan = new Scanner(System.in); \n\t\tscan.useDelimiter(\"\\n\");\t\t\t//修改输入数据的分隔符，不然空格以后的字符串不能显示，\\n回车\n\t\tSystem.out.println(\"输入数据：\");\n\t\tString str = scan.next();\n\t\tSystem.out.println(\"输入数据的数据为：\"+str);\n\t}\n\n}\n\n```\n\n如果输入int或者是float类型的数据，scanner类中也支持，不过最好先使用hasNextXxx()方法进行验证\n\n```\nimport java.util.Scanner;\n\npublic class Scanner_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\n\t\tScanner scan = new Scanner(System.in); \t//从键盘接收数据\n\t\tint i =0;\n\t\tfloat f = 0.0f;\n\t\tSystem.out.println(\"输入整数：\");\n\t\tif(scan.hasNextInt()){\n\t\t\ti = scan.nextInt();\n\t\t\tSystem.out.println(\"输入的整数\"+i);\n\t\t}else{\n\t\t\tSystem.out.println(\"输入的不是整数\");\n\t\t}\n\t\t\n\t\tSystem.out.println(\"输入小数：\");\n\t\tif(scan.hasNextFloat()){\n\t\t\tf = scan.nextFloat();\n\t\t\tSystem.out.println(\"输入的小数\"+f);\n\t\t}else{\n\t\t\tSystem.out.println(\"输入的不是小数\");\n\t\t}\n\t}\n\n}\n\n```\n\n#### **<2>实例操作一：实现日期格式的数据输入**\n\n**使用hasNext()对输入的数据进行正则验证，如果合法，则转换成Date类型**\n\n```\nimport java.text.ParseException;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\nimport java.util.Scanner;\n\npublic class Scanner_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n//\t\tScanner scan = new Scanner(System.in); \n//\t\tscan.useDelimiter(\"\\n\");\t\t\t//修改输入数据的分隔符，不然空格以后的字符串不能显示，\\n回车\n//\t\tSystem.out.println(\"输入数据：\");\n//\t\tString str = scan.next();\n//\t\tSystem.out.println(\"输入数据的数据为：\"+str);\n\t\t\n//\t\tScanner scan = new Scanner(System.in); \t//从键盘接收数据\n//\t\tint i =0;\n//\t\tfloat f = 0.0f;\n//\t\tSystem.out.println(\"输入整数：\");\n//\t\tif(scan.hasNextInt()){\n//\t\t\ti = scan.nextInt();\n//\t\t\tSystem.out.println(\"输入的整数\"+i);\n//\t\t}else{\n//\t\t\tSystem.out.println(\"输入的不是整数\");\n//\t\t}\n//\t\t\n//\t\tSystem.out.println(\"输入小数：\");\n//\t\tif(scan.hasNextFloat()){\n//\t\t\tf = scan.nextFloat();\n//\t\t\tSystem.out.println(\"输入的小数\"+f);\n//\t\t}else{\n//\t\t\tSystem.out.println(\"输入的不是小数\");\n//\t\t}\n\t\t\n\t\tScanner scan = new Scanner(System.in); \t//从键盘接收数据\n\t\tSystem.out.println(\"输入日期(yyyy-MM-dd):\");\n\t\tString str = null;\n\t\tDate date = null;\n\t\tif(scan.hasNext(\"^\\\\d{4}-\\\\d{2}-\\\\d{2}$\")){\t\t//判断输入格式是否是日期\n\t\t\tstr = scan.next(\"^\\\\d{4}-\\\\d{2}-\\\\d{2}$\");\t\t//接收日期格式的字符串\n\t\t\ttry{\n\t\t\t\tdate = new SimpleDateFormat(\"yyyy-MM-dd\").parse(str);\n\t\t\t}catch(ParseException e){\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t\t}else{\n\t\t\tSystem.out.println(\"输入的日期格式错误\");\n\t\t}\n\t\tSystem.out.println(date);\n\t}\n\n}\n\n```\n\n#### **<3>实例操作三：从文件中得到数据**\n\n```\nimport java.io.File;\nimport java.io.FileNotFoundException;\nimport java.text.ParseException;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\nimport java.util.Scanner;\n\npublic class Scanner_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n//\t\tScanner scan = new Scanner(System.in); \n//\t\tscan.useDelimiter(\"\\n\");\t\t\t//修改输入数据的分隔符，不然空格以后的字符串不能显示，\\n回车\n//\t\tSystem.out.println(\"输入数据：\");\n//\t\tString str = scan.next();\n//\t\tSystem.out.println(\"输入数据的数据为：\"+str);\n\t\t\n//\t\tScanner scan = new Scanner(System.in); \t//从键盘接收数据\n//\t\tint i =0;\n//\t\tfloat f = 0.0f;\n//\t\tSystem.out.println(\"输入整数：\");\n//\t\tif(scan.hasNextInt()){\n//\t\t\ti = scan.nextInt();\n//\t\t\tSystem.out.println(\"输入的整数\"+i);\n//\t\t}else{\n//\t\t\tSystem.out.println(\"输入的不是整数\");\n//\t\t}\n//\t\t\n//\t\tSystem.out.println(\"输入小数：\");\n//\t\tif(scan.hasNextFloat()){\n//\t\t\tf = scan.nextFloat();\n//\t\t\tSystem.out.println(\"输入的小数\"+f);\n//\t\t}else{\n//\t\t\tSystem.out.println(\"输入的不是小数\");\n//\t\t}\n\t\t\n//\t\tScanner scan = new Scanner(System.in); \t//从键盘接收数据\n//\t\tSystem.out.println(\"输入日期(yyyy-MM-dd):\");\n//\t\tString str = null;\n//\t\tDate date = null;\n//\t\tif(scan.hasNext(\"^\\\\d{4}-\\\\d{2}-\\\\d{2}$\")){\t\t//判断输入格式是否是日期\n//\t\t\tstr = scan.next(\"^\\\\d{4}-\\\\d{2}-\\\\d{2}$\");\t\t//接收日期格式的字符串\n//\t\t\ttry{\n//\t\t\t\tdate = new SimpleDateFormat(\"yyyy-MM-dd\").parse(str);\n//\t\t\t}catch(ParseException e){\n//\t\t\t\te.printStackTrace();\n//\t\t\t}\n//\t\t}else{\n//\t\t\tSystem.out.println(\"输入的日期格式错误\");\n//\t\t}\n//\t\tSystem.out.println(date);\n\t\t\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.txt\");//路径\n\t\tScanner scan = null;\n\t\ttry{\n\t\t\tscan = new Scanner(f);\t\t\t\t\t//从文件接收数据\n\t\t}catch(FileNotFoundException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\tStringBuffer str = new StringBuffer();\t\t//用于接收数据\n\t\twhile(scan.hasNext()){\t\t\t\t\t\t\t\t//判断是否还有内容\n\t\t\tstr.append(scan.next()).append(\"\\n\");\t//分隔符分隔字符串，在每个字符串之后加上回车\n\t\t}\n\t\tSystem.out.println(str);\n\t}\n\n}\n\n```\n\n### 2.BufferedReader类\n\nBufferedReader类用于从缓冲区中读取内容，多有的输入字节数据都将放在缓冲区中。\n\n<img src=\"/images/517519-20160315092235787-257496897.png\" alt=\"\" />\n\nBufferedReader中定义的构造方法只能接收**字符输入流**的实例，所以必须使用字符输入流和字节输入流的转换类InputStreamReader将字节输入流System.in变为字符流\n\n以下程序没有长度的限制，也可以正确地接收中文，所以以下代码就是键盘输入数据的标准格式\n\n```\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\n\npublic class BufferedReader_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tBufferedReader buf = null;\t\t\t\t//声明BufferedReader对象\n\t\tbuf = new BufferedReader(new InputStreamReader(System.in));\t//实例化BufferedReader对象\n\t\tString str = null;\t\t\t\t\t\t\t\t//接收输入内容\n\t\tSystem.out.print(\"请输入内容：\");\n\t\ttry{\n\t\t\tstr = buf.readLine();\t\t\t\t\t\t//读取输入内容\n\t\t}catch(IOException e){\t\t\t\t\t\t//要进行异常处理\n\t\t\te.printStackTrace();\n\t\t}\n\t\tSystem.out.println(\"输入的内容：\"+str);\n\t}\n}\n\n```\n\n#### **实例操作一：加法操作**\n\n**输入两个数字，并让两个数字相加**\n\n**可以实现整数、小数、字符串和日期类型数据的输入**\n\n```\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.text.ParseException;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\n\nclass InputData{\n\tprivate BufferedReader buf = null;\t\t\t//声明BufferReader对象\n\n\tpublic InputData() {\t\t//构造方法\n\t\tthis.buf = new BufferedReader(new InputStreamReader(System.in));\t//实例化BufferedReader对象\n\t}\n\t\n\tpublic String getString(String info){\t\t//从此方法中得到字符串的信息\n\t\tString temp = null;\t\t\t\t\t\t\t\t//接收输入内容\n\t\tSystem.out.println(info);\n\t\t try{\n\t\t\t temp = this.buf.readLine();                     //读取输入内容\n\t        }catch(IOException e){                      \t\t//要进行异常处理\n\t            e.printStackTrace();\n\t        }\n\t\treturn temp;\n\t}\n\t\n\tpublic int getInt(String info,String err){\t\t//得到一个整数的输入数据\n\t\tint temp = 0;\n\t\tString str = null;\n\t\tboolean flag = true;\t\t\t\t\t\t\t\t\t//定义一个循环的处理标志\n\t\twhile(flag){\n\t\t\tstr = this.getString(info);\n\t\t\tif(str.matches(\"^\\\\d+$\")){\t\t\t\t\t\t\t//符合数字的格式\n\t\t\t\ttemp = Integer.parseInt(str);\n\t\t\t\tflag = false;\n\t\t\t}else{\n\t\t\t\tSystem.out.println(err);\n\t\t\t}\n\t\t}\n\t\treturn temp;\n\t}\n\t\n\tpublic Float getFloat(String info,String err){\t\t//得到一个小数的输入数据\n\t\tFloat temp = 0f;\n\t\tString str = null;\n\t\tboolean flag = true;\t\t\t\t\t\t\t\t\t//定义一个循环的处理标志\n\t\twhile(flag){\n\t\t\tstr = this.getString(info);\n\t\t\tif(str.matches(\"^\\\\d+.?\\\\d+$\")){\t\t\t\t\t\t\t//符合小数的格式\n\t\t\t\ttemp = Float.parseFloat(str);\n\t\t\t\tflag = false;\n\t\t\t}else{\n\t\t\t\tSystem.out.println(err);\n\t\t\t}\n\t\t}\n\t\treturn temp;\n\t}\n\t\n\tpublic Date getDate(String info,String err){\t\t//得到一个日期的输入数据\n\t\tDate d = null;\n\t\tString str = null;\n\t\tboolean flag = true;\t\t\t\t\t\t\t\t\t//定义一个循环的处理标志\n\t\twhile(flag){\n\t\t\tstr = this.getString(info);\n\t\t\tif(str.matches(\"^\\\\d{4}+\\\\d{2}$\")){\t\t\t\t\t\t\t//符合小数的格式\n\t\t\t\tSimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd\");\n\t\t\t\ttry{\n\t\t\t\t\td = sdf.parse(str);\t\t\t\t//将字符串变为Date型数据\n\t\t\t\t}catch(ParseException e){\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n\t\t\t\tflag = false;\t\t\t\t\t\t\t//更改标志位后，将退出循环\n\t\t\t}else{\n\t\t\t\tSystem.out.println(err);\n\t\t\t}\n\t\t}\n\t\treturn d;\n\t}\n}\n\npublic class BufferedReader_demo2 {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tint i;\n\t\tint j;\n\t\tInputData input = new InputData();\n\t\ti = input.getInt(\"请输入第一个数字：\", \"输入的数据必须是数字，请重新输入！\");\n\t\tj = input.getInt(\"请输入第二个数字：\", \"输入的数据必须是数字，请重新输入！\");\n\t\tSystem.out.println(i+\"+\"+j+\"=\"+(i+j));\n\t}\n}\n\n```\n\n#### 实例操作二：菜单显示\n\n```\nclass Operation{\n\tpublic static void add(){\n\t\tSystem.out.println(\"你选择的是增加操作！\");\n\t}\n\tpublic static void delete(){\n\t\tSystem.out.println(\"你选择的是删除操作！\");\n\t}\n\tpublic static void update(){\n\t\tSystem.out.println(\"你选择的是更新操作！\");\n\t}\n\tpublic static void find(){\n\t\tSystem.out.println(\"你选择的是查看操作！\");\n\t}\n}\n\n//主类\n//Function        : \tmenu_demo;\npublic class menu_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\twhile(true){\n\t\t\tshow();\n\t\t}\n\t}\n\t\n\tpublic static void show(){\n\t\tSystem.out.println(\"菜单系统\");\n\t\tSystem.out.println(\"【1】增加数据\");\n\t\tSystem.out.println(\"【2】删除数据\");\n\t\tSystem.out.println(\"【3】修改数据\");\n\t\tSystem.out.println(\"【4】查看数据\");\n\t\tSystem.out.println(\"【0】系统退出\");\n\t\tInputData input = new InputData();\n\t\tint i = input.getInt(\"请选择\", \"请输入正确的选项\");\n\t\tswitch(i){\n\t\t\tcase 1:{\n\t\t\t\tOperation.add();\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase 2:{\n\t\t\t\tOperation.delete();\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase 3:{\n\t\t\t\tOperation.update();\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase 4:{\n\t\t\t\tOperation.find();\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase 0:{\n\t\t\t\tSystem.exit(1);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdefault:{\n\t\t\t\tSystem.out.println(\"请选择正确的操作\");\n\t\t\t}\n\t\t}\n\t}\n}\n\n```\n\n## 20.File类\n\n使用**File类**可以进行创建或者删除文件等常用操作。\n\n<img src=\"/images/517519-20160313114615850-626091528.png\" alt=\"\" />\n\n**<1>创建一个新文件**\n\n```\nimport java.io.File;\nimport java.io.IOException;\n\npublic class File_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.txt\");//路径\n\t\tSystem.out.println(\"pathSeparator:\"+File.pathSeparator);\t\t//调用静态常量\n\t\tSystem.out.println(\"separator:\"+File.separator);\t\t\t\t\t\t\t//调用静态常量\n\t\ttry{\n\t\t\tf.createNewFile();\n\t\t}catch(IOException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160313151800397-1900167991.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160313151820179-605460839.png\" alt=\"\" />\n\n&nbsp;\n\n**<2>删除一个指定的文件**\n\n&nbsp;使用File类中的delete()方法\n\n```\nimport java.io.File;\nimport java.io.IOException;\n\npublic class delete_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.txt\");//路径\n\t\tif(f.exists()){\n\t\t\tf.delete();\n\t\t}else{\n\t\t\ttry{\n\t\t\t\tf.createNewFile();\n\t\t\t}catch(IOException e){\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t\t}\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**<3>创建一个文件夹**\n\n使用mkdir()方法完成\n\n```\nimport java.io.File;\nimport java.io.IOException;\n\npublic class delete_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.txt\");//路径\n\t\tif(f.exists()){\n\t\t\tf.delete();\n\t\t}else{\n\t\t\ttry{\n\t\t\t\tf.createNewFile();\n\t\t\t}catch(IOException e){\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t\t}\n\t\tFile f1 = new File(\"/home/common/software/coding/HelloWord/HelloWord/test\");//路径\n\t\tf1.mkdirs();\n\t}\n\n}\n\n```\n\n&nbsp;\n\n**<4>列出指定目录的全部文件**\n\n<img src=\"/images/517519-20160313153552788-1113195284.png\" alt=\"\" />\n\n```\nimport java.io.File;\nimport java.io.IOException;\n\npublic class listFile_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord\");//路径\n//\t\tString str[] = f.list();\t\t\t\t\t\t//列出给定目录中的内容\n\t\tFile files[] = f.listFiles();\t\t\t\t\t\t//列出给定目录中的文件，包括路径\n\t\tfor(int i = 0;i<files.length;i++){\n\t\t\tSystem.out.println(files[i]);\n\t\t}\n\t}\n}\n\n```\n\n&nbsp;\n\n**<5>判断一个给定的路径是否是目录**\n\n使用isDirectory()方法判断给定的路径是否是目录\n\n```\nimport java.io.File;\nimport java.io.IOException;\n\npublic class listFile_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord\");//路径\n\t\t\n\t\tif(f.isDirectory()){\n\t\t\tSystem.out.println(\"是路径\");\n\t\t}\n\t}\n}\n\n```\n\n&nbsp;\n\n**<strong><6>**列出指定目录的全部内容</strong>\n\n```\nimport java.io.File;\nimport java.io.IOException;\n\npublic class File_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord\");//路径\n\t\tprint(f);\n\t}\n\t\n\tpublic static void print(File file){\n\t\tif(file != null){\n\t\t\tif(file.isDirectory()){\t\t\t\t\t\t\t//判断是否是目录\n\t\t\t\tFile f[] = file.listFiles();\t\t\t\t\t//如果是目录，则列出全部的内容\n\t\t\t\tif(f != null){\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\tfor(int i=0;i<f.length;i++){\t\t//列出目录下的全部内容\n\t\t\t\t\t\tprint(f[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}else{\n\t\t\t\tSystem.out.println(file);\t\t\t\t//如果不是目录，则直接打印路径信息\n\t\t\t}\n\t\t}\n\t}\n}\n\n```\n\n&nbsp;\n\nFile类只是针对文件本身进行操作，而如果要对文件内容进行操作，则可以使用RandomAccessFile类，此类属于随机读取类，可以随机地读取一个文件中指定位置的数据。\n\n<img src=\"/images/517519-20160314093844474-526130520.png\" alt=\"\" />\n\n```\nimport java.io.File;\nimport java.io.RandomAccessFile;\n\npublic class RandomAccessFile_demo {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.txt\");//路径\n\t\tRandomAccessFile rdf = null;\t\t\t\t//声明一个RandomAccessFile类对象\n\t\trdf = new RandomAccessFile(f,\"r\");\t//以读写方式打开文件，会自动创建新文件\n\t\t\n\t\tString name = \"zhangsan\";\n\t\tint age = 30;\n\t\t\n\t\trdf.writeBytes(name);\n\t\trdf.writeInt(age);\n\t\t\n\t\tname = \"lisi\";\n\t\tage = 22;\n\t\t\n\t\trdf.writeBytes(name);\n\t\trdf.writeInt(age);\n\t\t\n\t\tname = \"wangwu\";\n\t\tage = 32;\n\t\t\n\t\trdf.writeBytes(name);\n\t\trdf.writeInt(age);\n\t\trdf.close();\n\t}\n}\n\n```\n\n## 21.日期操作类\n\n主要使用java.util包中的Date、Calendar以及java.text包中的SimpleDateFormat\n\n### **1.Date类**\n\n```\nimport java.util.Date;\n\npublic class Date_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tDate date = new Date();\t\t\t\t\t\t\t\t\t\t//实例化Date类对象\n\t\tSystem.out.println(\"当前日期为：\"+date);\t\t//输出日期\n\t}\n\n}\n\n```\n\n### **2.Calendar类**\n\n<img src=\"/images/517519-20160311152620772-344557843.png\" alt=\"\" />\n\n```\nimport java.util.Calendar;\nimport java.util.GregorianCalendar;\n\npublic class Calendar_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tCalendar calendar = null;\t\t\t\t\t\t\t//声明一个Calendar对象\n\t\tcalendar = new GregorianCalendar();\t\t//通过子类为其实例化\n\t\tSystem.out.println(\"年：\"+calendar.get(Calendar.YEAR));\n\t\tSystem.out.println(\"月：\"+(calendar.get(Calendar.MONTH)+1));\n\t\tSystem.out.println(\"日：\"+calendar.get(Calendar.DAY_OF_MONTH));\n\t\tSystem.out.println(\"时：\"+calendar.get(Calendar.HOUR_OF_DAY));\n\t\tSystem.out.println(\"分：\"+calendar.get(Calendar.MINUTE));\n\t\tSystem.out.println(\"秒：\"+calendar.get(Calendar.SECOND));\n\t\tSystem.out.println(\"毫秒：\"+calendar.get(Calendar.MILLISECOND));\n\t}\n\n}\n\n```\n\n输出\n\n```\n年：2017\n月：5\n日：17\n时：20\n分：56\n秒：8\n毫秒：427\n\n```\n\n&nbsp;\n\n**DateFormat类**\n\n对java.util.Date进行格式化操作，为了符合中国人的习惯\n\nDateFormat类和MessageFormat类都属于Format类的子类，专门用于格式化数据使用\n\nDateFormat类是一个抽象类，无法直接实例化，但是在此抽象类中提供了一个静态方法，可以直接取得本类的实例。\n\n<img src=\"/images/517519-20160311161112975-998713610.png\" alt=\"\" />\n\n```\nimport java.util.Date;\nimport java.text.DateFormat;\n\npublic class DateFormat_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tDateFormat df1 = null;\t\t\t\t\t\t\t\t\t\t//声明DateFormat类对象\n\t\tDateFormat df2 = null;\t\t\t\t\t\t\t\t\t\t//声明DateFormat类对象\n\t\tdf1 = DateFormat.getDateInstance();\t\t\t\t//取得日期\n\t\tdf2 = DateFormat.getDateTimeInstance();\t\t//取得日期时间\n\t\tSystem.out.println(\"DATE:\"+df1.format(new Date()));\t\t//格式化日期\n\t\tSystem.out.println(\"DATETIME:\"+df2.format(new Date()));\n\t}\n\n}\n&nbsp;\n```\n\n&nbsp;\n\n**指定显示的风格**\n\n```\nimport java.util.Date;\nimport java.util.Locale;\nimport java.text.DateFormat;\n\npublic class DateFormat_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tDateFormat df1 = null;\t\t\t\t\t\t\t\t\t\t//声明DateFormat类对象\n\t\tDateFormat df2 = null;\t\t\t\t\t\t\t\t\t\t//声明DateFormat类对象\n\n\t\t\n\t\t//取得日期时间，设置日期的显示格式、时间的显示格式\n\t\tdf1 = DateFormat.getDateInstance(DateFormat.YEAR_FIELD,new Locale(\"zh\",\"CN\"));\n\t\tdf2 = DateFormat.getDateTimeInstance(DateFormat.YEAR_FIELD,DateFormat.ERA_FIELD,new Locale(\"zh\",\"CN\"));\n\t\tSystem.out.println(\"DATE:\"+df1.format(new Date()));\t\t//格式化日期\n\t\tSystem.out.println(\"DATETIME:\"+df2.format(new Date()));\n\t}\n\n}\n\n```\n\n输出\n\n```\nDATE:2017年5月17日\nDATETIME:2017年5月17日 下午09时38分22秒 CST\n\n```\n\n&nbsp;\n\n**格式化日期操作**\n\n首先使用第1个模板将字符串中表示的日期数字取出，然后再使用第2个模板将这些日期数字重新转化为新的格式表示。\n\n```\nimport java.util.Date;\nimport java.text.ParseException;\nimport java.text.SimpleDateFormat;\n\npublic class DateFormat_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n//\t\tDateFormat df1 = null;\t\t\t\t\t\t\t\t\t\t//声明DateFormat类对象\n//\t\tDateFormat df2 = null;\t\t\t\t\t\t\t\t\t\t//声明DateFormat类对象\n////\t\tdf1 = DateFormat.getDateInstance();\t\t\t\t//取得日期\n////\t\tdf2 = DateFormat.getDateTimeInstance();\t\t//取得日期时间\n//\t\t\n//\t\t//取得日期时间，设置日期的显示格式、时间的显示格式\n//\t\tdf1 = DateFormat.getDateInstance(DateFormat.YEAR_FIELD,new Locale(\"zh\",\"CN\"));\n//\t\tdf2 = DateFormat.getDateTimeInstance(DateFormat.YEAR_FIELD,DateFormat.ERA_FIELD,new Locale(\"zh\",\"CN\"));\n//\t\tSystem.out.println(\"DATE:\"+df1.format(new Date()));\t\t//格式化日期\n//\t\tSystem.out.println(\"DATETIME:\"+df2.format(new Date()));\n\t\t\n\t\tString strDate = \"2016-3-11 10:20:30.123\";\t\t//定义日期时间的字符串\n\t\tString pat1 = \"yyyy-MM-dd HH:mm:ss.SSS\";\t\t//准备第1个模板，从字符串中提取数字\n\t\tString pat2 = \"yyyy年MM月dd日HH时mm分ss秒SSS毫秒\";\t\t//准备第1个模板，从字符串中提取数字\n\t\tSimpleDateFormat sdf1 = new SimpleDateFormat(pat1);\t\t\t//实例化模板对象\n\t\tSimpleDateFormat sdf2 = new SimpleDateFormat(pat2);\t\t\t//实例化模板对象\n\t\t\n\t\tDate d = null;\n\t\ttry{\n\t\t\td = sdf1.parse(strDate);\t\t\t\t//将给定字符串中的日期提取出来\n\t\t}catch(ParseException e){\t\t\t\t//如果提供的字符串格式有错误，则进行异常处理\n\t\t\te.printStackTrace();\n\t\t}\n\t\tSystem.out.println(sdf2.format(d));\t\t\t\t//将日期变成新的格式\n\t}\n\n}\n\n```\n\n&nbsp;输出\n\n```\n2016年03月11日10时20分30秒123毫秒\n\n```\n\n**SimpleDateFormat的parse方法**\n\n```\n\tpublic static void main(String[] args) throws ParseException {\n\t\t// TODO 自动生成的方法存根\n\t\tSimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd\");\n\t\tString dateString = \"2017-08-01\";\n\t\tDate date = sdf.parse(dateString);\n\t\tSystem.out.println(date);\n\t}\n\n```\n\n&nbsp;输出\n\n```\nTue Aug 01 00:00:00 CST 2017\n\n```\n\n&nbsp;\n\n**实现：基于Calendar类**\n\n```\nimport java.sql.Date;\nimport java.util.Calendar;\nimport java.util.GregorianCalendar;\n\nclass DateTime {\n\tprivate Calendar calendar = null;\t\t//定义一个Calendar对象，可以取得时间\n\n\tpublic DateTime() {\t\t\t\t\t\t\t\t\t\n\t\tsuper();\n\t\tthis.calendar = new GregorianCalendar();\t\t//通过Calendar类的子类实例化\n\t}\n\t\n\tpublic String getDate(){\t\t//得到完整的日期，格式为：yyyy-MM-dd HH:mm:ss.SSS\n\t\t//考虑到程序要频繁修改字符串，所以使用StringBuffer提升性能\n\t\tStringBuffer buf = new StringBuffer();\n\t\t//依次取得时间\n\t\tbuf.append(calendar.get(Calendar.YEAR)).append(\"-\");\n\t\tbuf.append(this.addZero(calendar.get(Calendar.MONTH)+1, 2));\n\t\tbuf.append(\"-\");\n\t\tbuf.append(this.addZero(calendar.get(Calendar.DAY_OF_MONTH), 2));\n\t\tbuf.append(\" \");\n\t\tbuf.append(this.addZero(calendar.get(Calendar.HOUR_OF_DAY), 2));\n\t\tbuf.append(\":\");\n\t\tbuf.append(this.addZero(calendar.get(Calendar.MINUTE), 2));\n\t\tbuf.append(\":\");\n\t\tbuf.append(this.addZero(calendar.get(Calendar.SECOND), 2));\n\t\tbuf.append(\".\");\n\t\tbuf.append(this.addZero(calendar.get(Calendar.MILLISECOND), 3));\n\t\t\n\t\treturn buf.toString();\n\t\t\n\t}\n\t\n\tpublic String getDateComplete(){\t\t//得到完整的日期，格式为：yyyy年MM月dd日HH时mm分ss秒SSS毫秒\n\t\t//考虑到程序要频繁修改字符串，所以使用StringBuffer提升性能\n\t\tStringBuffer buf = new StringBuffer();\n\t\t//依次取得时间\n\t\tbuf.append(calendar.get(Calendar.YEAR)).append(\"年\");\n\t\tbuf.append(this.addZero(calendar.get(Calendar.MONTH)+1, 2));\n\t\tbuf.append(\"月\");\n\t\tbuf.append(this.addZero(calendar.get(Calendar.DAY_OF_MONTH), 2));\n\t\tbuf.append(\"日\");\n\t\tbuf.append(this.addZero(calendar.get(Calendar.HOUR_OF_DAY), 2));\n\t\tbuf.append(\"时\");\n\t\tbuf.append(this.addZero(calendar.get(Calendar.MINUTE), 2));\n\t\tbuf.append(\"分\");\n\t\tbuf.append(this.addZero(calendar.get(Calendar.SECOND), 2));\n\t\tbuf.append(\"秒\");\n\t\tbuf.append(this.addZero(calendar.get(Calendar.MILLISECOND), 3));\n\t\tbuf.append(\"毫秒\");\n\t\t\n\t\treturn buf.toString();\n\t\t\n\t}\n\t\n\t//考虑到日期中有前导0，所以在此处加上了补零的方法\n\tprivate String addZero(int num,int len){\n\t\tStringBuffer s = new StringBuffer(); \n\t\ts.append(num);\n\t\twhile(s.length()<len){\t\t//如果长度不足，则继续补零\n\t\t\ts.insert(0,\"0\");\t\t\t\t\t//在第1个位置处补零\n\t\t}\n\t\treturn s.toString();\n\t}\n\t\n\tpublic String getTimeStamp(){\t\t//得到时间戳：yyyyMMddHHmmssSSS\n\t\tStringBuffer buf = new StringBuffer();\n\t\tbuf.append(calendar.get(Calendar.YEAR));\n\t\tbuf.append(this.addZero(calendar.get(Calendar.MONTH)+1, 2));\n\t\tbuf.append(this.addZero(calendar.get(Calendar.DAY_OF_MONTH), 2));\n\t\tbuf.append(this.addZero(calendar.get(Calendar.HOUR_OF_DAY), 2));\n\t\tbuf.append(this.addZero(calendar.get(Calendar.MINUTE), 2));\n\t\tbuf.append(this.addZero(calendar.get(Calendar.SECOND), 2));\n\t\tbuf.append(this.addZero(calendar.get(Calendar.MILLISECOND), 3));\n\t\t\n\t\treturn buf.toString();\n\t\t\n\t}\n\t\n}\n\npublic class CalendarClass_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tDateTime dt = new DateTime();\t\t\t//实例化DateTime对象\n\t\tSystem.out.println(\"系统时间：\"+dt.getDate());\n\t\tSystem.out.println(\"中文时间：\"+dt.getDateComplete());\n\t\tSystem.out.println(\"系统时间：\"+dt.getTimeStamp());\n\t}\n\n}\n\n```\n\n&nbsp;输出\n\n```\n系统时间：2017-05-17 20:41:06.508\n中文时间：2017年05月17日20时41分06秒508毫秒\n系统时间：20170517204106508\n\n```\n\n&nbsp;\n\n**实现：基于SimpleDateFormat类**\n\n```\nimport java.util.Date;\nimport java.text.SimpleDateFormat;\n\nclass DateTime_1 {\n\tprivate SimpleDateFormat sdf = null;\t\t//声明日期格式化操作对象，直接对new Date()进行实例化\n\t\n\t//得到日期，格式为：yyyy-MM-dd HH:mm:ss.SSS\n\tpublic String getDate(){\t\t\n\t\tthis.sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS\");\n\t\treturn this.sdf.format(new Date());\t\t\n\t}\n\t\n\t//得到完整的日期，格式为：yyyy年MM月dd日HH时mm分ss秒SSS毫秒\n\tpublic String getDateComplete(){\t\t\n\t\tthis.sdf = new SimpleDateFormat(\"yyyy年MM月dd日HH时mm分ss秒SSS毫秒\");\n\t\treturn this.sdf.format(new Date());\t\t\n\t}\n\t\n\t//得到时间戳，格式为：yyyyMMddHHmmssSSS\n\tpublic String getDateStamp(){\t\t\n\t\tthis.sdf = new SimpleDateFormat(\"yyyyMMddHHmmssSSS\");\n\t\treturn this.sdf.format(new Date());\t\t\n\t}\n}\n\npublic class SimpleDateFormat_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tDateTime dt = new DateTime();\t\t\t//实例化DateTime对象\n\t\tSystem.out.println(\"系统时间：\"+dt.getDate());\n\t\tSystem.out.println(\"中文时间：\"+dt.getDateComplete());\n\t\tSystem.out.println(\"系统时间：\"+dt.getTimeStamp());\n\t}\n}\n\n```\n\n&nbsp;输出\n\n```\n系统时间：2017-05-17 20:59:26.224\n中文时间：2017年05月17日20时59分26秒224毫秒\n系统时间：20170517205926224\n\n```\n\n## 22.Properties属性类\n\n在一个属性文件中保存了多个属性，每一个属性就是直接用字符串表示出来的\"key=value对\"，而如果想要轻松地操作这些属性文件中的属性，可以通过Properties类方便地完成。\n\n<img src=\"/images/517519-20160317163615365-993635125.png\" alt=\"\" />\n\n&nbsp;\n\n**<1>设置和取得属性**\n\n```\nimport java.util.Properties;\n\npublic class Properties_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tProperties pro = new Properties();\n\t\tpro.setProperty(\"BJ\", \"Beijing\");\n\t\tpro.setProperty(\"NJ\", \"Nanjing\");\n\t\tpro.setProperty(\"TJ\", \"Tianjin\");\n\t\tSystem.out.println(\"获得属性\"+pro.getProperty(\"BJ\"));\n\t\tSystem.out.println(\"获得属性不存在\"+pro.getProperty(\"HB\"));\n\t\tSystem.out.println(\"获得属性不存在，同时设置默认的显示值\"+pro.getProperty(\"HB\",\"：：：没有发现\"));\n\t}\n}\n\n```\n\n&nbsp;\n\n**<2>将属性保存在普通文件中**\n\n```\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.util.Properties;\n\npublic class Properties_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tProperties pro = new Properties();\n\t\tpro.setProperty(\"BJ\", \"Beijing\");\n\t\tpro.setProperty(\"NJ\", \"Nanjing\");\n\t\tpro.setProperty(\"TJ\", \"Tianjin\");\n\t\tSystem.out.println(\"获得属性\"+pro.getProperty(\"BJ\"));\n\t\tSystem.out.println(\"获得属性不存在\"+pro.getProperty(\"HB\"));\n\t\tSystem.out.println(\"获得属性不存在，同时设置默认的显示值\"+pro.getProperty(\"HB\",\"：：：没有发现\"));\n\t\t\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.properties\");//路径\n\t\ttry{\n\t\t\tpro.store(new FileOutputStream(f),\"pro info\");\t\t//保存并添加注释信息\n\t\t}catch(Exception e){\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n}\n\n```\n\n&nbsp;\n\n**<3>从普通文件中读取属性内容**\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.util.Properties;\n\npublic class Properties_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tProperties pro = new Properties();\n\t\tpro.setProperty(\"BJ\", \"Beijing\");\n\t\tpro.setProperty(\"NJ\", \"Nanjing\");\n\t\tpro.setProperty(\"TJ\", \"Tianjin\");\n\t\tSystem.out.println(\"获得属性\"+pro.getProperty(\"BJ\"));\n\t\tSystem.out.println(\"获得属性不存在\"+pro.getProperty(\"HB\"));\n\t\tSystem.out.println(\"获得属性不存在，同时设置默认的显示值\"+pro.getProperty(\"HB\",\"：：：没有发现\"));\n\t\t\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.properties\");//路径\n\t\ttry{\n\t\t\tpro.load(new FileInputStream(f));\t\t//读取属性文件\n\t\t}catch(Exception e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\tSystem.out.println(\"BJ属性值为\"+pro.getProperty(\"BJ\"));\t\n\t}\n}\n\n```\n\n&nbsp;\n\n**<4>将属性保存在XML文件中**\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.util.Properties;\n\npublic class Properties_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tProperties pro = new Properties();\n\t\tpro.setProperty(\"BJ\", \"Beijing\");\n\t\tpro.setProperty(\"NJ\", \"Nanjing\");\n\t\tpro.setProperty(\"TJ\", \"Tianjin\");\n\t\tSystem.out.println(\"获得属性\"+pro.getProperty(\"BJ\"));\n\t\tSystem.out.println(\"获得属性不存在\"+pro.getProperty(\"HB\"));\n\t\tSystem.out.println(\"获得属性不存在，同时设置默认的显示值\"+pro.getProperty(\"HB\",\"：：：没有发现\"));\n\t\t\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.xml\");//路径\n\t\ttry{\n\t\t\tpro.storeToXML(new FileOutputStream(f),\"pro info\");\t\t//保存并添加注释信息\n\t\t}catch(Exception e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\tSystem.out.println(\"BJ属性值为\"+pro.getProperty(\"BJ\"));\t\n\t}\n}\n\n```\n\n&nbsp;\n\n**<5>从XML文件中读取属性**\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.util.Properties;\n\npublic class Properties_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tProperties pro = new Properties();\n\t\tpro.setProperty(\"BJ\", \"Beijing\");\n\t\tpro.setProperty(\"NJ\", \"Nanjing\");\n\t\tpro.setProperty(\"TJ\", \"Tianjin\");\n\t\tSystem.out.println(\"获得属性\"+pro.getProperty(\"BJ\"));\n\t\tSystem.out.println(\"获得属性不存在\"+pro.getProperty(\"HB\"));\n\t\tSystem.out.println(\"获得属性不存在，同时设置默认的显示值\"+pro.getProperty(\"HB\",\"：：：没有发现\"));\n\t\t\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.xml\");//路径\n\t\ttry{\n\t\t\tpro.loadFromXML(new FileInputStream(f));\t\t//读取属性文件\n\t\t}catch(Exception e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\tSystem.out.println(\"BJ属性值为\"+pro.getProperty(\"BJ\"));\t\n\t}\n}\n\n```\n\n## 23.runtime类\n\n**Runtime类**表示运行时的操作类，是一个封装了JVM进程的类，每一个JVM都对应着一个Runtime类的实例，此实例由JVM运行时为其实例化。\n\n<img src=\"/images/517519-20160310222953475-174399599.png\" alt=\"\" />\n\n```\npublic class Runtime_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tRuntime run = Runtime.getRuntime();\n\t\t\n\t\tSystem.out.println(\"JVM最大内存：\"+run.maxMemory());\n\t\tSystem.out.println(\"JVM空闲内存量：\"+run.freeMemory());\n\t\tString str = \"HELLO\";\n\t\tfor (int i=0;i<1000;i++){\n\t\t\tSystem.out.println(str += i);\n\t\t}\n\t\tSystem.out.println(\"JVM空闲内存量：\"+run.freeMemory());\n\t\trun.gc();\t\t\t\t\t//进行垃圾收集，释放空间\n\t\tSystem.out.println(\"JVM空闲内存量：\"+run.freeMemory());\n\t}\n}\n\n```\n\n&nbsp;\n\n**Runtime类与Process类**\n\n可以直接使用Runtime类运行本机的可执行程序\n\n```\npublic class Runtime_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tRuntime run = Runtime.getRuntime();\n\t\n\t\ttry{\n\t\t\trun.exec(\"notepad.exe\");\n\t\t}catch(Exception e){\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n}\n\n```\n\n&nbsp;\n\n可以通过控制Process进行系统的进程控制，如果想要让进程消失，则可以使用destroy()方法\n\n```\npublic class Runtime_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tRuntime run = Runtime.getRuntime();\n\t\n\t\tProcess pro = null;\t\t//声明一个Process对象，接收启动的进程\n\t\t\n\t\ttry{\n\t\t\tpro = run.exec(\"notepad.exe\");\n\t\t}catch(Exception e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\ttry{\n\t\t\tThread.sleep(5000);\n\t\t}catch(Exception e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\tpro.destroy();\n\t}\n}\n\n```\n\n## 24.system类\n\n**System类**是一些与系统相关属性和方法的集合，而且System类中所有的属性都是静态的，要想引用这些属性和方法，直接使用System类调用即可。\n\n<img src=\"/images/517519-20160311115602507-1863057445.png\" alt=\"\" />\n\n```\npublic class System_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tlong startTime = System.currentTimeMillis();\t//取得开始计算之前的时间\n\t\t\n\t\tint sum = 0;\t\t\t\t\t//累加操作\n\t\tfor(int i=0;i<300000000;i++){\n\t\t\tsum += i;\n\t\t}\n\t\t\n\t\tlong endTime = System.currentTimeMillis();\t//取得开始计算之后的时间\n\t\tSystem.out.println(\"计算所花费的时间：\"+(endTime-startTime)+\"毫秒\");\n\t\t\n\t\tSystem.getProperties().list(System.out);\t\t\t//列出系统的全部属性\n\t\t\n\t\tSystem.out.println(\"系统版本为：\"+System.getProperty(\"os.name\")+System.getProperty(\"os.version\")+System.getProperty(\"os.arch\"));\n\t\tSystem.out.println(\"系统用户为：\"+System.getProperty(\"user.name\"));\n\t\tSystem.out.println(\"当前用户目录：\"+System.getProperty(\"user.home\"));\n\t\tSystem.out.println(\"当前用户工作目录：\"+System.getProperty(\"user.dir\"));\n\t}\n}\n\n```\n\n&nbsp;\n\n**垃圾对象的回收**\n\nSystem类中也有一个rc()方法，此方法也可以进行垃圾的收集，而且此方法实际上是对Runtime类中的gc()方法的封装，功能与其类似。\n\n对一个对象进行回收，一个对象如果不再被任何栈内存所引用，那么此对象就可以被成为垃圾对象，等待被回收。实际上，等待的时间是不确定的，所以可以直接调用System.gc()方法进行垃圾的回收。\n\n<img src=\"/images/517519-20160311150449725-1985863748.png\" alt=\"\" />\n\n&nbsp;\n\n**System类对IO的支持**\n\n**<img src=\"/images/517519-20160314230953303-783469616.png\" alt=\"\" />**\n\n&nbsp;\n\n**<1>System.out**\n\n**System.out是PrintStream的对象，在<strong>PrintStream**中定义了一系列的print()和println()方法</strong>\n\n**<img src=\"/images/517519-20160314231342271-1073940635.png\" alt=\"\" />**\n\n**<img src=\"/images/517519-20160314231358615-254643874.png\" alt=\"\" />**\n\n&nbsp;\n\n**<2>System.err**\n\n**<strong>System.err表示的是错误信息输出，如果程序出现错误，则可以直接使用<strong>System.err**进行输出</strong></strong>\n\n**<strong><img src=\"/images/517519-20160314231516459-541554909.png\" alt=\"\" />**</strong>\n\n&nbsp;\n\n**<strong><2>System.in**</strong>\n\n**<strong><strong><strong>System.in**实际上是一个键盘的输入流，其本身是InputStream类型的对象，可以利用**<strong><strong><strong>System.in**</strong>完成从键盘读取数据的功能。</strong></strong></strong></strong></strong>\n\n**<strong><strong><strong><strong>指定空间的大小会出现空间限制，不指定大小则会在输入中文的时候产生乱码**</strong></strong></strong></strong>\n\n**<strong><strong><strong><strong><img src=\"/images/517519-20160314233345834-480253394.png\" alt=\"\" />**</strong></strong></strong></strong>\n\n```\npackage System;\n\nimport java.io.InputStream;\n\npublic class Systemin_demo {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tInputStream input = System.in;\t\t\t//从键盘接收数据\n\t\tbyte b[] = new byte[1024];\t\t\t\t\t//开辟空间，接收数据\n\t\tSystem.out.println(\"请输入内容：\");\n\t\tint len = input.read(b);\t\t\t\t\t\t\t//接收数据\n\t\tSystem.out.println(\"输入的内容：\"+new String(b,0,len));\n\t\tinput.close();\n\t}\n}\n\n```\n\n&nbsp;\n\n**输入/输出重定向**\n\n通过System类也可以改变System.in的输入流来源和System.out和System.err两个输出流的输出位置\n\n<img src=\"/images/517519-20160314233837443-1921193317.png\" alt=\"\" />\n\n## 25.Java字节流\n\n在程序中所有的数据都是以流的方式进行传输或保存的，程序需要数据时要使用输入流读取数据，而当程序需要将一些数据保存起来时，就要使用输出流。\n\n在java.io包中流的操作主要有字节流、字符流两大类，两类都有输入和输出操作。\n\n在字节流中输出数据主要使用OutStream类完成，输入使用的是InputStream类。\n\n在字符流中输出主要使用Write类完成，输入主要是使用Reader类完成。\n\n**字节流：**字节流主要操作byte类型数据，以byte数组为准\n\n主要操作类是OutputStream类和InputStream类。\n\n### **<1>字节输出流：OutputStream类**\n\n&nbsp;OutputStream是整个IO包中字节输出流的最大父类\n\n<img src=\"/images/517519-20160314155705131-1603689244.png\" alt=\"\" />\n\n&nbsp;\n\n**向文件中写入字符串**\n\n文件不存在则会自动创建\n\n直接将一个字符串变为byte数组，然后将byte数组直接写入到文件中\n\n```\nimport java.io.File;\nimport java.io.OutputStream;\nimport java.io.FileOutputStream;\n\npublic class OutputStream_demo {\n\n\tpublic static void main(String[] args) throws Exception {\t\t//异常抛出，不处理\n\t\t// TODO 自动生成的方法存根\n\t\t//第一步，找到一个文件\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.txt\");//路径\n\t\t//第二步，通过子类实例化父类对象\n\t\tOutputStream out = null;\t\t\t\t//准备好一个输出的对象\n\t\tout = new FileOutputStream(f);\t\t//通过对象多态性，进行实例化\n\t\t//第三步，进行写操作\n\t\tString str = \"HelloWord\";\t\t\t\t//准备一个字符串\n\t\tbyte b[] = str.getBytes();\t\t\t\t//只能输出byte数组，所以将字符串变成byte数组\n\t\t\n\t\tout.write(b); \t\t\t\t\t\t\t\t\t//将内容输出，保存文件\n\t\t//第四步，关闭输出流\n\t\tout.close(); \t\t\t\t\t\t\t\t\t//关闭输出流\n\t}\n}\n\n```\n\n&nbsp;\n\n**追加新内容**\n\n可以通过FileOutputStream向文件中追加内容。\n\n```\nimport java.io.File;\nimport java.io.OutputStream;\nimport java.io.FileOutputStream;\n\npublic class OutputStream_demo {\n\n\tpublic static void main(String[] args) throws Exception {\t\t//异常抛出，不处理\n\t\t// TODO 自动生成的方法存根\n\t\t//第一步，找到一个文件\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.txt\");//路径\n\t\t//第二步，通过子类实例化父类对象\n\t\tOutputStream out = null;\t\t\t\t//准备好一个输出的对象\n//\t\tout = new FileOutputStream(f);\t\t//通过对象多态性，进行实例化\n\t\tout = new FileOutputStream(f,true);\t\t//此处表示在文件末尾追加内容\n\t\t//第三步，进行写操作\n\t\tString str = \"HelloWord\";\t\t\t\t//准备一个字符串\n\t\tbyte b[] = str.getBytes();\t\t\t\t//只能输出byte数组，所以将字符串变成byte数组\n\t\t\n//\t\tout.write(b); \t\t\t\t\t\t\t\t\t//将内容输出，保存文件\n\t\t\n\t\tfor(int i=0;i<b.length;i++){\n\t\t\tout.write(b[i]);\n\t\t}\n\t\t//第四步，关闭输出流\n\t\tout.close(); \t\t\t\t\t\t\t\t\t//关闭输出流\n\t}\n}\n\n```\n\n&nbsp;\n\n### **<2>字节输入流：InputStream类**\n\n和OutputStream一样，InputStream也是抽象类，必须依靠子类\n\n<img src=\"/images/517519-20160314163818349-891968343.png\" alt=\"\" />\n\n```\nimport java.io.File;\nimport java.io.InputStream;\nimport java.io.FileInputStream;\nimport java.io.FileNotFoundException;\n\npublic class InputStream_demo {\n\n\tpublic static void main(String[] args) throws Exception {\t\t//异常抛出，不处理\n\t\t// TODO 自动生成的方法存根\n\t\t//第一步，找到一个文件\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.txt\");//路径\n\t\t//第二步，通过子类实例化父类对象\n\t\tInputStream input = null;\t\t\t\t//准备好一个输出的对象\n\t\tinput = new FileInputStream(f);\t\t//通过对象多态性，进行实例化\n\t\t//第三步，进行读操作\n//\t\tbyte b[] = new byte[1024];\t\t\t\t//所有的内容读到此数组中\n\t\tbyte b[] = new byte[(int)f.length()];\t\t\t\t//所有的内容读到此数组中，数组大小由文件决定\n//\t\tinput.read(b);\t\t\t\t\t\t\t\t\t\t//把内容取出，内容读到byte数组中\n\t\tint len = input.read(b);\t\t\t\t\t\t\n\t\t//第四步，关闭输入流\n\t\tinput.close();\n\t\t\n\t\tSystem.out.println(\"读入数据的长度：\"+len);\t\t\t\t//没有多余的空格产生\n\t\tSystem.out.println(\"内容为：\"+new String(b,0,len));//把byte数组变为字符串输出\n//\t\tSystem.out.println(\"内容为：\"+new String(b));//把byte数组变为字符串输出\n\t}\n}\n\n```\n\n&nbsp;\n\n在IO包中，提供了两个与平台无关的数据操作流，分别是**数据输出流(DataOuputStream)**和**数据输入流(DataInputStream).**\n\n**<strong>1.写入数据**</strong>\n\n**<img src=\"/images/517519-20160315142147631-784997358.png\" alt=\"\" />**\n\n&nbsp;\n\n```\nimport java.io.DataOutputStream;\nimport java.io.File;\nimport java.io.FileNotFoundException;\nimport java.io.FileOutputStream;\n\npublic class DataOutputStream_demo {\n\n\tpublic static void main(String[] args) throws Exception {\n\t\t// TODO 自动生成的方法存根\n\n\t\tDataOutputStream dos = null;\t\t\t//声明数据输出流对象\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/order.txt\");//路径\n\t\tdos = new DataOutputStream(new FileOutputStream(f));\t//实例化数据输出流对象\n\t\t\n\t\tString names[] = {\"衬衣\",\"手套\",\"围巾\"};\n\t\tfloat prices[] = {98.3f,30.0f,50.5f};\n\t\tint nums[] = {3,2,1};\n\t\tfor(int i=0;i<names.length;i++){\t//循环写入\n\t\t\tdos.writeChars(names[i]);\t\t\t//写入字符串\n\t\t\tdos.writeChar('\\t'); \t\t\t\t\t//加入分隔符\n\t\t\tdos.writeFloat(prices[i]);\t\t\t//写入字符串\n\t\t\tdos.writeChar('\\t'); \t\t\t\t\t//加入分隔符\n\t\t\tdos.writeInt(nums[i]);\t\t\t\t\t//写入字符串\n\t\t\tdos.writeChar('\\t'); \t\t\t\t\t//加入分隔符\n\t\t}\n\t\tdos.close();\n\t}\n}\n\n```\n\n&nbsp;<img src=\"/images/517519-20160315143638287-789653952.png\" alt=\"\" />\n\n**2.读取数据**\n\n```\nimport java.io.DataInputStream;\nimport java.io.DataOutputStream;\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileNotFoundException;\nimport java.io.FileOutputStream;\n\npublic class DataInputStream_demo {\n\n\tpublic static void main(String[] args) throws Exception {\n\t\t// TODO 自动生成的方法存根\n\n\t\tDataInputStream dis = null;\t\t\t//声明数据输出流对象\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/order.txt\");//路径\n\t\tdis = new DataInputStream(new FileInputStream(f));\t//实例化数据输出流对象\n\t\t\n\t\tString name = null;\n\t\tfloat price = 0.0f;\n\t\tint num = 0;\n\t\tchar temp[] = null;\n\t\tchar c = 0;\t\t\t\t\t\t\t\t\t//存放接收的字符\n\t\tint len = 0;\t\t\t\t\t\t\t\t\t//接收的字符的个数\n\t\ttry{\n\t\t\twhile(true){\n\t\t\t\ttemp = new char[200];\n\t\t\t\tlen = 0;\n\t\t\t\twhile((c=dis.readChar()) != '\\t'){\t//读取字符\n\t\t\t\t\ttemp[len] = c;\t\t\t\t\t\t\t\t//接收内容\n\t\t\t\t\tlen++;\t\t\t\t\t\t\t\t\t\t\t//读取长度加1\n\t\t\t\t}\t\t\n\t\t\t\tname = new String(temp,0,len);\t//将字符数组变成String\n\t\t\t\tprice = dis.readFloat();\t\t\t\t\t//读取float\n\t\t\t\tdis.readChar();\t\t\t\t\t\t\t\t//读取\\t\n\t\t\t\tnum = dis.readInt();\t\t\t\t\t\t//读取int\n\t\t\t\tdis.readChar();\t\t\t\t\t\t\t\t//读取\\n\n\t\t\t\tSystem.out.printf(\"名称：%s,名称：%5.2f,名称：%d\\n\",name,price,num);\n\t\t\t}\n\t\t}catch(Exception e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\tdis.close();\n\t}\n}\n\n```\n\n## 26.java字符流\n\n在程序中一个字符等于两个字节，Java提供了Reader和Writer两个专门操作字符流的类。\n\n### **<1>字符输出流Writer**\n\n也是一个抽象类\n\n<img src=\"/images/517519-20160314170437396-51722425.png\" alt=\"\" />\n\n和OutputStream相比，可以直接输出字符串，而不用将字符串变为byte数组之后再输出。\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileWriter;\nimport java.io.InputStream;\nimport java.io.Writer;\n\npublic class Write_demo {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\t//第一步，找到一个文件\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.txt\");//路径\n\t\t//第二步，通过子类实例化父类对象\n\t\tWriter out = null;\t\t\t\t//准备好一个输出的对象\n//\t\tout = new FileWriter(f);\t\t//通过对象多态性，进行实例化\n\t\tout = new FileWriter(f,true);\t\t//通过对象多态性，进行实例化\n\t\t//第三步，进行写操作\n\t\tString str = \"HelloWord!!!\";\t\t\t\t//准备一个字符串\n\t\tout.write(str);\t\t\t\t\t\t\t\t\t\t//把内容取出，内容读到byte数组中\n\t\t//第四步，关闭输入流\n\t\tout.close();\n\t}\n}\n\n```\n\n&nbsp;\n\n### **<2>字符输入流Reader**\n\nReader是使用字符的方式从文件中取出数据\n\n<img src=\"/images/517519-20160314172203115-1433146961.png\" alt=\"\" />\n\n```\nimport java.io.File;\nimport java.io.FileReader;\nimport java.io.Reader;\n\npublic class Reader_demo {\n\n\tpublic static void main(String[] args) throws Exception {\n\t\t// TODO 自动生成的方法存根\n\t\t//第一步，找到一个文件\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.txt\");//路径\n\t\t//第二步，通过子类实例化父类对象\n\t\tReader reader = null;\t\t\t\t//准备好一个输出的对象\n\t\treader = new FileReader(f);\t\t//通过对象多态性，进行实例化\n\t\t//第三步，进行读操作\n\t\tchar c[] = new char[1024];\t\t//所有的内容读到此数组中\n\t\tint len = reader.read(c);\t\t\t//将内容输出\n\t\t//第四步，关闭输入流\n\t\treader.close();\t\t\t\t\t\t\t//关闭输入流\n\t\tSystem.out.println(\"内容为：\"+new String(c,0,len));\t\t//把char数组变为字符串输出\n\t}\n\n}\n\n```\n\n### **字节流与字符流的区别**\n\n**字节流**在操作时本身不会用到缓冲区（内存），是文件本身直接操作的\n\n**字符流**在操作时使用了**缓冲区**，通过缓冲区再操作文件\n\n如果一个程序频繁地操作一个资源（如文件或者数据库），则性能会很低，此时为了提升性能，就可以将一部分数据暂时读入到内存的一块区域中，以后直接从此区域中读取数据即可，因为读取内存速度会比较快，这样可以提升程序的性能。\n\n<img src=\"/images/517519-20160314174939709-228062901.png\" alt=\"\" />\n\n**使用字节流更好**\n\n## **27.java转换流：字节流和字符流的转换类**\n\n### **<strong>InputStreamReader**：是Reader的子类，将输入的**字节流**变成**字符流**</strong>\n\n**将字节输出流变成字符输出流**\n\n```\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.OutputStreamWriter;\nimport java.io.Writer;\n\npublic class OutputStreamWriter_demo {\n\n\tpublic static void main(String[] args) throws Exception {\n\t\t// TODO 自动生成的方法存根\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.txt\");//路径\n\t\tWriter out = null;\n\t\tout = new OutputStreamWriter(new FileOutputStream(f));\t//字节流变成字符流\n\t\tout.write(\"HelloWord\");\n\t\tout.close();\n\t}\n}\n\n```\n\n&nbsp;\n\n### **<strong>OutputStreamWriter**：是Writer的子类，将输出的**字符流**变成**字节流**</strong>\n\n**将字节输出流变成字符输出流**\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.InputStreamReader;\nimport java.io.OutputStreamWriter;\nimport java.io.Reader;\nimport java.io.Writer;\n\npublic class InputStreamReader_demo {\n\n\tpublic static void main(String[] args) throws Exception {\n\t\t// TODO 自动生成的方法存根\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.txt\");//路径\n\t\tReader reader = null;\n\t\treader = new InputStreamReader(new FileInputStream(f));\t//字节流变成字符流\n\t\tchar c[] = new char[1024];\n\t\tint len = reader.read(c);\n\t\treader.close();\n\t\tSystem.out.println(new String(c,0,len));\n\t}\n}\n\n```\n\n<img src=\"/images/517519-20160314203600787-1605393319.png\" alt=\"\" />\n\n## 28.管道流：主要作用是可以进行两个线程间的通信\n\n分为**管道输出流(PipedOutputStream)**和**管道输入流(PipedInputStream)**\n\n**定义两个线程对象,在发送的线程类中定义了管道输出类,在接收的线程类中定义了管道的输入类,在操作时只需要使用PipedOutputStream类中提供的connection()方法就可以将两个线程冠带连接在一起,线程启动后会自动进行管道的输入和输出操作**\n\n```\nimport java.io.IOException;\nimport java.io.PipedInputStream;\nimport java.io.PipedOutputStream;\n\nclass Send implements Runnable{\t\t//实现Runnable接口\n\t\n\tprivate PipedOutputStream pos = null;\t//管道输出流\n\t\n\tpublic Send() {\t//实例化输出流\n\t\tsuper();\n\t\tthis.pos = new PipedOutputStream();\n\t}\n\t\n\tpublic PipedOutputStream getPos() {\t\t//通过线程类得到输出流\n\t\treturn pos;\n\t}\n\n\t@Override\n\tpublic void run() {\n\t\t// TODO 自动生成的方法存根\n\t\tString str = \"HelloWord!!\";\n\t\ttry{\n\t\t\tthis.pos.write(str.getBytes());\t\t//输出信息\n\t\t}catch(IOException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\ttry{\n\t\t\tthis.pos.close(); \t\t\t\t\t\t\t//关闭输出流\n\t\t}catch(IOException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n}\n\nclass Receive implements Runnable{\t\t//实现Runnable接口\n\t\n\tprivate PipedInputStream pis = null;\t//管道输入流\n\t\n\tpublic Receive() {\t//实例化输出流\n\t\tsuper();\n\t\tthis.pis = new PipedInputStream();\n\t}\n\t\n\tpublic PipedInputStream getPis() {\t\t//通过线程类得到输入流\n\t\treturn pis;\n\t}\n\n\t@Override\n\tpublic void run() {\n\t\t// TODO 自动生成的方法存根\n\t\tbyte b[] = new byte[1024];\t//实例化输入流\n\t\tint len = 0;\n\t\ttry{\n\t\t\tlen = this.pis.read(b);\t\t\t//接收数据\n\t\t}catch(IOException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\ttry{\n\t\t\tthis.pis.close(); \t\t\t\t\t\t\t//关闭输入流\n\t\t}catch(IOException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\tSystem.out.println(\"接收的内容为\"+new String(b,0,len));\n\t}\n}\n\npublic class Pipe_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tSend s = new Send();\n\t\tReceive r = new Receive();\n\t\ttry{\n\t\t\ts.getPos().connect(r.getPis());  \t//连接管道\n\t\t}catch(IOException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\tnew Thread(s).start();\n\t\tnew Thread(r).start();\n\t}\n}\n&nbsp;\n```\n\n## 29.内存操作流：可以将输出的位置设置在内存上\n\n此时就要使用ByteArrayInputStream、ByteArrayOutputStream来完成输入和输出功能。\n\nByteArrayInputStream主要完成将内容写入到内存中\n\nByteArrayOutputStream的功能主要是将内存中的数据输出\n\n<img src=\"/images/517519-20160314204828240-2072941031.png\" alt=\"\" />\n\n&nbsp;\n\n```\nimport java.io.ByteArrayInputStream;\nimport java.io.ByteArrayOutputStream;\nimport java.io.IOException;\n\npublic class ByteArrayStream_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tString str = \"HELLOWORD\";\n\t\tByteArrayInputStream bis = null;\t\t\t//声明一个内存的输入流\n\t\tByteArrayOutputStream bos = null;\t\t//声明一个内存的输出流\n\t\tbis = new ByteArrayInputStream(str.getBytes());\t//向内存中输入内容\n\t\tbos = new ByteArrayOutputStream();\t\t\t\t\t\t//准备从ByteArrayInputStream中读数据\n\t\t\n\t\tint temp = 0;\n\t\twhile((temp=bis.read()) != -1){\n\t\t\tchar c = (char)temp;\t\t//将读取的数字变为字符\n\t\t\tbos.write(Character.toLowerCase(c));\t\t//将字符变为小写\n\t\t}\n\t\tString newStr  = bos.toString();\t\t\t\t\t//取出内容\n\t\ttry{\n\t\t\tbis.close();\n\t\t\tbos.close();\n\t\t}catch(IOException e){\n\t\t\te.printStackTrace();\n\t\t}\n\t\tSystem.out.println(newStr);\n\t}\n}\n&nbsp;\n```\n\n<img src=\"/images/517519-20160314210921037-576057824.png\" alt=\"\" />\n\n## 30.压缩流\n\nZIP是一种较为常见的压缩形式，在Java中要实现ZIP的压缩需要导入java.util.zip包，可以使用此包中的ZipFile、ZipOutputStream、ZipInputStream和ZipEntry几个类完成操作。\n\n<img src=\"/images/517519-20160315153252474-26923124.png\" alt=\"\" />\n\n&nbsp;\n\n### **<1>ZipFile类**\n\n**压缩一个文件**\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.InputStream;\nimport java.util.zip.ZipEntry;\nimport java.util.zip.ZipOutputStream;\n\npublic class ZipOutputStream_demo {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tFile file = new File(\"/home/common/software/coding/HelloWord/HelloWord/order.txt\");//路径\n\t\tFile zipfile = new File(\"/home/common/software/coding/HelloWord/HelloWord/order.zip\");//路径\n\t\t\n\t\tInputStream input = new FileInputStream(file);\t\t//定义输入文件流\n\t\tZipOutputStream zipout = null;\t\t\t\t\t\t\t\t\t//定义压缩输出流\n\t\t//实例化压缩输出流对象，并指定压缩文件的输出路径\n\t\tzipout = new ZipOutputStream(new FileOutputStream(zipfile));\n\t\t//每一个被压缩的文件都用ZipEntry表示，需要为每一个压缩后的文件设置名称\n\t\tzipout.putNextEntry(new ZipEntry(file.getName()));\t\t//创建ZipEntry\n\t\tzipout.setComment(\"这是压缩后的文件\");\t\t\t\t\t\t//设置注释\n\t\tint temp = 0;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//接收输入的数据\n\t\twhile((temp = input.read()) != -1){\t\t\t\t\t\t\t\t\t//读取内容，采用了边读边写的方式\n\t\t\tzipout.write(temp);\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//压缩输出内容\n\t\t}\n\t\tinput.close();\n\t\tzipout.close();\n\t}\n}\n\n```\n\n**压缩一个文件夹**\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.InputStream;\nimport java.util.zip.ZipEntry;\nimport java.util.zip.ZipOutputStream;\n\npublic class ZipOutputStream_demo {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n//\t\tFile file = new File(\"/home/common/software/coding/HelloWord/HelloWord/order.txt\");//路径\n//\t\tFile zipfile = new File(\"/home/common/software/coding/HelloWord/HelloWord/order.zip\");//路径\n//\t\t\n//\t\tInputStream input = new FileInputStream(file);\t\t//定义输入文件流\n//\t\tZipOutputStream zipout = null;\t\t\t\t\t\t\t\t\t//定义压缩输出流\n//\t\t//实例化压缩输出流对象，并指定压缩文件的输出路径\n//\t\tzipout = new ZipOutputStream(new FileOutputStream(zipfile));\n//\t\t//每一个被压缩的文件都用ZipEntry表示，需要为每一个压缩后的文件设置名称\n//\t\tzipout.putNextEntry(new ZipEntry(file.getName()));\t\t//创建ZipEntry\n//\t\tzipout.setComment(\"这是压缩后的文件\");\t\t\t\t\t\t//设置注释\n//\t\tint temp = 0;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//接收输入的数据\n//\t\twhile((temp = input.read()) != -1){\t\t\t\t\t\t\t\t\t//读取内容，采用了边读边写的方式\n//\t\t\tzipout.write(temp);\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//压缩输出内容\n//\t\t}\n//\t\tinput.close();\n//\t\tzipout.close();\n\t\t\n\t\tFile file = new File(\"/home/common/software/coding/HelloWord/HelloWord/test\");//路径\n\t\tFile zipfile = new File(\"/home/common/software/coding/HelloWord/HelloWord/order.zip\");//路径\n\t\t\n\t\tInputStream input = null;\t\t//定义输入文件流\n\t\tZipOutputStream zipout = null;\t\t\t\t\t\t\t\t\t//定义压缩输出流\n\t\t//实例化压缩输出流对象，并指定压缩文件的输出路径\n\t\tzipout = new ZipOutputStream(new FileOutputStream(zipfile));\n\t\tzipout.setComment(\"这是压缩后的文件\");\t\t\t\t\t\t//设置注释\n\t\tif(file.isDirectory()){\t\t\t\t\t\t//判断是否是目录\n\t\t\tFile lists[] = file.listFiles();\t\t\t//列出全部文件\n\t\t\tfor(int i=0;i<lists.length;i++){\n\t\t\t\tinput = new FileInputStream(lists[i]);\t\t//设置文件输入流\n\t\t\t\t//每一个被压缩的文件都用ZipEntry表示，需要为每一个压缩后的文件设置名称\n\t\t\t\tzipout.putNextEntry(new ZipEntry(file.getName()+File.separator+lists[i].getName()));\t\t//创建ZipEntry\n\t\t\t\tint temp = 0;\n\t\t\t\twhile((temp = input.read()) != -1){\n\t\t\t\t\tzipout.write(temp);\n\t\t\t\t}\n\t\t\t\tinput.close();\n\t\t\t}\n\t\t}\n\t\tzipout.close();\n\t}\n}\n\n```\n\n&nbsp;\n\n```\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.util.zip.ZipEntry;\nimport java.util.zip.ZipFile;\n\npublic class ZipFile_demo {\n\n\tpublic static void main(String[] args)  throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tFile file = new File(\"/home/common/software/coding/HelloWord/HelloWord/b.zip\");//路径\n\t\tFile outputfile = new File(\"/home/common/software/coding/HelloWord/HelloWord/b1.txt\");//解压缩文件名称\n\t\t\n\t\tZipFile zipfile = new ZipFile(file);\t\t\t\t\t\t\t\t\t\t\t//实例化ZipFile对象\n//\t\tSystem.out.println(\"压缩文件的名称：\"+zipfile.getName());\t//得到压缩文件的名称\n\t\tZipEntry entry = zipfile.getEntry(\"b2.txt\");\t\t\t\t\t\t//得到一个压缩实体\n\t\tInputStream input = zipfile.getInputStream(entry);\t\t\t//取得ZipEntry输入流\n\t\tOutputStream out = new FileOutputStream(outputfile);\t\t\t//取得ZipEntry输入流\n\t\t\n\t\tint temp = 0;\t\t\t\t//保存接收数据\n\t\twhile((temp = input.read()) != -1){\n\t\t\tout.write(temp);\n\t\t}\n\t\tinput.close();\n\t\tout.close();\n\t}\n}\n\n```\n\n&nbsp;\n\n### **<2>ZipInputStream类**\n\n**ZipInputStream是InputStream的子类，通过此类可以方便地读取ZIP格式的压缩文件**\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.util.zip.ZipEntry;\nimport java.util.zip.ZipException;\nimport java.util.zip.ZipFile;\nimport java.util.zip.ZipInputStream;\n\npublic class ZipInputStream_demo {\n\n\tpublic static void main(String[] args) throws Exception {\n\t\t// TODO 自动生成的方法存根\n\t\tFile file = new File(\"/home/common/software/coding/HelloWord/HelloWord/HelloWord.zip\");//路径\n\t\tFile outfile = null;\t\t\t\t//定义输出的文件对象\n\t\tZipFile zipfile = new ZipFile(file);\t//实例化ZipFile对象\n\t\tZipInputStream zipinput = new ZipInputStream(new FileInputStream(file)); \t//实例化ZIP输入流\n\t\tZipEntry entry = null;\t\t\t\t\t\t//定义一个ZipEntry对象，用于接收压缩文件中的每一个实体\n\t\tInputStream input = null;\t\t\t\t//定义输入流，用于读取每一个ZipEntry\n\t\tOutputStream out = null;\t\t\t\t\t//定义输出流，用于输出每一个实体内容\n\t\twhile((entry = zipinput.getNextEntry()) != null){\t\t//得到每一个ZipEntry\n\t\t\tSystem.out.println(\"解压缩\"+entry.getName()+\"文件\");\n\t\t\toutfile = new File(\"/home/common/software/coding/HelloWord/HelloWord\"+entry.getName());\n\t\t\tif(!outfile.getParentFile().exists()){\t\t//判断文件夹是否存在\n\t\t\t\toutfile.getParentFile().mkdirs();\n\t\t\t}\n\t\t\tif(!outfile.exists()){\t\t\t\t//判断文件是否存在\n\t\t\t\toutfile.createNewFile();\n\t\t\t}\n\t\t\tinput = zipfile.getInputStream(entry);\t\t//得到压缩实体的输入流\n\t\t\tout = new FileOutputStream(outfile);\t\t//实例化输入流对象\n\t\t\tint temp = 0;\n\t\t\twhile((temp = input.read()) != -1){\n\t\t\t\tout.write(temp);\n\t\t\t}\n\t\t\tinput.close();\n\t\t\tout.close();\n\t\t}\n\t}\n}\n\n```\n\n## 31.**合并流：**主要功能是将两个文件的内容合并成一个文件\n\n如果要实现合并流，则必须使用SequenceInputStream类\n\n<img src=\"/images/517519-20160315145717943-27972741.png\" alt=\"\" />\n\n&nbsp;\n\n```\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.io.SequenceInputStream;\n\npublic class SequenceInputStream_demo {\n\n\tpublic static void main(String[] args) throws Exception{\n\t\t// TODO 自动生成的方法存根\n\t\tInputStream is1 = null;\t\t\t\t//输入流1\n\t\tInputStream is2 = null;\t\t\t\t//输入流2\n\t\tOutputStream os = null;\t\t\t\t//输出流\n\t\tSequenceInputStream sis = null;\t//合并流\n\t\t\n\t\tis1 = new FileInputStream(\"/home/common/software/coding/HelloWord/HelloWord/a.txt\");\n\t\tis2 = new FileInputStream(\"/home/common/software/coding/HelloWord/HelloWord/b.txt\");\n\t\tos = new FileOutputStream(\"/home/common/software/coding/HelloWord/HelloWord/ab.txt\");\n\t\tsis = new SequenceInputStream(is1,is2);\t\t\t\t//实例化合并流\n\t\tint temp = 0;\n\t\twhile((temp = sis.read()) != -1){\n\t\t\tos.write(temp);\n\t\t}\n\t\tsis.close();\n\t\tis1.close();\n\t\tis2.close();\n\t\tos.close();\n\t}\n}\n\n```\n\n## 32.对象序列化**<br />**\n\n**对象序列化**就是**把一个对象变为二进制的数据流的一种方法**。\n\n通过对象序列化可以**方便地实现对象的传输或存储**。\n\n如果一个类的对象想被序列化，则对象所在的类必须实现java.io.Serializable接口\n\n```\nclass Person_3 implements Serializable{\t\t//此类的对象可以被序列化\n\tprivate String name;\n\tprivate int age;\n\t\n\tpublic Person_3(String name, int age) {\n\t\tsuper();\n\t\tthis.name = name;\n\t\tthis.age = age;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn \"姓名：\" + name + \", 年龄：\" + age;\n\t}\n}\n\n```\n\n&nbsp;此类的对象是可以经过二进制数据流进行传输的\n\n如果要完成对象的输入或者输出，还必须依靠**对象输出流(ObjectOutputStream)**和**对象输入流(ObjectInputStream)**.\n\n使用对象输出流输出序列化对象的步骤有时也称为**序列化**\n\n而使用对象输入流读入对象的过程有时也称为**反序列化**\n\n**<img src=\"/images/517519-20160316100705896-427995571.png\" alt=\"\" />**\n\n### **<1>对象输出流ObjectOutputStream**\n\n```\nimport java.io.File;\nimport java.io.FileNotFoundException;\nimport java.io.FileOutputStream;\nimport java.io.ObjectOutputStream;\nimport java.io.OutputStream;\nimport java.io.Serializable;\n\nclass Person_3 implements Serializable{\t\t//此类的对象可以被序列化\n\tprivate String name;\n\tprivate int age;\n\t\n\tpublic Person_3(String name, int age) {\n\t\tsuper();\n\t\tthis.name = name;\n\t\tthis.age = age;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn \"姓名：\" + name + \", 年龄：\" + age;\n\t}\n}\n\npublic class Serializable_demo {\n\n\tpublic static void main(String[] args) throws Exception {\n\t\t// TODO 自动生成的方法存根\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.txt\");//路径\n\t\tObjectOutputStream oos = null;\n\t\tOutputStream out = new FileOutputStream(f);\t\t//文件输出流\n\t\toos = new ObjectOutputStream(out);\t\t\t\t\t\t//为对象输出流实例化\n\t\toos.writeObject(new Person_3(\"张三\", 30));\n\t\toos.close();\n\t}\n\n}\n```\n\n### **<2>对象输出流ObjectInputStream**\n\n```\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileNotFoundException;\nimport java.io.FileOutputStream;\nimport java.io.InputStream;\nimport java.io.ObjectInputStream;\nimport java.io.ObjectOutputStream;\nimport java.io.OutputStream;\nimport java.io.Serializable;\n\nclass Person_3 implements Serializable{\t\t//此类的对象可以被序列化\n\tprivate String name;\n\tprivate int age;\n\t\n\tpublic Person_3(String name, int age) {\n\t\tsuper();\n\t\tthis.name = name;\n\t\tthis.age = age;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn \"姓名：\" + name + \", 年龄：\" + age;\n\t}\n}\n\npublic class Serializable_demo {\n\n\tpublic static void main(String[] args) throws Exception {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tFile f = new File(\"/home/common/software/coding/HelloWord/HelloWord/test.txt\");//路径\n\t\tObjectInputStream ois = null;\n\t\tInputStream input = new FileInputStream(f);\t\t//文件输入流\n\t\tois = new ObjectInputStream(input);\t\t\t\t\t\t//为对象输入流实例化\n\t\tObject obj = ois.readObject();\t\t\t\t\t\t\t\t\t//读取对象\n\t\tois.close();\n\t\tSystem.out.println(obj);\n\t}\n}\n\n```\n\n## 33.Java国际化程序\n\n根据不同的国家配置不同的资源文件（资源文件有时也称为属性文件，后缀为.properties），所有的资源文件以键值对的形式出现。\n\n<img src=\"/images/517519-20160310233215741-1724066580.png\" alt=\"\" />\n\n&nbsp;\n\n**Locale类**\n\n**<img src=\"/images/517519-20160310234457960-1058568311.png\" alt=\"\" />**\n\n&nbsp;\n\n**ResourceBundle类**\n\n**<img src=\"/images/517519-20160310234614288-633332045.png\" alt=\"\" />**\n\n```\nimport java.util.ResourceBundle;;\n\npublic class Locale_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\tResourceBundle rb = ResourceBundle.getBundle(\"Message\"); //找到资源文件\n\t\tSystem.out.println(\"内容：\"+rb.getString(\"info\"));\t\t\t\t\t\t//从资源文件中取得内容\n\t}\n\n}\n\n```\n\n　　\n\n**根据Locale所选择的国家不同，输出不同国家的&ldquo;你好&rdquo;。**\n\n在属性文件中不能直接写入中文，读出来也是乱码，因此要变成Unicode编码\n\n<img src=\"/images/517519-20160311101309757-780795703.png\" alt=\"\" />\n\n```\nimport java.util.Locale;\nimport java.util.ResourceBundle;\n\npublic class Locale_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n\t\t\n\t\tLocale zhLoc = new Locale(\"zh\",\"CN\");\t\t//表示中国地区\n\t\tLocale enLoc = new Locale(\"en\",\"US\");\t\t//表示美国地区\n\t\tLocale frLoc = new Locale(\"fr\",\"FR\");\t\t//表示法国地区\n\t\t\n\t\tResourceBundle zhrb = ResourceBundle.getBundle(\"Message\", zhLoc);\t//找到中文的属性文件\n\t\tResourceBundle enrb = ResourceBundle.getBundle(\"Message\",enLoc);\t//找到英文的属性文件\n\t\tResourceBundle frrb = ResourceBundle.getBundle(\"Message\",frLoc);\t//找到法语的属性文件\n\t\t\n\t\tSystem.out.println(\"中文：\"+zhrb.getString(\"info\"));\n\t\tSystem.out.println(\"英文：\"+enrb.getString(\"info\"));\n\t\tSystem.out.println(\"法语：\"+frrb.getString(\"info\"));\n\t}\n\n}\n\n```\n\n&nbsp;\n\n使用MessageFormat格式化动态文本\n\n所有资源内容都是个固定的，但是输出的消息中如果包含一些动态文本，则必须使用占位符清楚地表示出动态文本的位置，占位符使用&ldquo;{编号}&rdquo;的格式出现。\n\n```\nimport java.text.MessageFormat;\nimport java.util.Locale;\nimport java.util.ResourceBundle;\n\npublic class Locale_demo {\n\n\tpublic static void main(String[] args) {\n\t\t// TODO 自动生成的方法存根\n//\t\tResourceBundle rb = ResourceBundle.getBundle(\"Message\"); //找到资源文件\n//\t\tSystem.out.println(\"内容：\"+rb.getString(\"info\"));\t\t\t\t\t\t//从资源文件中取得内容\n\t\t\n\t\tLocale zhLoc = new Locale(\"zh\",\"CN\");\t\t//表示中国地区\n\t\tLocale enLoc = new Locale(\"en\",\"US\");\t\t//表示美国地区\n\t\tLocale frLoc = new Locale(\"fr\",\"FR\");\t\t//表示法国地区\n\t\t\n\t\tResourceBundle zhrb = ResourceBundle.getBundle(\"Message\", zhLoc);\t//找到中文的属性文件\n\t\tResourceBundle enrb = ResourceBundle.getBundle(\"Message\",enLoc);\t//找到英文的属性文件\n\t\tResourceBundle frrb = ResourceBundle.getBundle(\"Message\",frLoc);\t//找到法语的属性文件\n\n\t\tSystem.out.println(\"中文：\"+zhrb.getString(\"info\"));\n\t\tSystem.out.println(\"英文：\"+enrb.getString(\"info\"));\n\t\tSystem.out.println(\"法语：\"+frrb.getString(\"info\"));\n\t\t\n\t\t//依次读取各个属性文件的内容，通过键值读取，此时的键值名称为&ldquo;info_1&rdquo;\n\t\tString str1 = zhrb.getString(\"info_1\");\n\t\tString str2 = enrb.getString(\"info_1\");\n\t\tString str3 = frrb.getString(\"info_1\");\n\t\tSystem.out.println(\"中文：\"+MessageFormat.format(str1,\"张三\"));\n\t\tSystem.out.println(\"英文：\"+MessageFormat.format(str2,\"zhangsan\"));\n\t\tSystem.out.println(\"法语：\"+MessageFormat.format(str3,\"zhangsan\"));\n\t}\n\n}\n\n```\n\n**&nbsp;properties文件，文件名Message_zh_CN.properties**\n\n```\ninfo = \\u4F60\\u597D\ninfo_1 = \\u4F60\\u597D\\uFF0C{0}\\uFF01\n\n```\n\n&nbsp;\n","tags":["Java"]},{"title":"Flink学习笔记——报错合集","url":"/Flink学习笔记——报错合集.html","content":"遇到的flink任务各种报错合集\n\n1.Exception in thread \"main\" java.lang.NoSuchMethodError: org.apache.commons.cli.Option.builder(Ljava/lang/String;)Lorg/apache/commons/cli/Option$Builder;\n\n```\nException in thread \"main\" java.lang.NoSuchMethodError: org.apache.commons.cli.Option.builder(Ljava/lang/String;)Lorg/apache/commons/cli/Option$Builder;\n\tat org.apache.flink.runtime.entrypoint.parser.CommandLineOptions.<clinit>(CommandLineOptions.java:27)\n\tat org.apache.flink.runtime.entrypoint.DynamicParametersConfigurationParserFactory.options(DynamicParametersConfigurationParserFactory.java:43)\n\tat org.apache.flink.runtime.entrypoint.DynamicParametersConfigurationParserFactory.getOptions(DynamicParametersConfigurationParserFactory.java:50)\n\tat org.apache.flink.runtime.entrypoint.parser.CommandLineParser.parse(CommandLineParser.java:42)\n\tat org.apache.flink.runtime.entrypoint.ClusterEntrypointUtils.parseParametersOrExit(ClusterEntrypointUtils.java:70)\n\tat org.apache.flink.yarn.entrypoint.YarnApplicationClusterEntryPoint.main(YarnApplicationClusterEntryPoint.java:87)\n\n```\n\n这是由于commons-cli包版本过低导致的，从1.2.升级到1.5.0可以解决这个问题\n\n2.Caused by: java.lang.ClassCastException: org.codehaus.janino.CompilerFactory cannot be cast to org.codehaus.commons.compiler.ICompilerFactory\n\n```\norg.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Unable to instantiate java compiler\n\tat org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372)\n\tat org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)\n\tat org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114)\n\tat org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:836)\n\tat org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:247)\n\tat org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1078)\n\tat org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1156)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)\n\tat org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1156)\nCaused by: java.lang.IllegalStateException: Unable to instantiate java compiler\n\tat org.apache.calcite.rel.metadata.JaninoRelMetadataProvider.compile(JaninoRelMetadataProvider.java:428)\n\tat org.apache.calcite.rel.metadata.JaninoRelMetadataProvider.load3(JaninoRelMetadataProvider.java:374)\n\tat org.apache.calcite.rel.metadata.JaninoRelMetadataProvider.lambda$static$0(JaninoRelMetadataProvider.java:109)\n\tat org.apache.flink.calcite.shaded.com.google.common.cache.CacheLoader$FunctionToCacheLoader.load(CacheLoader.java:165)\n\tat org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)\n\tat org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)\n\tat org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)\n\tat org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)\n\tat org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache.get(LocalCache.java:3951)\n\tat org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3974)\n\tat org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4958)\n\tat org.apache.calcite.rel.metadata.JaninoRelMetadataProvider.create(JaninoRelMetadataProvider.java:469)\n\tat org.apache.calcite.rel.metadata.JaninoRelMetadataProvider.revise(JaninoRelMetadataProvider.java:481)\n\tat org.apache.calcite.rel.metadata.RelMetadataQueryBase.revise(RelMetadataQueryBase.java:95)\n\tat org.apache.calcite.rel.metadata.RelMetadataQuery.getPulledUpPredicates(RelMetadataQuery.java:784)\n\tat org.apache.calcite.rel.rules.ReduceExpressionsRule$FilterReduceExpressionsRule.onMatch(ReduceExpressionsRule.java:151)\n\tat org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)\n\tat org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)\n\tat org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)\n\tat org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)\n\tat org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)\n\tat org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)\n\tat org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)\n\tat org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:64)\n\tat org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:78)\n\tat org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)\n\tat scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)\n\tat scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)\n\tat scala.collection.Iterator.foreach(Iterator.scala:937)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:937)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1425)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:70)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:69)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)\n\tat scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)\n\tat org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55)\n\tat org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:175)\n\tat org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:82)\n\tat org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:75)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:306)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:186)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1656)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:782)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:861)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:701)\n\tat com.xxx.data.FlinkSQLDemo.main(FlinkSQLDemo.java:415)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)\n\t... 11 more\nCaused by: java.lang.ClassCastException: org.codehaus.janino.CompilerFactory cannot be cast to org.codehaus.commons.compiler.ICompilerFactory\n\tat org.codehaus.commons.compiler.CompilerFactoryFactory.getCompilerFactory(CompilerFactoryFactory.java:129)\n\tat org.codehaus.commons.compiler.CompilerFactoryFactory.getDefaultCompilerFactory(CompilerFactoryFactory.java:79)\n\tat org.apache.calcite.rel.metadata.JaninoRelMetadataProvider.compile(JaninoRelMetadataProvider.java:426)\n\t... 63 more\n\n```\n\n在尝试了网上搜到的各种解决方案没有效果之后，将flink任务提交的命令从<!--more-->\n&nbsp;/usr/lib/flink/bin/flink run 改成&nbsp;/usr/lib/flink/bin/flink run-application 解决了\n\n&nbsp;\n","tags":["Flink"]},{"title":"JavaScript排序算法——冒泡排序","url":"/JavaScript排序算法——冒泡排序.html","content":"```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\" />\n<title>&aring;&dagger;&rsquo;&aelig;&sup3;&iexcl;</title>\n<!--<link rel=\"stylesheet\" type=\"text/css\" href=\"../style/fdt.css\" />-->\n<script type=\"text/javascript\" src=\"../js/jquery-1.6.2.min.js\"></script>\n<script type=\"text/javascript\" src=\"../js/jquery.easydrag.handler.beta2.js\"></script>\n<script type=\"text/javascript\">\n\n\t$(document).ready(\n\t\tfunction() { \n\t\t\t\n\t\t\tvar array_1 = [9,8,7,6,5,4,3,2,1];\n\t\t\talert(array_1);\n\t\t\t/*bubbleSort*/\n\t\t    alert(bubbleSort(array_1));\n\n\t\t}\n\t); \n\n\n\n</script>\n\n<style type=\"text/css\">\n\t\n\t* { padding:0; margin:0; }\n\n\tbody {\n\t\tpadding: 100px;\n\t\tfont-size: 15px; \n\t}\n\n\t\n\n\n</style>\n\n\n<script type=\"text/javascript\">\n\tfunction bubbleSort(array){\n\t\tvar i = 0;len = array.length;\n\t\tvar j, d;\n\t\tfor (; i < len; i++) {\n\t\t    for (j = 0; j < len; j++) {\n\t\t       \tif (array[i] < array[j]) {\n\t\t            d = array[j];\n\t\t            array[j] = array[i];\n\t\t            array[i] = d;\n\t\t        }\n\t\t    }\n\t\t}\n\t\treturn array;\n\t}\n\n\n</script>\n\n\n</head>\n\n\n\n\n<body>\n\t&Atilde;&deg;&Aring;&Yacute;&Aring;&Aring;&ETH;&ograve;\n</body>\n</html>\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["算法"]},{"title":"JavaScript排序算法——堆排序","url":"/JavaScript排序算法——堆排序.html","content":"```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>堆排序</title>\n<!--<link rel=\"stylesheet\" type=\"text/css\" href=\"../style/fdt.css\" />-->\n<script type=\"text/javascript\" src=\"../js/jquery-1.6.2.min.js\"></script>\n<script type=\"text/javascript\" src=\"../js/jquery.easydrag.handler.beta2.js\"></script>\n<script type=\"text/javascript\">\n\n    $(document).ready(\n        function() { \n            \n            // var array_1 = [9,8,7,6,5,4,3,2,1];\n            var array_1 = [4,5,3,1,2,0];\n            alert(array_1);\n            /*heapSort*/\n            alert(heapSort(array_1));\n\n        }\n    ); \n\n\n\n</script>\n\n<style type=\"text/css\">\n    \n    * { padding:0; margin:0; }\n\n    body {\n        padding: 100px;\n        font-size: 15px; \n    }\n\n    \n\n\n</style>\n\n\n<script type=\"text/javascript\">\n    function heapSort(elements){\n        \n\n\n        //调整函数\n        function headAdjust(elements, pos, len){\n          //将当前节点值进行保存\n          var swap = elements[pos];\n\n          //定位到当前节点的左边的子节点\n          var child = pos * 2 + 1;\n\n          //递归，直至没有子节点为止\n          while(child < len){\n            //如果当前节点有右边的子节点，并且右子节点较大的场合，采用右子节点和当前节点进行比较\n            if(child + 1 < len &amp;&amp; elements[child] < elements[child + 1]){\n              child += 1;\n            }\n\n            //比较当前节点和最大的子节点，小于则进行值交换，交换后将当前节点定位于子节点上\n            if(elements[pos] < elements[child]){\n              elements[pos] = elements[child];\n              pos = child;\n              child = pos * 2 + 1;\n            }\n            else{\n              break;\n            }\n\n            elements[pos] = swap;\n          }\n        }\n\n        //构建堆\n        function buildHeap(elements){\n          //从最后一个拥有子节点的节点开始，将该节点连同其子节点进行比较，\n          //将最大的数交换与该节点,交换后，再依次向前节点进行相同交换处理，\n          //直至构建出大顶堆（升序为大顶，降序为小顶）\n          for(var i=elements.length/2; i>=0; i--){\n            headAdjust(elements, i, elements.length);\n          }\n        }\n\n        function sort(elements){\n          //构建堆\n          buildHeap(elements);\n\n          //从数列的尾部开始进行调整\n          for(var i=elements.length-1; i>0; i--){\n            //堆顶永远是最大元素，故，将堆顶和尾部元素交换，将\n            //最大元素保存于尾部，并且不参与后面的调整\n            alert(elements);\n            var swap = elements[i];\n            elements[i] = elements[0];\n            elements[0] = swap;\n            alert(elements);\n            //进行调整，将最大的元素调整至堆顶\n            headAdjust(elements, 0, i);\n            alert(elements);\n          }\n        }\n\n        sort(elements);\n        return elements;\n        \n\n    }\n\n\n</script>\n\n\n</head>\n\n\n\n\n<body>\n    堆排序\n</body>\n</html>\n```\n\n<!--more-->\n&nbsp;\n","tags":["算法"]},{"title":"JavaScript排序算法——希尔排序","url":"/JavaScript排序算法——希尔排序.html","content":"```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>希尔排序</title>\n<!--<link rel=\"stylesheet\" type=\"text/css\" href=\"../style/fdt.css\" />-->\n<script type=\"text/javascript\" src=\"../js/jquery-1.6.2.min.js\"></script>\n<script type=\"text/javascript\" src=\"../js/jquery.easydrag.handler.beta2.js\"></script>\n<script type=\"text/javascript\">\n\n\t$(document).ready(\n\t\tfunction() { \n\t\t\t\n\t\t\tvar array_1 = [9,8,7,6,5,4,3,2,1];\n\t\t\talert(array_1);\n\t\t\t/*shellSort*/\n\t\t    alert(shellSort(array_1));\n\n\t\t}\n\t); \n\n\n\n</script>\n\n<style type=\"text/css\">\n\t\n\t* { padding:0; margin:0; }\n\n\tbody {\n\t\tpadding: 100px;\n\t\tfont-size: 15px; \n\t}\n\n\t\n\n\n</style>\n\n\n<script type=\"text/javascript\">\n\tfunction shellSort(array){\n\t\tvar stepArr = [1031612713, 217378076, 45806244, 9651787, 2034035, 428481, 90358, 19001, 4025, 1750, 836, 701, 301, 132, 57, 23, 10, 4, 1]; // reverse() 在维基上看到这个最优的步长 较小数组\n\t\tvar i = 0;\n\t\tvar stepArrLength = stepArr.length;\n\t\tvar len = array.length;\n\t\tvar len2 =  parseInt(len/2);\n\t\t\n\t\tfor(;i < stepArrLength; i++){\n\t\t\tif(stepArr[i] > len2){\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tstepSort(stepArr[i]);\n\t\t}\n\t\t// 排序一个步长\n\t\tfunction stepSort(step){\n\t\t\t\n\t\t\t//console.log(step) 使用的步长统计\n\t\t\t\n\t\t\tvar i = 0, j = 0, f, tem, key;\n\t\t\t\n\t\t\t\n\t\t\tfor(;i < step; i++){// 依次循环列\n\t\t\t\tfor(j=1; step * j + i < len; j++){//依次循环每列的每行\n\t\t\t\t\ttem = f = step * j + i;\n\t\t\t\t\tkey = array[f];\n\t\t\t\t\twhile((tem-=step) >= 0){// 依次向上查找\t\t<-\n\t\t\t\t\t\t\t\t\t\t\t//\t\t\t\t\t<----\n\t\t\t\t\t\t\t\t\t\t\t//\t\t\t\t\t<-------\n\n\t\t\t\t\t\tif(array[tem] > key){\n\t\t\t\t\t\t\tarray[tem+step] = array[tem];\n\t\t\t\t\t\t}else{\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tarray[tem + step ] = key;\n\t\t\t\t}\n\t\t\t}\n\t\t\t\n\t}\n\t\n\treturn array;\n\t\n}\n\n\n</script>\n\n\n</head>\n\n\n\n\n<body>\n\t希尔排序\n</body>\n</html>\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["算法"]},{"title":"JavaScript排序算法——归并排序","url":"/JavaScript排序算法——归并排序.html","content":"```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>归并排序</title>\n<!--<link rel=\"stylesheet\" type=\"text/css\" href=\"../style/fdt.css\" />-->\n<script type=\"text/javascript\" src=\"../js/jquery-1.6.2.min.js\"></script>\n<script type=\"text/javascript\" src=\"../js/jquery.easydrag.handler.beta2.js\"></script>\n<script type=\"text/javascript\">\n\n\t$(document).ready(\n\t\tfunction() { \n\t\t\t\n\t\t\tvar array_1 = [9,8,7,6,5,4,3,2,1];\n\t\t\talert(array_1);\n\t\t\t/*mergeSort*/\n\t\t\t//迭代实现\n\t\t    alert(mergeSort(array_1));\n\n\t\t}\n\t); \n\n\n\n</script>\n\n<style type=\"text/css\">\n\t\n\t* { padding:0; margin:0; }\n\n\tbody {\n\t\tpadding: 100px;\n\t\tfont-size: 15px; \n\t}\n\n\t\n\n\n</style>\n\n\n<script type=\"text/javascript\">\n\tfunction merge(left,right){    \n\t\tvar result=[];    \n\t\twhile(left.length>0 &amp;&amp; right.length>0){        \n\t\t\tif(left[0]<right[0]){            \n\t\t\t\tresult.push(left.shift());        \n\t\t\t}else{            \n\t\t\t\tresult.push(right.shift());        \n\t\t\t}    \n\t\t}   \n\t\talert(\" left=\"+left+\" right\"+right+\" result=\"+result); \n\t\talert(result.concat(left).concat(right));\n\t\treturn result.concat(left).concat(right);\n\t}\n\n\tfunction mergeSort(items){\n\t\tif(items.length == 1){        \n\t\t\treturn items;    \n\t\t}    \n\t\tvar middle=Math.floor(items.length/2),    \n\t\tleft=items.slice(0,middle),    \n\t\tright=items.slice(middle);\n\t\talert(\" middle=\"+middle+\" items.length=\"+items.length+\" left=\"+left+\" right\"+right+\" items\"+items); \n\t\treturn merge(mergeSort(left),mergeSort(right));\n\t}\n\n\n\n</script>\n\n\n</head>\n\n\n\n\n<body>\n\t归并排序\n</body>\n</html>\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["算法"]},{"title":"JavaScript排序算法——插入排序","url":"/JavaScript排序算法——插入排序.html","content":"```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>插入</title>\n<!--<link rel=\"stylesheet\" type=\"text/css\" href=\"../style/fdt.css\" />-->\n<script type=\"text/javascript\" src=\"../js/jquery-1.6.2.min.js\"></script>\n<script type=\"text/javascript\" src=\"../js/jquery.easydrag.handler.beta2.js\"></script>\n<script type=\"text/javascript\">\n\n    $(document).ready(\n        function() { \n            \n            var array_1 = [9,8,7,6,5,4,3,2,1];\n            alert(array_1);\n            /*insertSort*/\n            alert(insertSort(array_1));\n        }\n    ); \n\n</script>\n    \n\n<script type=\"text/javascript\">\n    function insertSort(array){\n        var i = 1,j, step, key, len = array.length;\n\n        for (; i < len; i++) {\n\n            step = j = i;\n            key = array[j];\n\n            while (--j > -1) {\n                if (array[j] > key) {\n                    array[j + 1] = array[j];\n                } \n                else {\n                    break;\n                }\n            }\n            array[j + 1] = key;\n        }\n        return array;\n    }\n\n</script>\n\n\n\n<style type=\"text/css\">\n    \n    * { padding:0; margin:0; }\n\n    body {\n        padding: 100px;\n        font-size: 15px; \n    }\n\n    \n\n\n</style>\n\n</head>\n\n\n\n\n<body>\n    快速排序\n</body>\n</html>\n```\n\n<!--more-->\n&nbsp;\n","tags":["算法"]},{"title":"JavaScript排序算法——快速排序","url":"/JavaScript排序算法——快速排序.html","content":"```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>快速排序</title>\n<!--<link rel=\"stylesheet\" type=\"text/css\" href=\"../style/fdt.css\" />-->\n<script type=\"text/javascript\" src=\"../js/jquery-1.6.2.min.js\"></script>\n<script type=\"text/javascript\" src=\"../js/jquery.easydrag.handler.beta2.js\"></script>\n<script type=\"text/javascript\">\n\n\t$(document).ready(\n\t\tfunction() { \n\t\t\t\n\t\t\tvar array_1 = [4,5,3,1,2];\n\t\t\talert(array_1);\n\t\t\t/*quickSort*/\n\t\t    alert(quickSort(array_1));\n\n\t\t}\n\t); \n\n\n\n</script>\n\n<style type=\"text/css\">\n\t\n\t* { padding:0; margin:0; }\n\n\tbody {\n\t\tpadding: 100px;\n\t\tfont-size: 15px; \n\t}\n\n\t\n\n\n</style>\n\n\n<script type=\"text/javascript\">\n\tfunction quickSort(array){\n\t\t//var array = [8,4,6,2,7,9,3,5,74,5];\n\t\t//var array = [0,1,2,44,4,324,5,65,6,6,34,4,5,6,2,43,5,6,62,43,5,1,4,51,56,76,7,7,2,1,45,4,6,7];\n\t\tvar i = 0;\n\t\tvar j = array.length - 1;\n\t\tvar Sort = function(i, j){\n\t\t\t\n\t\t\t// 结束条件\n\t\t\tif(i == j ){ return };\n\t\t\t\n\t\t\tvar key = array[i];\n\t\t\tvar stepi = i; // 记录开始位置\n\t\t\tvar stepj = j; // 记录结束位置\n\t\t\t\n\t\t\twhile(j > i){\n\t\t\t\talert(array);\n\t\t\t\t// j <<-------------- 向前查找\n\t\t\t\tif(array[j] >= key){\n\t\t\t\t\tj--;\n\t\t\t\t}else{\n\t\t\t\t\tarray[i] = array[j]\n\t\t\t\t\t//i++ ------------>>向后查找\n\t\t\t\t\twhile(j > ++i){\n\t\t\t\t\t\tif(array[i] > key){\n\t\t\t\t\t\t\tarray[j] = array[i];\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\talert(array);\n\t\t\talert(\"i=\"+i+\",j=\"+j+\",stepi=\"+stepi+\",stepj=\"+stepj);\n\t\t\t\n\t\t\t// 如果第一个取出的 key 是最小的数\n\t\t\tif(stepi == i){\n\t\t\t\tSort(++i, stepj);\n\t\t\t\treturn ;\n\t\t\t}\n\t\t\t\n\t\t\t// 最后一个空位留给 key\n\t\t\tarray[i] = key;\n\t\t\t\n\t\t\t// 递归\n\t\t\tSort(stepi, i);\n\t\t\tSort(j, stepj);\n\t\t}\n\t\t\n\t\tSort(i, j);\n\t\t\n\t\treturn array;\n\t}\n\n\n</script>\n\n\n</head>\n\n\n\n\n<body>\n\t快速排序\n</body>\n</html>\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["算法"]},{"title":"JavaScript排序算法——选择排序","url":"/JavaScript排序算法——选择排序.html","content":"```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>选择排序</title>\n<!--<link rel=\"stylesheet\" type=\"text/css\" href=\"../style/fdt.css\" />-->\n<script type=\"text/javascript\" src=\"../js/jquery-1.6.2.min.js\"></script>\n<script type=\"text/javascript\" src=\"../js/jquery.easydrag.handler.beta2.js\"></script>\n<script type=\"text/javascript\">\n\n\t$(document).ready(\n\t\tfunction() { \n\t\t\t\n\t\t\tvar array_1 = [9,8,7,6,5,4,3,2,1];\n\t\t\talert(array_1);\n\t\t\t/*selectionSort*/\n\t\t    alert(selectionSort(array_1));\n\n\t\t}\n\t); \n\n\n\n</script>\n\n<style type=\"text/css\">\n\t\n\t* { padding:0; margin:0; }\n\n\tbody {\n\t\tpadding: 100px;\n\t\tfont-size: 15px; \n\t}\n\n\t\n\n\n</style>\n\n\n<script type=\"text/javascript\">\n\tfunction selectionSort(array){\n\t\tvar len = array.length;\n\t\tvar index = 0;\n\t\tvar k;\n\t\tvar item;\n\t\tvar c;\n\t\tfor(var i=0; i<len; i++){\n\t\t\t\n\t\t\t//最小的数\n\t\t\titem = array[i];\n\t\t\t//最小的数的位置编号\n\t\t\tindex = i;\n\t\t\t//寻找最小的数位置\n\t\t\tfor(j=i+1; j<len;j++){\n\t\t\t\tif(array[j] < item){\n\t\t\t\t\tindex = j;\n\t\t\t\t\titem = array[j];\n\t\t\t\t}\n\t\t\t}\n\t\t\tif(index != i){\n\t\t\t\tc = array[i];\n\t\t\t\tarray[i] = array[index];\n\t\t\t\tarray[index] = c;\n\t\t\t}\n\t\t}\n\t\treturn array;\n\t}\n\n\n</script>\n\n\n</head>\n\n\n\n\n<body>\n\t选择排序\n</body>\n</html>\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["算法"]},{"title":"Ubuntu16.04安装opentsdb2.4.1","url":"/Ubuntu16.04安装opentsdb2.4.1.html","content":"官方文档\n\n```\nhttps://github.com/OpenTSDB/opentsdb/wiki/Installation-on-Cloudera-Virtual-Machine\n\n```\n\nopentsdb的安装依赖Hbase，本文中使用的是CDH5.16.2中的Hbase 1.2.0+cdh5.16.2\n\n1.git clone\n\n```\ngit clone https://github.com/OpenTSDB/opentsdb.git\n\n```\n\n2.安装\n\n```\ncd opentsdb\n./build.sh\nenv COMPRESSION=none HBASE_HOME=/opt/cloudera/parcels/CDH/lib/hbase ./src/create_table.sh\n\n```\n\n如果Hbase启用了kerberos，则需要先认证\n\n```\nkinit -kt ~/下载/hbase.keytab hbase/master@HADOOP.COM\n\n```\n\n上面的脚本会在hbase中初始化4张table\n\n```\nhbase(main):004:0> list\nTABLE\ntsdb\ntsdb-meta\ntsdb-tree\ntsdb-uid\n4 row(s) in 0.0370 seconds\n\n=> [\"tsdb\", \"tsdb-meta\", \"tsdb-tree\", \"tsdb-uid\"]\n\n```\n\n如果遇到\n\n```\ncreate 'tsdb',\n  {NAME => 't', VERSIONS => 1, COMPRESSION => 'NONE', BLOOMFILTER => 'ROW', DATA_BLOCK_ENCODING => 'DIFF', TTL => 'FOREVER'}\n\nERROR: For input string: \"FOREVER\"\n\n```\n\n请修改 create_table.sh 脚本\n\n```\nlintong@master:~/software/opentsdb/src$ vim create_table.sh\n\n```\n\n将 TSDB_TTL=${TSDB_TTL-'FOREVER'} 修改成 TSDB_TTL=${TSDB_TTL-'2147483647'}\n\n如果你的hbase开启了kerberos，则需要\n\n```\nmv ./src/opentsdb.conf ./build/\n\n```\n\n然后编译opentsdb.conf配置文件，添加KERBEROS相关配置\n\n```\n --------- NETWORK ----------\n# The TCP port TSD should use for communications\n# *** REQUIRED ***\ntsd.network.port = 10015\n\n# ----------- HTTP -----------\n# The location of static files for the HTTP GUI interface.\n# *** REQUIRED ***\ntsd.http.staticroot = ./staticroot\n# Where TSD should write it's cache files to\n# *** REQUIRED ***\ntsd.http.cachedir = /tmp/tsd\n\n# --------- CORE ----------\n# Whether or not to automatically create UIDs for new metric types, default\n# is False\ntsd.core.auto_create_metrics = true\n# Name of the HBase table where data points are stored, default is \"tsdb\"\ntsd.storage.hbase.data_table = tsdb\n# Path under which the znode for the -ROOT- region is located, default is \"/hbase\"\ntsd.storage.hbase.zk_basedir = /hbase\n# A comma separated list of Zookeeper hosts to connect to, with or without\n# port specifiers, default is \"localhost\"\ntsd.storage.hbase.zk_quorum = master\n\n#-------- KERBEROS -----\nhbase.security.auth.enable=true\nhbase.security.authentication=kerberos\nhbase.sasl.clientconfig=Client\nhbase.kerberos.regionserver.principal=hbase/_HOST@HADOOP.COM\n\n```\n\n同时添加jass文件\n\n```\nlintong@master:~/software/opentsdb/build/jaas$ ls\njaas.conf\nlintong@master:~/software/opentsdb/build/jaas$ cat jaas.conf\nClient {\n    com.sun.security.auth.module.Krb5LoginModule required\n    useKeyTab=true\n    useTicketCache=false\n    keyTab=\"/home/lintong/下载/hbase.keytab\"\n    principal=\"hbase/master@HADOOP.COM\";\n};\n\n```\n\n参考：[OpenTSDB搭建过程（CDH环境，kerberos认证）](https://www.jianshu.com/p/048a3d8b71dd)\n\n[HBase 1.1.2.2.6.0.3-8 TTL设置过之后，无法显示重置为FOREVER](https://blog.csdn.net/qq_35440040/article/details/105553004)\n\n3.创建缓存目录\n\n```\nmkdir /tmp/tsd\n\n```\n\n4.启动opentsdb\n\n```\nlintong@master:~/software/opentsdb/build$ JVMARGS=\"-Djava.security.auth.login.config=/home/lintong/software/opentsdb/build/jaas/jaas.conf -Dzookeeper.sasl.client=false\" ./tsdb tsd\n\n```\n\n也可以添加到./tsdb启动脚本中\n\n<img src=\"/images/517519-20220122150046357-1962933544.png\" width=\"800\" height=\"116\" loading=\"lazy\" />\n\n5.访问，我的是10015端口\n\n<img src=\"/images/517519-20211230001803231-1457375643.png\" width=\"800\" height=\"351\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n","tags":["OpenTSDB"]},{"title":"jQuery学习笔记——弹出对话框","url":"/jQuery学习笔记——弹出对话框.html","content":"**引用jQuery库文件的<script>标签，必须放在引用自定义脚本文件的<script>标签之前**。否则，在编写的代码中将不能引用到jQuery框架\n\n```\n<script type=\"text/javascript\" src=\"js/jquery-1.6.2.min.js\"></script>\n<script type=\"text/javascript\">\n\n        $(document).ready(function(){\n        　　alert(\"这是测试\");    \n        });\n```\n\n**其中的引用库文件的标签要放在后面自定义的标签的前面，不然不能实现弹出对话框的效果**\n\n<!--more-->\n&nbsp;\n\n**等待页面加载完毕**\n\n```\n$(document).ready(function(){\n\n        });\n```\n\n&nbsp;\n\n**点击的时候执行的方法，其中的.hdw是class的名字**\n\n```\n$(document).ready(function(){\n    \n    $('a').click(function(){      \n        alert(\"测试\");\n        });\n        \n        \n    $('input:text').click(function(){\n        alert(\"这是文本框弹出的内容\");\n        });    \n        \n    $('input:checkbox').click(function(){\n        alert(\"这是BOX的弹出内容\");\n        });    \n        \n    $('.hdw').click(function(){\n        alert(\"这是DIV弹出的内容\");\n        });    \n\n    });\n```\n\n&nbsp;\n","tags":["前端"]},{"title":"系统设计——秒杀系统","url":"/系统设计——秒杀系统.html","tags":["系统设计"]},{"title":"go学习笔记——gorm","url":"/go学习笔记——gorm.html","content":"gen是gorm官方推出的一个GORM代码生成工具\n\n官方文档：[https://gorm.io/zh_CN/gen/](https://gorm.io/zh_CN/gen/)\n\n## 1.使用gen框架生成model和dao\n\n安装gorm gen\n\n```\ngo get -u gorm.io/gen\n\n```\n\n假设有如下用户表\n\n```\nCREATE TABLE user\n(\n    `id`     bigint unsigned NOT NULL AUTO_INCREMENT COMMENT '主键',\n    `username`  varchar(128) NOT NULL COMMENT '用户名',\n    `email` varchar(128) NOT NULL COMMENT '邮箱',\n    PRIMARY KEY (`id`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='用户表';\n\n```\n\n在cmd目录下创建gen/generate.go文件\n\n```\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"gorm.io/driver/mysql\"\n\t\"gorm.io/gen\"\n\t\"gorm.io/gorm\"\n)\n\nconst MySQLDSN = \"root:123456@tcp(127.0.0.1:55000)/default?charset=utf8mb4&amp;parseTime=True\"\n\nfunc connectDB(dsn string) *gorm.DB {\n\tdb, err := gorm.Open(mysql.Open(dsn))\n\tif err != nil {\n\t\tpanic(fmt.Errorf(\"connect mysql fail: %w\", err))\n\t}\n\treturn db\n}\n\nfunc main() {\n\t// 指定生成代码的具体相对目录(相对当前文件)，默认为：./query\n\t// 默认生成需要使用WithContext之后才可以查询的代码，但可以通过设置gen.WithoutContext禁用该模式\n\tg := gen.NewGenerator(gen.Config{\n\t\t// 默认会在 OutPath 目录生成CRUD代码，并且同目录下生成 model 包\n\t\t// 所以OutPath最终package不能设置为model，在有数据库表同步的情况下会产生冲突\n\t\t// 若一定要使用可以通过ModelPkgPath单独指定model package的名称\n\t\tOutPath:      \"./internal/query\",\n\t\tModelPkgPath: \"./internal/model\",\n\n\t\t// gen.WithoutContext：禁用WithContext模式\n\t\t// gen.WithDefaultQuery：生成一个全局Query对象Q\n\t\t// gen.WithQueryInterface：生成Query接口\n\t\tMode: gen.WithoutContext | gen.WithDefaultQuery | gen.WithQueryInterface,\n\t})\n\n\t// 通常复用项目中已有的SQL连接配置db(*gorm.DB)\n\t// 非必需，但如果需要复用连接时的gorm.Config或需要连接数据库同步表信息则必须设置\n\tg.UseDB(connectDB(MySQLDSN))\n\n\t// 从连接的数据库为所有表生成Model结构体和CRUD代码\n\tg.ApplyBasic(g.GenerateAllTable()...)\n\t// 也可以手动指定需要生成代码的数据表\n\tg.ApplyBasic(g.GenerateModel(\"user\"))\n\n\t// 执行并生成代码\n\tg.Execute()\n}\n\n```\n\n运行generate.go，将会生成model和dao文件\n\n<img src=\"/images/517519-20240201000056892-326773222.png\" width=\"300\" height=\"234\" loading=\"lazy\" />\n\n需要注意mysql的tinyint(1)生成的时候会映射成bool，这可能会有些问题\n\n## 2.gorm框架CRUD\n\n### 1.insert\n\n```\nfunc init() {\n\tSetDefault(database.DB)\n}\n\nfunc Test_userDo_Create(t *testing.T) {\n\tuser := model.User{\n\t\tUsername: \"test\",\n\t\tEmail:    \"test@test\",\n\t}\n\terr := User.Create(&amp;user)\n\tif err != nil {\n\t\tfmt.Println(\"create user fail\")\n\t}\n}\n\n```\n\n### 2.delete\n\n```\nfunc Test_userDo_Delete(t *testing.T) {\n\t/*\n\t\tuser := model.User{\n\t\t\tID: 1,\n\t\t}\n\t\tresult, err := User.Delete(&amp;user)\n\t*/\n\tresult, err := User.Where(User.ID.Eq(2)).Delete()\n\tif err != nil {\n\t\tfmt.Println(\"delete user fail\")\n\t}\n\tfmt.Println(result)\n}\n\n```\n\n### 3.update\n\n```\nfunc Test_userDo_Update(t *testing.T) {\n\t/*\n\t\tuser := model.User{\n\t\t\tID:       2,\n\t\t\tUsername: \"test2\",\n\t\t\tEmail:    \"test2@test\",\n\t\t}\n\t\tresult, err := User.Updates(&amp;user)\n\t*/\n\tresult, err := User.\n\t\tWhere(User.ID.Eq(2)).\n\t\tUpdate(User.Username, \"test22\")\n\n\tif err != nil {\n\t\tfmt.Println(\"update user fail\")\n\t}\n\tfmt.Println(result.RowsAffected)\n}\n\n```\n\n### 4.select\n\n```\nfunc Test_userDo_Scan(t *testing.T) {\n\tuser := model.User{}\n\terr := User.Where(User.ID.Eq(2)).Scan(&amp;user)\n\tif err != nil {\n\t\tfmt.Println(\"scan user fail\")\n\t}\n\tfmt.Println(user)\n}\n\n```\n\n参考：[GORM Gen使用指南](https://www.liwenzhou.com/posts/Go/gen/)\n\n### 5.Gorm处理可变结果集\n\n在上面例子中使用scan获得查询结果的时候，字段的个数是固定的，如果当字段的个数是不定长度的时候，可以使用gorm来处理可变结果集\n\n可以将**[]interface{}**或者**<strong>[]orm.Params**</strong>来保存查询结果，参考：[Golang开发实践:把数据库数据保存到map[string]interface{}中](https://www.cnblogs.com/liujie-php/p/10434545.html) 或者[【巧妙】GO + MySQL的通用查询方法](https://blog.csdn.net/rockage/article/details/103776251)\n\n```\nfunc (repo *userRepo) ListUsers(ctx context.Context) (*model.User, error) {\n\tconnect, err := repo.data.db.DB()\n\tif err != nil {\n\t\trepo.log.Error(fmt.Sprintf(\"connect fail, error: %v\", err))\n\t\treturn nil, err\n\t}\n\trows, err := connect.Query(\"select * from `user`\")\n\tif err != nil {\n\t\trepo.log.Error(fmt.Sprintf(\"query fail, error: %v\", err))\n\t\treturn nil, err\n\t}\n\tdefer rows.Close()\n\tcols, err := rows.Columns()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvalues := make([]interface{}, 0)\n\tfor i := 0; i < len(cols); i++ {\n\t\tvar value interface{}\n\t\tvalues = append(values, &amp;value)\n\t}\n\tfor rows.Next() {\n\t\terr = rows.Scan(values...)\n\t\tif err != nil {\n\t\t\trepo.log.Error(fmt.Sprintf(\"scan fail, error: %v\", err))\n\t\t\treturn nil, err\n\t\t}\n\t\tfor k, v := range values {\n\t\t\tkey := cols[k]\n\t\t\tvar rawValue = *(v.(*interface{}))\n\t\t\tswitch v := rawValue.(type) {\n\t\t\tcase string:\n\t\t\t\tfmt.Print(\"key=>\" + key + \":string \")\n\t\t\t\tfmt.Print(v)\n\t\t\tcase int32:\n\t\t\t\tfmt.Print(\"key=>\" + key + \":int32 \")\n\t\t\t\tfmt.Println(v)\n\t\t\tcase []uint8:\n\t\t\t\tfmt.Print(\"key=>\" + key + \",type=>uint8[],value=>\")\n\t\t\t\tfmt.Print(string(v))\n\t\t\t\tfmt.Print(\" \")\n\t\t\tdefault:\n\t\t\t\tfmt.Print(v)\n\t\t\t}\n\t\t}\n\t\tfmt.Println()\n\t}\n\treturn nil, nil\n}\n\n```\n\n输出结果\n\n```\nkey=>id,type=>uint8[],value=>5 key=>username,type=>uint8[],value=>test key=>email,type=>uint8[],value=>test@test \nkey=>id,type=>uint8[],value=>6 key=>username,type=>uint8[],value=>test key=>email,type=>uint8[],value=>test@test \nkey=>id,type=>uint8[],value=>7 key=>username,type=>uint8[],value=>test key=>email,type=>uint8[],value=>test@test \nkey=>id,type=>uint8[],value=>8 key=>username,type=>uint8[],value=>test key=>email,type=>uint8[],value=>test@test \nkey=>id,type=>uint8[],value=>9 key=>username,type=>uint8[],value=>test key=>email,type=>uint8[],value=>test@test \n\n```\n\n### 6.分页\n\n可以使用gorm的scopes来实现分页（先count再分页），参考：[Gorm Scopes复用你的逻辑](https://tehub.com/a/c4lp7ydJFd) 和 [学习gorm系列十之：使用gorm.Scopes函数复用你的查询逻辑](https://juejin.cn/post/7313979390840258597)\n\n```\nfunc Paginate(pageNum int, pageSize int) func(db *gorm.DB) *gorm.DB {\n\treturn func(db *gorm.DB) *gorm.DB {\n\t\tif pageNum <= 0 {\n\t\t\tpageNum = 1\n\t\t}\n\t\tif pageSize > 100 {\n\t\t\tpageSize = 100\n\t\t} else if pageSize <= 0 {\n\t\t\tpageSize = 10\n\t\t}\n\t\toffset := (pageNum - 1) * pageSize\n\t\treturn db.Offset(offset).Limit(pageSize)\n\t}\n}\n\nfunc (repo *userRepo) ListUser(ctx context.Context, pageNum int, pageSize int) ([]*model.User, error) {\n\tvar result []*model.User\n\tvar total int64\n\terr := repo.data.db.Model(&amp;model.User{}).Count(&amp;total).Error\n\tif err != nil {\n\t\trepo.log.Error(fmt.Sprintf(\"list user fail, error: %v\", err))\n\t}\n\terr = repo.data.db.Scopes(Paginate(pageNum, pageSize)).Find(&amp;result).Error\n\tif err != nil {\n\t\trepo.log.Error(fmt.Sprintf(\"update user fail, error: %v\", err))\n\t\treturn nil, err\n\t}\n\treturn result, nil\n}\n\n```\n\n测试分页函数\n\n```\nfunc Test_userRepo_ListUsers(t *testing.T) {\n\tdata, _, err := NewData(zap_logger, db, rdb)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tuserRepo := userRepo{data: data, log: zap_logger}\n\tresult, err := userRepo.ListUser(context.Background(), 1, 5)\n\tif err != nil {\n\t\tfmt.Println(err)\n\t}\n\tfor _, user := range result {\n\t\tfmt.Println(user)\n\t}\n}\n\n```\n\n输出\n\n```\n&amp;{5 test test@test}\n&amp;{6 test test@test}\n&amp;{7 test test@test}\n&amp;{8 test test@test}\n&amp;{9 test test@test}\n\n```\n\nMybatis Page Helper分页插件原理：[Mybatis分页插件PageHelper的配置和使用方法](https://www.cnblogs.com/kangoroo/p/7998433.html)\n\n## 3.gorm框架自定义SQL\n\ngorm还支持编写sql模板，来添加自定义sql的函数逻辑，其中使用的语法是text template，可以参考：<!--more-->\n&nbsp;[go学习笔记&mdash;&mdash;text template](https://www.cnblogs.com/tonglin0325/p/5559650.html)\n\n```\npackage model\n\nimport \"gorm.io/gen\"\n\ntype Filter interface {\n\t// FilterWithColumn SELECT * FROM @@table WHERE @@column=@value\n\tFilterWithColumn(column string, value string) ([]*gen.T, error)\n\n\t// FilterWithObject\n\t//\n\t// SELECT * FROM @@table where id > 0\n\t// {{if user != nil}}\n\t//   {{if user.ID > 0}}\n\t//     AND id = @user.ID\n\t//   {{else if user.Username != \"\"}}\n\t//     AND username=@user.Username\n\t//   {{else if user.Email != \"\"}}\n\t//     AND email=@user.Email\n\t//   {{end}}\n\t// {{end}}\n\tFilterWithObject(user *gen.T) ([]*gen.T, error)\n}\n\n```\n\n然后在generate.go文件中添加\n\n```\ng.ApplyInterface(func(model.Filter) {}, g.GenerateModel(\"user\"))\n\n```\n\n使用自定义SQL生成的函数\n\n```\nfunc Test_userDo_FilterWithColumn(t *testing.T) {\n\tresult, err := User.FilterWithColumn(\"username\", \"test\")\n\tif err != nil {\n\t\tfmt.Println(\"filter user fail\")\n\t}\n\tfor _, each := range result {\n\t\tfmt.Println(each)\n\t}\n}\n\nfunc Test_userDo_FilterWithObject(t *testing.T) {\n\tuser := model.User{\n\t\tID:       3,\n\t\tUsername: \"test2\",\n\t\tEmail:    \"test2@test\",\n\t}\n\tresult, err := User.FilterWithObject(&amp;user)\n\tif err != nil {\n\t\tfmt.Println(\"filter user fail\")\n\t}\n\tfor _, each := range result {\n\t\tfmt.Println(each)\n\t}\n}\n\n```\n\n其中FilterWithObject函数的User对象的ID不为空，且大于的时候，执行的SQL如下\n\n```\nSELECT * FROM user where id > 0 AND id = 3 \n\n```\n\nID为空的时候，执行的SQL如下\n\n```\nSELECT * FROM user where id > 0 AND username='test2' \n\n```\n\nID和Username都为空的时候，执行的SQL如下\n\n```\nSELECT * FROM user where id > 0 AND email='test2@test' \n\n```\n\n参考：[GORM Gen使用指南](https://www.liwenzhou.com/posts/Go/gen/)\n\n## 4.gorm框架关联查询\n\n### 1.has one\n\n使用外键关联查询，得到一个struct结果，比如查询一个用户，一个用户只有一张信用卡，同时查询用户信息和用户的这张信用卡struct\n\n参考：[https://gorm.io/zh_CN/docs/has_one.html](https://gorm.io/zh_CN/docs/has_one.html)\n\n### 2.has many\n\n使用外键关联查询，得到一个list of struct结果，比如查询一个用户，一个用户拥有多张信用卡，同时查询用户信息和用户所有的信用卡list\n\n参考：[https://gorm.io/zh_CN/docs/has_many.html](https://gorm.io/zh_CN/docs/has_many.html)\n\n## 5.gorm事务\n\n### 1.data层承载事务\n\n参考：[https://gorm.io/zh_CN/docs/transactions.html](https://gorm.io/zh_CN/docs/transactions.html)\n\n### 2.biz层承载事务\n\n参考：[在 Go-Kratos 框架中优雅的使用 GORM 完成事务](https://www.cnblogs.com/zhanchenjin/p/17855944.html)\n\n以及 [https://github.com/go-kratos/examples/blob/main/transaction/gorm/internal/biz/biz.go](https://github.com/go-kratos/examples/blob/main/transaction/gorm/internal/biz/biz.go)\n\n&nbsp;\n","tags":["golang"]},{"title":"MySQL学习笔记——undo log和redo log","url":"/MySQL学习笔记——undo log和redo log.html","content":"MySQL的架构可以分成连接层，Server层和存储引擎层。\n\nundo log<!--more-->\n&nbsp;和&nbsp;redo log 是 MySQL InnoDB **存储引擎**管理的数据日志类型，它们主要用于支持事务的 ACID 特性，即原子性、一致性、隔离性和持久性。\n\n## 1.Undo Log（回滚日志）\n\nundo log&nbsp;用于支持事务的**原子性（Atomicity）**和**隔离性（Isolation）**。它在事务发生更改之前记录数据的原始状态，以便在事务回滚时能够将数据恢复到其原始状态。\n\n**undo log工作原理**：\n\n- 当一个事务对数据库中的数据进行修改（如 `INSERT`、`UPDATE` 或 `DELETE` 操作）时，InnoDB 会先将原始数据的副本写入 `undo log`。\n- 如果事务需要回滚（无论是由于用户请求还是由于系统错误），InnoDB 使用 `undo log` 将更改撤销，使数据恢复到事务开始之前的状态。\n- `undo log` 也用于实现 **一致性非锁定读（Consistent Non-Locking Reads）**，**一致性非锁定读** 是 MVCC 的核心特性之一。这意味着在事务的隔离级别为 **REPEATABLE READ** 或 **READ COMMITTED** 时，可以提供一致的读取视图。它允许事务在读取数据时不加锁，从而避免阻塞其他事务。这种机制依赖于 `undo log` 和 `ReadView`，确保在高并发的情况下提供一致的读取视图。\n\n**<strong>undo log**存储位置</strong>：\n\n- `undo log` 存储在专用的 **回滚段（rollback segment）** 中，通常位于与数据文件相同的表空间内。\n\nundo log主要用于**事务回滚**和**MVCC（多并发版本控制）**。\n\n**MVCC工作原理：**\n\n- **可重复读（REPEATABLE READ）**隔离级别：**事务在开始时**创建一个`ReadView`，并在整个事务期间使用这个`ReadView`，确保即使有其他事务提交了更改，当前事务的查询结果也保持一致。这样就保证了事务内的多次读取是**可重复**的。\n- **读已提交（READ COMMITTED）**隔离级别下：`ReadView`是在**每次执行查询时**生成的，因此一个事务内的不同查询可能看到其他事务已经提交的更改。这样可以保证读取到的是其他事务已提交的数据，但无法保证事务内的多次读取结果一致。当事务执行读取操作时，`ReadView` 会判断数据的每个版本的**可见性**，这主要取决于版本的 **事务 ID**（`trx_id`）\n\n## 2.Redo Log（重做日志）\n\nredo log&nbsp;用于支持事务的**持久性（Durability）**。它记录了已提交的事务对数据库进行的更改，这样在系统崩溃后可以通过redo log来恢复数据。\n\n在redo log记录完成后，事务才算完成。后续，InnoDB 引擎会在适当的时候，由后台线程将缓存在 Buffer Pool 的脏页刷新到磁盘里，这就是 **WAL （Write-Ahead Logging）技术**。\n\n**工作原理**:\n\n- 在事务提交之前，所有的更改都会被写入 `redo log` 的日志缓冲区（`redo log buffer`）。\n- 当事务提交时，`redo log` 缓冲区会刷新到磁盘上的 `redo log` 文件。\n- 如果数据库崩溃，在重启时，InnoDB 可以使用 `redo log` 来重新应用（重做）日志中的所有更改，以确保所有已提交的事务持久化到数据库中。\n\n**存储位置**:\n\n- `redo log` 存储在专用的日志文件中，InnoDB存储引擎有一个redo log group，由2个文件组成（通常为 `ib_logfile0` 和 `ib_logfile1`）中，这些文件位于 InnoDB 的日志目录中。\n<li>\n重做日志文件组是以**循环写**的方式工作的，从头开始写，写到末尾就又回到开头，相当于一个环形。所以 InnoDB 存储引擎会先写 ib_logfile0 文件，当 ib_logfile0 文件被写满的时候，会切换至 ib_logfile1 文件，当 ib_logfile1 文件也被写满时，会切换回 ib_logfile0 文件。\n</li>\n\n参考：[MySQL 日志：undo log、redo log、binlog 有什么用？](https://xiaolincoding.com/mysql/log/how_update.html#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-undo-log)\n\n## 3.两阶段提交（Two-Phase Commit, 2PC）\n\n在 MySQL 中，事务的操作首先写入到 **redo log**（InnoDB 的物理日志）中，用于崩溃恢复；同时，如果 MySQL 服务器配置了 **binlog**（二进制日志），事务的变化也需要记录到 **binlog** 中，作为逻辑日志，用于主从复制和增量备份。\n\n为了**确保这两个日志之间的一致性**（即要么都成功，要么都失败），MySQL 使用了两阶段提交。如果没有这种机制，在事务提交过程中如果崩溃，可能会出现 redo log 和 binlog 之间的不一致，从而导致数据不一致的情况。\n\n&nbsp;\n","tags":["MySQL"]},{"title":"使用postman创建mock server","url":"/使用postman创建mock server.html","content":"可以使用postman创建一个mock server用于临时测试API，参考官方文档：[Configure and use a Postman mock server](https://learning.postman.com/docs/designing-and-developing-your-api/mocking-data/setting-up-mock/)\n\n选择Mock servers，点击+号创建一个mock server\n\n<img src=\"/images/517519-20240803160021992-1483411254.png\" width=\"900\" height=\"392\" loading=\"lazy\" />\n\n创建\n\n<img src=\"/images/517519-20240803160550642-209588604.png\" width=\"700\" height=\"443\" loading=\"lazy\" />\n\n最后会得到一个URL，这就是mock server请求的URL\n\n<img src=\"/images/517519-20240803160731454-1354174032.png\" width=\"600\" height=\"376\" loading=\"lazy\" />\n\n测试一下\n\n<img src=\"/images/517519-20240803160912383-823795157.png\" width=\"600\" height=\"43\" loading=\"lazy\" />\n\n查看postman的logs\n\n<img src=\"/images/517519-20240803164157840-474031179.png\" width=\"800\" height=\"324\" loading=\"lazy\" />\n\n使用mock server也可以使用变量和模板定义response\n\n参考：[https://learning.postman.com/docs/designing-and-developing-your-api/mocking-data/creating-dynamic-responses/](https://learning.postman.com/docs/designing-and-developing-your-api/mocking-data/creating-dynamic-responses/)\n\n在 Response Body 中可以使用模板来自定义response，比如<br />\n\n```\n{\n    \"name\": \"{{$randomFullName}}\",\n    \"userName\": \"{{$randomUserName}}\",\n    \"location\": \"{{$randomCity}}\",\n    \"company\": \"{{$randomCompanyName}}\",\n    \"jobTitle\": \"{{$randomJobTitle}}\",\n    \"updatedAt\": \"{{$timestamp}}\"\n}\n\n```\n\n<!--more-->\n&nbsp;如下，这时请求接口返回的结果就会是随机的\n\n<img src=\"/images/517519-20240824113845254-677132941.png\" width=\"800\" height=\"99\" loading=\"lazy\" />\n\n```\ncurl https://xxxx.mock.pstmn.io/tonglin0325\n{\n    \"name\": \"Sara Kassulke\",\n    \"userName\": \"Kari.Leannon\",\n    \"location\": \"Adriannabury\",\n    \"company\": \"Gleichner, Lowe and Christiansen\",\n    \"jobTitle\": \"Customer Branding Manager\",\n    \"updatedAt\": \"1724255442\"\n}\n\n```\n\n也可以指定输出的字段为请求的参数abc，如下\n\n```\n{\n    \"name\": \"{{$randomFullName}}\",\n    \"userName\": \"{{$randomUserName}}\",\n    \"location\": \"{{$randomCity}}\",\n    \"company\": \"{{$queryParams 'abc'}}\",\n    \"jobTitle\": \"{{$randomJobTitle}}\",\n    \"updatedAt\": \"{{$timestamp}}\"\n}\n\n```\n\n请求接口的时候带上abc=test123，输出就会返回对应的值\n\n```\ncurl https://xxxx.mock.pstmn.io/tonglin0325\\?abc\\=test123\n{\n    \"name\": \"Lyle Baumbach\",\n    \"userName\": \"Van_Jacobson\",\n    \"location\": \"South Shayna\",\n    \"company\": \"test123\",\n    \"jobTitle\": \"Internal Identity Strategist\",\n    \"updatedAt\": \"1724470931\"\n}\n\n```\n\n&nbsp;\n","tags":["开发工具"]},{"title":"Python爬虫——selenium语法","url":"/Python爬虫——selenium语法.html","content":"## 1.获取元素\n\n通过a标签的文本筛选\n\n```\ndriver.find_element(By.LINK_TEXT, 'xx').click()\n\n```\n\n通过css筛选\n\n```\ndriver.find_element(By.CSS_SELECTOR, \"input[type='email']\").send_keys(\"xxx\")\ndriver.find_element(By.CSS_SELECTOR, \"button[type='button']\").click()\ndriver.find_element(By.CSS_SELECTOR, \"a.btn_download\").click()\n\n```\n\n通过element name或者css name筛选\n\n```\ndriver.find_element(By.NAME, \"xx\").send_keys(\"xxx\")\ndriver.find_element(By.CLASS_NAME, \"xx\").click()\n\n```\n\n通过xpath筛选，只会contains，starts-with等语法\n\n```\ndriver.find_element(By.XPATH, '//a[starts-with(@title, \"xx\")]').click()\ndriver.find_element(By.XPATH, '//iframe[contains(@title, \"xx\")]').click()\n\n```\n\n## 2.时间筛选器选择时间\n\n可以使用driver.execute_script(js)将元素的readonly属性去掉，再click后clear掉日期，最后send_keys输入新的日期时间\n\n```\nelement = driver.find_elements(By.CSS_SELECTOR, \"input[class='xxx']\")\njs = 'document.getElementsByClassName(\"xxx\")[0].removeAttribute(\"readonly\");'\ndriver.execute_script(js)\nelement[0].click()\nelement[0].clear()\nelement[0].send_keys(\"2022-01-01\")\n\n```\n\n参考：[selenium控制日历控件，readonly为true](https://blog.csdn.net/xjhzjut/article/details/136392800)\n\n## 3.指定chrome下载路径\n\n在使用selenium下载文件的时候，如果不指定chrome的下载路径，会弹出下载框让你确定文件名和下载路径\n\n需要在selenium的driver中使用prefs`属性添加如下属性`\n\n```\nprefs = {'profile.default_content_settings.popups': 0,  # 设置为 0 禁止弹出窗口\n         'download.default_directory': '/Users/seluser/Downloads'}          # 指定下载路径\n\n```\n\n代码如下\n\n```\nfrom selenium import webdriver\n\nprofile = webdriver.ChromeOptions()\n\nprefs = {'profile.default_content_settings.popups': 0,  # 设置为 0 禁止弹出窗口\n         'download.default_directory': 'd:\\\\'}          # 指定下载路径\n\nprofile.add_experimental_option('prefs', prefs)\n\n# executable_path这个是chromedriver的路径 如果设置过环境变量，此参数可以省略\nchromedriver_path = \"D:\\\\path\\\\chromedriver.exe\"   # 自己本地电脑路径\ndriver = webdriver.Chrome(executable_path=chromedriver_path, chrome_options=profile)\n\n```\n\n参考：[selenium+python自动化80-文件下载（不弹询问框）](https://www.cnblogs.com/yoyoketang/p/7657436.html)\n\n如果使用的是undetected_chromedriver，则不支持chrome_options参数，需要使用如下方式进行添加prefs\n\n```\nimport json\nimport os\nimport tempfile\nfrom functools import reduce\n\nimport undetected_chromedriver as webdriver\n\nclass ChromeWithPrefs(webdriver.Chrome):\n    def __init__(self, *args, options=None, **kwargs):\n        if options:\n            self._handle_prefs(options)\n\n        super().__init__(*args, options=options, **kwargs)\n\n        # remove the user_data_dir when quitting\n        self.keep_user_data_dir = False\n\n    @staticmethod\n    def _handle_prefs(options):\n        if prefs := options.experimental_options.get(\"prefs\"):\n            # turn a (dotted key, value) into a proper nested dict\n            def undot_key(key, value):\n                if \".\" in key:\n                    key, rest = key.split(\".\", 1)\n                    value = undot_key(rest, value)\n                return {key: value}\n\n            # undot prefs dict keys\n            undot_prefs = reduce(\n                lambda d1, d2: {**d1, **d2},  # merge dicts\n                (undot_key(key, value) for key, value in prefs.items()),\n            )\n\n            # create an user_data_dir and add its path to the options\n            user_data_dir = os.path.normpath(tempfile.mkdtemp())\n            options.add_argument(f\"--user-data-dir={user_data_dir}\")\n\n            # create the preferences json file in its default directory\n            default_dir = os.path.join(user_data_dir, \"Default\")\n            os.mkdir(default_dir)\n\n            prefs_file = os.path.join(default_dir, \"Preferences\")\n            with open(prefs_file, encoding=\"latin1\", mode=\"w\") as f:\n                json.dump(undot_prefs, f)\n\n            # pylint: disable=protected-access\n            # remove the experimental_options to avoid an error\n            del options._experimental_options[\"prefs\"]\n\n\nif __name__ == \"__main__\":\n    prefs = {\n        \"profile.default_content_setting_values.images\": 2,\n        # \"download.default_directory\": \"d:/temp\",\n        # \"plugins.always_open_pdf_externally\": True,\n    }\n    options = webdriver.ChromeOptions()\n    options.add_experimental_option(\"prefs\", prefs)\n\n    # use the derived Chrome class that handles prefs\n    driver = ChromeWithPrefs(options=options)\n\n```\n\n参考：[https://github.com/ultrafunkamsterdam/undetected-chromedriver/issues/524](https://github.com/ultrafunkamsterdam/undetected-chromedriver/issues/524)\n","tags":["Python"]},{"title":"parquet-tools使用","url":"/parquet-tools使用.html","content":"使用parquet-tools的方法有2种\n\n1.在安装了CDH的机器上，会自动有parquet-tools命令\n\n```\nlintong@master:/opt/cloudera/parcels/CDH/bin$ ls| grep parquet-tools\nparquet-tools\nlintong@master:/opt/cloudera/parcels/CDH/bin$ parquet-tools\n\n```\n\n<!--more-->\n&nbsp;\n\n2.自行编辑jar\n\ngit clone并指定分支，master分支已经删除了parquet-tools\n\n```\ngit clone git@github.com:apache/parquet-mr.git -b apache-parquet-1.10.1\n\n```\n\n编译\n\n```\ncd parquet-tools &amp;&amp; mvn clean package -Plocal\n\n```\n\n&nbsp;\n\nparquet-tools可以使用的命令，参考：[How to build and use parquet-tools to read parquet files](https://support.datafabric.hpe.com/s/article/How-to-build-and-use-parquet-tools-to-read-parquet-files?language=en_US)\n\n**1.查看parquet文件的schema**\n\n由AvroParquet写的parquet文件的schema\n\n```\nlintong@lintongdeMacBook-Pro ~/coding/java/parquet-mr/parquet-tools/target $ java -jar parquet-tools-1.10.1.jar schema /xxx/avro_parquet/part-r-00000.snappy.parquet\n\nmessage com.linkedin.haivvreo.test_serializer {\n  required binary string1 (UTF8);\n  required int32 int1;\n  required int32 tinyint1;\n  required int32 smallint1;\n  required int64 bigint1;\n  required boolean boolean1;\n  required float float1;\n  required double double1;\n  required group list1 (LIST) {\n    repeated binary array (UTF8);\n  }\n  required group map1 (MAP) {\n    repeated group map (MAP_KEY_VALUE) {\n      required binary key (UTF8);\n      required int32 value;\n    }\n  }\n  required group struct1 {\n    required int32 sInt;\n    required boolean sBoolean;\n    required binary sString (UTF8);\n  }\n  required binary enum1 (ENUM);\n  optional int32 nullableint;\n}\n\n```\n\n由ThriftParquet写的parquet文件的schema\n\n```\nlintong@lintongdeMacBook-Pro ~/coding/java/parquet-mr/parquet-tools/target $ java -jar parquet-tools-1.10.1.jar schema /xxx/thrift_parquet/part-r-00000.snappy.parquet\n\nmessage ParquetSchema {\n  required binary string1 (UTF8) = 1;\n  required int32 int1 = 2;\n  required int32 tinyint1 = 3;\n  required int32 smallint1 = 4;\n  required int64 bigint1 = 5;\n  required boolean boolean1 = 6;\n  required double float1 = 7;\n  required double double1 = 8;\n  required group list1 (LIST) = 9 {\n    repeated binary list1_tuple (UTF8);\n  }\n  required group map1 (MAP) = 10 {\n    repeated group map (MAP_KEY_VALUE) {\n      required binary key (UTF8);\n      optional int32 value;\n    }\n  }\n  required group struct1 = 11 {\n    required int32 sInt = 1;\n    required boolean sBoolean = 2;\n    required binary sString (UTF8) = 3;\n  }\n  required binary enum1 (UTF8) = 12;\n  optional int32 nullableint = 13;\n}\n\n```\n\n由hive job写的parquet文件的schema\n\n```\nmessage hive_schema {\n  optional binary appid (UTF8);\n  optional int64 ts;\n  optional int32 userid;\n  optional binary countries (UTF8);\n}\n\n```\n\n　　\n\n**2.查看parquet文件的head**\n\n```\njava -jar parquet-tools-1.10.1.jar head -n 1 /xxx/thrift_parquet/part-r-00000.snappy.parquet\nstring1 = ecAsz6ca7E\nint1 = 64676\ntinyint1 = 8\nsmallint1 = 0\nbigint1 = -9081354042296389692\nboolean1 = true\nfloat1 = 0.1271180510520935\ndouble1 = 0.011293589263621895\nlist1:\n.list1_tuple = v8gCJFRBIb\n.list1_tuple = nfvrI1Rltp\nmap1:\n.map:\n..key = v8gCJFRBIb\n..value = 428\n.map:\n..key = nfvrI1Rltp\n..value = 1257\nstruct1:\n.sInt = 740564\n.sBoolean = true\n.sString = RuiVISF2BI\nenum1 = BLUE\nnullableint = 5559\n\n```\n\n&nbsp;\n","tags":["parquet"]},{"title":"使用avro-protobuf将protobuf转换成avro","url":"/使用avro-protobuf将protobuf转换成avro.html","content":"avro-protobuf项目提供ProtobufDatumReader类，可以用于从protobuf定义生成的java class中获得avro schema\n\n使用方法如下：\n\n1.引入依赖\n\n```\n        <dependency>\n            <groupId>org.apache.avro</groupId>\n            <artifactId>avro-protobuf</artifactId>\n            <version>1.11.1</version>\n        </dependency>\n        <dependency>\n            <groupId>com.google.protobuf</groupId>\n            <artifactId>protobuf-java</artifactId>\n            <version>3.21.7</version>\n        </dependency>\n\n```\n\n2.定义protobuf schema，名为other.proto，schema如下\n\n```\nsyntax = \"proto3\";\npackage com.acme;\n\nmessage MyRecord {\n  string f1 = 1;\n  OtherRecord f2 = 2;\n}\n\nmessage OtherRecord {\n  int32 other_id = 1;\n}\n\n```\n\n从使用protobuf定义生成java class\n\n```\nprotoc -I=./ --java_out=./src/main/java ./src/main/proto/other.proto\n\n```\n\n3.编写java代码\n\n```\npackage com.example.demo;\n\nimport com.acme.Other;\n\nimport java.util.*;\nimport org.apache.avro.protobuf.ProtobufDatumReader;\n\npublic class Demo {\n\n    public static void main(String[] args) throws Exception { \n\n        ProtobufDatumReader<Other.MyRecord> datumReader = new ProtobufDatumReader<Other.MyRecord>(Other.MyRecord.class);\n        System.out.println(datumReader.getSchema().toString(true));\n\n    }\n\n}\n\n```\n\n输出如下\n\n```\n{\n  \"type\" : \"record\",\n  \"name\" : \"MyRecord\",\n  \"namespace\" : \"com.acme.Other\",\n  \"fields\" : [ {\n    \"name\" : \"f1\",\n    \"type\" : {\n      \"type\" : \"string\",\n      \"avro.java.string\" : \"String\"\n    },\n    \"default\" : \"\"\n  }, {\n    \"name\" : \"f2\",\n    \"type\" : [ \"null\", {\n      \"type\" : \"record\",\n      \"name\" : \"OtherRecord\",\n      \"fields\" : [ {\n        \"name\" : \"other_id\",\n        \"type\" : \"int\",\n        \"default\" : 0\n      } ]\n    } ],\n    \"default\" : null\n  } ]\n}\n\n```\n\n**注意**：该工具在把protobuf schema转换成avro schema的时候，可能会出现不严谨的时候，比如在转换protobuf的uint32（0 到 2^32 -1）的时候，会统一转换成int（-2^31 ~ 2^31-1），这可能会产生问题，解决方法是使用confluent schema registry提供的工具，参考：[使用confluent schema registry将protobuf schema转换成avro schema](https://www.cnblogs.com/tonglin0325/p/4642622.html)\n","tags":["avro","google"]},{"title":"Amazon EMR使用指南","url":"/Amazon EMR使用指南.html","content":"Amazon EMR是Amazon提供的托管大数据套件，可选的组件包括Hadoop，Hive，Hue，Hbase，Presto，Spark等\n\n使用Amazon EMR的好处是快速伸缩，版本升级也较为方便，如果配合S3存储，可以做到计算和存储分离，这样对于运维的压力会小一些，存储的稳定性交给S3，计算集群即使故障也可以很方便的进行重建，很适合小团队。缺点是界面友好程度远不如CDH和HDP。\n\n如果使用Amazon EMR，最好阅读一下官方的2个文档：\n\n1.Amazon EMR最佳实践\n\n```\nhttps://d0.awsstatic.com/whitepapers/aws-amazon-emr-best-practices.pdf\n\n```\n\n2.Amazon EMR迁移指南\n\n```\nhttps://d1.awsstatic.com/whitepapers/amazon_emr_migration_guide.pdf\n```\n\n## 1.创建EMR集群\n\n在创建Amazon EMR集群的时候可以选择**快速模式**，界面如下\n\n<img src=\"/images/517519-20220129151433350-335157056.png\" alt=\"\" loading=\"lazy\" />\n\n也可以选择**高级模式**\n\n<img src=\"/images/517519-20220129154043393-629951081.png\" alt=\"\" loading=\"lazy\" />\n\n集群启动了之后，EMR大数据组件的安装目录在/usr/lib\n\n```\n[hadoop@ip-xxxxxxxx lib]$ ls\nbigtop-groovy  dracut      hadoop            hadoop-yarn    java        jvm          ld-linux-aarch64.so.1  modules-load.d  rpm               systemd\nbigtop-utils   fontconfig  hadoop-hdfs       hbase          java-1.5.0  jvm-commmon  livy                   oozie           sendmail          tez\nbinfmt.d       games       hadoop-httpfs     hive           java-1.6.0  jvm-exports  locale                 presto          sendmail.postfix  tmpfiles.d\ncpp            gcc         hadoop-kms        hive-hcatalog  java-1.7.0  jvm-private  lsb                    python2.7       spark             udev\ncups           gems        hadoop-lzo        hudi           java-1.8.0  kbd          modprobe.d             python3.7       sse2              yum-plugins\ndebug          grub        hadoop-mapreduce  hue            java-ext    kernel       modules                ranger-kms      sysctl.d          zookeeper\n\n```\n\nEMR管理组件的安装目录在/usr/share/aws/emr\n\n```\n[hadoop@ip-xxxxxxxxxx emr]$ ls\ncloudwatch-sink  ddb    emr-log-analytics-metrics  goodies              instance-controller  node-provisioner  s3select\ncommand-runner   emrfs  emr-metrics-collector      hadoop-state-pusher  kinesis              s3-dist-cp        scripts\n\n```\n\n## 2.EMR CLI使用\n\n查看集群的cluster id，参考：https://docs.aws.amazon.com/zh_cn/emr/latest/ManagementGuide/emr-manage-view-clusters.html\n\n```\naws emr list-clusters\n\n```\n\n根据cluster id查看集群配置\n\n```\naws emr describe-cluster --cluster-id j-xxxxxx\n\n```\n\n## 3.EMR角色分布\n\n<!--more-->\n&nbsp;amazon EMR的节点分成3类：主节点，核心节点和任务节点\n\n参考：[了解节点类型：主节点、核心节点和任务节点](https://docs.aws.amazon.com/zh_cn/emr/latest/ManagementGuide/emr-master-core-task-nodes.html)&nbsp;和&nbsp;[部署高可用的EMR集群，为您的业务连续性保驾护航](https://aws.amazon.com/cn/blogs/china/amazon-emr-ha-for-your-service/)\n\n主节点：\n\n- HDFS Name Node运行在三个主节点中的其中两个。一个为active状态，另一个为 standby状态。当active的Name Node出现故障时，standby的Name Node会变为active并接管所有客户端的操作。EMR会启用一个新的主节点将故障的Name Node替换，并设为standby状态。\n- Yarn ResourceManager(RM)运行在所有的三个主节点上。其中，一个RM是active状态，另外两个是standby状态。当active的RM出现故障时，EMR会进行自动故障转移，将其中一个standby节点变为新的active来接管所有操作。之后EMR会将出现故障的master 节点替换，并设为standby状态。\n\n核心节点：\n\n<li class=\"listitem\">\n为确保核心节点实例组也具有高可用性，建议您至少启动四个核心节点。如果您决定启动具有三个或更少核心节点的较小集群，请通过将&nbsp;dfs.replication&nbsp;parameter 设置为至少 2 来配置具有足够 DFS 复制的 HDFS。有关更多信息，请参阅&nbsp;[HDFS 配置](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hdfs-config.html)。\n</li>\n\n\n\n|节点类型/组件名称|HDFS Namenode|HDFS SecondaryNamenode|HDFS Datanode|JournalNodes|Zookeeper Server|YARN ResourceManager|YARN NodeManager|YARN JobHistoryServer|presto coordinator|presto worker|HiveServer2|HiveMetastore|Hive Gateway|Spark Gateway|Spark HistoryServer|HUE Server\n| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- \n|主节点（高可用部署的时候节点数据为3个）分配任务，不需要 Amazon EC2 实例和很高的处理能力|✅|✅|&nbsp;|✅|✅|✅|&nbsp;|&nbsp;✅|✅|&nbsp;|&nbsp;✅|&nbsp;✅|✅&nbsp;|&nbsp;✅|&nbsp;✅|✅&nbsp;\n|核心节点（处理任务并将数据存储在 HDFS 中）需要处理能力和存储容量|&nbsp;|&nbsp;|✅|&nbsp;|&nbsp;|&nbsp;|✅|&nbsp;|&nbsp;|✅|&nbsp;|&nbsp;|&nbsp;|&nbsp;|&nbsp;|&nbsp;\n|任务节点（不存储数据）只需要处理能力|&nbsp;|&nbsp;|&nbsp;|&nbsp;|&nbsp;|&nbsp;|✅|&nbsp;|&nbsp;|✅|&nbsp;|&nbsp;|&nbsp;|&nbsp;|&nbsp;|&nbsp;\n\n如果在YARN上运行的是flink任务的话，job manager只能运行在core节点上，不能运行在task节点，因为task节点并不会去/var/lib/flink路径下初始化flink相关组件，而task manager可以运行在task节点上\n\n## 4.EMR配置文件\n\n在EMR中对配置进行了修改，比如hive-metastore.xml, hive-server2.xml，core-site.xml，对应如何配置以及需要重启的组件可以参考如下表格：[https://docs.aws.amazon.com/zh_cn/emr/latest/ReleaseGuide/emr-630-release.html](https://docs.aws.amazon.com/zh_cn/emr/latest/ReleaseGuide/emr-630-release.html)\n\n<img src=\"/images/517519-20221108110850032-771586928.png\" width=\"1000\" height=\"353\" loading=\"lazy\" />\n\n## 5.EMR启动步骤\n\n参考：[Amazon EMR实战心得浅谈](https://aws.amazon.com/cn/blogs/china/brief-introduction-to-emr-practical-experience/)\n\n&nbsp;<img src=\"/images/517519-20221108171243676-922070496.png\" alt=\"\" loading=\"lazy\" />\n\n如果EMR集群启动失败的话，可以去Log URI查看日志，Terminated with errors后面会告诉你是哪台实例在哪个步骤的时候报错\n\n&nbsp;<img src=\"/images/517519-20221109130236672-464082794.png\" alt=\"\" loading=\"lazy\" />\n\n## 6.EMR集群的动态伸缩\n\n在创建AWS EMR集群的时候，可以选择开启cluster scaling，auto scaling有2种伸缩策略，一种是**EMR-manager scaling**，另一种是**用户自行配置scaling策略**来进行扩缩容\n\n注意在**只有core节点和task节点是可以进行扩缩容的**，master主节点在EMR集群创建后就无法进行变更\n\n### **1.EMR-manager scaling配置**\n\n这里需要配置的参数主要控制的是core和task节点的扩缩容实例数量，参考：[Using managed scaling in Amazon EMR](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-scaling.html)\n\n```\nMinimum 3 (core + task 最小为3台机器)\nMaximum 15 （core + task 最大为15台机器)\nOn-demand limit (非必须参数，假设Min为2台机器，max为100台机器，On-demand limit为10台，则扩容的时候会有10台机器的购买方式是按需购买，剩余的机器是竞价购买)\nMaximum Core Node (非必须参数，core 节点最多为3台)\n```\n\n<img src=\"/images/517519-20230328141847752-1483912536.png\" alt=\"\" loading=\"lazy\" />\n\n在使用了**EMR-manager scaling**之后，EMR集群的扩容主要由**集群容量指标**控制，比如&nbsp;TaskNodesRequested，CoreNodesRequested`等，``参考：`[了解托管扩展指标](https://docs.aws.amazon.com/zh_cn/emr/latest/ManagementGuide/managed-scaling-metrics.html)\n\n这里我们手动将task节点的数量从0点resizing成5个，或者可以往YARN上提交需要较大资源的任务，参考：[实验9: EMR AUTO SCALING](https://bigdata.awspsa.com/5-%E8%BF%9B%E9%98%B6%E8%AF%BE%E7%A8%8B/5_4-%E8%87%AA%E5%8A%A8%E5%8C%96%E5%BC%B9%E6%80%A7%E7%9A%84%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0/5_4_1-emr-auto-scaling.html)\n\n<img src=\"/images/517519-20230328161757640-1190497564.png\" alt=\"\" loading=\"lazy\" />\n\n可以在cloud watch中的EMR相关指标中看到，TaskNodesRequested从0上升到5个\n\n<img src=\"/images/517519-20230328165351692-83639080.png\" width=\"1000\" height=\"324\" loading=\"lazy\" />\n\n<img src=\"/images/517519-20230328161328073-1555814068.png\" width=\"1400\" height=\"408\" loading=\"lazy\" />\n\n如果实际任务不需要这么多资源，过一小段时间后就会自动缩容&nbsp;\n\n但是实际测试下来，EMR-manager scaling有时候会出现无法扩容的问题，例如我创建了如下扩容策略的EMR集群\n\n<img src=\"/images/517519-20230417162026376-320671083.png\" alt=\"\" loading=\"lazy\" />\n\n这时即使在创建集群时指定了task的instance group，但是因为其中task节点的数量初始为0，所以集群创建后不会有task的instance group，之后也不能进行扩容\n\n然后我使用如下命令创建了一个flink session cluster\n\n```\n/usr/lib/flink/bin/yarn-session.sh -s 1 -jm 10240 -tm 10240 --detached\n\n```\n\n但是该flink集群缺无法正常启动，一直提示获取不到足够的资源\n\n<img src=\"/images/517519-20230417162153478-60969885.png\" alt=\"\" loading=\"lazy\" />\n\n在cloudwatch和EMR监控上也看不到任何扩容的请求\n\n<img src=\"/images/517519-20230417162309472-473159116.png\" alt=\"\" loading=\"lazy\" />\n\n所以这里推荐使用用户自行配置scaling策略\n\n参考：[Amazon EMR Managed Scaling 介绍&mdash;&mdash;自动调整集群大小，高效节约运营成本](https://aws.amazon.com/cn/blogs/china/introducing-amazon-emr-managed-scaling-automatically-resize-clusters-to-lower-cost/)\n\n同时EMR托管的扩容只支持YARN应用程序，如 Spark、Hadoop、Hive、Flink；不支持不基于YARN的应用程序，例如 Presto 或 HBase。如果想要对Presto进行扩缩容，只能使用指标来自定义自动扩展，比如查询延迟，CPU使用率，内存使用率以及磁盘I/O等\n\n<img src=\"/images/517519-20231201224638285-1651194717.png\" width=\"700\" height=\"281\" loading=\"lazy\" />\n\n参考：[https://docs.aws.amazon.com/zh_cn/emr/latest/ManagementGuide/emr-scale-on-demand.html](https://docs.aws.amazon.com/zh_cn/emr/latest/ManagementGuide/emr-scale-on-demand.html)\n\n### **2.用户自行配置scaling策略**\n\n这里配置的扩缩容的节点是Task节点，Core节点由于有HDFS，所以不进行扩缩容\n\n<img src=\"/images/517519-20230328141435585-1463801639.png\" alt=\"\" loading=\"lazy\" />\n\n自行编辑扩容策略，这里设置的扩容策略是当AppsPending这个指标大于等于1的时候，扩容2台机器\n\n<img src=\"/images/517519-20230417163300598-67773596.png\" alt=\"\" loading=\"lazy\" />\n\n扩缩容可以支持的指标有很多，如下\n\n<img src=\"/images/517519-20231201221532740-598880353.png\" width=\"300\" height=\"337\" loading=\"lazy\" />\n\n<img src=\"/images/517519-20231201222511862-495790288.png\" width=\"300\" height=\"311\" loading=\"lazy\" />\n\n<img src=\"/images/517519-20231201222602615-1317760889.png\" width=\"300\" height=\"310\" loading=\"lazy\" />\n\n<img src=\"/images/517519-20231201222642508-264363271.png\" width=\"300\" height=\"159\" loading=\"lazy\" />\n\n指标分成3类，Cluster Status，Node Status以及IO\n\n含义参考：[https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html#UsingEMR_ViewingMetrics_MetricsReported](https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html#UsingEMR_ViewingMetrics_MetricsReported)\n\n&nbsp;\n","tags":["AWS"]},{"title":"Grafana学习笔记——filter语法","url":"/Grafana学习笔记——filter语法.html","content":"在使用grafana的filter的时候，其支持一些语法用于对指标进行过滤，如下\n\n<img src=\"/images/517519-20230601152518908-497096837.png\" width=\"500\" height=\"317\" loading=\"lazy\" />\n\nliteral_or ： tagv的过滤规则: 精确匹配多项迭代值，多项迭代值以'|'分隔，大小写敏感\n\niliteral_or： tagv的过滤规则: 精确匹配多项迭代值，多项迭代值以'|'分隔，忽略大小写\n\nwildcard： tagv的过滤规则: 通配符匹配，大小写敏感\n\niwildcard： tagv的过滤规则: 通配符匹配，忽略大小写\n\nregexp： tagv的过滤规则: 正则表达式匹配\n\nnot_literal_or： tagv的过滤规则: 通配符取非匹配，大小写敏感\n\nnot_iliteral_or： tagv的过滤规则: 通配符取非匹配，忽略大小写\n\n<!--more-->\n&nbsp;\n","tags":["Grafana"]},{"title":"HAProxy学习笔记——HAProxy Data Plane API","url":"/HAProxy学习笔记——HAProxy Data Plane API.html","content":"HAProxy1.9.0及其以上版本支持了Data Plane API功能，可以使用API的方式来管理HAProxy\n\n官方网址\n\n```\nhttps://github.com/haproxytech/dataplaneapi\n\n```\n\n以及\n\n```\nhttps://ci-jie.github.io/2020/10/25/HAProxy-Data-Plane-API/\n\n```\n\n　　\n","tags":["HAProxy"]},{"title":"react学习笔记——基础","url":"/react学习笔记——基础.html","content":"## 1.webstorm开发环境调试\n\n配置npm，然后run start\n\n<img src=\"/images/517519-20220419082012376-1571267013.png\" width=\"600\" height=\"400\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\ncommand+shift，并点击url\n\n<img src=\"/images/517519-20220419082216875-1793122081.png\" width=\"300\" height=\"51\" loading=\"lazy\" />\n\n&nbsp;\n\n这时webstorm会自动添加JavaScript debug\n\n<img src=\"/images/517519-20220419082350594-2131897545.png\" width=\"600\" height=\"171\" loading=\"lazy\" />\n\n&nbsp;\n\n&nbsp;此时在浏览器点击页面，产生的日志就会在webstorm的console中显示\n\n<img src=\"/images/517519-20220419082607338-1042586956.png\" width=\"600\" height=\"334\" loading=\"lazy\" />\n\n## 2.react动态路由\n\n&nbsp;\n\n## 3.react hook\n\n参考：[React Hooks教程--useState、useEffect以及如何创建自定义钩子](https://juejin.cn/post/7023213712789995550)\n\n&nbsp;\n","tags":["React"]},{"title":"Mongo的oplog和change stream","url":"/Mongo的oplog和change stream.html","content":"## 1.oplog\n\nMongoDB副本集由一组服务器组成，这些服务器都具有相同数据的副本，复制可确保客户端对副本集主副本集上的文档所做的所有更改都正确应用于其他副本集的服务器，称为secondaries副本。\n\nMongoDB 复制的工作原理是让主节点在其操作日志（或操作日志）中记录更改，然后每个从节点读取主节点的操作日志并将所有操作按顺序应用到它们自己的文档中。\n\n将新服务器添加到副本集时，该服务器首先对主服务器上的所有数据库和集合执行snapshot，然后读取主服务器的<!--more-->\n&nbsp;**oplog**&nbsp;以应用自启动快照以来可能所做的所有更改。这个新服务器在赶上主服务器oplog的尾部时成为secondary服务器（并且能够处理查询）。\n\n## 2.change stream\n\nDebezium MongoDB 连接器使用与上述类似的复制机制，尽管它实际上并没有成为副本集的成员。主要区别在于连接器不直接读取 oplog，而是将捕获和解码 oplog 委托给 MongoDB 的&nbsp;**Change Streams**&nbsp;功能。\n\n使用change stream，MongoDB服务器将集合的更改公开为事件流。Debezium 连接器监视流并将更改传递到下游。而且，当连接器第一次看到副本集时，它会查看 oplog 以获取上次记录的事务，然后执行主数据库和集合的快照。复制完所有数据后，连接器会从之前从操作日志中读取的位置创建更改流。\n\n当MongoDB连接器进程发生变化时，它会定期记录oplog/stream中事件发起的位置。当连接器停止时，它会记录它处理的最后一个oplog/流位置，以便在重新启动时它只是从该位置开始流式传输。换句话说，连接器可以停止、升级或维护，并在一段时间后重新启动，它将准确地从中断的位置继续，而不会丢失单个事件。当然，MongoDB的oplog通常被限制在最大大小，这意味着连接器不应该停止太久，否则oplog中的某些操作可能会在连接器有机会读取它们之前被清除。在这种情况下，重新启动时，连接器将检测缺少的oplog操作，执行快照，然后继续流式传输更改。\n\n限制：change stream只能用于 replica set 集群和 sharded cluster 集群，单节点mongo因为没有oplog，所以不支持\n\n参考：[MongoDB Change Stream之一&mdash;&mdash;上手及初体验](https://cloud.tencent.com/developer/article/1711794)\n\n## 3.change stream相关操作\n\n1.获取某个database的某个collection的Change Stream\n\n```\nuse xx_db\ndb.your_collection.watch([],{maxAwaitTimeMS:60000})\n\n```\n\n2.指定从resume token开始获取某个database的某个collection的Change Stream\n\n```\ndb.document.watch([],{maxAwaitTimeMS:60000,resumeAfter:{\"_data\": \"xxxxxxx\"}})\n\n```\n\n其他参数可以参考官方文档：[https://www.mongodb.com/docs/manual/reference/method/db.collection.watch/#mongodb-method-db.collection.watch](https://www.mongodb.com/docs/manual/reference/method/db.collection.watch/#mongodb-method-db.collection.watch)\n\n&nbsp;\n","tags":["mongo"]},{"title":"Spark学习笔记——Spark加载jar包的过程","url":"/Spark学习笔记——Spark加载jar包的过程.html","content":"## 给spark任务添加jar包的方式\n\nspark中使用的jar分成2种，一种是用户自行添加的jar，另一种是spark环境依赖的jar\n\n### 1.spark环境依赖的jar\n\n其添加的方式有有下面几种\n\n1.<!--more-->\n&nbsp;`--conf spark.driver.extraClassPath=...`&nbsp;or&nbsp;`--driver-class-path ...`\n\n2.&nbsp;`--conf spark.driver.extraLibraryPath=...`, or&nbsp;`--driver-library-path ...`\n\n3.&nbsp;`--conf spark.executor.extraClassPath=...`\n\n4.&nbsp;`--conf spark.executor.extraLibraryPath=...`\n\n上面的配置参数指定的jar包最终都是放到了**系统类加载器**的classpath里，由系统类加载器完成加载。\n\n&nbsp;\n\n### 2.用户自行添加的jar\n\n其添加的方式有有下面几种\n\n1.&nbsp;`--jars`\n\n2.&nbsp;[`SparkContext.addJar(...)`](http://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#addJar(java.lang.String))&nbsp;method\n\n3.&nbsp;[`SparkContext.addFile(...)`](http://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#addFile(java.lang.String))&nbsp;method\n\n4. not to forget, the last parameter of the spark-submit is also a .jar file. 也是通过&nbsp;[`SparkContext.addJar(...)`](http://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#addJar(java.lang.String))\n\n用户自行添加的jar会放到Spark自定义的**MutableURLClassLoader**或者**ChildFirstURLClassLoader**的classpath中，由其完成加载。\n\n&nbsp;\n\n### 3.spark类加载的顺序\n\nClassLoader使用的是双亲委托模型来搜索类的，每个ClassLoader实例都有一个父类加载器的引用（不是继承的关系，是一个包含的关系），虚拟机内置的类加载器（Bootstrap ClassLoader）本身没有父类加载器，但可以用作其它ClassLoader实例的的父类加载器。\n\n当一个ClassLoader实例需要加载某个类时，它会试图亲自搜索某个类之前，先把这个任务委托给它的父类加载器，这个过程是由上至下依次检查的，首先由最顶层的类加载器&nbsp;**Bootstrap ClassLoader&nbsp;**试图加载，如果没加载到，则把任务转交给&nbsp;**Extension ClassLoader&nbsp;**试图加载，如果也没加载到，则转交给&nbsp;**App ClassLoader**&nbsp;进行加载，如果它也没有加载得到的话，则返回给委托的发起者，由它到&nbsp;**指定的文件系统或网络等URL&nbsp;**中加载该类。\n\n如果它们都没有加载到这个类时，则抛出ClassNotFoundException异常。否则将这个找到的类生成一个类的定义，并将它加载到内存当中，最后返回这个类在内存中的Class实例对象。\n\n#### 1 启动类加载器（引导类加载器，Bootstrap classLoader)\n\n1）这个类加载使用c/C++语言实现的，嵌套在JVM内部。<br />2）它用来加载Java的核心库（JAVA_HOME/jre/ lib/rt.jar、resources.jar或sun.boot.class.path路径下的内容），用于提供JVM自身需要的类。<br />3）并不继承自java.lang.classLoader，没有父加载器。加载扩展类和应用程序类加载器，并指定为他们的父类加载器。出于安全考虑，Bootstrap启动类加载器只加载包名为java、javax、sun等开头的类。\n\n#### 2.2 扩展类加载器（Extension classLoader)\n\n1）Java语言编写，由sun.misc.Launcher$ExtClassLoader实现，派生于classLoader类。<br />2）父类加载器为启动类加载器。<br />3）从java.ext.dirs系统属性所指定的目录中加载类库，或从JDK的安装目录的ire/lib/ext子目录（扩展目录）下加载类库。如果用户创建的JAR放在此目录下，也会自动由扩展类加载器加载。\n\n#### 2.3 应用程序类加载器（系统类加载器，AppclassLoader）\n\n1）java语言编写，由sun.misc.Launcher$AppclassLoader实现。<br />2）派生于classLoader类，父类加载器为扩展类加载器。<br />3）它负责加载环境变量classpath或系统属性java.class.path指定路径下的类库。<br />4）该类加载是***程序中默认的类加载器***，一般来说，Java应用的类都是由它来完成加载。<br />5）通过classLoader#getsystemClassLoader ()方法可以获取到该类加载器。\n\n#### 2.4 用户自定义类加载器（User Defined classLoader）\n\n1）在Java的日常应用程序开发中，类的加载几乎是由上述3种类加载器相互配合执行的，在必要时，我们还可以自定义类加载器，来定制类的加载方式。<br />2）主要功能：\n\n隔离加载类、修改类加载的方式、扩展加载源防止源码泄漏\n\n**<strong><strong>&nbsp;**</strong></strong>\n\n所以在spark加载类的过程中，会优先加载**spark环境依赖的jar**，之后在加载**用户自行添加的jar**\n\n&nbsp;&nbsp;\n\n参考：[Spark 如何摆脱java双亲委托机制优先从用户jar加载类？](https://cloud.tencent.com/developer/article/1640518)\n\n[Add JAR files to a Spark job - spark-submit](https://stackoverflow.com/questions/37132559/add-jar-files-to-a-spark-job-spark-submit)\n\n[Spark 类加载器导致的Kryo序列化问题](https://blog.csdn.net/qq_41775852/article/details/117258621)\n\n&nbsp;\n\n\n","tags":["Spark"]},{"title":"广告系统——第三方广告平台","url":"/广告系统——第三方广告平台.html","content":"罗列了一下第三方广告平台产品，按照广告的流程，分成4类：\n\nDSP（广告需求方平台，广告主在上面进行投放）\n\nADX（广告交易平台，负责广告交易和竞价）\n\nSSP（媒体供应方平台，媒体可以在上面售卖网站app的曝光来获得盈利）\n\nDMP（为广告投放投放提供人群画像进行广告的受众定向，并进行人群标签画像的管理）\n\n<img src=\"/images/517519-20231231155634736-961359969.png\" width=\"600\" height=\"328\" loading=\"lazy\" />\n\n## 1.国内\n\n### 1.BAT\n\n### 2.字节快手\n\n### 3.手机厂商\n\n### 4.其他渠道\n\n1.微博（DSP投放）\n\n```\nhttps://tui.weibo.com/home\n\n```\n\n微博的广告产品有[超级粉丝通](https://tui.weibo.com/platform/superfans)，[粉丝头条](https://tui.weibo.com/platform/topfans)，[WAX（程序化广告交易平台）](https://tui.weibo.com/platform/wax)\n\n[360-搜索推广](https://manual.sensorsdata.cn/sensorsadstracking/latest/360-138871011.html)\n\n[360-移动推广](https://manual.sensorsdata.cn/sensorsadstracking/latest/360-138871046.html)\n\n[360-展示广告](https://manual.sensorsdata.cn/sensorsadstracking/latest/360-138871102.html)\n\n[知乎效果营销](https://manual.sensorsdata.cn/sensorsadstracking/latest/%E7%9F%A5%E4%B9%8E%E6%95%88%E6%9E%9C%E8%90%A5%E9%94%80-118425339.html)\n\n[爱奇艺-奇麟营销](https://manual.sensorsdata.cn/sensorsadstracking/latest/%E7%88%B1%E5%A5%87%E8%89%BA-%E5%A5%87%E9%BA%9F%E8%90%A5%E9%94%80-121833274.html)\n\n[B站-必选广告](https://manual.sensorsdata.cn/sensorsadstracking/latest/b-121833393.html)\n\n[sigmob](https://manual.sensorsdata.cn/sensorsadstracking/latest/sigmob-121833602.html)\n\n[喜马拉雅-声量](https://manual.sensorsdata.cn/sensorsadstracking/latest/%E5%96%9C%E9%A9%AC%E6%8B%89%E9%9B%85-%E5%A3%B0%E9%87%8F-121833634.html)\n\n[快看-后羿智投](https://manual.sensorsdata.cn/sensorsadstracking/latest/%E5%BF%AB%E7%9C%8B-%E5%90%8E%E7%BE%BF%E6%99%BA%E6%8A%95-138871183.html)\n\n[网易-有道智选](https://manual.sensorsdata.cn/sensorsadstracking/latest/%E7%BD%91%E6%98%93-%E6%9C%89%E9%81%93%E6%99%BA%E9%80%89-119275675.html)\n\n[小红书-聚光平台](https://manual.sensorsdata.cn/sensorsadstracking/latest/%E5%B0%8F%E7%BA%A2%E4%B9%A6-%E8%81%9A%E5%85%89%E5%B9%B3%E5%8F%B0-119275707.html)\n\n[网易-易效 (网易新闻)](https://manual.sensorsdata.cn/sensorsadstracking/latest/%E7%BD%91%E6%98%93-%E6%98%93%E6%95%88-%28%E7%BD%91%E6%98%93%E6%96%B0%E9%97%BB%29-121832729.html)\n\n[美柚-女人通](https://manual.sensorsdata.cn/sensorsadstracking/latest/%E7%BE%8E%E6%9F%9A-%E5%A5%B3%E4%BA%BA%E9%80%9A-138871218.html)\n\n[ASA-苹果搜索广告](https://manual.sensorsdata.cn/sensorsadstracking/latest/asa-146276472.html)\n\n参考：[神策渠道配置详情](https://manual.sensorsdata.cn/sensorsadstracking/latest/app_tg-135234118.html)\n\n## 2.国外\n\n### 1.Google系 AdMob（DSP投放+SSP变现）\n\n官网：[https://admob.google.com/home/](https://admob.google.com/home/)\n\n主要是Google面向移动端的广告平台，不仅是一个移动广告联盟，而且还是一个获利平台，可帮助移动开发者利用广告创收、获取富有实用价值的分析洞见以及发展应用业务。作为一个广告联盟，AdMob 可协助您在全球范围内投放广告，从而让您利用自己的移动应用变现。作为一个变现平台，AdMob 可以协助与多个广告联盟合作的开发者最大限度地提升通过所有第三方广告联盟合作伙伴获得的广告收入。\n\n### 2.Google系 GAM（Google Ad Manager）（DSP投放）\n\n官网：[https://admanager.google.com/home/](https://admanager.google.com/home/)\n\n是一款广告管理平台，适合拥有大量直销业务的大型发布商使用。Ad Manager 可提供精细控制，并支持多个广告交易平台和广告联盟，包括 AdSense、Ad Exchange、第三方广告联盟和第三方广告交易平台等。\n\ngoogle ad manager旨在帮助媒体公司和发布商管理和优化他们的广告库存。它提供广告投放、定价、报告和监控功能，以及与广告交易相关的工具。\n\n### 3.Google系 AdSense（SSP变现）\n\n官网：[https://www.google.com/adsense](https://www.google.com/adsense)\n\nAdSense 是一个面向网站所有者的广告网络，它允许网站发布商在他们的网站上展示广告，并通过用户对这些广告的点击或查看来获得收入。AdSense使用自动化技术根据网站内容和访问者兴趣匹配广告，从而提供更相关的广告体验。\n\n比如我们可以利用AdSense给博客添加广告，参考：<!--more-->\n&nbsp;[Github博客接入谷歌广告AdSense](https://www.cnblogs.com/tonglin0325/p/4685818.html)\n\n### 3.Facebook系\n\n官网：[https://www.facebook.com/business/ads](https://www.facebook.com/business/ads)\n\n### 4.Amazon\n\n官网：[https://advertising.amazon.com/zh-cn](https://advertising.amazon.com/zh-cn)\n\n### 5.AppLovin（DSP投放+SSP变现）\n\n官网：[https://www.applovin.com/cn/](https://www.applovin.com/cn/)\n\n### 6.Unity\n\n官网：\n\n### 7.OpenX\n\n官网：[https://www.openx.com/](https://www.openx.com/)\n\n### 8.triplelift\n\n官网：[https://triplelift.com/](https://triplelift.com/)\n\n### 9.InMobi\n\n官网：[https://www.inmobi.cn/](https://www.inmobi.cn/)\n\n### 10.Smaato\n\n官网：[https://www.smaato.com/](https://www.smaato.com/)\n\n### 11.Magnite\n\n官网：[https://www.magnite.com/](https://www.magnite.com/)\n\n### 12.Liftoff\n\n官网：[https://liftoff.io/](https://liftoff.io/)\n\n### 13.PubMatic\n\n官网：[https://pubmatic.com/](https://pubmatic.com/)\n\n### 14.PulsePoint\n\n官网：[https://www.pulsepoint.com/](https://www.pulsepoint.com/)\n\n### 15.AppNexus（DSP投放+SSP变现）\n\n由Xandr（AT&amp;T子公司）旗下的广告技术公司提供的广告交易平台，允许广告主和媒体公司进行广告交易。\n\n官网：[https://docs.xandr.com/category/xandr-products](https://docs.xandr.com/category/xandr-products)\n\n### 16.LiveIntent\n\n官网：\n\n### 17.Index Exchange\n\n官网：\n\n还有很多其他的第三方广告平台\n\n```\nhttps://help.branch.io/zh/using-branch/docs/ad-partners-list\n\n```\n\n　　\n","tags":["广告系统"]},{"title":"Github博客接入谷歌广告AdSense","url":"/Github博客接入谷歌广告AdSense.html","content":"AdSense是Google GAM广告系统中的一个产品，可以在博客中接入Google AdSense来创造收入，下面是Google AdSense的官网\n\n```\nhttps://www.google.com/intl/zh-CN_cn/adsense/start/\n\n```\n\n<!--more-->\n&nbsp;<img src=\"/images/517519-20220913233111128-161097715.png\" width=\"1000\" height=\"493\" loading=\"lazy\" />\n\n## 1.注册AdSense账号\n\n登录后会提示你没有AdSense账号，这时选择注册一个\n\n<img src=\"/images/517519-20220914002155291-1971574863.png\" width=\"600\" height=\"231\" loading=\"lazy\" />\n\n## 2.填写信息\n\n填写你的网站，并开始使用AdSense\n\n<img src=\"/images/517519-20220914002752196-1710288552.png\" width=\"500\" height=\"774\" loading=\"lazy\" />\n\n进入Google AdSense页面，完善付款信息\n\n<img src=\"/images/517519-20220914003113243-2086443620.png\" width=\"800\" height=\"298\" loading=\"lazy\" />&nbsp;\n\n## 3.给网站添加AdSense代码\n\n点击获得代码\n\n<img src=\"/images/517519-20220914003347802-665176065.png\" width=\"800\" height=\"316\" loading=\"lazy\" />\n\n将会得到如下代码\n\n<img src=\"/images/517519-20220914003840863-1228347693.png\" width=\"600\" height=\"276\" loading=\"lazy\" />\n\n将其复制到博客的代码中，我是将其直接贴到了head.ejs中\n\n<img src=\"/images/517519-20220924222908582-342180148.png\" width=\"700\" height=\"260\" loading=\"lazy\" />\n\n完成后就可以再首页看到已经完成所有步骤\n\n<img src=\"/images/517519-20220914143952925-1762986566.png\" width=\"800\" height=\"317\" loading=\"lazy\" />\n\n## 4.设置广告\n\n设置广告按网站来展现，启用了自动广告，自动广告的话会自动将广告添加到网站合适的位置\n\n你可以在编辑中设置广告的样式等\n\n<img src=\"/images/517519-20220914144802451-635372767.png\" width=\"800\" height=\"351\" loading=\"lazy\" />\n\n如果不选择自动广告的话，还可以自行指定广告的广告位，比如添加到顶栏或者侧边栏等你要想展现广告的位置\n\n设置好后，查看网站广告的状态，会显示Google正在审核你的网站，等待审批通知后即可，这个可能会等待几天\n\n<img src=\"/images/517519-20220914092047692-800686515.png\" width=\"800\" height=\"275\" />\n\n## 5.查看广告\n\n等待了大概一周之后，审核通过的邮件来了\n\n<img src=\"/images/517519-20220924221611220-841341444.png\" width=\"500\" height=\"79\" loading=\"lazy\" />\n\n&nbsp;\n\n调整了一下自动广告的格式，只保留页内广告\n\n<img src=\"/images/517519-20220924222050083-1134830175.png\" width=\"300\" height=\"693\" loading=\"lazy\" />\n\n最终效果\n\n<img src=\"/images/517519-20220924222135980-81792572.png\" width=\"700\" height=\"618\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["广告系统"]},{"title":"Ubuntu16.04安装presto","url":"/Ubuntu16.04安装presto.html","content":"presto有2个社区，一个是PrestoDB（由Facebook员工维护，版本号是0.xxx），一个是PrestoSQL（由一些离开Facebook的Presto主力开发者维护，版本号是xxx，PrestoSQL 从版本 351 开始将其名称变更为 Trino）\n\n\n\n|社区版本|官网|安装包下载地址|版本号|安装文档\n| ---- | ---- | ---- | ---- | ---- \n|PrestoDB|https://prestodb.io/|https://repo1.maven.org/maven2/com/facebook/presto/|0.xxx（比如0.245）|https://prestodb.io/docs/current/installation/deployment.html#installing-presto\n|PrestoSQL（Trino）|https://trino.io/|https://repo1.maven.org/maven2/io/prestosql/|xxx（比如330）|https://trino.io/docs/current/installation/deployment.html\n\n## PrestoSQL安装步骤\n\n### 安装prestoSQL330\n\n#### 1.下载和安装\n\n330是presto最后一个支持java8的版本，高于330的版本需要java11的支持，且java8的版本最低要8u161，否则会报下面错误：ERROR: Presto requires Java 11+ (found 1.8.0_121) 以及 ERROR: Presto requires Java 8u161+ (found 1.8.0_121)\n\n```\nwget https://repo1.maven.org/maven2/io/prestosql/presto-server/330/presto-server-330.tar.gz\ntar -zxvf presto-server-330.tar.gz\n\n```\n\n创建presto用户\n\n```\nsudo groupadd presto\nsudo useradd presto -g presto -r --no-log-init -d /var/lib/presto\nsudo mkdir /var/lib/presto\nsudo mv ~/software/presto-server-330 /opt/cloudera/parcels\nsudo ln -s /opt/cloudera/parcels/presto-server-330 /opt/cloudera/parcels/presto\n\n```\n\n#### 2.配置文件\n\n在presto的安装目录下创建etc目录，以及若干配置文件\n\n```\nlintong@master:/opt/cloudera/parcels/presto$ ls\ncatalog  config.properties  jvm.config  log.properties  node.properties\nlintong@master:/opt/cloudera/parcels/presto$ ls ./catalog/\njmx.properties\n\n```\n\n具体配置文件内容请参考上面表格中列出的presto安装文档\n\n#### 3.启动和停止\n\n```\nsudo -iu presto\n$ cd /opt/cloudera/parcels/presto\n$ ./bin/launcher status\nNot running\n$ ./bin/launcher start\nStarted as 14252\n$ ./bin/launcher stop\nStopped 1425\n\n```\n\n<img src=\"/images/517519-20220917140050876-818842263.png\" width=\"800\" height=\"383\" loading=\"lazy\" />\n\n#### 4.集成CDH的hive\n\n由于CDH的hive开启了kerberos，且集成了LDAP，所以需要进行额外的配置，参考：[如何在CDH集群中部署Presto](https://cloud.tencent.com/developer/article/1158360) 以及 [presto安装和集成kerberos的hive](https://www.cnblogs.com/erlou96/p/14592749.html)\n\n同步presto用户和组的信息到ldap中，参考：[Ubuntu16.04安装openldap和phpldapadmin](https://www.cnblogs.com/tonglin0325/p/13661323.html)\n\n<img src=\"/images/517519-20220918205948699-1646626289.png\" width=\"300\" height=\"1018\" loading=\"lazy\" /><img src=\"/images/517519-20220918210025927-1847875947.png\" width=\"300\" height=\"660\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n然后在ldap中将presto添加到hive group当中，这样做的目的是使得presto用户拥有hive组的权限，从而在查询HDFS文件用户是hive用户的hive表的时候不会报如下错误\n\n```\nQuery failed (#20220918_133211_00216_x47mw): Error opening Hive split hdfs://master:8020/user/hive/warehouse/test_table/000000_0.lzo_deflate (offset=0, length=46) using org.apache.hadoop.mapred.TextInputFormat: Permission denied: user=presto, access=READ, inode=\"/user/hive/warehouse/test_table/000000_0.lzo_deflate\":hive:hive:-rwxrwx--x at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkAccessAcl(DefaultAuthorizationProvider.java:363) at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:256) at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkPermission(DefaultAuthorizationProvider.java:1 ...\n\n```\n\n<img src=\"/images/517519-20220918210131679-649832799.png\" width=\"1000\" height=\"397\" loading=\"lazy\" />\n\n然后在HUE中点击sync，从LDAP上同步用户和组，可以看到在LDAP上配置的用户和组的关系已经同步到了HUE上\n\n同步presto用户\n\n<img src=\"/images/517519-20220918210333615-529506285.png\" width=\"1000\" height=\"240\" loading=\"lazy\" />\n\n同步presto组\n\n<img src=\"/images/517519-20220918210400023-615471858.png\" width=\"1000\" height=\"223\" loading=\"lazy\" />\n\n由于hive启用了sentry，所以需要给presto用户配置一下hive表的权限，这里配置了所有hive表的权限\n\n<img src=\"/images/517519-20220918213042799-458201523.png\" width=\"800\" height=\"259\" loading=\"lazy\" />\n\n配置hive connection\n\n```\nlintong@master:/opt/cloudera/parcels/presto/etc/catalog$ cat hive.properties\nconnector.name=hive-hadoop2\nhive.metastore.uri=thrift://master:9083\nhive.metastore.authentication.type=KERBEROS\nhive.metastore.service.principal=hive/_HOST@HADOOP.COM\nhive.metastore.client.principal=presto/_HOST@HADOOP.COM\nhive.metastore.client.keytab=/var/lib/presto/presto.keytab\n\n```\n\n此时presto就可以正常查询hive表了，但是无论使用presto cli还是datagrip进行查询，最终的用户都是presto，如果要进一步的话，则需要集成ranger\n\n<img src=\"/images/517519-20220918211743709-1889361327.png\" width=\"800\" height=\"561\" loading=\"lazy\" />\n\n&nbsp;\n\n下载presto cli验证一下\n\n```\nwget https://repo1.maven.org/maven2/io/prestosql/presto-cli/330/presto-cli-330-executable.jar\nsudo chown presto:presto ./presto-cli-330-executable.jar\nsudo chmod +x presto-cli-330-executable.jar\nsudo -iu presto\n\n./presto-cli-330-executable.jar --server localhost:10019 --catalog=hive --schema=default\npresto:default> show tables;\n    Table\n--------------\n kst\n kst2\n kst2_parquet\n kst3\n test\n test1\n test2\n test_parquet\n test_table\n test_table2\n(10 rows)\n\nQuery 20220918_130558_00164_x47mw, FINISHED, 1 node\nSplits: 19 total, 19 done (100.00%)\n0:00 [10 rows, 240B] [31 rows/s, 752B/s]\n\n```\n\n也可以使用datagrip验证一下，可以正常查询hive表\n\n&nbsp;<img src=\"/images/517519-20220918211622692-423703942.png\" width=\"800\" height=\"407\" loading=\"lazy\" />\n\n&nbsp;\n\n### 安装prestoSQL333\n\n使用老版本是没有前途的，所以还是装了java11来使用prestoSQL333，参考：[Ranger+LDAP+Presto实现权限控制](https://blog.csdn.net/pandani/article/details/120771547)\n\n333要求java11，首先下载和解压java11，下载java11去oracle官网下载，需要注册oracle账号才可以下载\n\n```\nlintong@master:/usr/java$ ls | grep jdk-11\njdk-11.0.16.1\njdk-11.0.16.1_linux-x64_bin.tar.gz\n\n```\n\n给presto用户专门指定java11环境，配置.bash_profile和.bashrc文件\n\n```\npresto@master:~$ cat ~/.bash_profile\n# .bash_profile\n\n# Get the aliases and functions\nif [ -f ~/.bashrc ]; then\n        . ~/.bashrc\nfi\n\n# User specific environment and startup programs\n\nPATH=$PATH:$HOME/bin\n\nexport PATH\n\npresto@master:~$ cat ~/.bashrc\n# java\nexport JAVA_HOME=/usr/java/jdk-11.0.16.1\nexport JRE_HOME=${JAVA_HOME}/jre\nexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib\nexport PATH=${JAVA_HOME}/bin:$PATH\n\n```\n\n给presto用户指定解析器，否则bash_profile不会生效\n\n```\nsudo usermod -s /bin/bash presto\n\n```\n\n验证解析器是否添加成功\n\n```\nlintong@master:~$ cat /etc/passwd | grep presto\npresto:x:978:1011::/var/lib/presto:/bin/bash\n\n```\n\n验证切换到presto用户下的java版本\n\n```\npresto@master:~$ echo $JAVA_HOME\n/usr/java/jdk-11.0.16.1\n\n```\n\n　　\n\n&nbsp;\n\n## Amazon EMR Presto\n\n如果是在Amazon EMR中使用presto的话，可以在创建集群的时候选择是使用PrestoDB还是Trino（PrestoSQL），但不能在同一个集群上同时安装两者。如果在尝试创建集群时同时指定了 PrestoDB 和 Trino，则会发生验证错误，而且集群创建请求失败。\n\n创建emr6.3.0\n\n<img src=\"/images/517519-20220914161006650-195549206.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\n创建emr6.5.0\n\n<img src=\"/images/517519-20220914160348340-1192378928.png\" alt=\"\" loading=\"lazy\" />\n\n&nbsp;\n\nAmazon EMR和PrestoDB和Trino（PrestoSQL）的发行版对应关系见如下文档\n\n```\nhttps://docs.amazonaws.cn/emr/latest/ReleaseGuide/Presto-release-history.html\n\n```\n\n　　\n\n&nbsp;\n","tags":["presto"]},{"title":"在minikube下创建kafka集群","url":"/在minikube下创建kafka集群.html","content":"在minikube下安装的kafka集群分成4个步骤\n\n## 1.在mac上安装minikube\n\n这里安装的minikube是基于virtualbox的，也就是minikube是运行在virtualbox启动的一个虚拟机中\n\n参考：[Mac下安装minikube](https://www.cnblogs.com/tonglin0325/p/4584170.html)\n\n## 2.给zk和kafka创建local persistence volumn\n\n参考：[Helm 安装Kafka](https://www.jianshu.com/p/9307d5b95b0b)\n\nzk和kafka的数据需要落盘，所以需要依赖pv，这里创建的是k8s的local pv，注意如果volumeBindingMode选择WaitForFirstConsumer的话，只有在pod创建的时候，pvc才会绑定到pv上，没有pod就话pvc就一直是pending状态\n\nStorageClass的yaml，local-storage.yaml\n\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: local-storage\nprovisioner: kubernetes.io/no-provisioner\nvolumeBindingMode: Immediate\nreclaimPolicy: Retain\n\n```\n\n创建一个叫local-storage的StorageClass\n\n```\nkubectl apply -f ./local-storage.yaml\n\n```\n\n进入virtualbox的虚拟器中创建如下的linux目录，minikube的虚拟器账号密码是docker tcuser\n\n<img src=\"/images/517519-20220714153104124-1134513096.png\" width=\"800\" height=\"471\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n\n在local-storage的sc下面创建3个local的pv用于存储zookeeper的数据，zookeeper-local-pv.yaml\n\n```\nkubectl apply -f ./zookeeper-local-pv.yaml\n\n```\n\nyaml，其中minikube是k8s node的name，可以使用minikube get pod -A查看\n\n```\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: data-kafka-zookeeper-0\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n  - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: local-storage\n  local:\n    path: /tmp/zookeeper/data-0\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - minikube\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: data-kafka-zookeeper-1\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n  - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: local-storage\n  local:\n    path: /tmp/zookeeper/data-1\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - minikube\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: data-kafka-zookeeper-2\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n  - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: local-storage\n  local:\n    path: /tmp/zookeeper/data-2\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - minikube\n\n```\n\n创建zk的pvc\n\n```\nkubectl apply -f ./zookeeper-local-pvc.yaml\n\n```\n\nzookeeper-local-pvc.yaml\n\n```\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-kafka-zookeeper-0\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n  storageClassName: local-storage\n  volumeMode: Filesystem\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-kafka-zookeeper-1\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n  storageClassName: local-storage\n  volumeMode: Filesystem\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-kafka-zookeeper-2\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n  storageClassName: local-storage\n  volumeMode: Filesystem\n\n```\n\n　　\n\n在local-storage的sc下面创建3个local的pv用于存储kafka的数据，kafka-local-pv.yaml\n\n```\nkubectl apply -f ./kafka-local-pv.yaml\n\n```\n\nyaml，其中minikube是k8s node的name，可以使用minikube get pod -A查看\n\n```\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: data-kafka-0\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n  - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: local-storage\n  local:\n    path: /tmp/kafka/data-0\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - minikube\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: data-kafka-1\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n  - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: local-storage\n  local:\n    path: /tmp/kafka/data-1\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - minikube\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: data-kafka-2\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n  - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: local-storage\n  local:\n    path: /tmp/kafka/data-2\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - minikube\n\n```\n\n创建kafka的pvc\n\n```\nkubectl apply -f ./kafka-local-pvc.yaml\n\n```\n\nkafka-local-pvc.yaml\n\n```\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-kafka-0\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n  storageClassName: local-storage\n  volumeMode: Filesystem\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-kafka-1\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n  storageClassName: local-storage\n  volumeMode: Filesystem\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-kafka-2\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n  storageClassName: local-storage\n  volumeMode: Filesystem\n\n```\n\n查看pv，注意这里所有pv刚创建的status都是Available，如果创建了pvc，状态会变成Bound\n\n如果删除了pvc的话，pv的状态就会变成Released，这时这个pc已经无法被其他pvc绑定，需要edit这个pv，删除其中的claimRef配置\n\npv和pvc的各种状态可以参考文章：[Kubernetes 中 PV 和 PVC 的状态变化](https://www.qikqiak.com/post/status-in-pv-pvc/#:~:text=Available%20manual%207s-,%E6%96%B0%E5%BB%BAPVC,%E4%BF%9D%E8%AF%81%E7%9C%8B%E5%88%B0Pending%20%E7%8A%B6%E6%80%81%E3%80%82)\n\n参考：[https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-persistent-volume-storage/](https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-persistent-volume-storage/)\n\n```\nkubectl get pv -A\nNAME                     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                            STORAGECLASS    REASON   AGE\ndata-kafka-zookeeper-0   5Gi        RWO            Retain           Bound    default/data-kafka-zookeeper-1   local-storage            10h\ndata-kafka-zookeeper-1   5Gi        RWO            Retain           Bound    default/data-kafka-zookeeper-0   local-storage            10h\ndata-kafka-zookeeper-2   5Gi        RWO            Retain           Bound    default/data-kafka-zookeeper-2   local-storage            10h\ndatadir-kafka-0          5Gi        RWO            Retain           Bound    default/data-kafka-0             local-storage            10h\ndatadir-kafka-1          5Gi        RWO            Retain           Bound    default/data-kafka-1             local-storage            10h\ndatadir-kafka-2          5Gi        RWO            Retain           Bound    default/data-kafka-2             local-storage            10h\n\n```\n\n　　　　\n\n## 3.下载&nbsp;bitnami/kafka的chart，并使用helm来部署kafka集群\n\n添加 repo\n\n```\nhelm repo add incubator https://charts.helm.sh/incubator\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n\n```\n\n下载chart\n\n```\nhelm fetch bitnami/kafka\n\n```\n\n这边下载最新版本的chart是kafka-15.3.2.tgz\n\n最新的chart版本和对应的kafka版本可以去网站查看\n\n```\nhttps://artifacthub.io/packages/helm/bitnami/kafka\n\n```\n\n或者使用search命令查看可以下载的版本\n\n```\nhelm search repo bitnami/kafka -l\nNAME         \tCHART VERSION\tAPP VERSION\tDESCRIPTION\nbitnami/kafka\t25.3.1       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t25.3.0       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t25.2.0       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t25.1.12      \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t25.1.11      \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t25.1.10      \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t25.1.9       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t25.1.8       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t25.1.7       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t25.1.6       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t25.1.5       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t25.1.4       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t25.1.3       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t25.1.2       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t25.1.1       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t25.1.0       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t25.0.1       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t25.0.0       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t24.0.14      \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t24.0.13      \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t24.0.12      \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t24.0.11      \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t24.0.10      \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t24.0.9       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t24.0.8       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t24.0.7       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t24.0.6       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t24.0.5       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t24.0.4       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t24.0.3       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t24.0.2       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t24.0.1       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t24.0.0       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t23.0.7       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t23.0.6       \t3.5.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t23.0.5       \t3.5.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t23.0.4       \t3.5.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t23.0.3       \t3.5.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t23.0.2       \t3.5.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t23.0.1       \t3.5.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t23.0.0       \t3.5.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t22.1.6       \t3.4.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t22.1.5       \t3.4.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t22.1.4       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t22.1.3       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t22.1.2       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t22.1.1       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t22.0.3       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t22.0.2       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t22.0.1       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t22.0.0       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t21.4.6       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t21.4.5       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t21.4.4       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t21.4.3       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t21.4.2       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t21.4.1       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t21.4.0       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t21.3.1       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t21.3.0       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t21.2.0       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t21.1.1       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t21.1.0       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t21.0.1       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t21.0.0       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t20.1.1       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t20.1.0       \t3.4.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t20.0.6       \t3.3.2      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t20.0.5       \t3.3.2      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t20.0.4       \t3.3.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t20.0.3       \t3.3.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t20.0.2       \t3.3.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t20.0.1       \t3.3.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t20.0.0       \t3.3.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t19.1.5       \t3.3.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t19.1.4       \t3.3.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t19.1.3       \t3.3.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t19.1.2       \t3.3.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t19.1.1       \t3.3.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t19.1.0       \t3.3.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t19.0.2       \t3.3.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t19.0.1       \t3.3.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t19.0.0       \t3.3.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t18.5.0       \t3.2.3      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t18.4.4       \t3.2.3      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t18.4.3       \t3.2.3      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t18.4.2       \t3.2.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t18.4.1       \t3.2.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t18.4.0       \t3.2.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t18.3.1       \t3.2.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t18.3.0       \t3.2.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t18.2.0       \t3.2.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t18.1.3       \t3.2.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t18.1.2       \t3.2.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t18.1.1       \t3.2.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t18.0.8       \t3.2.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t18.0.7       \t3.2.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t18.0.6       \t3.2.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t18.0.5       \t3.2.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t18.0.4       \t3.2.1      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t18.0.3       \t3.2.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t18.0.2       \t3.2.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t18.0.0       \t3.2.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t17.2.6       \t3.2.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t17.2.5       \t3.2.0      \tApache Kafka is a distributed streaming platfor...\nbitnami/kafka\t17.2.3       \t3.2.0      \tApache Kafka is a distributed streaming platfor...\n\n```\n\n如果要指定版本的话，可以使用如下命令\n\n```\nhelm fetch bitnami/kafka --version 17.2.3\n\n```\n\n修改value.yaml，主要是修改了kafka和zk的persistence配置，添加了local-storage存储\n\n以及service的type改成了ClusterIP，并配置了在宿主机上暴露的端口\n\n参考：[k8s 集群暴露 kafka 端口&nbsp;](https://www.lixianyang.xyz/posts/bitnami-chart-kafka-%E6%B7%BB%E5%8A%A0%E8%AE%A4%E8%AF%81/)\n\n```\n## @section Global parameters\n## Global Docker image parameters\n## Please, note that this will override the image parameters, including dependencies, configured to use the global value\n## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass\n\n## @param global.imageRegistry Global Docker image registry\n## @param global.imagePullSecrets Global Docker registry secret names as an array\n## @param global.storageClass Global StorageClass for Persistent Volume(s)\n##\nglobal:\n  imageRegistry: \"\"\n  ## E.g.\n  ## imagePullSecrets:\n  ##   - myRegistryKeySecretName\n  ##\n  imagePullSecrets: []\n  storageClass: \"\"\n\n## @section Common parameters\n\n## @param kubeVersion Override Kubernetes version\n##\nkubeVersion: \"\"\n## @param nameOverride String to partially override common.names.fullname\n##\nnameOverride: \"\"\n## @param fullnameOverride String to fully override common.names.fullname\n##\nfullnameOverride: \"\"\n## @param clusterDomain Default Kubernetes cluster domain\n##\nclusterDomain: cluster.local\n## @param commonLabels Labels to add to all deployed objects\n##\ncommonLabels: {}\n## @param commonAnnotations Annotations to add to all deployed objects\n##\ncommonAnnotations: {}\n## @param extraDeploy Array of extra objects to deploy with the release\n##\nextraDeploy: []\n## Enable diagnostic mode in the statefulset\n##\ndiagnosticMode:\n  ## @param diagnosticMode.enabled Enable diagnostic mode (all probes will be disabled and the command will be overridden)\n  ##\n  enabled: false\n  ## @param diagnosticMode.command Command to override all containers in the statefulset\n  ##\n  command:\n    - sleep\n  ## @param diagnosticMode.args Args to override all containers in the statefulset\n  ##\n  args:\n    - infinity\n\n## @section Kafka parameters\n\n## Bitnami Kafka image version\n## ref: https://hub.docker.com/r/bitnami/kafka/tags/\n## @param image.registry Kafka image registry\n## @param image.repository Kafka image repository\n## @param image.tag Kafka image tag (immutable tags are recommended)\n## @param image.pullPolicy Kafka image pull policy\n## @param image.pullSecrets Specify docker-registry secret names as an array\n## @param image.debug Specify if debug values should be set\n##\nimage:\n  registry: docker.io\n  repository: bitnami/kafka\n  tag: 3.1.0-debian-10-r20\n  ## Specify a imagePullPolicy\n  ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n  ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n  ##\n  pullPolicy: IfNotPresent\n  ## Optionally specify an array of imagePullSecrets.\n  ## Secrets must be manually created in the namespace.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  ## e.g:\n  ## pullSecrets:\n  ##   - myRegistryKeySecretName\n  ##\n  pullSecrets: []\n  ## Set to true if you would like to see extra information on logs\n  ##\n  debug: false\n## @param config Configuration file for Kafka. Auto-generated based on other parameters when not specified\n## Specify content for server.properties\n## NOTE: This will override any KAFKA_CFG_ environment variables (including those set by the chart)\n## The server.properties is auto-generated based on other parameters when this parameter is not specified\n## e.g:\n## config: |-\n##   broker.id=-1\n##   listeners=PLAINTEXT://:9092\n##   advertised.listeners=PLAINTEXT://KAFKA_IP:9092\n##   num.network.threads=3\n##   num.io.threads=8\n##   socket.send.buffer.bytes=102400\n##   socket.receive.buffer.bytes=102400\n##   socket.request.max.bytes=104857600\n##   log.dirs=/bitnami/kafka/data\n##   num.partitions=1\n##   num.recovery.threads.per.data.dir=1\n##   offsets.topic.replication.factor=1\n##   transaction.state.log.replication.factor=1\n##   transaction.state.log.min.isr=1\n##   log.flush.interval.messages=10000\n##   log.flush.interval.ms=1000\n##   log.retention.hours=168\n##   log.retention.bytes=1073741824\n##   log.segment.bytes=1073741824\n##   log.retention.check.interval.ms=300000\n##   zookeeper.connect=ZOOKEEPER_SERVICE_NAME\n##   zookeeper.connection.timeout.ms=6000\n##   group.initial.rebalance.delay.ms=0\n##\nconfig: \"\"\n## @param existingConfigmap ConfigMap with Kafka Configuration\n## NOTE: This will override `config` AND any KAFKA_CFG_ environment variables\n##\nexistingConfigmap: \"\"\n## @param log4j An optional log4j.properties file to overwrite the default of the Kafka brokers\n## An optional log4j.properties file to overwrite the default of the Kafka brokers\n## ref: https://github.com/apache/kafka/blob/trunk/config/log4j.properties\n##\nlog4j: \"\"\n## @param existingLog4jConfigMap The name of an existing ConfigMap containing a log4j.properties file\n## The name of an existing ConfigMap containing a log4j.properties file\n## NOTE: this will override `log4j`\n##\nexistingLog4jConfigMap: \"\"\n## @param heapOpts Kafka Java Heap size\n##\nheapOpts: -Xmx1024m -Xms1024m\n## @param deleteTopicEnable Switch to enable topic deletion or not\n##\ndeleteTopicEnable: false\n## @param autoCreateTopicsEnable Switch to enable auto creation of topics. Enabling auto creation of topics not recommended for production or similar environments\n##\nautoCreateTopicsEnable: true\n## @param logFlushIntervalMessages The number of messages to accept before forcing a flush of data to disk\n##\nlogFlushIntervalMessages: _10000\n## @param logFlushIntervalMs The maximum amount of time a message can sit in a log before we force a flush\n##\nlogFlushIntervalMs: 1000\n## @param logRetentionBytes A size-based retention policy for logs\n##\nlogRetentionBytes: _1073741824\n## @param logRetentionCheckIntervalMs The interval at which log segments are checked to see if they can be deleted\n##\nlogRetentionCheckIntervalMs: 300000\n## @param logRetentionHours The minimum age of a log file to be eligible for deletion due to age\n##\nlogRetentionHours: 168\n## @param logSegmentBytes The maximum size of a log segment file. When this size is reached a new log segment will be created\n##\nlogSegmentBytes: _1073741824\n## @param logsDirs A comma separated list of directories under which to store log files\n##\nlogsDirs: /bitnami/kafka/data\n## @param maxMessageBytes The largest record batch size allowed by Kafka\n##\nmaxMessageBytes: _1000012\n## @param defaultReplicationFactor Default replication factors for automatically created topics\n##\ndefaultReplicationFactor: 1\n## @param offsetsTopicReplicationFactor The replication factor for the offsets topic\n##\noffsetsTopicReplicationFactor: 1\n## @param transactionStateLogReplicationFactor The replication factor for the transaction topic\n##\ntransactionStateLogReplicationFactor: 1\n## @param transactionStateLogMinIsr Overridden min.insync.replicas config for the transaction topic\n##\ntransactionStateLogMinIsr: 1\n## @param numIoThreads The number of threads doing disk I/O\n##\nnumIoThreads: 8\n## @param numNetworkThreads The number of threads handling network requests\n##\nnumNetworkThreads: 3\n## @param numPartitions The default number of log partitions per topic\n##\nnumPartitions: 1\n## @param numRecoveryThreadsPerDataDir The number of threads per data directory to be used for log recovery at startup and flushing at shutdown\n##\nnumRecoveryThreadsPerDataDir: 1\n## @param socketReceiveBufferBytes The receive buffer (SO_RCVBUF) used by the socket server\n##\nsocketReceiveBufferBytes: 102400\n## @param socketRequestMaxBytes The maximum size of a request that the socket server will accept (protection against OOM)\n##\nsocketRequestMaxBytes: _104857600\n## @param socketSendBufferBytes The send buffer (SO_SNDBUF) used by the socket server\n##\nsocketSendBufferBytes: 102400\n## @param zookeeperConnectionTimeoutMs Timeout in ms for connecting to ZooKeeper\n##\nzookeeperConnectionTimeoutMs: 6000\n## @param zookeeperChrootPath Path which puts data under some path in the global ZooKeeper namespace\n## ref: https://kafka.apache.org/documentation/#brokerconfigs_zookeeper.connect\n##\nzookeeperChrootPath: \"\"\n## @param authorizerClassName The Authorizer is configured by setting authorizer.class.name=kafka.security.authorizer.AclAuthorizer in server.properties\n##\nauthorizerClassName: \"\"\n## @param allowEveryoneIfNoAclFound By default, if a resource has no associated ACLs, then no one is allowed to access that resource except super users\n##\nallowEveryoneIfNoAclFound: true\n## @param superUsers You can add super users in server.properties\n##\nsuperUsers: User:admin\n## Authentication parameters\n## https://github.com/bitnami/bitnami-docker-kafka#security\n##\nauth:\n  ## Authentication protocol for client and inter-broker communications\n  ## This table shows the security provided on each protocol:\n  ## | Method    | Authentication                | Encryption via TLS |\n  ## | plaintext | None                          | No                 |\n  ## | tls       | None                          | Yes                |\n  ## | mtls      | Yes (two-way authentication)  | Yes                |\n  ## | sasl      | Yes (via SASL)                | No                 |\n  ## | sasl_tls  | Yes (via SASL)                | Yes                |\n  ## @param auth.clientProtocol Authentication protocol for communications with clients. Allowed protocols: `plaintext`, `tls`, `mtls`, `sasl` and `sasl_tls`\n  ## @param auth.externalClientProtocol Authentication protocol for communications with external clients. Defaults to value of `auth.clientProtocol`. Allowed protocols: `plaintext`, `tls`, `mtls`, `sasl` and `sasl_tls`\n  ## @param auth.interBrokerProtocol Authentication protocol for inter-broker communications. Allowed protocols: `plaintext`, `tls`, `mtls`, `sasl` and `sasl_tls`\n  ##\n  clientProtocol: plaintext\n  # Note: empty by default for backwards compatibility reasons, find more information at\n  # https://github.com/bitnami/charts/pull/8902/\n  externalClientProtocol: \"\"\n  interBrokerProtocol: plaintext\n  ## SASL configuration\n  ##\n  sasl:\n    ## @param auth.sasl.mechanisms SASL mechanisms when either `auth.interBrokerProtocol`, `auth.clientProtocol` or `auth.externalClientProtocol` are `sasl`. Allowed types: `plain`, `scram-sha-256`, `scram-sha-512`\n    ##\n    mechanisms: plain,scram-sha-256,scram-sha-512\n    ## @param auth.sasl.interBrokerMechanism SASL mechanism for inter broker communication.\n    ##\n    interBrokerMechanism: plain\n    ## JAAS configuration for SASL authentication.\n    ##\n    jaas:\n      ## @param auth.sasl.jaas.clientUsers Kafka client user list\n      ##\n      ## clientUsers:\n      ##   - user1\n      ##   - user2\n      ##\n      clientUsers:\n        - user\n      ## @param auth.sasl.jaas.clientPasswords Kafka client passwords. This is mandatory if more than one user is specified in clientUsers\n      ##\n      ## clientPasswords:\n      ##   - password1\n      ##   - password2\"\n      ##\n      clientPasswords: []\n      ## @param auth.sasl.jaas.interBrokerUser Kafka inter broker communication user for SASL authentication\n      ##\n      interBrokerUser: admin\n      ## @param auth.sasl.jaas.interBrokerPassword Kafka inter broker communication password for SASL authentication\n      ##\n      interBrokerPassword: \"\"\n      ## @param auth.sasl.jaas.zookeeperUser Kafka ZooKeeper user for SASL authentication\n      ##\n      zookeeperUser: \"\"\n      ## @param auth.sasl.jaas.zookeeperPassword Kafka ZooKeeper password for SASL authentication\n      ##\n      zookeeperPassword: \"\"\n      ## @param auth.sasl.jaas.existingSecret Name of the existing secret containing credentials for clientUsers, interBrokerUser and zookeeperUser\n      ## Create this secret running the command below where SECRET_NAME is the name of the secret you want to create:\n      ##       kubectl create secret generic SECRET_NAME --from-literal=client-passwords=CLIENT_PASSWORD1,CLIENT_PASSWORD2 --from-literal=inter-broker-password=INTER_BROKER_PASSWORD --from-literal=zookeeper-password=ZOOKEEPER_PASSWORD\n      ##\n      existingSecret: \"\"\n  ## TLS configuration\n  ##\n  tls:\n    ## @param auth.tls.type Format to use for TLS certificates. Allowed types: `jks` and `pem`\n    ##\n    type: jks\n    ## @param auth.tls.existingSecrets Array existing secrets containing the TLS certificates for the Kafka brokers\n    ## When using 'jks' format for certificates, each secret should contain a truststore and a keystore.\n    ## Create these secrets following the steps below:\n    ## 1) Generate your truststore and keystore files. Helpful script: https://raw.githubusercontent.com/confluentinc/confluent-platform-security-tools/master/kafka-generate-ssl.sh\n    ## 2) Rename your truststore to `kafka.truststore.jks`.\n    ## 3) Rename your keystores to `kafka-X.keystore.jks` where X is the ID of each Kafka broker.\n    ## 4) Run the command below one time per broker to create its associated secret (SECRET_NAME_X is the name of the secret you want to create):\n    ##       kubectl create secret generic SECRET_NAME_0 --from-file=kafka.truststore.jks=./kafka.truststore.jks --from-file=kafka.keystore.jks=./kafka-0.keystore.jks\n    ##       kubectl create secret generic SECRET_NAME_1 --from-file=kafka.truststore.jks=./kafka.truststore.jks --from-file=kafka.keystore.jks=./kafka-1.keystore.jks\n    ##       ...\n    ##\n    ## When using 'pem' format for certificates, each secret should contain a public CA certificate, a public certificate and one private key.\n    ## Create these secrets following the steps below:\n    ## 1) Create a certificate key and signing request per Kafka broker, and sign the signing request with your CA\n    ## 2) Rename your CA file to `kafka.ca.crt`.\n    ## 3) Rename your certificates to `kafka-X.tls.crt` where X is the ID of each Kafka broker.\n    ## 3) Rename your keys to `kafka-X.tls.key` where X is the ID of each Kafka broker.\n    ## 4) Run the command below one time per broker to create its associated secret (SECRET_NAME_X is the name of the secret you want to create):\n    ##       kubectl create secret generic SECRET_NAME_0 --from-file=ca.crt=./kafka.ca.crt --from-file=tls.crt=./kafka-0.tls.crt --from-file=tls.key=./kafka-0.tls.key\n    ##       kubectl create secret generic SECRET_NAME_1 --from-file=ca.crt=./kafka.ca.crt --from-file=tls.crt=./kafka-1.tls.crt --from-file=tls.key=./kafka-1.tls.key\n    ##       ...\n    ##\n    existingSecrets: []\n    ## @param auth.tls.autoGenerated Generate automatically self-signed TLS certificates for Kafka brokers. Currently only supported if `auth.tls.type` is `pem`\n    ## Note: ignored when using 'jks' format or `auth.tls.existingSecrets` is not empty\n    ##\n    autoGenerated: false\n    ## @param auth.tls.password Password to access the JKS files or PEM key when they are password-protected.\n    ## Note: ignored when using 'existingSecret'.\n    ##\n    password: \"\"\n    ## @param auth.tls.existingSecret Name of the secret containing the password to access the JKS files or PEM key when they are password-protected. (`key`: `password`)\n    ##\n    existingSecret: \"\"\n    ## @param auth.tls.jksTruststoreSecret Name of the existing secret containing your truststore if truststore not existing or different from the ones in the `auth.tls.existingSecrets`\n    ## Note: ignored when using 'pem' format for certificates.\n    ##\n    jksTruststoreSecret: \"\"\n    ## @param auth.tls.jksKeystoreSAN The secret key from the `auth.tls.existingSecrets` containing the keystore with a SAN certificate\n    ## The SAN certificate in it should be issued with Subject Alternative Names for all headless services:\n    ##  - kafka-0.kafka-headless.kafka.svc.cluster.local\n    ##  - kafka-1.kafka-headless.kafka.svc.cluster.local\n    ##  - kafka-2.kafka-headless.kafka.svc.cluster.local\n    ## Note: ignored when using 'pem' format for certificates.\n    ##\n    jksKeystoreSAN: \"\"\n    ## @param auth.tls.jksTruststore The secret key from the `auth.tls.existingSecrets` or `auth.tls.jksTruststoreSecret` containing the truststore\n    ## Note: ignored when using 'pem' format for certificates.\n    ##\n    jksTruststore: \"\"\n    ## @param auth.tls.endpointIdentificationAlgorithm The endpoint identification algorithm to validate server hostname using server certificate\n    ## Disable server host name verification by setting it to an empty string.\n    ## ref: https://docs.confluent.io/current/kafka/authentication_ssl.html#optional-settings\n    ##\n    endpointIdentificationAlgorithm: https\n## @param listeners The address(es) the socket server listens on. Auto-calculated it's set to an empty array\n## When it's set to an empty array, the listeners will be configured\n## based on the authentication protocols (auth.clientProtocol, auth.externalClientProtocol and auth.interBrokerProtocol parameters)\n##\nlisteners: []\n## @param advertisedListeners The address(es) (hostname:port) the broker will advertise to producers and consumers. Auto-calculated it's set to an empty array\n## When it's set to an empty array, the advertised listeners will be configured\n## based on the authentication protocols (auth.clientProtocol, auth.externalClientProtocol and auth.interBrokerProtocol parameters)\n##\nadvertisedListeners: []\n## @param listenerSecurityProtocolMap The protocol->listener mapping. Auto-calculated it's set to nil\n## When it's nil, the listeners will be configured based on the authentication protocols (auth.clientProtocol, auth.externalClientProtocol and auth.interBrokerProtocol parameters)\n##\nlistenerSecurityProtocolMap: \"\"\n## @param allowPlaintextListener Allow to use the PLAINTEXT listener\n##\nallowPlaintextListener: true\n## @param interBrokerListenerName The listener that the brokers should communicate on\n##\ninterBrokerListenerName: INTERNAL\n## @param command Override Kafka container command\n##\ncommand:\n  - /scripts/setup.sh\n## @param args Override Kafka container arguments\n##\nargs: []\n## @param extraEnvVars Extra environment variables to add to Kafka pods\n## ref: https://github.com/bitnami/bitnami-docker-kafka#configuration\n## e.g:\n## extraEnvVars:\n##   - name: KAFKA_CFG_BACKGROUND_THREADS\n##     value: \"10\"\n##\nextraEnvVars: []\n## @param extraEnvVarsCM ConfigMap with extra environment variables\n##\nextraEnvVarsCM: \"\"\n## @param extraEnvVarsSecret Secret with extra environment variables\n##\nextraEnvVarsSecret: \"\"\n\n## @section Statefulset parameters\n\n## @param replicaCount Number of Kafka nodes\n##\nreplicaCount: 3\n## @param minBrokerId Minimal broker.id value, nodes increment their `broker.id` respectively\n## Brokers increment their ID starting at this minimal value.\n## E.g., with `minBrokerId=100` and 3 nodes, IDs will be 100, 101, 102 for brokers 0, 1, and 2, respectively.\n##\nminBrokerId: 0\n## @param containerPorts.client Kafka client container port\n## @param containerPorts.internal Kafka inter-broker container port\n## @param containerPorts.external Kafka external container port\n##\ncontainerPorts:\n  client: 9092\n  internal: 9093\n  external: 9094\n## Configure extra options for Kafka containers' liveness, readiness and startup probes\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes\n## @param livenessProbe.enabled Enable livenessProbe on Kafka containers\n## @param livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe\n## @param livenessProbe.periodSeconds Period seconds for livenessProbe\n## @param livenessProbe.timeoutSeconds Timeout seconds for livenessProbe\n## @param livenessProbe.failureThreshold Failure threshold for livenessProbe\n## @param livenessProbe.successThreshold Success threshold for livenessProbe\n##\nlivenessProbe:\n  enabled: true\n  initialDelaySeconds: 10\n  timeoutSeconds: 5\n  failureThreshold: 3\n  periodSeconds: 10\n  successThreshold: 1\n## @param readinessProbe.enabled Enable readinessProbe on Kafka containers\n## @param readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe\n## @param readinessProbe.periodSeconds Period seconds for readinessProbe\n## @param readinessProbe.timeoutSeconds Timeout seconds for readinessProbe\n## @param readinessProbe.failureThreshold Failure threshold for readinessProbe\n## @param readinessProbe.successThreshold Success threshold for readinessProbe\n##\nreadinessProbe:\n  enabled: true\n  initialDelaySeconds: 5\n  failureThreshold: 6\n  timeoutSeconds: 5\n  periodSeconds: 10\n  successThreshold: 1\n## @param startupProbe.enabled Enable startupProbe on Kafka containers\n## @param startupProbe.initialDelaySeconds Initial delay seconds for startupProbe\n## @param startupProbe.periodSeconds Period seconds for startupProbe\n## @param startupProbe.timeoutSeconds Timeout seconds for startupProbe\n## @param startupProbe.failureThreshold Failure threshold for startupProbe\n## @param startupProbe.successThreshold Success threshold for startupProbe\n##\nstartupProbe:\n  enabled: false\n  initialDelaySeconds: 30\n  periodSeconds: 10\n  timeoutSeconds: 1\n  failureThreshold: 15\n  successThreshold: 1\n## @param customLivenessProbe Custom livenessProbe that overrides the default one\n##\ncustomLivenessProbe: {}\n## @param customReadinessProbe Custom readinessProbe that overrides the default one\n##\ncustomReadinessProbe: {}\n## @param customStartupProbe Custom startupProbe that overrides the default one\n##\ncustomStartupProbe: {}\n## @param lifecycleHooks lifecycleHooks for the Kafka container to automate configuration before or after startup\n##\nlifecycleHooks: {}\n## Kafka resource requests and limits\n## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n## @param resources.limits The resources limits for the container\n## @param resources.requests The requested resources for the container\n##\nresources:\n  limits: {}\n  requests: {}\n## Kafka pods' Security Context\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod\n## @param podSecurityContext.enabled Enable security context for the pods\n## @param podSecurityContext.fsGroup Set Kafka pod's Security Context fsGroup\n##\npodSecurityContext:\n  enabled: true\n  fsGroup: 1001\n## Kafka containers' Security Context\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n## @param containerSecurityContext.enabled Enable Kafka containers' Security Context\n## @param containerSecurityContext.runAsUser Set Kafka containers' Security Context runAsUser\n## @param containerSecurityContext.runAsNonRoot Set Kafka containers' Security Context runAsNonRoot\n## e.g:\n##   containerSecurityContext:\n##     enabled: true\n##     capabilities:\n##       drop: [\"NET_RAW\"]\n##     readOnlyRootFilesystem: true\n##\ncontainerSecurityContext:\n  enabled: true\n  runAsUser: 1001\n  runAsNonRoot: true\n## @param hostAliases Kafka pods host aliases\n## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/\n##\nhostAliases: []\n## @param hostNetwork Specify if host network should be enabled for Kafka pods\n##\nhostNetwork: false\n## @param hostIPC Specify if host IPC should be enabled for Kafka pods\n##\nhostIPC: false\n## @param podLabels Extra labels for Kafka pods\n## Ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n##\npodLabels: {}\n## @param podAnnotations Extra annotations for Kafka pods\n## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\n##\npodAnnotations: {}\n## @param podAffinityPreset Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n##\npodAffinityPreset: \"\"\n## @param podAntiAffinityPreset Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n##\npodAntiAffinityPreset: soft\n## Node affinity preset\n## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity\n##\nnodeAffinityPreset:\n  ## @param nodeAffinityPreset.type Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n  ##\n  type: \"\"\n  ## @param nodeAffinityPreset.key Node label key to match Ignored if `affinity` is set.\n  ## E.g.\n  ## key: \"kubernetes.io/e2e-az-name\"\n  ##\n  key: \"\"\n  ## @param nodeAffinityPreset.values Node label values to match. Ignored if `affinity` is set.\n  ## E.g.\n  ## values:\n  ##   - e2e-az1\n  ##   - e2e-az2\n  ##\n  values: []\n## @param affinity Affinity for pod assignment\n## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set\n##\naffinity: {}\n## @param nodeSelector Node labels for pod assignment\n## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n##\nnodeSelector: {}\n## @param tolerations Tolerations for pod assignment\n## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n##\ntolerations: []\n## @param topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains. Evaluated as a template\n## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods\n##\ntopologySpreadConstraints: {}\n## @param terminationGracePeriodSeconds Seconds the pod needs to gracefully terminate\n## ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#hook-handler-execution\n##\nterminationGracePeriodSeconds: \"\"\n## @param podManagementPolicy StatefulSet controller supports relax its ordering guarantees while preserving its uniqueness and identity guarantees. There are two valid pod management policies: OrderedReady and Parallel\n## ref: https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy\n##\npodManagementPolicy: Parallel\n## @param priorityClassName Name of the existing priority class to be used by kafka pods\n## Ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/\n##\npriorityClassName: \"\"\n## @param schedulerName Name of the k8s scheduler (other than default)\n## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n##\nschedulerName: \"\"\n## @param updateStrategy.type Kafka statefulset strategy type\n## @param updateStrategy.rollingUpdate Kafka statefulset rolling update configuration parameters\n## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies\n##\nupdateStrategy:\n  type: RollingUpdate\n  rollingUpdate: {}\n## @param extraVolumes Optionally specify extra list of additional volumes for the Kafka pod(s)\n## e.g:\n## extraVolumes:\n##   - name: kafka-jaas\n##     secret:\n##       secretName: kafka-jaas\n##\nextraVolumes: []\n## @param extraVolumeMounts Optionally specify extra list of additional volumeMounts for the Kafka container(s)\n## extraVolumeMounts:\n##   - name: kafka-jaas\n##     mountPath: /bitnami/kafka/config/kafka_jaas.conf\n##     subPath: kafka_jaas.conf\n##\nextraVolumeMounts: []\n## @param sidecars Add additional sidecar containers to the Kafka pod(s)\n## e.g:\n## sidecars:\n##   - name: your-image-name\n##     image: your-image\n##     imagePullPolicy: Always\n##     ports:\n##       - name: portname\n##         containerPort: 1234\n##\nsidecars: []\n## @param initContainers Add additional Add init containers to the Kafka pod(s)\n## e.g:\n## initContainers:\n##   - name: your-image-name\n##     image: your-image\n##     imagePullPolicy: Always\n##     ports:\n##       - name: portname\n##         containerPort: 1234\n##\ninitContainers: []\n## Kafka Pod Disruption Budget\n## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\n## @param pdb.create Deploy a pdb object for the Kafka pod\n## @param pdb.minAvailable Maximum number/percentage of unavailable Kafka replicas\n## @param pdb.maxUnavailable Maximum number/percentage of unavailable Kafka replicas\n##\npdb:\n  create: false\n  minAvailable: \"\"\n  maxUnavailable: 1\n\n## @section Traffic Exposure parameters\n\n## Service parameters\n##\nservice:\n  ## @param service.type Kubernetes Service type\n  ##\n  type: ClusterIP\n  ## @param service.ports.client Kafka svc port for client connections\n  ## @param service.ports.internal Kafka svc port for inter-broker connections\n  ## @param service.ports.external Kafka svc port for external connections\n  ##\n  ports:\n    client: 9092\n    internal: 9093\n    external: 9094\n  ## @param service.nodePorts.client Node port for the Kafka client connections\n  ## @param service.nodePorts.external Node port for the Kafka external connections\n  ## NOTE: choose port between <30000-32767>\n  ##\n  nodePorts:\n    client: \"\"\n    external: \"\"\n  ## @param service.sessionAffinity Control where client requests go, to the same pod or round-robin\n  ## Values: ClientIP or None\n  ## ref: https://kubernetes.io/docs/user-guide/services/\n  ##\n  sessionAffinity: None\n  ## @param service.clusterIP Kafka service Cluster IP\n  ## e.g.:\n  ## clusterIP: None\n  ##\n  clusterIP: \"\"\n  ## @param service.loadBalancerIP Kafka service Load Balancer IP\n  ## ref: https://kubernetes.io/docs/user-guide/services/#type-loadbalancer\n  ##\n  loadBalancerIP: \"\"\n  ## @param service.loadBalancerSourceRanges Kafka service Load Balancer sources\n  ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service\n  ## e.g:\n  ## loadBalancerSourceRanges:\n  ##   - 10.10.10.0/24\n  ##\n  loadBalancerSourceRanges: []\n  ## @param service.externalTrafficPolicy Kafka service external traffic policy\n  ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip\n  ##\n  externalTrafficPolicy: Cluster\n  ## @param service.annotations Additional custom annotations for Kafka service\n  ##\n  annotations: {}\n  ## @param service.extraPorts Extra ports to expose in the Kafka service (normally used with the `sidecar` value)\n  ##\n  extraPorts: []\n## External Access to Kafka brokers configuration\n##\nexternalAccess:\n  ## @param externalAccess.enabled Enable Kubernetes external cluster access to Kafka brokers\n  ##\n  enabled: true\n  ## External IPs auto-discovery configuration\n  ## An init container is used to auto-detect LB IPs or node ports by querying the K8s API\n  ## Note: RBAC might be required\n  ##\n  autoDiscovery:\n    ## @param externalAccess.autoDiscovery.enabled Enable using an init container to auto-detect external IPs/ports by querying the K8s API\n    ##\n    enabled: false\n    ## Bitnami Kubectl image\n    ## ref: https://hub.docker.com/r/bitnami/kubectl/tags/\n    ## @param externalAccess.autoDiscovery.image.registry Init container auto-discovery image registry\n    ## @param externalAccess.autoDiscovery.image.repository Init container auto-discovery image repository\n    ## @param externalAccess.autoDiscovery.image.tag Init container auto-discovery image tag (immutable tags are recommended)\n    ## @param externalAccess.autoDiscovery.image.pullPolicy Init container auto-discovery image pull policy\n    ## @param externalAccess.autoDiscovery.image.pullSecrets Init container auto-discovery image pull secrets\n    ##\n    image:\n      registry: docker.io\n      repository: bitnami/kubectl\n      tag: 1.23.3-debian-10-r19\n      ## Specify a imagePullPolicy\n      ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n      ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n      ##\n      pullPolicy: IfNotPresent\n      ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n      ## e.g:\n      ## pullSecrets:\n      ##   - myRegistryKeySecretName\n      ##\n      pullSecrets: []\n    ## Init Container resource requests and limits\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n    ## @param externalAccess.autoDiscovery.resources.limits The resources limits for the auto-discovery init container\n    ## @param externalAccess.autoDiscovery.resources.requests The requested resources for the auto-discovery init container\n    ##\n    resources:\n      limits: {}\n      requests: {}\n  ## Parameters to configure K8s service(s) used to externally access Kafka brokers\n  ## Note: A new service per broker will be created\n  ##\n  service:\n    ## @param externalAccess.service.type Kubernetes Service type for external access. It can be NodePort or LoadBalancer\n    ##\n    type: NodePort\n    ## @param externalAccess.service.ports.external Kafka port used for external access when service type is LoadBalancer\n    ##\n    ports:\n      external: 9094\n    ## @param externalAccess.service.loadBalancerIPs Array of load balancer IPs for each Kafka broker. Length must be the same as replicaCount\n    ## e.g:\n    ## loadBalancerIPs:\n    ##   - X.X.X.X\n    ##   - Y.Y.Y.Y\n    ##\n    loadBalancerIPs: []\n    ## @param externalAccess.service.loadBalancerNames Array of load balancer Names for each Kafka broker. Length must be the same as replicaCount\n    ## e.g:\n    ## loadBalancerNames:\n    ##   - broker1.external.example.com\n    ##   - broker2.external.example.com\n    ##\n    loadBalancerNames: []\n    ## @param externalAccess.service.loadBalancerAnnotations Array of load balancer annotations for each Kafka broker. Length must be the same as replicaCount\n    ## e.g:\n    ## loadBalancerAnnotations:\n    ##   - external-dns.alpha.kubernetes.io/hostname: broker1.external.example.com.\n    ##   - external-dns.alpha.kubernetes.io/hostname: broker2.external.example.com.\n    ##\n    loadBalancerAnnotations: []\n    ## @param externalAccess.service.loadBalancerSourceRanges Address(es) that are allowed when service is LoadBalancer\n    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service\n    ## e.g:\n    ## loadBalancerSourceRanges:\n    ## - 10.10.10.0/24\n    ##\n    loadBalancerSourceRanges: []\n    ## @param externalAccess.service.nodePorts Array of node ports used for each Kafka broker. Length must be the same as replicaCount\n    ## e.g:\n    ## nodePorts:\n    ##   - 30001\n    ##   - 30002\n    ##\n    nodePorts: [30001,30002,30003]\n    ## @param externalAccess.service.useHostIPs Use service host IPs to configure Kafka external listener when service type is NodePort\n    ##\n    useHostIPs: false\n    ## @param externalAccess.service.usePodIPs using the MY_POD_IP address for external access.\n    ##\n    usePodIPs: false\n    ## @param externalAccess.service.domain Domain or external ip used to configure Kafka external listener when service type is NodePort\n    ## If not specified, the container will try to get the kubernetes node external IP\n    ##\n    domain: \"stage-kafka.info\"\n    ## @param externalAccess.service.annotations Service annotations for external access\n    ##\n    annotations: {}\n    ## @param externalAccess.service.extraPorts Extra ports to expose in the Kafka external service\n    ##\n    extraPorts: []\n## Network policies\n## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/\n##\nnetworkPolicy:\n  ## @param networkPolicy.enabled Specifies whether a NetworkPolicy should be created\n  ##\n  enabled: false\n  ## @param networkPolicy.allowExternal Don't require client label for connections\n  ## When set to false, only pods with the correct client label will have network access to the port Redis&trade; is\n  ## listening on. When true, zookeeper accept connections from any source (with the correct destination port).\n  ##\n  allowExternal: true\n  ## @param networkPolicy.explicitNamespacesSelector A Kubernetes LabelSelector to explicitly select namespaces from which traffic could be allowed\n  ## If explicitNamespacesSelector is missing or set to {}, only client Pods that are in the networkPolicy's namespace\n  ## and that match other criteria, the ones that have the good label, can reach the kafka.\n  ## But sometimes, we want the kafka to be accessible to clients from other namespaces, in this case, we can use this\n  ## LabelSelector to select these namespaces, note that the networkPolicy's namespace should also be explicitly added.\n  ##\n  ## e.g:\n  ## explicitNamespacesSelector:\n  ##   matchLabels:\n  ##     role: frontend\n  ##   matchExpressions:\n  ##    - {key: role, operator: In, values: [frontend]}\n  ##\n  explicitNamespacesSelector: {}\n  ## @param networkPolicy.externalAccess.from customize the from section for External Access on tcp-external port\n  ## e.g:\n  ## - ipBlock:\n  ##    cidr: 172.9.0.0/16\n  ##    except:\n  ##    - 172.9.1.0/24\n  ##\n  externalAccess:\n    from: []\n  ## @param networkPolicy.egressRules.customRules [object] Custom network policy rule\n  ##\n  egressRules:\n    ## Additional custom egress rules\n    ## e.g:\n    ## customRules:\n    ##   - to:\n    ##       - namespaceSelector:\n    ##           matchLabels:\n    ##             label: example\n    customRules: []\n\n## @section Persistence parameters\n\n## Enable persistence using Persistent Volume Claims\n## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/\n##\npersistence:\n  ## @param persistence.enabled Enable Kafka data persistence using PVC, note that ZooKeeper persistence is unaffected\n  ##\n  enabled: true\n  ## @param persistence.existingClaim A manually managed Persistent Volume and Claim\n  ## If defined, PVC must be created manually before volume will be bound\n  ## The value is evaluated as a template\n  ##\n  existingClaim: \"\"\n  ## @param persistence.storageClass PVC Storage Class for Kafka data volume\n  ## If defined, storageClassName: <storageClass>\n  ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n  ## If undefined (the default) or set to null, no storageClassName spec is\n  ## set, choosing the default provisioner.\n  ##\n  storageClass: local-storage\n  ## @param persistence.accessModes Persistent Volume Access Modes\n  ##\n  accessModes:\n    - ReadWriteOnce\n  ## @param persistence.size PVC Storage Request for Kafka data volume\n  ##\n  size: 5Gi\n  ## @param persistence.annotations Annotations for the PVC\n  ##\n  annotations: {}\n  ## @param persistence.selector Selector to match an existing Persistent Volume for Kafka data PVC. If set, the PVC can't have a PV dynamically provisioned for it\n  ## selector:\n  ##   matchLabels:\n  ##     app: my-app\n  ##\n  selector: {}\n  ## @param persistence.mountPath Mount path of the Kafka data volume\n  ##\n  mountPath: /bitnami/kafka\n## Log Persistence parameters\n##\nlogPersistence:\n  ## @param logPersistence.enabled Enable Kafka logs persistence using PVC, note that ZooKeeper persistence is unaffected\n  ##\n  enabled: false\n  ## @param logPersistence.existingClaim A manually managed Persistent Volume and Claim\n  ## If defined, PVC must be created manually before volume will be bound\n  ## The value is evaluated as a template\n  ##\n  existingClaim: \"\"\n  ## @param logPersistence.storageClass PVC Storage Class for Kafka logs volume\n  ## If defined, storageClassName: <storageClass>\n  ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n  ## If undefined (the default) or set to null, no storageClassName spec is\n  ## set, choosing the default provisioner.\n  ##\n  storageClass: \"\"\n  ## @param logPersistence.accessModes Persistent Volume Access Modes\n  ##\n  accessModes:\n    - ReadWriteOnce\n  ## @param logPersistence.size PVC Storage Request for Kafka logs volume\n  ##\n  size: 5Gi\n  ## @param logPersistence.annotations Annotations for the PVC\n  ##\n  annotations: {}\n  ## @param logPersistence.selector Selector to match an existing Persistent Volume for Kafka log data PVC. If set, the PVC can't have a PV dynamically provisioned for it\n  ## selector:\n  ##   matchLabels:\n  ##     app: my-app\n  ##\n  selector: {}\n  ## @param logPersistence.mountPath Mount path of the Kafka logs volume\n  ##\n  mountPath: /opt/bitnami/kafka/logs\n\n## @section Volume Permissions parameters\n##\n\n## Init containers parameters:\n## volumePermissions: Change the owner and group of the persistent volume(s) mountpoint(s) to 'runAsUser:fsGroup' on each node\n##\nvolumePermissions:\n  ## @param volumePermissions.enabled Enable init container that changes the owner and group of the persistent volume\n  ##\n  enabled: false\n  ## @param volumePermissions.image.registry Init container volume-permissions image registry\n  ## @param volumePermissions.image.repository Init container volume-permissions image repository\n  ## @param volumePermissions.image.tag Init container volume-permissions image tag (immutable tags are recommended)\n  ## @param volumePermissions.image.pullPolicy Init container volume-permissions image pull policy\n  ## @param volumePermissions.image.pullSecrets Init container volume-permissions image pull secrets\n  ##\n  image:\n    registry: docker.io\n    repository: bitnami/bitnami-shell\n    tag: 10-debian-10-r339\n    pullPolicy: IfNotPresent\n    ## Optionally specify an array of imagePullSecrets.\n    ## Secrets must be manually created in the namespace.\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n    ## Example:\n    ## pullSecrets:\n    ##   - myRegistryKeySecretName\n    ##\n    pullSecrets: []\n  ## Init container resource requests and limits\n  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ## @param volumePermissions.resources.limits Init container volume-permissions resource limits\n  ## @param volumePermissions.resources.requests Init container volume-permissions resource requests\n  ##\n  resources:\n    limits: {}\n    requests: {}\n  ## Init container' Security Context\n  ## Note: the chown of the data folder is done to containerSecurityContext.runAsUser\n  ## and not the below volumePermissions.containerSecurityContext.runAsUser\n  ## @param volumePermissions.containerSecurityContext.runAsUser User ID for the init container\n  ##\n  containerSecurityContext:\n    runAsUser: 0\n\n## @section Other Parameters\n\n## ServiceAccount for Kafka\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n##\nserviceAccount:\n  ## @param serviceAccount.create Enable creation of ServiceAccount for Kafka pods\n  ##\n  create: true\n  ## @param serviceAccount.name The name of the service account to use. If not set and `create` is `true`, a name is generated\n  ## If not set and create is true, a name is generated using the kafka.serviceAccountName template\n  ##\n  name: \"\"\n  ## @param serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created\n  ## Can be set to false if pods using this serviceAccount do not need to use K8s API\n  ##\n  automountServiceAccountToken: true\n  ## @param serviceAccount.annotations Additional custom annotations for the ServiceAccount\n  ##\n  annotations: {}\n## Role Based Access Control\n## ref: https://kubernetes.io/docs/admin/authorization/rbac/\n##\nrbac:\n  ## @param rbac.create Whether to create &amp; use RBAC resources or not\n  ## binding Kafka ServiceAccount to a role\n  ## that allows Kafka pods querying the K8s API\n  ##\n  create: false\n\n## @section Metrics parameters\n\n## Prometheus Exporters / Metrics\n##\nmetrics:\n  ## Prometheus Kafka exporter: exposes complimentary metrics to JMX exporter\n  ##\n  kafka:\n    ## @param metrics.kafka.enabled Whether or not to create a standalone Kafka exporter to expose Kafka metrics\n    ##\n    enabled: false\n    ## Bitnami Kafka exporter image\n    ## ref: https://hub.docker.com/r/bitnami/kafka-exporter/tags/\n    ## @param metrics.kafka.image.registry Kafka exporter image registry\n    ## @param metrics.kafka.image.repository Kafka exporter image repository\n    ## @param metrics.kafka.image.tag Kafka exporter image tag (immutable tags are recommended)\n    ## @param metrics.kafka.image.pullPolicy Kafka exporter image pull policy\n    ## @param metrics.kafka.image.pullSecrets Specify docker-registry secret names as an array\n    ##\n    image:\n      registry: docker.io\n      repository: bitnami/kafka-exporter\n      tag: 1.4.2-debian-10-r147\n      ## Specify a imagePullPolicy\n      ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n      ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n      ##\n      pullPolicy: IfNotPresent\n      ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n      ## e.g:\n      ## pullSecrets:\n      ##   - myRegistryKeySecretName\n      ##\n      pullSecrets: []\n\n    ## @param metrics.kafka.certificatesSecret Name of the existing secret containing the optional certificate and key files\n    ## for Kafka exporter client authentication\n    ##\n    certificatesSecret: \"\"\n    ## @param metrics.kafka.tlsCert The secret key from the certificatesSecret if 'client-cert' key different from the default (cert-file)\n    ##\n    tlsCert: cert-file\n    ## @param metrics.kafka.tlsKey The secret key from the certificatesSecret if 'client-key' key different from the default (key-file)\n    ##\n    tlsKey: key-file\n    ## @param metrics.kafka.tlsCaSecret Name of the existing secret containing the optional ca certificate for Kafka exporter client authentication\n    ##\n    tlsCaSecret: \"\"\n    ## @param metrics.kafka.tlsCaCert The secret key from the certificatesSecret or tlsCaSecret if 'ca-cert' key different from the default (ca-file)\n    ##\n    tlsCaCert: ca-file\n    ## @param metrics.kafka.extraFlags Extra flags to be passed to Kafka exporter\n    ## e.g:\n    ## extraFlags:\n    ##   tls.insecure-skip-tls-verify: \"\"\n    ##   web.telemetry-path: \"/metrics\"\n    ##\n    extraFlags: {}\n    ## @param metrics.kafka.command Override Kafka exporter container command\n    ##\n    command: []\n    ## @param metrics.kafka.args Override Kafka exporter container arguments\n    ##\n    args: []\n    ## @param metrics.kafka.containerPorts.metrics Kafka exporter metrics container port\n    ##\n    containerPorts:\n      metrics: 9308\n    ## Kafka exporter resource requests and limits\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n    ## @param metrics.kafka.resources.limits The resources limits for the container\n    ## @param metrics.kafka.resources.requests The requested resources for the container\n    ##\n    resources:\n      limits: {}\n      requests: {}\n    ## Kafka exporter pods' Security Context\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod\n    ## @param metrics.kafka.podSecurityContext.enabled Enable security context for the pods\n    ## @param metrics.kafka.podSecurityContext.fsGroup Set Kafka exporter pod's Security Context fsGroup\n    ##\n    podSecurityContext:\n      enabled: true\n      fsGroup: 1001\n    ## Kafka exporter containers' Security Context\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n    ## @param metrics.kafka.containerSecurityContext.enabled Enable Kafka exporter containers' Security Context\n    ## @param metrics.kafka.containerSecurityContext.runAsUser Set Kafka exporter containers' Security Context runAsUser\n    ## @param metrics.kafka.containerSecurityContext.runAsNonRoot Set Kafka exporter containers' Security Context runAsNonRoot\n    ## e.g:\n    ##   containerSecurityContext:\n    ##     enabled: true\n    ##     capabilities:\n    ##       drop: [\"NET_RAW\"]\n    ##     readOnlyRootFilesystem: true\n    ##\n    containerSecurityContext:\n      enabled: true\n      runAsUser: 1001\n      runAsNonRoot: true\n    ## @param metrics.kafka.hostAliases Kafka exporter pods host aliases\n    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/\n    ##\n    hostAliases: []\n    ## @param metrics.kafka.podLabels Extra labels for Kafka exporter pods\n    ## Ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n    ##\n    podLabels: {}\n    ## @param metrics.kafka.podAnnotations Extra annotations for Kafka exporter pods\n    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\n    ##\n    podAnnotations: {}\n    ## @param metrics.kafka.podAffinityPreset Pod affinity preset. Ignored if `metrics.kafka.affinity` is set. Allowed values: `soft` or `hard`\n    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n    ##\n    podAffinityPreset: \"\"\n    ## @param metrics.kafka.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `metrics.kafka.affinity` is set. Allowed values: `soft` or `hard`\n    ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n    ##\n    podAntiAffinityPreset: soft\n    ## Node metrics.kafka.affinity preset\n    ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity\n    ##\n    nodeAffinityPreset:\n      ## @param metrics.kafka.nodeAffinityPreset.type Node affinity preset type. Ignored if `metrics.kafka.affinity` is set. Allowed values: `soft` or `hard`\n      ##\n      type: \"\"\n      ## @param metrics.kafka.nodeAffinityPreset.key Node label key to match Ignored if `metrics.kafka.affinity` is set.\n      ## E.g.\n      ## key: \"kubernetes.io/e2e-az-name\"\n      ##\n      key: \"\"\n      ## @param metrics.kafka.nodeAffinityPreset.values Node label values to match. Ignored if `metrics.kafka.affinity` is set.\n      ## E.g.\n      ## values:\n      ##   - e2e-az1\n      ##   - e2e-az2\n      ##\n      values: []\n    ## @param metrics.kafka.affinity Affinity for pod assignment\n    ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n    ## Note: metrics.kafka.podAffinityPreset, metrics.kafka.podAntiAffinityPreset, and metrics.kafka.nodeAffinityPreset will be ignored when it's set\n    ##\n    affinity: {}\n    ## @param metrics.kafka.nodeSelector Node labels for pod assignment\n    ## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n    ##\n    nodeSelector: {}\n    ## @param metrics.kafka.tolerations Tolerations for pod assignment\n    ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n    ##\n    tolerations: []\n    ## @param metrics.kafka.schedulerName Name of the k8s scheduler (other than default) for Kafka exporter\n    ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n    ##\n    schedulerName: \"\"\n    ## @param metrics.kafka.extraVolumes Optionally specify extra list of additional volumes for the Kafka exporter pod(s)\n    ## e.g:\n    ## extraVolumes:\n    ##   - name: kafka-jaas\n    ##     secret:\n    ##       secretName: kafka-jaas\n    ##\n    extraVolumes: []\n    ## @param metrics.kafka.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the Kafka exporter container(s)\n    ## extraVolumeMounts:\n    ##   - name: kafka-jaas\n    ##     mountPath: /bitnami/kafka/config/kafka_jaas.conf\n    ##     subPath: kafka_jaas.conf\n    ##\n    extraVolumeMounts: []\n    ## @param metrics.kafka.sidecars Add additional sidecar containers to the Kafka exporter pod(s)\n    ## e.g:\n    ## sidecars:\n    ##   - name: your-image-name\n    ##     image: your-image\n    ##     imagePullPolicy: Always\n    ##     ports:\n    ##       - name: portname\n    ##         containerPort: 1234\n    ##\n    sidecars: []\n    ## @param metrics.kafka.initContainers Add init containers to the Kafka exporter pods\n    ## e.g:\n    ## initContainers:\n    ##   - name: your-image-name\n    ##     image: your-image\n    ##     imagePullPolicy: Always\n    ##     ports:\n    ##       - name: portname\n    ##         containerPort: 1234\n    ##\n    initContainers: []\n    ## Kafka exporter service configuration\n    ##\n    service:\n      ## @param metrics.kafka.service.ports.metrics Kafka exporter metrics service port\n      ##\n      ports:\n        metrics: 9308\n      ## @param metrics.kafka.service.clusterIP Static clusterIP or None for headless services\n      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#choosing-your-own-ip-address\n      ##\n      clusterIP: \"\"\n      ## @param metrics.kafka.service.sessionAffinity Control where client requests go, to the same pod or round-robin\n      ## Values: ClientIP or None\n      ## ref: https://kubernetes.io/docs/user-guide/services/\n      ##\n      sessionAffinity: None\n      ## @param metrics.kafka.service.annotations [object] Annotations for the Kafka exporter service\n      ##\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"{{ .Values.metrics.kafka.service.ports.metrics }}\"\n        prometheus.io/path: \"/metrics\"\n    ## Kafka exporter pods ServiceAccount\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n    ##\n    serviceAccount:\n      ## @param metrics.kafka.serviceAccount.create Enable creation of ServiceAccount for Kafka exporter pods\n      ##\n      create: true\n      ## @param metrics.kafka.serviceAccount.name The name of the service account to use. If not set and `create` is `true`, a name is generated\n      ## If not set and create is true, a name is generated using the kafka.metrics.kafka.serviceAccountName template\n      ##\n      name: \"\"\n      ## @param metrics.kafka.serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created\n      ## Can be set to false if pods using this serviceAccount do not need to use K8s API\n      ##\n      automountServiceAccountToken: true\n  ## Prometheus JMX exporter: exposes the majority of Kafkas metrics\n  ##\n  jmx:\n    ## @param metrics.jmx.enabled Whether or not to expose JMX metrics to Prometheus\n    ##\n    enabled: false\n    ## Bitnami JMX exporter image\n    ## ref: https://hub.docker.com/r/bitnami/jmx-exporter/tags/\n    ## @param metrics.jmx.image.registry JMX exporter image registry\n    ## @param metrics.jmx.image.repository JMX exporter image repository\n    ## @param metrics.jmx.image.tag JMX exporter image tag (immutable tags are recommended)\n    ## @param metrics.jmx.image.pullPolicy JMX exporter image pull policy\n    ## @param metrics.jmx.image.pullSecrets Specify docker-registry secret names as an array\n    ##\n    image:\n      registry: docker.io\n      repository: bitnami/jmx-exporter\n      tag: 0.16.1-debian-10-r208\n      ## Specify a imagePullPolicy\n      ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n      ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n      ##\n      pullPolicy: IfNotPresent\n      ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n      ## e.g:\n      ## pullSecrets:\n      ##   - myRegistryKeySecretName\n      ##\n      pullSecrets: []\n    ## Prometheus JMX exporter containers' Security Context\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n    ## @param metrics.jmx.containerSecurityContext.enabled Enable Prometheus JMX exporter containers' Security Context\n    ## @param metrics.jmx.containerSecurityContext.runAsUser Set Prometheus JMX exporter containers' Security Context runAsUser\n    ## @param metrics.jmx.containerSecurityContext.runAsNonRoot Set Prometheus JMX exporter containers' Security Context runAsNonRoot\n    ## e.g:\n    ##   containerSecurityContext:\n    ##     enabled: true\n    ##     capabilities:\n    ##       drop: [\"NET_RAW\"]\n    ##     readOnlyRootFilesystem: true\n    ##\n    containerSecurityContext:\n      enabled: true\n      runAsUser: 1001\n      runAsNonRoot: true\n    ## @param metrics.jmx.containerPorts.metrics Prometheus JMX exporter metrics container port\n    ##\n    containerPorts:\n      metrics: 5556\n    ## Prometheus JMX exporter resource requests and limits\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n    ## @param metrics.jmx.resources.limits The resources limits for the JMX exporter container\n    ## @param metrics.jmx.resources.requests The requested resources for the JMX exporter container\n    ##\n    resources:\n      limits: {}\n      requests: {}\n    ## Prometheus JMX exporter service configuration\n    ##\n    service:\n      ## @param metrics.jmx.service.ports.metrics Prometheus JMX exporter metrics service port\n      ##\n      ports:\n        metrics: 5556\n      ## @param metrics.jmx.service.clusterIP Static clusterIP or None for headless services\n      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#choosing-your-own-ip-address\n      ##\n      clusterIP: \"\"\n      ## @param metrics.jmx.service.sessionAffinity Control where client requests go, to the same pod or round-robin\n      ## Values: ClientIP or None\n      ## ref: https://kubernetes.io/docs/user-guide/services/\n      ##\n      sessionAffinity: None\n      ## @param metrics.jmx.service.annotations [object] Annotations for the Prometheus JMX exporter service\n      ##\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"{{ .Values.metrics.jmx.service.ports.metrics }}\"\n        prometheus.io/path: \"/\"\n    ## @param metrics.jmx.whitelistObjectNames Allows setting which JMX objects you want to expose to via JMX stats to JMX exporter\n    ## Only whitelisted values will be exposed via JMX exporter. They must also be exposed via Rules. To expose all metrics\n    ## (warning its crazy excessive and they aren't formatted in a prometheus style) (1) `whitelistObjectNames: []`\n    ## (2) commented out above `overrideConfig`.\n    ##\n    whitelistObjectNames:\n      - kafka.controller:*\n      - kafka.server:*\n      - java.lang:*\n      - kafka.network:*\n      - kafka.log:*\n    ## @param metrics.jmx.config [string] Configuration file for JMX exporter\n    ## Specify content for jmx-kafka-prometheus.yml. Evaluated as a template\n    ##\n    ## Credits to the incubator/kafka chart for the JMX configuration.\n    ## https://github.com/helm/charts/tree/master/incubator/kafka\n    ##\n    config: |-\n      jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:5555/jmxrmi\n      lowercaseOutputName: true\n      lowercaseOutputLabelNames: true\n      ssl: false\n      {{- if .Values.metrics.jmx.whitelistObjectNames }}\n      whitelistObjectNames: [\"{{ join \"\\\",\\\"\" .Values.metrics.jmx.whitelistObjectNames }}\"]\n      {{- end }}\n    ## @param metrics.jmx.existingConfigmap Name of existing ConfigMap with JMX exporter configuration\n    ## NOTE: This will override metrics.jmx.config\n    ##\n    existingConfigmap: \"\"\n  ## Prometheus Operator ServiceMonitor configuration\n  ##\n  serviceMonitor:\n    ## @param metrics.serviceMonitor.enabled if `true`, creates a Prometheus Operator ServiceMonitor (requires `metrics.kafka.enabled` or `metrics.jmx.enabled` to be `true`)\n    ##\n    enabled: false\n    ## @param metrics.serviceMonitor.namespace Namespace in which Prometheus is running\n    ##\n    namespace: \"\"\n    ## @param metrics.serviceMonitor.interval Interval at which metrics should be scraped\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint\n    ##\n    interval: \"\"\n    ## @param metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint\n    ##\n    scrapeTimeout: \"\"\n    ## @param metrics.serviceMonitor.labels Additional labels that can be used so ServiceMonitor will be discovered by Prometheus\n    ##\n    labels: {}\n    ## @param metrics.serviceMonitor.selector Prometheus instance selector labels\n    ## ref: https://github.com/bitnami/charts/tree/master/bitnami/prometheus-operator#prometheus-configuration\n    ##\n    selector: {}\n    ## @param metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping\n    ##\n    relabelings: []\n    ## @param metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion\n    ##\n    metricRelabelings: []\n    ## @param metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint\n    ##\n    honorLabels: false\n    ## @param metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in prometheus.\n    ##\n    jobLabel: \"\"\n\n## @section Kafka provisioning parameters\n\n## Kafka provisioning\n##\nprovisioning:\n  ## @param provisioning.enabled Enable kafka provisioning Job\n  ##\n  enabled: false\n  ## @param provisioning.numPartitions Default number of partitions for topics when unspecified\n  ##\n  numPartitions: 1\n  ## @param provisioning.replicationFactor Default replication factor for topics when unspecified\n  ##\n  replicationFactor: 1\n  ## @param provisioning.topics Kafka provisioning topics\n  ## - name: topic-name\n  ##   partitions: 1\n  ##   replicationFactor: 1\n  ##   ## https://kafka.apache.org/documentation/#topicconfigs\n  ##   config:\n  ##     max.message.bytes: 64000\n  ##     flush.messages: 1\n  ##\n  topics: []\n  ## @param provisioning.command Override provisioning container command\n  ##\n  command: []\n  ## @param provisioning.args Override provisioning container arguments\n  ##\n  args: []\n  ## @param provisioning.podAnnotations Extra annotations for Kafka provisioning pods\n  ##\n  podAnnotations: {}\n  ## @param provisioning.podLabels Extra labels for Kafka provisioning pods\n  ## Ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n  ##\n  podLabels: {}\n  ## Kafka provisioning resource requests and limits\n  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ## @param provisioning.resources.limits The resources limits for the Kafka provisioning container\n  ## @param provisioning.resources.requests The requested resources for the Kafka provisioning container\n  ##\n  resources:\n    limits: {}\n    requests: {}\n  ## Kafka provisioning pods' Security Context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod\n  ## @param provisioning.podSecurityContext.enabled Enable security context for the pods\n  ## @param provisioning.podSecurityContext.fsGroup Set Kafka provisioning pod's Security Context fsGroup\n  ##\n  podSecurityContext:\n    enabled: true\n    fsGroup: 1001\n  ## Kafka provisioning containers' Security Context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n  ## @param provisioning.containerSecurityContext.enabled Enable Kafka provisioning containers' Security Context\n  ## @param provisioning.containerSecurityContext.runAsUser Set Kafka provisioning containers' Security Context runAsUser\n  ## @param provisioning.containerSecurityContext.runAsNonRoot Set Kafka provisioning containers' Security Context runAsNonRoot\n  ## e.g:\n  ##   containerSecurityContext:\n  ##     enabled: true\n  ##     capabilities:\n  ##       drop: [\"NET_RAW\"]\n  ##     readOnlyRootFilesystem: true\n  ##\n  containerSecurityContext:\n    enabled: true\n    runAsUser: 1001\n    runAsNonRoot: true\n  ## @param provisioning.schedulerName Name of the k8s scheduler (other than default) for kafka provisioning\n  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n  ##\n  schedulerName: \"\"\n  ## @param provisioning.extraVolumes Optionally specify extra list of additional volumes for the Kafka provisioning pod(s)\n  ## e.g:\n  ## extraVolumes:\n  ##   - name: kafka-jaas\n  ##     secret:\n  ##       secretName: kafka-jaas\n  ##\n  extraVolumes: []\n  ## @param provisioning.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the Kafka provisioning container(s)\n  ## extraVolumeMounts:\n  ##   - name: kafka-jaas\n  ##     mountPath: /bitnami/kafka/config/kafka_jaas.conf\n  ##     subPath: kafka_jaas.conf\n  ##\n  extraVolumeMounts: []\n  ## @param provisioning.sidecars Add additional sidecar containers to the Kafka provisioning pod(s)\n  ## e.g:\n  ## sidecars:\n  ##   - name: your-image-name\n  ##     image: your-image\n  ##     imagePullPolicy: Always\n  ##     ports:\n  ##       - name: portname\n  ##         containerPort: 1234\n  ##\n  sidecars: []\n  ## @param provisioning.initContainers Add additional Add init containers to the Kafka provisioning pod(s)\n  ## e.g:\n  ## initContainers:\n  ##   - name: your-image-name\n  ##     image: your-image\n  ##     imagePullPolicy: Always\n  ##     ports:\n  ##       - name: portname\n  ##         containerPort: 1234\n  ##\n  initContainers: []\n\n## @section ZooKeeper chart parameters\n\n## ZooKeeper chart configuration\n## https://github.com/bitnami/charts/blob/master/bitnami/zookeeper/values.yaml\n##\nzookeeper:\n  ## @param zookeeper.enabled Switch to enable or disable the ZooKeeper helm chart\n  ##\n  enabled: true\n  ## @param zookeeper.replicaCount Number of ZooKeeper nodes\n  ##\n  replicaCount: 3\n  ## ZooKeeper authenticaiton\n  ##\n  auth:\n    ## @param zookeeper.auth.enabled Enable ZooKeeper auth\n    ##\n    enabled: false\n    ## @param zookeeper.auth.clientUser User that will use ZooKeeper clients to auth\n    ##\n    clientUser: \"\"\n    ## @param zookeeper.auth.clientPassword Password that will use ZooKeeper clients to auth\n    ##\n    clientPassword: \"\"\n    ## @param zookeeper.auth.serverUsers Comma, semicolon or whitespace separated list of user to be created. Specify them as a string, for example: \"user1,user2,admin\"\n    ##\n    serverUsers: \"\"\n    ## @param zookeeper.auth.serverPasswords Comma, semicolon or whitespace separated list of passwords to assign to users when created. Specify them as a string, for example: \"pass4user1, pass4user2, pass4admin\"\n    ##\n    serverPasswords: \"\"\n  ## ZooKeeper Persistence parameters\n  ## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/\n  ## @param zookeeper.persistence.enabled Enable persistence on ZooKeeper using PVC(s)\n  ## @param zookeeper.persistence.storageClass Persistent Volume storage class\n  ## @param zookeeper.persistence.accessModes Persistent Volume access modes\n  ## @param zookeeper.persistence.size Persistent Volume size\n  ##\n  persistence:\n    enabled: true\n    storageClass: local-storage\n    accessModes:\n      - ReadWriteOnce\n    size: 5Gi\n\n## External Zookeeper Configuration\n## All of these values are only used if `zookeeper.enabled=false`\n##\nexternalZookeeper:\n  ## @param externalZookeeper.servers List of external zookeeper servers to use\n  ##\n  servers: []\n\n```\n\n使用helm部署，需要在fetch的chart的解压目录下\n\n```\n(⎈ |minikube:default)➜  /Users/lintong/coding/helm/kafka git:(master) ✗ $ helm install kafka -f values.yaml .\n\n```\n\n查看pv和pvc，已经是Bound状态\n\n```\nkubectl get pv,pvc -A\nNAME                                      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                            STORAGECLASS    REASON   AGE\npersistentvolume/data-kafka-zookeeper-0   5Gi        RWO            Retain           Bound    default/data-kafka-zookeeper-1   local-storage            10h\npersistentvolume/data-kafka-zookeeper-1   5Gi        RWO            Retain           Bound    default/data-kafka-zookeeper-0   local-storage            10h\npersistentvolume/data-kafka-zookeeper-2   5Gi        RWO            Retain           Bound    default/data-kafka-zookeeper-2   local-storage            10h\npersistentvolume/datadir-kafka-0          5Gi        RWO            Retain           Bound    default/data-kafka-0             local-storage            10h\npersistentvolume/datadir-kafka-1          5Gi        RWO            Retain           Bound    default/data-kafka-1             local-storage            10h\npersistentvolume/datadir-kafka-2          5Gi        RWO            Retain           Bound    default/data-kafka-2             local-storage            10h\n\nNAMESPACE   NAME                                           STATUS   VOLUME                   CAPACITY   ACCESS MODES   STORAGECLASS    AGE\ndefault     persistentvolumeclaim/data-kafka-0             Bound    datadir-kafka-0          5Gi        RWO            local-storage   22m\ndefault     persistentvolumeclaim/data-kafka-1             Bound    datadir-kafka-1          5Gi        RWO            local-storage   22m\ndefault     persistentvolumeclaim/data-kafka-2             Bound    datadir-kafka-2          5Gi        RWO            local-storage   22m\ndefault     persistentvolumeclaim/data-kafka-zookeeper-0   Bound    data-kafka-zookeeper-1   5Gi        RWO            local-storage   10h\ndefault     persistentvolumeclaim/data-kafka-zookeeper-1   Bound    data-kafka-zookeeper-0   5Gi        RWO            local-storage   10h\ndefault     persistentvolumeclaim/data-kafka-zookeeper-2   Bound    data-kafka-zookeeper-2   5Gi        RWO            local-storage   10h\n\n```\n\n查看pod状态\n\n```\nkubectl get pod -A\nNAMESPACE       NAME                                        READY   STATUS      RESTARTS   AGE\ndefault         kafka-0                                     1/1     Running     2          7m14s\ndefault         kafka-1                                     1/1     Running     3          7m14s\ndefault         kafka-2                                     1/1     Running     3          7m14s\ndefault         kafka-zookeeper-0                           1/1     Running     0          7m14s\ndefault         kafka-zookeeper-1                           1/1     Running     0          7m14s\ndefault         kafka-zookeeper-2                           1/1     Running     0          7m14s\n\n```\n\n　　\n\n## 4.连接kafka集群\n\n查看service，可以看到kafka-0-external，kafka-1-external，kafka-2-external都是NodePort，所以可以使用minikube的ip来访问\n\n```\nkubectl get service -A\nNAMESPACE       NAME                                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE\ndefault         kafka                                ClusterIP   10.101.193.49    <none>        9092/TCP                     11m\ndefault         kafka-0-external                     NodePort    10.107.22.127    <none>        9094:30001/TCP               11m\ndefault         kafka-1-external                     NodePort    10.106.222.15    <none>        9094:30002/TCP               11m\ndefault         kafka-2-external                     NodePort    10.111.166.28    <none>        9094:30003/TCP               11m\ndefault         kafka-headless                       ClusterIP   None             <none>        9092/TCP,9093/TCP            11m\ndefault         kafka-zookeeper                      ClusterIP   10.110.168.238   <none>        2181/TCP,2888/TCP,3888/TCP   11m\ndefault         kafka-zookeeper-headless             ClusterIP   None             <none>        2181/TCP,2888/TCP,3888/TCP   11m\n\n```\n\nminikube的ip\n\n```\n(⎈ |minikube:default)➜  /Users/lintong $ minikube ip\n192.168.99.100\n\n```\n\n使用来offset explorer连接，成功连接\n\n<img src=\"/images/517519-20220714173144455-1673378660.png\" width=\"800\" height=\"138\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["k8s","kafka"]},{"title":"K8S学习笔记——创建pv","url":"/K8S学习笔记——创建pv.html","content":"在K8S上使用存储的时候，需要创建Persistence Volumes（持久化卷）用于持久化数据，否则当pod重启后，数据将会丢失，可以参考官方文档：\n\n```\nhttps://kubernetes.io/zh-cn/docs/concepts/storage/persistent-volumes/\n\n```\n\n常用的持久卷类型有local（<!--more-->\n&nbsp;节点上挂载的本地存储设备），rbd（依赖Ceph，RBD 卷只能由单个使用者以读写模式安装。不允许同时写入），cephfs（依赖Ceph，同一&nbsp;`cephfs`&nbsp;卷可同时被多个写者挂载）等\n\n```\nhttps://kubernetes.io/zh-cn/docs/concepts/storage/volumes/\n\n```\n\npersistent volume，persistent volume claim和storage class的区别可以参考：[Kubernetes对象之PersistentVolume，StorageClass和PersistentVolumeClaim](https://www.jianshu.com/p/99e610067bc8)\n\n默认的storage class有HostPath，HostPath 卷 （仅供单节点测试使用；不适用于多节点集群； 请尝试使用&nbsp;`local`&nbsp;卷作为替代）\n\n```\nkubectl get sc -A\nNAME                 PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\nstandard (default)   k8s.io/minikube-hostpath   Delete          Immediate           false                  40d\n\n```\n\n如果要创建一个local的storage class，首先需要编写一个local-storage.yaml\n\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: local-storage\nprovisioner: kubernetes.io/no-provisioner\nvolumeBindingMode: WaitForFirstConsumer\nreclaimPolicy: Retain\n\n```\n\n创建local sc\n\n```\nkubectl apply -f local-storage.yaml\n\n```\n\n查看创建的sc\n\n```\nkubectl get sc -A\nNAME                 PROVISIONER                    RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\nlocal-storage        kubernetes.io/no-provisioner   Retain          WaitForFirstConsumer   false                  2m41s\nstandard (default)   k8s.io/minikube-hostpath       Delete          Immediate              false                  41d\n\n```\n\n给local storage class下面增加一个kafka pv，kafka-local-pv.yaml\n\n```\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: data-kafka-0\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n  - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: local-storage\n  local:\n    path: /tmp/kafka/data-0\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - minikube\n\n```\n\n创建kafka pv\n\n```\nkubectl apply -f kafka-local-pv.yaml\n\n```\n\n查看创建的kafka pv\n\n```\nkubectl get pv -A\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                              STORAGECLASS    REASON   AGE\ndata-kafka-0                               5Gi        RWO            Retain           Available                                      local-storage            6s\n\n```\n\n给local storage class下面增加一个zk pv，zookeeper-local-pv.yaml\n\n```\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: data-zookeeper-0\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n  - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: local-storage\n  local:\n    path: /tmp/zookeeper/data-0\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - master\n\n```\n\n创建zookeeper pv\n\n```\nkubectl apply -f zookeeper-local-pv.yaml\n\n```\n\n查看创建的zookeeper pv\n\n```\nkubectl get pv,pvc --all-namespaces\nNAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                              STORAGECLASS    REASON   AGE\npersistentvolume/data-kafka-0                               5Gi        RWO            Retain           Available                                      local-storage            12m\npersistentvolume/data-zookeeper-0                           5Gi        RWO            Retain           Available                                      local-storage            2m5s\n\n```\n\n　　\n\n&nbsp;\n","tags":["k8s"]},{"title":"JavaScript学习笔记——基本知识","url":"/JavaScript学习笔记——基本知识.html","content":"## **1.JavaScript的放置和注释**\n\n### **1.输出工具**\n\nA.alert();\n\nB.document.write();\n\nC.prompt(\"\",\"\");\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\n<title>输出函数测试</title>\n</head>\n\n<body>\n<script>\n    <!-- 弹出对话框显示 -->\n    alert(\"<h1>测试</h1>\");\n\n    <!-- 页面显示 -->\n    document.write(\"<h1>测试教程</h1>\");\n\n    <!-- 弹出输入对话框，一个提示参数，一个输入参数 -->\n       var value=prompt(\"please enter your name\",\"\")\n       <!-- 显示输入的参数 -->\n       alert(value);\n</script>\n</body>\n</html>\n```\n\n# 测试教程\n\n### **2.JavaScript如何在html页面当中进行放置**\n\nA.<script></script>　　放在<head></head>中间，也可以放在<body></body>中间，有两个属性，一个是**type**，另外一个是**language**\n\n**div中加样式：写class，，然后在<head></head>中写<style><strong></style>**</strong>\n\n```\n<style>\n   .one{\n     width:100px;\n     height:100px;\n     background:red;\n     font-size:12px;\n     color:blue;\n   }\n</style>\n```\n\nJavaScript可以在html页面当中的任何位置来进行调用，但是他们还是一个整体，是相互联系，相互影响。\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\" />\n<title>javascript放置</title>\n<script type=\"text/javascript\" language=\"javascript\" >\n   var a=\"这是测试\"\n</script>\n<style>\n   .one{\n     width:100px;\n     height:100px;\n     background:red;\n     font-size:12px;\n     color:blue;\n   }\n</style>\n</head>\n\n<body>\n\n<script type=\"text/javascript\" language=\"javascript\">\n   a=\"这是测试demo\"\n   document.write(a);\n</script>\n\n \n    测试demo！\n \n\n <script type=\"text/javascript\" language=\"javascript\">\n   alert(a);\n</script>\n</body>\n</html>\n```\n\nB.可以在超链接或是重定向的位置调用javascript代码\n\n格式：\"javascript:alert('我是超链接')\"<br />重定向格式：action=\"javascript:alert('我是表单')\"\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\" />\n<title>place2</title>\n<style>\n   .one{\n     width:100px;\n     height:100px;\n     background:red;\n     font-size:12px;\n     color:blue;\n   }\n</style>\n</head>\n\n<body>\n  \n　　 测试demo！\n \n\n <!-- 点击链接，弹出对话框 -->\n [链接](javascript:alert('我是超链接'))\n\n <!-- from表单，提交表单后弹出对话框 -->\n <form action=\"javascript:alert('我是表单')\" method=\"post\">\n  <input type=\"text\" name=\"names\">\n  <!-- type是submit，点击按钮后直接提交表单 -->\n  <input type=\"submit\" value=\"提交\">\n </form>\n</body>\n</html>\n```\n\nC.在事件后面进行调用<br />1>.格式：onclick=\"alert('我是事件')\"<br />2>.<script for=\"two\" event=\"onclick\"><br />　　alert(\"我是DIV2\");<br /></script>\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\" />\n<title>place3</title>\n<style>\n   .one{\n     width:100px;\n     height:100px;\n     background:red;\n     font-size:12px;\n     color:blue;\n   }\n\n    .two{\n     width:200px;\n     height:300px;\n     background:blue;\n     font-size:12px;\n     color:red;\n   }\n</style>\n\n<!-- for属性表示为了那个ID，一般不用 -->\n<script for=\"two\" event=\"onclick\">\n  alert(\"我是DIV2\");\n</script>\n</head>\n\n<body>\n  <!-- 点击这个div弹出对话框 -->\n  \n　　 测试demo！\n  \n\n  \n　　 测试demo！\n  \n\n</body>\n</html>\n```\n\n### **3.调用外部javascript文件**\n\n**格式： <script src=\"2.js\"></script>　　在<head></head>中加入**\n\n```\n<script src=\"2.txt\"></script>\n```\n\n　　　　js文件\n\n```\nvar a=\"test\";\nalert(a);\n```\n\n**注意**：在调用页面<script>标签对当中不能有任何代码、在js脚本中不能出现<script>标签对、但是他们还是一个整体，是相互联系，相互影响。\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\" />\n<title>place4</title>\n  <!-- 调用外部js文件 -->\n  <script src=\"2.txt\"></script>\n<style>\n   .one{\n     width:100px;\n     height:100px;\n     background:red;\n     font-size:12px;\n     color:blue;\n   }\n\n    .two{\n     width:200px;\n     height:300px;\n     background:blue;\n     font-size:12px;\n     color:red;\n   }\n</style>\n<script>\n  <!--\n    //a=\"test\";\n    //alert(a);\n</script>\n</head>\n\n<body>\n  \n 　　测试demo！\n \n\n   \n 　　测试demo！\n \n\n</body>\n</html>\n```\n\n### **4.javascript注释:**\n\n1.对付旧的浏览器<br />　　<!--   --><br />2.真正注释符号。<br />　　A.行内注释  //<br />　　B.块注释/**/\n\n<!--more-->\n&nbsp;\n\n## 2.JavaScript的变量和数据类型\n\n### **一、javascript命名规范**\n\n　　1. 严格区分大小写<br />　　2. 变量的命名必须以字母或 _或 $开头，余下的部分可以是任意的字母，数字，或者是_ 或者是$\n\n　　3.不能用关键字或者是保留字命名。<br />　　4.javascript自己的命名习惯<br />　　　　驼峰命名法：getElementById<br />　　　　首字母大写：Object<br />　　5.命名一定要有意义。<br />　　6. ;的用法\n\n### **二、javascript变量**\n\n　　变量：可以存储数据的一个容器。\n\n　　**1.变量如何创建(声明)**<br />　　　　**必须以\"var\"关键来修饰**。<br />　　　　　　A.先声明，后赋值<br />　　　　　　　　var bbs;<br />　　　　　　　　bbs=\"bbs.houdunwang.com\";<br />　　　　　　B.声明和赋值同时进行<br />　　　　　　　　var url=\"www.houdunwang.com\";\n\n　　　　　　C.一次声明多个变量，然后再赋值<br />　　　　　　　　var name, age , sex;<br />　　　　　　　　name=\"lisi\";<br />　　　　　　　　age=14;<br />　　　　　　　　sex=\"boy\";<br />　　　　　　D.一次声明多个变量同时进行赋值。<br />　　　　　　　　var name=\"wangwu\",age=17,sex=\"girl\";\n\n　　**2.如何覆盖已有变量**<br />　　　　A.如何重新声明该变量，而没有赋值，该变量的值不会改变<br />　　　　B.如果重新声明该变量并且重新赋值，那么旧的变量值会删除，改为新的变量值。\n\n　　**3.不用var 关键字来修饰变量**<br />　　　　aa=\"我是没有声明的\"<br />　　　　alert(aa);<br />　　　　*****************************************、\n\n　　　　A.如果不用var关键来修饰的变量，并且这个所谓的变量也**没有赋值**，那么javascript**会报错**。<br />　　　　B.如果不用var关键来修饰的变量，但是**变量赋值**了，那么javascript会把他**当作一个全局变量**来处理，不会报错。但是我们不推介使用。<br /><br />　　　　*****************************************\n\n### **三、javascript中的数据类型**\n\n　　**1.初始类型**\n\n　　　　Undefined<br />　　　　Null<br />　　　　Number<br />　　　　String<br />　　　　Boolean<br />　　**2.引用类型**<br />　　　　object (class)\n\n### **四、**JavaScript把内存分成四个部分：**栈、堆、代码段和静态区**\n\n　　　　　　原始数据类型都是存储在栈中，长度是固定的，在栈中的查询速度比较快\n\n　　　　　　在栈中申明了object对象，只是一个引用地址，实际的内容在堆中\n\n### **五、typeof操作符**\n\n　　他是用来检测数据类型的一元运算符，并且返回的结果始终是一个字符串。\n\n　　知识点：== 比较值是不是相等于的　　全等于 ===比较值和类型是不是相等于的\n\n### **六、数据类型**\n\n　　1.初始类型<br />　　　　**Undefined**　　指的就是变量创建后但是没有赋值，而变量的默认值就是undefined<br />　　　　**Null**　　　　　 指的是什么都没有，仅仅是一个占位符。<br />　　　　**Number　　　**包括整型和浮点型。支持二进制、八进制(以0开头)、十进制、十六进制(以0x开头)。用科学计数法来表示(2e2=200)，\n\n　　　　　　　　　　　还包括一些特殊的值：Number.MAX_VALUE 最大值&nbsp;Number.MIN_VALUE 最小值\n\n　　　　**String**　　　　用单双引号来说明，他所包围的值都可以是字符串。<br />　　　　　　　　　　　单双引号的用法：<br />　　　　　　　　　　　　　　1>效率是一样的<br />　　　　　　　　　　　　　　2>只能成对出现，不能相互交叉使用<br />　　　　　　　　　　　　　　3>可以相互嵌套。<br />　　　　　　　　　　　　　　4>还包括一些特殊的字符:<br />　　　　　　　　　　　　　　　　\\n &nbsp;换行<br />　　　　　　　　　　　　　　　　\\t &nbsp;制表符\n\n　　　　　　　　　　　　　　　　\\b 空格<br />　　　　　　　　　　　　　　　　\\r &nbsp;回车<br />　　　　　　　　　　　　　　　　\\' &nbsp;&lsquo;<br />　　　　　　　　　　　　　　　　\\\" &ldquo;<br />　　　　　　　　　　　　　　　　\\\\ \\<br />　　　　**Boolean　　**只有两个特殊的值 true false\n\n　　2.引用类型<br />　　　　object (class)　　　　包含相关属性和方法的一个集合。new 关键字。var obj=new Object();\n\n&nbsp;\n\n　**　类型 　　　　 　　 　　 值 　　　　 　　　　&nbsp;typeof运算的结果**<br />　　Undefined 　　 　　 undefined 　　 　　　　 &nbsp;　 \"undefined\"　　undefined是一个字符串，等于Null，但是不全等于Null\n\n　　Null 　　　　　　 &nbsp;　　null 　　　　 　　　　　　&nbsp;\"object\"　　　 &nbsp;null是一个对象\n\n　　String 　　 在单双引号之间的值，特殊字符 　　　&nbsp;\"string\"\n\n　　Boolean 　　　&nbsp;　　true false 　　&nbsp; 　　　　　 &nbsp;&nbsp;\"boolean\"\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\" />\n<title>数据类型下</title>\n</head>\n\n<body>\n<Script>\n   //Number类型\n       //整型\n//        var num=999.0;\n//        alert(num);\n//        alert(typeof (num))\n    //浮点型\n//      var flo=3.140;\n//      alert(flo);\n//      alert(typeof (flo))\n\n//二进制 八进制  十进制   十六进制\n       //八进制   必须以\"0\",后面的数字不能超过&ldquo;7&rdquo;,如果后面的数字超过8,那么会把这个数字当成十进制的数字来处理。\n         //var num8=077;\n         //alert(typeof    num8);\n\n      //十六进制  从0-9  A-F  ,必须以\"0x\"开头.,\n\n        //var num16=0xhff;\n        //alert(num16);\n\n//科学计数法   用e来表示某个数的10的一次方，e后面加数字表示某个数的N次方\n//\n//   var num=2e-2;\n//   alert(num)\n\n\n//最大值   Number.MAX_VALUE   最大值\n\n    //alert(Number.MAX_VALUE );\n \n\n\n  //最小值   Number.MIN_VALUE   最小值\n   //alert(Number.MIN_VALUE);\n         \n\n//Number.POSITIVE_INFINITY\n\n//Number.NEGATIVE_INFINITY\n\n//Infinity无穷大\n//-Infinity无穷小\n\n//alert(Number.POSITIVE_INFINITY===Infinity)\n//alert(Number.NEGATIVE_INFINITY===-Infinity)\n\n//isFinite()  判断一个数字是否是无穷的\n\n\n//NaN  not a  number\n\n//    var num=1;\n//  var str=\"abc\";\n//  alert(num*str);\n//isNaN()    not a  number?  不能转换成数字返回为真，能转换为数字返回为假。\n\n//   var str=\"123\";\n//   alert(isNaN(str));\n \n\n\n\n  //引用类型\n//     obj=new  Object(2); \n//     alert(typeof    obj);\n//     obj={};\n//     alert(typeof    obj);\n\n//    var arr=[1,2,3];\n//    alert(typeof    arr);\n\n\n//   function fun () {\n//   alert(\"我是一个函数\");\n//   \n//   }\n//      \n//   alert(typeof    fun)\n//          \n     \n</script>\n</body>\n</html>\n```\n\n&nbsp;\n\n## 3.javascript运算符\n\n**一、运算符和操作数的组合就称为表达式。**\n\n<br />**二、javascript运算符**<br /><br />**(一) 算术运算符**<br />　　+ - * / % var++ ++var var-- --var<br />　　A. +<br />　　　　(1) 用于数值的运算<br />　　　　(2) 用于字符串的连接<br />　　***************************<br />　　任何的数据类型和字符串相加都是等于相加以后的字符串<br />　　*************************<br /><br />　　B. %<br />　　　　(1)用于取余数，判断奇数或者是偶数<br />　　　　(2)一般不用于小数，因为结果不确定。<br /><br />　　C. var++和++var<br />　　　　++在前面，自己先加，然后再赋值。<br />　　　　++在后面，先赋值，然后自己再加。\n\n<br />**(二) 关系运算符(比较运算符)**<br />　　< > <= >= == === != !==<br />　　(1)他运算的结果**都是布尔值**<br />　　(2)都是**字符串**的时候，他会先转换成ASCII码然后进行比较他们的第一个字母。<br />　　(3)都是**数值**的时候，他会正常比较<br />　　(4)当一个**字符串**，另一个是**数值**的时候，把字符串尝试转换成数值类型，然后进行比较，如果不能转换成数值类型，则会返回NaN,然后返回假<br />　　(5) undefined null<br />　　(6)如果两个都是**数值型字符串**，那么他们也是只比较第一个<br />　　(7)如果一个数值和布尔值进行比较，会把布尔值转换为数值再进行比较，true为1，false为 0\n\n　　**A. == 只比较值是否相等**<br />　　(1) 比较字符串的时候是比较他们的ASCII码是否相等<br />　　(2) 比较两个数值的时候是比较他们的数值是否相等<br />　　(3) 比较函数的时候，判断他们的内存位置是否相等。<br />　　**B. === 不但比较值是否相等，还比较类型是否相等。**\n\n**(三) 赋值运算符**<br />　　= += -= *= /= %=<br />　　A. +=<br />　　var a=1;<br />　　a+=3； a=a+3<br />　　(1) 用于数值的相加再赋值<br />　　(2) 用于字符串的连接再赋值<br />　　***************************<br />　　任何的数据类型和字符串相加都是等于相加以后的字符串<br />　　*************************\n\n<br />**(四) 逻辑运算符(布尔运算符)**<br />　　与 and &amp;&amp; 或 or || 非not !\n\n　　**A. &amp;&amp;**<br />　　if(a &amp;&amp; b){<br />　　　　alert(\"两个都是真的\");<br />　　}else{<br />　　　　alert(\"至少有一个是假的\")<br />　　}<br />　　运算符两边只要有一个是假，那么他的运算结果就是假，只有两个都为真的时候，运算结果才是真的。\n\n　　**B.||**<br />　　if(a || b){<br />　　　　alert(\"至少有一个是真的\");<br />　　}else{<br />　　　　alert(\"两个都是假的\")<br />　　}<br />　　运算符两边只要有一个是真的那么他就是真的，只有当两个都是假的时候，他才是假的。\n\n**　　C.not !**<br />　　取反，假的变成真的，真的变成假的。\n\n　　(1)逻辑运算符可以对**任何类型的数据进行运算**但是在运算的时候，可以转换为对应的布尔值<br />　　　　**转换的规则**\n\n　　　　　　　　　Undefined-->false<br />　　　　　　　　　　　　&nbsp;Null-->false<br />　　　　 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Boolean-->就是本身的值<br />　　　　　　　　　　Number-->除了0以外都是真的<br />　　　　 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 　&nbsp;String-->除了空字符串意外都是真的<br />　　　　　　　　　　　 &nbsp;对象-->真的\n\n　　(2)类变量进行赋值<br />　　　　**var a= b &amp;&amp; c**<br />　　　　如果一个运算数是**对象**，另一个是&nbsp;**Boolean 值**，返回**该对象**。<br />　　　　如果**两个运算数都是对象**，返回**第二个对象**。<br />　　　　如果**某个运算数是 null**，返回&nbsp;**null**。<br />　　　　如果**某个运算数是 NaN**，返回&nbsp;**NaN**。<br />　　　　如果**某个运算数是 undefined**，**发生错误**。<br />　　　　**var a=b ||c**<br />　　　　如果一个运算数是**对象**，并且该对象**左边的运算数值均为 false**，则返回**该对象**。<br />　　　　如果**两个运算数都是对象**，返回**第一个对象**。<br />　　　　如果**最后一个运算数是 null**，并且**其他运算数值均为 false**，则返回&nbsp;**null**。<br />　　　　如果**最后一个运算数是 NaN**，并且**其他运算数值均为 false**，则返回&nbsp;**NaN**。<br />　　　　如果**某个运算数是 undefined**，**发生错误**。\n\n&nbsp;\n\n**(五) 一元运算符**<br />　　typeof + - delete new ++ --<br /><br />　　**A. +**<br />　　　　正号、正数<br /><br />　　**B.delete**<br />　　　　删除对象的方法或是属性<br /><br />　　**C.new**<br />　　　　用来创建一个对象\n\n**(六) 特殊的运算符**\n\n　　， () = ? :<br />　　A. ,<br />　　用来一次声明多个变量<br /><br />　　B.()<br />　　(1) 运算的时候有优先级的作用<br />　　　　a*(b-c)<br />　　(2) 运行一段函数<br /><br />　　C.根据表达式的计算结果有条件的为变量赋值<br />　　格式: var 变量= Boolean expression?真值:假值\n\n<br />**(七) 位运算符**\n\n&nbsp;\n\n## 4.javascript数据类型强制转换和隐式转换\n\n### **javascript数据类型强制转换**\n\n### **一、转换为数值类型**\n\n**Number(参数)**&nbsp;把**任何的类型**转换为**数值类型**\n\n　　A.如果是**布尔值**，false为0，true为1<br />　　B.如果是**数字**，转换成为本身。将无意义的后导0去掉。<br />　　C.如果**Null**转换为0<br />　　D.如果是**undefined**&nbsp;转换为NaN not a number<br />　　E.如果**对象**则会先调用对象的 valueOf(),如果valueOf()返回的是NaN,然后再调用对象的 toString()<br />　　F.如果是**字符串**<br />　　　　1.如果字符串当中只有数字，转换为10进制(忽略前导0和后导0)<br />　　　　2.如果是有效的规范的浮点型，转换为浮点值(忽略前导0和后导0)<br />　　　　3.如果是空字符串，则转换为0<br />　　　　4.如果是其他的值，返回NaN<br /><br />**parseInt(参数1，参数2)**&nbsp;将**字符串**转换为**整数**<br />　　A.如果一个字符串**只包含数字**，则以10进制的方式转换为整型。<br />　　B.他会自动忽略字符串前面的空格，直到找到**第一个非空的数值字符串**，直到解析到**第一个非数值的字符串**结束。<br />　　C.如果字符串的第一个字符不是空格、数字、-，那么返回**NaN**<br />　　D.**参数1**<br />　　　　八进制 十进制 十六进制<br />　　　　**0**&nbsp;后面的数字不能超过7&nbsp;**0x**&nbsp;0-9 a-f<br />　　　**参数2**<br />　　　　控制解析模式 2-32<br /><br />**parseFloat()**&nbsp;将**字符串**转换为**浮点数**<br />　　A.字符串当中的.**只有第一个有效**，其他的都是无效的。<br />　　B.如果字符串是一个**有效的整数**，他返回的是整数，不会返回浮点数。\n\n&nbsp;\n\n### **二、转换为字符串类型**\n\n**1. String(参数)**\n\n可以将**任何的类型**转换为**字符串**<br />　　**null和undefined**: 也都会转换为字符串，分别是&nbsp;**null和undefined**<br />　　**布尔类型**:会返回**true或者false**<br />　　**数值类型**:**本身的字符串**\n\n**2.toString()**<br />调用的格式&nbsp;**对象.toString()**<br />作用是**将对象以字符串的方式来表示**<br />　　array.toString() 　　返回　　由，分割的字符串<br />　　Boolean.toString() &nbsp;&nbsp;返回　　值 true或者false<br />　　String.toString() 　&nbsp; 返回　　本身<br />　　Number.toString(参数) 返回　　本身的字符串形式<br />　　控制输出模式 ：2-32<br /><br />注意：null和undefined没有toString()方法\n\n**三、转换为布尔类型**<br />Boolean() 可以将**任何类型的值**转换为**布尔值**<br />**转换为假**: \"\"、 0、 NaN 、undefined、 false<br />其他的全部都**转换为真**。\n\n&nbsp;\n\n### **javascript数据类型隐式转换**\n\n### **一、函数类**\n\n&nbsp;\n\n**isNaN()**\n\n　　该函数会对参数进行**隐式的Number()转换**，如果转换不成功则返回true;<br />**alert()**<br />　　输出的内容隐式的转换为字符串\n\n### **二、运算符类**\n\n**1.算数运算符**<br />- * / %<br />如果**操作数不是数值**，将会**隐式的调用Number()函数**，按照这个函数的转换规则进行转换，<br />如果**转换不成功**，整个表达式返回**NaN**\n\n+<br />如果操作数**都是数值**，然后进行**相加**<br />**任何数据类型**和**字符串**相加，都会隐私的调用他们的**toString()方法**，然后**返回他们拼接的结果**。<br />如果操作数**都是布尔值**，那么进行**Number()转换**，false为0，true为1，**进行相加**。\n\n**2.关系运算符**<br />关系运算符的操作数**可以是任何类型**，如果操作数**不是数值类型，将会隐式的转换**\n\n(1)他**运算的结果**都是**布尔值**<br />(2)**都是字符串**的时候，他会先**隐式转换成ASCII码**然后进行**比较他们的第一个字母**。<br />(3)**都是数值**的时候，他会**正常比较**<br />(4)当一个是**字符串**，另一个是**数值**的时候，把**字符串尝试转换成Number()数值类型**，然后进行比较，如果不能转换成数值类型，则会返回NaN,然后返回假<br />(5) undefined == null<br />(6)如果两个**都是数值型字符串**，那么他们**隐式转换成ASCII码**,也是**只比较第一个**<br />(7)如果是一个**数值**和**布尔值**进行比较，会把**布尔值隐式转换为数值**再进行比较，true为1， false为 0\n\n**3.等性运算符 == !=**<br />会**对操作数隐式的转换后再比较值**\n\n(1)如果其中至少有一个是**布尔值**，那么会**隐式的调用Number()进行转换**，然后比较。<br />(2)如果一个为**字符串**。另一个为**数值**，那么会**隐式的调用Number()**对字符串进行转换，<br />如果转换不成功，则返回false;<br />(3) undefined == null<br />(a) 比较**字符串**的时候是**比较他们的ASCII码是否相等**<br />(b) 比较**两个数值**的时候是**比较他们的数值是否相等**<br />(c) 比较**函数**的时候，**判断他们的位置是否相等**。<br /><br />**4.逻辑运算符**<br />A. 放在表达式里面用于判断。<br />B. 给变量赋值<br />var a= b &amp;&amp; c<br />如果一个运算数是**对象**，另一个是**隐式的调用Boolean()函数**，返回**该对象**。<br />如果两个运算数**都是对象**，返回**第二个对象**。<br />如果**某个运算数是 null**，返回&nbsp;**null**。<br />如果**某个运算数是 NaN**，返回&nbsp;**NaN**。<br />如果**某个运算数是 undefined**，发生**错误**。<br /><br />var a=b ||c<br />如果一个运算数是对象，并且该对象左边的运算数隐式的调用Boolean()函数,并且其值为 false，则返回该对象。<br />如果两个运算数**都是对象**，返回**第一个对象**。<br />如果**最后一个运算数是 null**，并且其他运算数值均为 false，则返回 null。<br />如果**最后一个运算数是 NaN**，并且其他运算数值均为 false，则返回 NaN。<br />如果某个运算数是 undefined，发生错误。\n\n&nbsp;\n\n### **三、语句类**\n\nif(表达式){\n\n}else{\n\n}\n\n格式: var 变量= Boolean expression?真值:假值\n\nwhile(){\n\n}<br />**if语句和三元表达式里面的表达式会隐式的调用Boolean()函数**，按照这个函数的转换规则，转换 为相应的布尔值\n\n&nbsp;\n\n## 5.javascript流程控制\n\n流程:就是程序代码的执行顺序。\n\n流程控制:通过规定的语句让程序代码有条件的按照一定的方式执行。\n\n### 一、顺序结构\n\n按照书写顺序来执行，是程序中最基本的流程结构。\n\n### 二、选择结构(分支结构、条件结构)\n\n根据给定的条件有选择的执行形相应的语句。\n\n（1） if else\n\n**1.单路分支**\n\n```\n//条件可以是表达式也可以是任何的数据类型\n//大括号会把他里面的代码当作一个整体来运行，如果只有一条语句，可以省略大括号\nif(条件){\n条件成立执行的语句\n}\n\n```\n\n**2.双路分支**\n\n```\nif(条件){\n条件成立的时候执行的代码\n}else{\n条件不成立的时候执行的代码\n}\n\n```\n\n**3.多路分支**\n\n```\nif(条件1){\n条件1成立执行的代码\n}else if(条件2){\n条件2成立执行的代码\n}else if(条件3){\n条件3成立执行的代码\n}......else{\n如果上述条件都不成立执行的代码\n}\n\n```\n\n**4.嵌套分支**\n\n```\nif(条件1){\nif(){\n}else if(){\n\n}....\n\n}else if(条件2){\n条件2成立执行的代码\n}else if(条件3){\n条件3成立执行的代码\n}......else{\n如果上述条件都不成立执行的代码\n}\n\n```\n\n（2）switch\n\n```\nswitch(变量任何的数据类型){\ncase 值1:\n表达式1；\nbreak;\ncase 值2:\n表达式3；\nbreak;\n.........\ndefalut:\n表达式\n}\n\n```\n\n**多个选择进行相应的匹配**\n\n*************************************************<br />1.当判断某种范围的时候最好用if语句，当判断单个值时候用switch<br />2.条件满足的情况不可以重复，会发生不可预期的错误。<br />*************************************************\n\n### <br />三、循环结构&nbsp;\n\njavascript&nbsp;循环结构中包含以下三种循环\n\n**for()循环**\n\n**while()循环**\n\n**do...while()循环**\n\n&nbsp;\n\nfor()循环: 重复执行，符合条件的代码段。\n\n**在for循环中须注意的一点：初始值与循环条件之间须用&ldquo;;&rdquo;分号分隔开来。**\n\n语法结构：\n\n**for(初始值;循环条件;值的递增与递减){**\n\n**&nbsp; 循环体代码；**\n\n**}**\n\n示例：\n\n```\n<script type=\"text/javascript\">\n\nvar arr=new Array(1,2,3,4,5,6);\nalert(arr.length);\nfor(i=0;i<arr.length;i++){\n  document.write(arr[i]);\n}\n\n</script>\n\n```\n\n　　\n\n**while循环，当满足循环条件时，执行循环。**\n\n示例：\n\n```\n<script>\n\nvar num=1; //初始化变量的值；\nwhile(num<=6){//循环条件\n document.write(num+\"<br/>\");\n num++; //值递增1；\n}\n\n</script>\n\n```\n\n　　\n\ndo...while()循环；\n\n语法结构：\n\n**do{**\n\n**&nbsp; 代码块；**\n\n**}while(循环条件)**\n\ndo...while循环，先执行一次，再判断条件是否符合;如符合，继续执行循环体中的代码块。\n\n示例：\n\n```\n<script>\n\nvar num=0;\ndo{\n    document.write(num);\n    num++;\n}while(num<10)\n\n</script>\n\n```\n\n　　\n\n### 四、跳转语句\n\n在循环控制语句中，当 满足指定条件 的时候，退出循环 或者是退出 当前 循环的语句。<br /><br />**1.break;**<br />格式:break;<br />跳出并且**终止循环**，如果后面有代码，则继续往下执行。<br />**2.continue;**<br />格式:continue;<br />跳出并且**终止当前的循环**，如果下个值仍满足循环条件，则继续循环。\n\n***********************************************************<br />**break:**<br />A.switch,指的是当满足某个条件后，退出switch语句<br />B.用在循环语句当中，跳出并且终止循环，如果后面有代码，则继续往下执行。<br />**continue:**<br />A.只能用在循环语句当中， 跳出并且终止当前的循环，如果下个值仍满足循环条件，则继续 循 环。<br />B.最好用适当的语句代替continue\n\n&nbsp;\n\n### 五、标签语句:\n\n用来退出多层循环<br /><br />格式:\n\n```\n   out:\n    for (var i=0; i<5; i++) {\n         document.write(\"第一层循环:\"+i);\n          document.write(\"<br/>&amp;nbsp;&amp;nbsp;\");\n          for (var j=0; j<5; j++) {\n              if(j==3){\n                 continue  out;   \n              }\n              document.write(\"第二层循环:\"+j);\n          }\n          document.write(\"<br/>\");\n    }\n    alert(\"我退出去了\");\n```\n\n标签名:语句；\n\n注意:**标签名只可以作用于break 或continue，可以用于跳出两层循环**\n\n### 六、with\n\nwith(){\n\n}\n\n```\n    //with语句      用于设置代码在  特定对象中 的  作用域。\n//   with(document){\n//    write(1);\n//    write(2);\n//    write(3);\n//    write(4);\n//    write(5);\n//    write(\"后盾网\");\n//    }\n```\n\n**就是把document.write(1);**\n\n**　　　document.write(2);....前面的document省略　**<br />**用于设置代码在对象中的作用域。**<br />*************************************************<br />不建议使用。<br />*************************************************\n\n**&nbsp;动态地改变document的title**\n\n```\n  //document.title=\"后盾网\";\n//  var a=\"后盾网\";\n//  var title=\"后盾网视频教程\";\n//  with(document){\n//    title=a;\n//  }\n```\n\n&nbsp;\n\n## 6.javascript函数\n\n将完成某一特定功能的代码集合起来，可以重复使用的代码块。\n\n### **一、函数的声明方式(创建)**\n\nA.基本语法<br />function 关键字<br />function 函数名([参数1],[参数2]....){<br />函数体<br />[retrun] //返回值<br />}<br /><br />B.字面量定义的形式(匿名函数)\n\nvar 变量=function ([参数1],[参数2]....){<br />函数体<br />[retrun] //返回值<br />}\n\nC.以对象的形式来声明\n\nnew 关键字。\n\nvar 变量=new Function([参数1],[参数2]...,\"函数体\");\n\n```\n//函数的第一种声明方式\n /*\n   function math () {\n   var num1=parseFloat(prompt(\"请输入\",\"\"));\n   var num2=parseFloat(prompt(\"请输入\",\"\"));\n   var operator=prompt(\"请输入运算符\",\"\");\n   var result;\n   switch (operator) {\n   case \"+\":\n      result=num1+num2;\n      break;\n   case \"-\":\n      result=num1-num2;\n      break;\n\n   case \"*\":\n      result=num1*num2;\n      break;\n   case \"/\":\n      result=num1/num2;\n      break;\n   default:\n      result=\"输入有误\";\n      \n   }\n     return result;\n   }\n\n   \n   alert(math());\n   alert(math())\n   */\n//函数的第二种声明方式\n /*\n  var fun=function  () {\n   var num1=parseFloat(prompt(\"请输入\",\"\"));\n   var num2=parseFloat(prompt(\"请输入\",\"\"));\n   var operator=prompt(\"请输入运算符\",\"\");\n   var result;\n   switch (operator) {\n   case \"+\":\n      result=num1+num2;\n      break;\n   case \"-\":\n      result=num1-num2;\n      break;\n\n   case \"*\":\n      result=num1*num2;\n      break;\n   case \"/\":\n      result=num1/num2;\n      break;\n   default:\n      result=\"输入有误\";\n      \n   }\n     return result;\n   }\n      \n alert(fun());\n */\n //函数的第三种声明方式\n\n//  var fun=new Function(alert(\"测试\"))\n//  fun();\n<br />\n```\n\n### 二、函数的调用方式:\n\nA.函数名() 、变量名();\n\nB.(function () {alert(\"测试\");})();\n\n### <br />三、两种声明方式的区别\n\n1.如果两个函数的命名相同，后面的将会覆盖前面的函数。<br /><br />2.**以基本语法声明的函数，会在代码运行的时候，提前加载到内存当中（就是以function开头的格式）**，以供以后使用，<br />但是**以字面量形式命名的函数，会在执行到的时候，才进行赋值**。\n\n3.在不同的<script></script>块中的函数，使用和调用的时候，**应该先定义，后执行**。\n\n&nbsp;\n\n### **四、函数的参数和return语句**\n\n**一、参数(最多是25)**<br />可以动态的改变函数体内对应的变量的类型或值，使同一函数体得到不同的结果。\n\n形参:在定义函数的时候，函数括号中定义的变量叫做形参。<br />实参:调用函数的时候，在括号中传入的变量或值叫做实参。\n\n**1.参数的类型**<br />可以是任何的数据类型<br /><br />**2.参数的个数(最多是25)**<br />A.实参和形参数量相等，一一对应。\n\nB.**形参**的数量**多于实参**<br />************************************************<br />不会报错，但是**多出的参数他的值**，**会自动赋值为undefined**<br />************************************************<br />C.**实参**的数量**多于形参**<br />************************************************<br />不会报错，但是**要得到多出的实参的值**，要**用arguments对象**\n\n************************************************<br /><br />**二、Arguments对象**\n\n每创建一个函数，该函数就会隐式创建一个**arguments对象**，他**包含有实际传入参数的信息**。\n\n**1.length 检测实际传入参数的个数**<br />**2.callee 对本身的调用**<br />**3.访问传入参数的具体的值。([下标])(如arguments[0])**\n\n**三、函数重载**<br /><br />**同一个函数因为参数的类型或数量不同，可以对应多个函数的实现，每种实现对应一个函数体。**\n\n<br />**四、return 语句**\n\n**一、停止并且跳出当前的函数**<br />1.在ruturn 语句后面的函数体内所有内容都不会输出。<br />2.在函数体内可以有多个return语句，但是只会执行一个。(判断语句)\n\n**二、给函数返回一个值 return [返回值];**<br />1.返回值可以是任何的数据类型<br />2.只能返回一个返回值。<br />3.如果函数没有返回值，那么这个函数的值就会自动的赋值为undefined\n\n&nbsp;\n\n### **五、javascript 内置顶层函数**\n\n名词解释:\n\n1.函数\n\n2.内置: ECMAscript<br /><br />内置函数:ECMAscript 自带的函数 Number()<br /><br />宿主函数: BOM DOM alert() prompt() confirm();<br /><br />**//confirm() 弹出一个带有确定和取消按钮的一个对话框，确定返回真，取消返回假。**\n\n3.顶层<br /><br /><br />字符串函数：字符串.函数（）<br /><br />数组函数<br /><br />顶层对象的函数，可以作用于任何对象。\n\n<br />**内置顶层函数**\n\n**1.escape() 对字符串进行编码**\n\n**2.unescape() 对编码的字符串进行解码**\n\n**3.Number() 转换成数值类型**\n\n**4.String() 转换成字符串类型**\n\n**5.Boolean() 转换成布尔类型**\n\n**6.parseInt() 将字符串转换为整型**\n\n**7.parseFloat() 转换为小数**\n\n**8.isNaN() 判断一个数能否转换为数值类型。**\n\n**9.isFinite() 判断一个数是否为有穷的数字。将不是有穷的数字或不能转换为数值类型的数返回假。**\n\n**10.eval() 将字符串转换成javascript命令执行(必须符合javascript语法规范，否则会出错)。**\n\n<br />IE:<br />eval() 在当前作用域生效<br />window.eval() 在当前作用域生效<br />execScript()\n\nFF:\n\neval() 在当前作用域生效<br />window.eval() 在全局生效\n\n&nbsp;\n\n**使代码在作用于上兼容IE**\n\n```\nfunction evals (str) {\n   if(typeof str!=\"string\"){\n     return;\n   }\n  if(window.execScript){\n     window.execScript(str);\n  }else{\n    window.eval(str);\n  }\n\n}\n```\n\n&nbsp;\n\n## 7.javascript数组\n\n数组是一个可以存储 一组 或是 一系列 相关数据 的 容器\n\njavascript数组可以存储任何类型的值\n\n### 一、如何创建数组\n\n**(1) 通过对象的方式来创建。**\n\n```\nvar a=new Array();\n\n```\n\n**A.直接赋值**\n\n```\nvar a=new Array(元素1，元素2，元素3，元素4,........)\n\nvar a=new Array(数值)\n\n```\n\n如果只有一个元素，并且这个元素是数值类型的，那么他就是指定数组的长度。\n\n并且他的值都是undefined\n\n数组的属性:length属性\n\n**B.声明以后再赋值**\n\n```\nvar a=new Array();\n\na[0]=1;\na[2]=2;\na[0]=3;\n\n```\n\n　　　　\n\n**(2)隐形声明的方式**\n\n```\nvar a=[];\n\n```\n\n**A.直接赋值:**\n\n```\nvar a=[1,2,3,4];\n\n```\n\n**B.声明以后再赋值**\n\n```\nvar a=[];\na[0]=1;\na[1]=2;\na[2]=3;\n```\n\n### 二、访问数组的元素\n\n通过数组的(中括号)下标访问。\n\n数组下标从0开始，他的最大值，是length属性-1&nbsp;\n\n### 三、遍历数组的元素\n\n(1) for 循环\n\n```\n//  for (var i=0; i<arr.length; i++) {\n//    alert(arr[i]);\n//  }\n//  var a=0\n```\n\n(2) while();\n\n```\n// while (a<arr.length) {\n//    alert(arr[a]);\n//    a++\n// }\n```\n\n(3) for in<br />有两个作用:<br />第一:用于数组的遍历<br />第二:用于对象属性的遍历\n\n```\n//for (var i in arr) {\n//  alert(arr[i]);\n//}&nbsp;\n```\n\n### 四、数组的分类\n\n**1.下标的类型**<br />　　A.下标是数字的类型的(索引数组)<br />　　B.下标是字符串类型的(关联数组)\n\n**2.维度来分类**<br />　　A.一维数组\n\n　　B.二维数组<br /><br />　　声明二维数组:<br />　　　　// var arr=[[1,2,3],[4,5,6]];<br />　　　　// alert(arr[1][1])\n\n注意:<br />1.可以存储任何类型的数据\n\n2.**只支持一维数组**。\n\n3.长度可变。\n\n4.如果是索引数组，下标始终从0开始，如果指定了长度，但是没有赋值，他的值就会自动赋值为undefined;\n\n## 8.javascript对象基础\n\n### **一、创建对象**\n\n**1.构造函数方法:**\n\n```\nfunction fun1 () {\n   \n}\n var obj=new fun1();\n alert(typeof obj)\n```\n\n&nbsp;**2.Object方法**\n\n```\n  var obj=new Object();\n  function Object () {\n  　　alert(123);\n  }\n```\n\n**3.json方法(javascript object notation) 原生格式**\n\n```\n    var obj={};\n```\n\n### **二、如何添加属性和方法**\n\n　　如果属性的值是函数，我们叫做他是对象的方法，否则叫做是属性。\n\n**1.构造方法**<br />　　A.声明以后再添加\n\n```\n//A.声明以后再添加\n    function fun1 () {\n    }\n    var obj=new fun1();\n    obj.name=\"zhangsan\";\n    obj.say=function  () {\n       var a=\"说话\";\n        return a;\n    }\n```\n\n　　B.声明的时候再添加\n\n```\n//A.声明的时候添加\nfunction fun1 () {\n  this.name=\"张三\";\n  this.eat=function  () {\n    alert(\"我能吃饭\");\n  }\n}\nvar obj=new fun1();\nobj.eat()\n```\n\n**2.json方法**<br />　　A.声明的时候添加<br />　　var obj={属性名:属性值，属性名2:属性值2，属性名3:属性值3，......};\n\n```\n//A.声明的时候添加\nvar obj={name:\"张三\",say:function  () {\n   alert(\"说话\");\n}};\n        \n        alert(obj.name);\n        obj.say();\n```\n\n　　B.声明以后再添加\n\n```\n//B.声明以后再添加\nvar obj={};\nobj.name=\"张三\";\nobj.sex=\"man\";\nobj.play=function  () {\n  alert(\"我会玩\");\n}\n\nalert(obj.sex);\nobj.play();\n```\n\n### **三、访问对象的属性和方法**\n\n引用值.属性\n\n引用值.属性();\n\n### **四、如何销毁对象**\n\njavascript自己的垃圾回收机制，就是在对象没有引用的时候释放内存(销毁);\n\n　　**对象=null;**\n\n### **五、如何删除对象的属性**\n\n**　　delete**\n\n## 9.javascript对象的遍历、内存分布和封装特性\n\n### **一、javascript对象遍历**\n\n**1.javascript属性访问**<br />对象.属性<br />对象[属性] //字符串格式\n\n```\n//javascript属性的访问方法\n   var ren ={};\n   ren.name=\"张三\";\n   ren.sex=\"男\";\n   ren.eat=function  () {\n      alert(\"吃饭\");\n   }\n   alert(ren.name);\n   alert(ren[\"name\"]);\n```\n\n**2.javascript属性遍历**\n\nfor in\n\n```\n //javascript属性遍历\n var ren ={};\n   ren.name=\"张三\";\n   ren.sex=\"男\";\n   ren.eat=function  () {\n      alert(\"吃饭\");\n   }\n\n  for (var i in ren) {\n     alert(ren[i])\n  }\n```\n\n&nbsp;通过arguments来遍历传入的参数\n\n```\n    function myArray () {\n    var lengs=  arguments.length;\n     for (var i=0; i<lengs; i++) {\n        this[i]=arguments[i];\n     }\n    }\n  var arr=new myArray(1,2,3);\n  alert(arr[0]);\n```\n\n### **二、内存分布**\n\n### **三、对象的特性之封装**\n\n把对象所有的组成部分组合起来，尽可能的隐藏对象的部分细节，使其受到保护。<br />只保留有限的接口和外部发生联系。\n\n　　一、工厂函数\n\n```\n     //工厂函数\n    function dianshi (color,size,brand) {\n      var Tv={};\n     Tv.color=color;\n     Tv.size=size;\n     Tv.brand=brand;\n     Tv.look=function  () {\n        alert(\"看电视\");\n     }\n     Tv.play=function  () {\n        alert(\"玩游戏\");\n     }\n     Tv.dvd=function  () { \n        alert(\"DVD\");\n     } \n       return Tv;\n      }\n\n    var ds=dianshi(\"red\",\"30inch\",\"sony\");\n    //alert(typeof ds)\n    alert(ds.color)\n\n    var ds1=dianshi(\"blue\",\"40inch\",\"changh\");\n    alert(ds1[\"size\"])\n```\n\n　　二、构造函数　\n\n```\n    //构造方法的形式\n    function Tv(color,size,brand) {\n      this.color=color;\n      this.size=size;\n      this.brand=brand;\n      this.play=function  () {\n         alert(\"玩游戏\");\n      }\n      this.look=function  () {\n         alert(\"看电视\");\n      }\n      this.dvd=function  () {\n         alert(\"DVD\");\n      }\n    }\n    var sony=new Tv(\"red\",\"20 inch\",\"sony\");\n      alert(sony.color)　\n```\n\n　　三、prototype方法\n\n**　　对原型属性的修改将影响到所有的实例**\n\n```\n //prototype方法\n    function Tv(color,size,brand) {\n      this.color=color;\n      this.size=size;\n      this.brand=brand;\n      this.play=function  () {\n         alert(\"玩游戏\");\n      }\n    \n    }\n\n      Tv.prototype.look=function  () {\n         alert(\"看电视\");\n      }\n      Tv.prototype.dvd=function  () {\n         alert(\"DVD\");\n      }\n      Tv.prototype.aaa={name:\"张三\"};\n    var sony=new Tv(\"red\",\"20 inch\",\"sony\");\n    var changhong =new Tv(\"red\",\"20 inch\",\"CH\");\n//      delete sony.color\n//      delete sony.play\n//      delete sony.look\n//      alert(sony.color)\n//       alert(sony.play)\n//       alert(sony.look)\n//   sony.look();\n//   changhong.look();\n\nalert(sony.aaa.name=\"李四\");\nalert(changhong.aaa.name);\n```\n\n　　四、混合方法\n\n```\n//混合方式\nfunction Tv(color,size,brand) {\n      this.color=color;\n      this.size=size;\n      this.brand=brand;\n      this.play=function  () {\n         alert(\"玩游戏\");\n      }\n    \n     Tv.prototype.aaa={name:\"张三\"};\n\n    }\n\n      Tv.prototype.look=function  () {\n         alert(\"看电视\");\n      }\n      Tv.prototype.dvd=function  () {\n         alert(\"DVD\");\n      }\n```\n\n## 10.javascript对象的继承和Object对象\n\n对象的一个类可以从现有的类中派生，并且拥有现有的类的方法或是属性，这和过程叫做继承。被继承的类叫做父类或是基类，继承的类叫做子类。\n\n(一个对象拥有另一个对象的属性和方法)\n\n<br />优点:<br /><br />提高代码的重用性\n\n提高代码的可维护性<br /><br />提高代码的逻辑性\n\n<br />**一、Object对象**\n\nvar obj=new Object()\n\n**属性：**\n\n**　　1.constructor**<br />　　对创建对象的函数的引用（指针）。\n\n```\n   //1.constructor \n       //对创建对象的函数的引用（指针）\n   \n     var obj=new Object();\n    alert(obj.constructor)\n```\n\n<br />**　　2.Prototype 原型**\n\n　　**********************************************<br />　　对该函数对象的对象原型的引用。是函数对象的默认属性\n\n　　**********************************************\n\n```\n//Prototype  \n      //对该函数对象的对象原型的引用。\n\n     var obj=new fun1();\n     function fun1 () {\n      this.name=\"zhangsan\";\n    }\n     alert(obj.prototype)\n    alert(fun1.prototype)\n```\n\n　　A.对象的共享属性存放到代码段当中。\n\n　　B.可以实现继承。\n\n**方法：**\n\n　　A.hasOwnProperty(property)<br />　　判断对象是否有某个特定的属性，返回true或者false\n\n```\nalert(obj.hasOwnProperty(\"name\"))\n```\n\n　　B.IsPrototypeOf(object)<br />　　判断该对象是否为另一个对象的原型。(用来检测对象的类型)\n\n　　var arr=new Array();<br />　　alert(Array.prototype.isPrototypeOf(arr))\n\n　　c.运算符<br />　　instanceof\n\n　　&nbsp;java 中的instanceof 运算符是用来在运行时指出对象是否是特定类的一个实例\n\n```\nalert(arr instanceof Array)\n```\n\n<br />**二、继承**\n\n1.原型继承\n\n```\nfunction person () {\n  this.name=\"张三\";\n  this.say=function  () {\n    alert(this.name)\n  }\n}\n\nfunction student () {\n}\nstudent.prototype=new person()\n\nvar zhangsan=new student ();\nzhangsan.say()\n```\n\n2.对象冒充的形式<br />　　A.call<br />　　obj1.fun.call(obj2,参数1......)\n\n　　B.apply<br />　　obj1.fun.call(obj2,[参数1，参数2....])\n\n　　让对象1的方法冒充成对象2的方法。\n\n```\n//对象冒充\n/*\nfunction person () {\n   this.name=\"张三\";\n   this.say=function  () {\n     alert(this.name)\n   }\n}\n\nfunction student () {\n  this.name=\"李四\";\n}\nvar ren=new person ();\nvar zhangsan=new student ();\n\nren.say.call(zhangsan)\n\n*/\nfunction person (name) {\n   this.name=name;\n   this.say=function  () {\n     alert(this.name)\n   }\n}\n\nfunction student () {\n window.person.apply(this,[\"zhangsan\"])\n}\n\nvar zhangsan=new student ();\n  alert(zhangsan.name)\n  zhangsan.say();\n```\n\n&nbsp;&nbsp;\n\n## 11.对象的继承顺序\n\n**一、对象的继承顺序**\n\n```\n//对象的继承顺序\n    Object.prototype.say=function  () {\n       alert(\"我是顶层的方法\");\n    }\n    function person () {\n        this.say=function  () {\n           alert(\"我是父类的方法\");\n        }\n    }\n\n    person.prototype.say=function  () {\n         alert(\"我是父类原型的方法\");\n    }\n\n    function study () {\n    this.say=function  () {\n       alert(\"本身的方法\");\n    }\n       \n    }\n    study.prototype=new person();\n     study.prototype.say=function  () {\n       alert(\"本身原型的方法\");\n     } \n    var zhangsan=new study ();\n    alert(zhangsan.say)\n```\n\n&nbsp;\n\n## 12.对象的分类\n\n### **一、对象的分类**\n\n**　　1.内置对象**<br />　　　　Global<br /><br />　　　　Math\n\n**　　2.本地对象**<br />　　　　Array<br />　　　　Number<br />　　　　String<br />　　　　Boolean<br />　　　　Function<br />　　　　RegExp\n\n　　3.宿主对象<br />　　　　DOM<br />　　　　BOM<br /><br />\n\n### **二、Math对象**\n\n格式: Math.方法(参数)<br /><br />　　**1.取绝对值**<br />　　　　Math.abs();\n\n```\nvar num1=-2.4;\nalert(Math.abs(num1))\n```\n\n　　**2.取近似整数**<br />　　　　//Math.round() 四舍五入\n\n```\n//Math.round()  //四舍五入\nvar num=2.1;\nalert(Math.round(num))\n```\n\n　　　　//Math.floor() 对数进行下取舍\n\n```\n//Math.floor()  对数进行下取舍\nalert(Math.floor(num))\n```\n\n　　　　//Math.ceil() 对数进行上取舍\n\n```\n//Math.ceil()  对数进行上取舍\nalert(Math.ceil(num))\n```\n\n**　　3.取最大值或最小值**<br />　　　　Math.max(参数....)<br />　　　　Math.min(参数.....)\n\n```\n      var num1=1;\n      var num2=2;\n      var num3=3;\n      var num4=4;\n      alert(Math.max(num1,num2,num3,num4))\n      alert(Math.min(num1,num2,num3,num4))\n```\n\n**　　4.取随机数**<br />　　　　Math.random();\n\n```\n      //Math.random();  从0到1的随机数\n```\n\n&nbsp;\n\n### **三、javascript字符串对象**\n\n　　**一、属性**<br />**　　　　1.length**<br />　　　　计算**字符串的长度**(不区分中英文)\n\n```\n     var str=\"bbs.houdunwang.com\";\n     alert(str.length)\n     var str1=\"你好\";\n     alert(str1.length);\n```\n\n**　　　　2.constructor**\n\n　　　　对象的构造函数\n\n```\n     var str=\"你好\";\n     alert(str.constructor)\n```\n\n&nbsp;\n\n**　　二、方法**\n\n**　　　　(1)获取类型**\n\n　　　　　　1.myString.charAt(num)<br />　　　　　　返回在**指定位置的字符**\n\n```\n    var str=\"你好\";\n    alert(str.charAt(2));\n    alert(str[0])\n```\n\n　　　　　　2.myString.charCodeAt(num)<br />　　　　　　返回**指定位置的字符的Unicode编码**\n\n```\n    var str=\"ABC\";\n      alert(str.charCodeAt(0))\n      alert(str.charCodeAt(1))\n      alert(str.charCodeAt(2))\n```\n\n　　　　　　3. String.fromCharCode()\n\n　　　　　　**接受**一个或多个指定的**Unicode值**，然后返回一个或多个**字符串**\n\n```\nalert(String.fromCharCode(65,66,67))\n```\n\n&nbsp;\n\n**　　　　(2)查找类型**<br />　　　　　　1.myString.indexOf()<br />　　　　　　返回**某个指定的字符串，在字符串中首次出现的位置**\n\n```\n    var str=\"你好\";\n      alert(str.indexOf(\"好\"))\n```\n\n　　　　　　2. myString.lastIndexOf()\n\n　　　　　　返回**一个字符串值末次出现的位置**\n\n```\n      var str=\"bbs.houdunwang.com\";\n      alert(str.lastIndexOf(\".\"))\n```\n\n　　　　　　3. myString.match()\n\n　　　　　　**在字符串中检索指定的值**，返回的值就是指定的类型(值)\n\n```\n      var str=\"后1盾2网3论4坛\";\n      alert(str.match(/\\d+/g))\n```\n\n　　　　　　4.search()\n\n　　　　　　只能作用于正则。\n\n　　　　　　5. myString.replace()<br />　　　　　　将字符串中的一些字符**替换**为另外一些字符\n\n```\n      var str=\"你好\";\n      alert(str.replace(\"好\",\"hao\"))\n```\n\n&nbsp;\n\n**　　　　(3) 截取类型**\n\n　　　　　　1.myString.slice(start,end)\n\n　　　　　　**截取从指定的开始位置，到结束位置(不包括)的所有字符串**。如果不指定结束位置，则从 指定的开始位置，取到结尾\n\n```\n    var str=\"你好\";\n    alert(str.slice(0,1))\n```\n\n　　　　　　2.substring(start,end)\n\n　　　　　　**截取从指定的开始位置，到结束位置(不包括)的所有字符串**。如果不指定结束位置，则从 指定的开始位置，取到结尾\n\n```\n   var str=\"你好\";\n    alert(str.substring(0))\n```\n\n　　　　　　3.substr(start,length)\n\n　　　　　　**从指定的位置开始截取指定长度的字符串**。如果没有指定长度，从指定开始的位置取到结尾\n\n```\n    var str=\"你好\";\n    alert(str.substr(0,1))\n```\n\n　　　　　　***************************************************************************\n\n　　　　　　　　slice(start,end) vs substring(start,end)<br />　　　　　　　　**slice参数可以是负数**，如果是负数，从-1开始指的是字符串结尾。<br />　　　　　　　　substring参数是负数的时候，会自动转换为0<br />　　　　　　***************************************************************************\n\n**　　　　(5)转换类型**\n\n　　　　　　1. split(\"分割位置\",[指定的长度])<br />　　　　　　**将一个字符串分割成数组**\n\n```\n  var str=\"你好，视频，教程\";\n  alert(str.split(\"，\"))\n```\n\n　　　　　　2.toLowerCase();\n\n　　　　　　**用于把字符串转换为小写**\n\n　　　　　　3.toUpperCase()<br />　　　　　　**将字符串转换为大写**\n\n```\n var str=\"HOUDUNWANG\";\n alert(str.toLowerCase(). toUpperCase())\n```\n\n&nbsp;\n\n**　　　　(6) 样式类型**\n\n　　　　　　1.fontcolor()<br />　　　　　　**给字符串指定颜色**，十六进制表示、red、rgb(255,0,0)\n\n```\n var str=\"你好\";\n //document.write(str.fontcolor(\"red\"))\n //document.write(str.fontcolor(\"#aaaaaa\"))\n```\n\n　　　　　　2.fontsize()\n\n　　　　　　**指定字符串的大小**&nbsp;(1-7)\n\n```\n document.write(str.fontsize(1))\n document.write(str.fontsize(3))\n document.write(str.fontsize(7))\n```\n\n&nbsp;\n\n### **四、JavaScript数组对象**\n\n**　　一、属性**<br />　　　　1.length<br />　　　　**设置或返回数组元素的数目**\n\n```\n       var a=[\"a\",\"b\",1,2];\n        alert(a.length=5)\n        alert(a.length)\n        alert(a)\n```\n\n　　　　2.constructor\n\n　　　　**返回构造函数的引用**\n\n```\n        var a=[\"a\",\"b\",1,2];\n        alert(a.constructor)\n```\n\n&nbsp;\n\n**　　二、方法**<br />**　　　　A.删除或添加类**\n\n　　　　　　1. myarr.push(数组元素......)<br />　　　　　　**向数组的末尾添加新的元素**，返回值是新数组的长度。<br />　　　　　　可以一次添加多个元素\n\n```\n           var a=[\"a\",\"b\",1,2];\n           alert(a.push(\"c\",\"d\"))\n           alert(a)\n```\n\n　　　　　　2. myarr.unshift(数组元素.....)\n\n　　　　　　**向数组的开头加入新的元素**，返回值是新数组的长度<br />　　　　　　可以一次添加多个元素\n\n```\n           var a=[\"a\",\"b\",1,2];\n           alert(a.unshift(\"c\",\"d\"))\n           alert(a)\n```\n\n　　　　　　3. myarr.pop()\n\n　　　　　　**删除数组的最后一个元素**，返回删除的元素\n\n```\n            var a=[\"a\",\"b\",1,2];\n           alert(a.pop())\n           alert(a)\n```\n\n　　　　　　4. myarr.shift()\n\n　　　　　　**删除数组的第一个元素**，返回删除的元素\n\n```\n           var a=[\"a\",\"b\",1,2];\n           alert(a.shift())\n           alert(a)\n```\n\n　　　　　　5.**万能的添加删除函数**\n\n　　　　　　myarr.splice(index,数量,添加的元素.....)<br />　　　　　　　　(1)**index　　 从何处开始添加或删除**，必须是数值类型(数组的下标)<br />　　　　　　　　(2)&nbsp;**数量 　　 规定了删除的个数**，如果是0，则不删除<br />　　　　　　　　(3)**&nbsp;需要添加的元素　　可以当作替换的元素**\n\n```\n            var a=[\"a\",\"b\",1,2];\n            alert(a.splice(1,0,\"c\",\"d\"))\n            alert(a)\n```\n\n　　　　　　　　************************************\n\n　　　　　　　　如果有删除的元素，返回删除的元素<br />　　　　　　　　************************************\n\n**　　　　B.数组的转换**<br />　　　　　　mystr.split()<br />　　　　　　myarr.join([分隔符])<br />　　　　　　**把数组元素按照指定分隔符组合成一个字符串**，如果没有指定分隔符，默认是用&ldquo;,&rdquo;<br />　　　　　　返回结果就是组合成的字符串\n\n```\n            var a=[\"a\",\"b\",1,2];\n            //alert(a.join())\n            alert(a.join(\"-\"))\n```\n\n&nbsp;\n\n**　　　　C.数组的分割**<br />　　　　　　myarr.slice()<br />　　　　　　**截取从指定的开始位置，到结束位置(不包括)的元素**。如果不指定结束位置，则从指定的开始位置，取到结尾(数组的下标)<br />　　　　　　支持负数(-1开头) 返回新数组。\n\n```\n            var a=[\"a\",\"b\",1,2,\"你好\",\"你好吗\"];\n             alert(a.slice(2,4))\n             alert(a.slice(2))\n             alert(a.slice(-2,-1))\n             alert(a)\n```\n\n&nbsp;\n\n**　　　　D.排序**<br />　　　　　　冒泡排序\n\n```\nfunction mySort (fun) {\n\nfor (var i=0; i<this.length; i++) {\n\n                  for (var j=0; j<this.length-i; j++) {\n                     if(this[j]>this[j+1]){\n                        var aa=this[j];\n                       this[j]=this[j+1]\n                       this[j+1]=aa;\n                     }\n                  }\n            }\n            return this\n}\n\n\n    Array.prototype.mySort=    mySort    \n      alert(arr.mySort())\n```\n\n**　　　　　　myarr.sort()**\n\n　　　　　　对数组进行排序，**如果没有参数，则按照字母的编码进行排序**，**如果要按照其他的顺序来排序，要提供一个函数**。<br />　　　　　　会提供两个参数(a,b)&nbsp;**此时按从小到大排序**<br />　　　　　　　　a<b a在b前<br />　　　　　　　　a=b<br />　　　　　　　　a<b\n\n```\n    var a=[12,34,123]\n    a.sort(function  (a,b) {\n       return a=b\n    });\n    alert(a)\n```\n\n**　　　　F.数组的连接**<br />　　　　　　myarr.concat()<br />　　　　　　连接两个或更多的数组，并返回新数组，但是对原数组没有任何影响.\n\n```\n    var a=[1,2,3];\n    //var b=[\"你好\",\"你好吗\",\"hello world\"]\n    //var c=[\"a\",\"b\",\"c\"]\n    //alert(a.concat(b,c))\n    //alert(a)\n    alert(a.concat(\"a\",\"c\"))\n    alert(a)\n```\n\n**　　　　G.自定义一个函数(删除数组的重复元素)**\n\n```\n    var arr=[1,5,2,3,5,4,5,6,7,8,5,9,5]\n    Array.prototype.deleteit=function deleteit(){\n        for(var i=0;i<=this.length;i++){\n            for(var j=i+1;j<=this.length;j++){\n                if(this[i]==this[j]){\n               //this.splice(this[j],1);\n                this.splice(j,1);\n                j--;\n               alert(\"第\"+i+\"个数字\"+\"[\"+this[i]+\"]\"+\"和第\"+j+\"个数字\"+\"[\"+this[j]+\"]\"+\"相同，已删除！\");\n                }\n            }\n        }return this;\n    }\n    alert(arr.deleteit())\n```\n\n&nbsp;\n\n### **五、javascript时间对象**\n\n日期对象\n\n在javascript中日期也是他的内置对象，所以我们要对日期进行获取和操作，必须实例化对象。\n\n**1.创建日期对象**<br />　　var dateobj=new Date();<br />　　将会包含本地时间的信息，包括年月日时分秒 星期<br /><br />　　A.可传入的参数格式<br />　　　　1.\"时:分:秒 日/月/年\" \"日/月/年 时:分:秒\" 字符串<br />　　　　2.年,月,日,时,分,秒 不能加\"\"<br />　　注意:月份和星期都是从0开始计算的\n\n<br />**2.获取日期信息的方法**<br />Date 对象方法<br />FF: Firefox, IE: Internet Explorer\n\n方法 描述 FF IE<br />Date() 返回当日的日期和时间。&nbsp;<br />getDate() 从 Date 对象返回一个月中的某一天 (1 ~ 31)。&nbsp;<br />getDay() 从 Date 对象返回一周中的某一天 (0 ~ 6)。&nbsp;<br />getMonth() 从 Date 对象返回月份 (0 ~ 11)。&nbsp;<br />getFullYear() 从 Date 对象以四位数字返回年份。&nbsp;<br />getYear() 请使用 getFullYear() 方法代替。&nbsp;<br />getHours() 返回 Date 对象的小时 (0 ~ 23)。&nbsp;<br />getMinutes() 返回 Date 对象的分钟 (0 ~ 59)。&nbsp;<br />getSeconds() 返回 Date 对象的秒数 (0 ~ 59)。 &nbsp;<br />getMilliseconds() 返回 Date 对象的毫秒(0 ~ 999)。&nbsp;<br />getTime() 返回 1970 年 1 月 1 日至今的毫秒数。&nbsp;<br />getTimezoneOffset() 返回本地时间与格林威治标准时间 (GMT) 的分钟差。&nbsp;\n\n&nbsp;\n\n**3.设置日期的方法**\n\n```\nsetDate() 设置 Date 对象中月的某一天 (1 ~ 31)。 \nsetMonth() 设置 Date 对象中月份 (0 ~ 11)。 \nsetFullYear() 设置 Date 对象中的年份（四位数字）。 \nsetYear() 请使用 setFullYear() 方法代替。 \nsetHours() 设置 Date 对象中的小时 (0 ~ 23)。 \nsetMinutes() 设置 Date 对象中的分钟 (0 ~ 59)。 \nsetSeconds() 设置 Date 对象中的秒钟 (0 ~ 59)。 \nsetMilliseconds() 设置 Date 对象中的毫秒 (0 ~ 999)。 \nsetTime() 以毫秒设置 Date 对象。 \nsetUTCDate() 根据世界时设置 Date 对象中月份的一天 (1 ~ 31)。 \nsetUTCMonth() 根据世界时设置 Date 对象中的月份 (0 ~ 11)。 \nsetUTCFullYear() 根据世界时设置 Date 对象中的年份（四位数字）。\nsetUTCHours() 根据世界时设置 Date 对象中的小时 (0 ~ 23)。 \nsetUTCMinutes() 根据世界时设置 Date 对象中的分钟 (0 ~ 59)。\nsetUTCSeconds() 根据世界时设置 Date 对象中的秒钟 (0 ~ 59)。\nsetUTCMilliseconds() 根据世界时设置 Date 对象中的毫秒 (0 ~ 999)。\n\n```\n\n　　\n\n## 13.DOM_document对象\n\n**javascript-document对象详解**\n\nDOM document(html xml) object modle\n\n**document对象(DOM核心对象)**\n\n<br />**作用：**<br />　　1.内容 innerHTML<br />　　2.属性<br />　　3.样式\n\n<br />**document对象**\n\n**一、属性**<br />　　title&nbsp;**返回或设置当前文档的标题**\n\n```\n   alert(document.title)\n   document.title=\"你好\";\n```\n\n　　URL&nbsp;**返回当前文档的url**\n\n```\n   alert(document.URL)\n   alert(location.href)\n```\n\n　　bgColor&nbsp;**设置文档的背景色**\n\n```\n   document.bgColor=\"red\";\n```\n\n　　fgColor&nbsp;**设置文档的前景色(设置文字颜色)**\n\n```\n   document.fgColor=\"blue\";\n```\n\n&nbsp;\n\n**二、方法**\n\n<br />　　getElementById(idname) 返回拥有指定id的（第一个）对象的引用 &nbsp;\n\n　　　　**innerHTML 属性设置或返回表格行的开始和结束标签之间的 HTML**\n\n```\n    var div1=document.getElementById(\"one\");\n    alert(div1.innerHTML)\n```\n\n```\n\n 你好\n\n```\n\n　　getElementsByTagName(tagname)&nbsp;**返回带有指定标签名的对象的集合**\n\n```\nvar divs=document.getElementsByTagName(\"div\");\n  var lengths=divs.length;\n  //alert(lengths)\n  //通过下标来访问\n    //alert(divs[1].innerHTML)\n    for (var i=0; i<lengths; i++) {\n    alert(divs[i].innerHTML)\n    }\n```\n\n　　getElementsByName(name)&nbsp;**返回带有指定name指定名称的对象的集合**\n\n```\n  var inputs=document.getElementsByName(\"text1\");\n  var lengths=inputs.length;\n  alert(lengths)\n```\n\n　　write()\n\n　　**************************************************************<br />　　**getElementsByName(name)在IE中是不兼容的，在IE的表单中是可以兼容的**。<br />　　　　如果是IE：<br />　　　　　　**如果该对象的标准属性包含有name,那么可以正确的使用**。否则不可以使用。<br />　　　　如果是FF:<br />　　　　　　该方法可以适用于任何情况。<br />　　　　结论:<br />　　　　　　**主要是适用于表单**。<br />　　**************************************************************\n\n&nbsp;\n\n　　getElementsByClassName()&nbsp;**返回带有指定classname指定名称的对象的集合**\n\n```\n   //var aaas=document.getElementsByClassName(\"aaa\");\n   //alert(aaas.length)\n```\n\n```\n\n test\n\n\n test\n\n```\n\n&nbsp;\n\n　　**********************************************************************<br />　　不兼容，可以自己写一个兼容性函数\n\n```\n   function byclass (classname) {\n      if(document.getElementsByClassName){\n        return document.getElementsByClassName(classname)\n      }else{\n        var tag= document.getElementsByTagName(\"*\");\n         var lengths=tag.length;\n         var divs=[];\n         for (var i=0; i<lengths; i++) {\n            if(tag[i].className==classname){\n               divs.push(tag[i])\n            }\n         }\n         return divs;\n      }\n      \n   }\n\n alert(byclass(\"aaa\").length)\n```\n\n　　**********************************************************************\n\n&nbsp;\n\n**三、dcoument对象的集合**\n\n　　A.all&nbsp;**所有元素的集合，不兼容**\n\n```\nalert(document.all)\n```\n\n　　B.forms 返回对文档中所有form对象的引用\n\n```\nalert(document.forms.length)\n```\n\n　　　　通过集合来访问相应的对象\n\n　　　　1.通过下标的形式，弹出**表单的name**\n\n```\n   //访问 1.下标\n     //alert(document.forms[1].name)\n```\n\n　　　　2.通过name形式\n\n```\n    //2.name属性\n      alert(document.forms[\"myform1\"].text1.value)\n```\n\n&nbsp;\n\n　　C. anchors&nbsp;**返回对文档中所有anchors 对象的引用(即所有a链接)**\n\n&nbsp;\n\n　　D.images 返回对文档中所有image 对象的引用<br />　　F.links 返回对文档中所有area 对象和link对象的引用\n\n&nbsp;\n\n**javascript-对文档对象的内容、属性、样式的操作**\n\n**一、操作内容**<br />　　1.　　innerHTML 用来设置或获取**对象起始和结束标签内的内容**(**识别html标签**)<br />　　2.　　innerText 用来设置或获取**对象起始和结束标签内的内容**&nbsp;(兼容IE，**获取文本**)<br />　　　　 &nbsp;&nbsp;textContent用来设置或获取**对象起始和结束标签内的内容**&nbsp;(兼容FF，**获取文本**)\n\n　　　　　　**注意区别innerHTML和innerText，第一个会识别样式，第二个只会识别文本**\n\n**　　　　　　　　但是在FF中不兼容，要使用textContent，以下是兼容函数****　**\n\n**　　　　　　　　支持document.all的是IE**\n\n```\nfunction getContent (objs,val) {\n     if(document.all){\n       if(val){\n         objs.innerText=val\n       }else{\n          return  objs.innerText\n       }\n     }else{\n        if(val){\n         objs.textContent=val\n       }else{\n          return  objs.textContent\n       }\n     \n     }\n  }\n  window.onload=function  () {\n    var div1=document.getElementById(\"div1\");\n    var div2=document.getElementById(\"div2\");\n    var but=document.getElementById(\"but\");\n     but.onclick=function  () {\n        //var contents=div1.innerHTML;\n        //div2.innerHTML=contents;\n         var contents=getContent(div1)\n        getContent(div2,contents);\n     }\n  }\n```\n\n```\n\n  <h3>\n    this is my test！\n  </h3>\n\n<input type=\"button\" value=\"&darr;&darr;\" id=\"but\" >\n\n\n```\n\n　　3.　　outerHTML 用来设置或获取包括本对象在内起始和结束标签内的内容(识别html标签)&nbsp;\n\n　　　　 &nbsp;&nbsp;outerText 用来设置或获取包括本对象在内起始和结束标签内的内容\n\n&nbsp;\n\n**二、操作属性**<br />　　**1.直接操作**<br />　　　　对象.属性<br />　　　　对象.属性=值 (设置、获取、添加属性(属性值))\n\n**　　2.获取和设置**\n\n　　　　getAttribute(\"属性\") 获取给定的属性的值<br />　　　　setAttribute(\"属性\",\"值\") 设置或是添加属性\n\n```\n  window.onload=function  () {\n    var links=document.getElementsByTagName(\"a\")[0];\n    //alert(links.href)\n    //links.href=\"2.html\";\n    //alert(links.title)\n    //links.title=\"test\";\n    //links.title=\"test\";\n     //getAttribute(\"属性\")  获取给定的属性的值\n     // setAttribute(\"属性\",\"值\")  设置或是添加属性\n\n     alert(links.getAttribute(\"href\"))\n     links.setAttribute(\"href\",\"2.html\")\n    \n  }\n```\n\n&nbsp;\n\n**三、操作样式**\n\n**　　1.行内样式**<br />　　　　对象.style.属性<br />　　　　对象.style.属性=值 (设置、获取、添加属性)\n\n```\nwindow.onload=function  () {\n  var one=document.getElementById(\"one\");\n  one.onmouseover=function  () {\n    //alert(one.style.color)\n     one.style.color=\"blue\";\n     one.style.backgroundColor=\"red\";\n     one.style.fontSize=\"13px\";\n  }\n  one.onmouseout=function  () {\n     one.style.color=\"red\";\n     one.style.backgroundColor=\"blue\";\n     one.style.fontSize=\"11px\";\n\n  }\n}\n```\n\n```\n[链接](#)\n```\n\n　　　　****************************************************\n\n　　　　遇到属性是\"-\"链接的，要将\"-\"去掉，后面的单词的首字母大写<br />　　　　****************************************************\n\n**　　**\n\n**　　2.css层叠样式**\n\n　　　　**1>通过更改ID来更改样式（一般不用，不更改ID）**\n\n```\n<style>\n   #one{\n     width:200px;\n     height:200px;\n     border:1px solid red;\n     color:red;\n     font-size:14px;\n     padding:24px;\n   }\n   #two{\n     width:200px;\n     height:200px;\n     border:1px solid blue;\n     color:blue;\n     font-size:19px;\n     padding:15px;\n   }\n</style>\n<script>\n   window.onload=function  () {\n      var one=document.getElementById(\"one\");\n      var but=document.getElementById(\"but\");\n      but.onclick=function  () {\n        one.id=\"two\";\n      }\n\n   }\n</script>\n```\n\n```\n\n   你好\n\n<input type=\"button\" value=\"更改样式\" id=\"but\">\n```\n\n　　　　**2>通过className更改样式**\n\n```\n<style>\n   .div1{\n     \n     height:200px;\n     border:1px solid red;\n     color:red;\n     font-size:14px;\n     padding:24px;\n   }\n   .div2{\n     width:200px;\n     height:200px;\n     border:1px solid blue;\n     color:blue;\n     font-size:19px;\n     padding:15px;\n   }\n</style>\n```\n\n```\n   window.onload=function  () {\n      var one=document.getElementById(\"one\");\n      var but=document.getElementById(\"but\");\n      but.onclick=function  () {\n        one.className=\"div2\";\n      }\n    }\n```\n\n```\n\n   你好\n\n<input type=\"button\" value=\"更改样式\" id=\"but\">\n```\n\n　　　　*******************************************\n\n　　　　适合批量更改<br />　　　　*******************************************<br /><br />　　　　**3>更改或者获取或者设置某个属性的值**\n\n<br />　　　　**************************************************************<br />　　　　document.styleSheets[下标].rules[下标].style.属性<br />　　　　document.styleSheets[下标].rules[下标].style.属性=值\n\n　　　　**document.styleSheets 样式表的集合，即是<style><strong></style>的数量**</strong><br />　　　　**document.styleSheets[0].rules**&nbsp;**样式规则的列表，即其中的等的个数**<br />　　　　document.styleSheets[0].rules.style 样式规则的集合<br />　　　　document.styleSheets[下标].rules[下标].style.属性\n\n```\nalert(document.styleSheets[0].rules[0].style.width)\n```\n\n**　　　　适用于IE**\n\n**　　　　****************************************************************\n\n&nbsp;\n\n　　　　**************************************************************<br />　　　　document.styleSheets[下标].cssRules[下标].style.属性<br />　　　　document.styleSheets[下标].cssRules[下标].style.属性=值<br />**　　　　适用于FF**<br />　　　　***************************************************************\n\n&nbsp;\n\n**　　　　4> 动态的添加删除css样式规则**<br />　　　　document.styleSheets[下标].insertRule(\"选择器{属性:值}\",位置)&nbsp;**FF**&nbsp;w3c<br />　　　　deleteRule(位置)&nbsp;**FF**&nbsp;w3c\n\n<br />　　　　document.styleSheets[下标].addRule(\"选择器\",\"属性:值\",位置)&nbsp;**IE**&nbsp;removeRule(位置) IE\n\n```\n //document.styleSheets[0].addRule(\".div1\",\"margin:200px\",0);\n//document.styleSheets[0].removeRule(1);\n```\n\n&nbsp;\n\n**　　3.行内样式和css层叠样式通用的方式**<br />　　　　对象.currentStyle.属性 IE 用来获得实际的样式属性<br />　　　　getComputedStyle(对象,null) FF 用来获得实际的样式属性\n\n```\n       //对象.currentStyle.属性  IE   用来获得实际的样式属性\n    //getComputedStyle(对象,null)   FF  用来获得实际的样式属性\n       //alert(one.currentStyle.width)\n       alert(getComputedStyle(one,null).width)\n```\n\n　　　　*******************************\n\n　　　　只能获取不能设置<br />　　　　*******************************\n\n&nbsp;\n\n## 14.BOM_window对象\n\n**javascript浏览器对象模型-window对象**\n\nBOM Browser Object Model<br />window对象 是BOM中所有对象的核心。\n\n**一、属性**\n\n**1.(位置类型-获得浏览器的位置)**<br />　　IE:<br />　　window.screenLeft<br />　　可以获得浏览器距屏幕左上角的**左边距**<br />　　window.screenTop<br />　　可以获得浏览器距屏幕左上角的**上边距**\n\n```\n//IE\n   //左边距\n   //alert(screenLeft)\n   //上边距\n   //alert(screenTop)\n```\n\n　　FF:<br />　　alert(screenX)<br />　　alert(screenY)\n\n```\n//FF\n  //左边距\n  // alert(screenX)\n  //上边距\n  // alert(screenY)\n```\n\n**　　(获得浏览器的尺寸)**\n\n　　FF:window.innerWidth 获得窗口的宽度<br />　　window.innerHeight 获得窗口的高度\n\n```\n//获取浏览器的尺寸\n\n    //FF:\n      //alert(window.innerWidth);\n      //alert(window.innerHeight);\n\n    //IE和FF通用:\n        alert(document.documentElement.clientWidth)\n        alert(document.documentElement.clientHeight)        \n```\n\n&nbsp;\n\n**2.关系类型**\n\n　　A.parent返回父窗口<br />　　B.top 返回顶层窗口\n\n　　C.self===window 相当于window\n\n**3.stutas 设置窗口状态栏的文本**\n\n```\n    window.status=\"自定义的状态栏文字\"\n```\n\n<br />**二、方法**\n\n**1.窗体控制**<br />　　**A.对窗体的移动**\n\n　　window.moveBy(x,y) 相对于当前位置沿着X\\Y轴移动指定的像素，如负数是反方向<br />　　moveTo(x,y) 相对于浏览器的左上角沿着X\\Y轴移动到指定的像素，如负数是反方向\n\n```\n     //位置\n     moveBy(100,100);\n      //moveTo(200,200)\n```\n\n　　**B.窗体尺寸的改变**\n\n　　resizeBy(x,y) 相对于当前窗体的大小，调整宽度和高度<br />　　resizeTo(x,y) 把窗体调整为指定宽度和高度\n\n```\n      //尺寸\n       window.resizeBy(100,100)\n       resizeTo(400,400)\n```\n\n<br />**2.对窗体滚动条的控制**\n\n　　scrollBy(x,y) 相对于当前滚动条的位置移动的像素(前提有滚动条)<br />　　scrollTo(x,y) 相对于当前窗口的高度或宽度，移动到指定的像素\n\n```\n     //scrollBy(0,100)\n    //scrollTo(0,200)\n```\n\n**3.时间间隔的函数**\n\n　　setInterval(\"函数或者代码串\",指定的时间(毫秒)) 按照指定的周期(毫秒)不断的执行函 数或是代码串\n\n```\n// setInterval(\"函数或者代码串\",指定的时间(毫秒))  按照指定的周期(毫秒)不断的执行函  数或是代码串\n\n//第一种调用方式\n // setInterval(\"alert('你好')\",1000);\\\n//  var i=0\n// setInterval(changes,1000)\n// function changes () {\n//  alert(i)\n//  i++\n// }\n\n//第二种调用方式\n//var a=0;\n//setInterval(function  () {\n//  alert(a);\n//  a++\n//},1000)\n\n//第三种调用方式\n //var i=0;\n// setInterval(\"changes(0)\",1000)\n// function changes (i) {\n//  alert(i)\n//  i++\n// }\n```\n\n　　clearInterval()\n\n```\n//停止调用\n \n  window.onload=function  () {\n    var t=setInterval('alert(\"你好\")',5000)\n   var aa=document.getElementById(\"stop\");\n   aa.onclick=function  () {\n    clearInterval(t)\n   }\n  }\n```\n\n```\n  <input type=\"button\" value=\"停止\" id=\"stop\">\n```\n\n　　setTimeout(\"函数或者代码串\",指定的时间(毫秒)) 在指定的毫秒数后只执行一次函数或代码。<br /><br />　　clearTimeout()\n\n```\nwindow.onload=function  () {\n    var aa =setTimeout(\"alert('bbs.houdunwang.com')\",5000)\n   var bb=document.getElementById(\"stop\");\n   bb.onclick=function  () {\n    clearTimeout(aa)\n   }\n  }\n```\n\n&nbsp;\n\n**4.打开新的窗口**\n\n　　open(url,name,feafurse,replace) 通过脚本打开新的窗口\n\n```\n    window.onload=function  () {\n    var names=document.getElementById(\"names\");\n      var but=document.getElementById(\"but\");\n      but.onclick=function  () {\n      open(\"26.1.html\",\"windows\",\"status=0,menubar=0,toolbar=0\")\n      }\n\n    }\n```\n\n&nbsp;\n\n**javascript-History、Location、Screnn对象**\n\n**一、history对象**<br />包含浏览器访问过的url\n\n　　**1.属性**<br />　　length 返回**浏览器历史记录的数量**\n\n```\nalert(history.length)\n```\n\n　　**2.方法**\n\n　　back() 后退<br />　　forward() 前进<br />　　go(number) 如果参数是**正数**，那么就是**前进相应的数目**，如果是**负数**那么反之，如果是**0那么就是刷新**\n\n```\n  window.onload=function  () {\n     var one=document.getElementById(\"one\");\n     one.onclick=function  () {\n        //history.forward()\n        history.go(3)\n     }\n  }\n```\n\n&nbsp;\n\n**二、location对象 包含有当前url的相关信息**\n\n**　　1.属性**<br />　　href&nbsp;**设置或 返回完整的url**\n\n```\nalert(location.href);\n```\n\n　　search&nbsp;**返回url?后面的查询部分**\n\n```\nalert(location.href=\"location2.html?1234\")\n```\n\n&nbsp;\n\n```\nalert(location.search)\n```\n\n&nbsp;\n\n**　　2.方法**<br />　　assign()&nbsp;**加载新的文档**\n\n```\nlocation.assign(\"location2.html\");\n```\n\n　　reload(boolean) 重新加载文档, 当参数是true，任何时候都会重新加载，false的时候，只有在文档改变的时候才会加载，否则直接读取内存当中的。\n\n```\nlocation.reload()\n```\n\n　　replace() 用新的文档代替当前的文档 （没有历史记录）\n\n```\nlocation.replace(\"location2.html\")\n```\n\n&nbsp;\n\n**三、screen对象**<br />记录了客户端显示屏的信息<br /><br />　　属性：<br />　　availHeight 返回显示屏幕的高度 (除 Windows 任务栏之外)。<br />　　availWidth 返回显示屏幕的宽度 (除 Windows 任务栏之外)。\n\n　　height 返回显示屏幕的高度。<br />　　width 返回显示屏幕的宽度。\n\n```\n  document.write(screen.availHeight)\n  document.write(\"<br/>\")\n  document.write(screen.height)\n   document.write(\"<hr/>\")\n  document.write(screen.availWidth)\n   document.write(\"<br/>\")\n  document.write(screen.width)\n```\n\n&nbsp;\n\n## 15.JavaScript操作节点\n\n**javascript-节点属性详解**\n\n根据 DOM，HTML 文档中的每个成分都是一个节点。\n\n　　DOM 是这样规定的：\n\n　　　　**整个文档**是一个**文档节点** <br />　　　　**每个 HTML 标签**是一个**元素节点** <br />　　　　**包含在 HTML 元素中的文本**是**文本节点** <br />　　　　**每一个 HTML 属性**是一个**属性节点** <br />　　　　**注释**属于**注释节点 **\n\n**一、如何获得节点引用**\n\n** 1.旧的获取节点引用方式**<br />      　　getElementById()\n\n```\n//查找文档中的一个特定的元素，最有效的方法是 getElementById(),一定要有document\nvar outdiv=document.getElementById(\"outdiv\");\n\nvar childs=outdiv.childNodes;\n     \nalert(childs.length);\n```\n\ngetElementByTagName() 　　//返回带有指定标签名的对象集合\n\n getElementByName()<br /> *******************************************<br />劣势：<br />  1.浪费内存<br />  2.逻辑性不强<br /> *******************************************\n\n** 2.通过节点 关系属性 获得节点的引用**<br />      对象.parentNode  获得**父节点**的引用\n\n```\n    //父节点\n     \n     var innerdiv=document.getElementById(\"innerdiv\");\n     var father=innerdiv.parentNode;\n     alert(father.nodeName);\n```\n\n对象.childNodes 获得**子节点**的集合\n\n 对象.firstChild 获得**第一个子节点**<br />      对象.lastChild   获得**最后个子节点**\n\n```\n     //最后一个子节点\n      \n     var innerdiv=document.getElementById(\"innerdiv\");\n     var last=innerdiv.lastChild;\n     alert(last.nodeName);\n```\n\n对象.nextSibling 获得**下一个兄弟节点**的引用\n\n对象.previousSibling 获得**上一个兄弟节点**的引用\n\n****************************************************************<br />劣势:兼容性不好。\n\nFF会把例如后面的所有的空白节点都读成一个子节点\n\nIE会把最后一个空白节点读成子节点\n\n****************************************************************<br />      <br />**二、节点的信息(属性)**\n\n             节点类型           节点名字        节点值<br />            nodeType(数值)       nodeName      nodeValue\n\nnodeType 属性返回以数字值返回指定节点的节点类型。\n\n如果节点是元素节点，则 nodeType 属性将返回 1。\n\n如果节点是属性节点，则 nodeType 属性将返回 2\n\n元素节点 1 标签名 null <br />属性节点         2                属性名        属性值<br />文本节点         3                 #text         文本<br />注释节点         8                 #comment      注释的文字<br />文档节点         9                 #document     null\n\n```\n        alert(outdiv.nodeType)     \n       alert(document.nodeType)\n       alert(document.nodeName)\n        alert(document.nodeValue)\n```\n\n&nbsp;\n\n**三、兼容性的方法**\n\n&nbsp;\n\n**javascript-节点的增、删、改、查实例讲解**\n\n**一、创建节点**\n\n** 　　1>创建元素节点**<br />      　　　　document.createElement(\"元素标签名\");\n\n```\n   var elea=document.createElement(\"a\");\n     elea.href=\"#\";\n     elea.title=\"我是一个链接\";\n     elea.innerHTML=\"链接\";\n     elea.style.color=\"red\";\n\n    document.body.appendChild(elea);\n```\n\n&nbsp;\n\n 　　**2>创建属性节点**<br />       　　　　document.createAttribute(\"属性名\");<br />       　　　　对象.属性=\"属性值\"<br />       　　　　对象.setAttribute(属性名,属性值)<br />       　　　　对象.getAttribute(属性名,属性值)<br />    　　**3>创建文本节点**<br />       　　　　对象.innerHTML=\"\";\n\n```\nelea.innerHTML=\"链接\";\n```\n\n　　　　document.createTextNode(\"文本\");\n\n```\n    var h3text=document.createTextNode(\"测试\");\n```\n\n&nbsp;\n\n**二、追加到页面当中**\n\n　　父对象.appendChild(追加的对象) 插入到最后\n\n```\ndocument.body.appendChild(elea);\n```\n\n　　父对象.insertBefore(要插入的对象,之前的对象) 插入到最后\n\n```\n   var innerdiv=document.getElementById(\"innerdiv\");\n    var spans=innerdiv.firstChild;\n    var eleh3=document.createElement(\"h3\");\n    var h3text=document.createTextNode(\"测试\");\n        eleh3.appendChild(h3text);\n        innerdiv.insertBefore(eleh3,spans);\n```\n\n&nbsp;\n\n**三、修改(替换)节点**\n\n　　父对象.replaceChild(要修改的对象,被修改的);\n\n```\n   innerdiv.replaceChild(elep,eleh3)\n```\n\n&nbsp;\n\n**四、删除节点**\n\n 　　父对象.removeChild(删除的对象)<br />　　如果确定要删除节点，最好也清空内存  对象=null;\n\n```\n   innerdiv.removeChild(elep)\n   elep=null;\n   alert(elep.innerHTML);\n```\n\n&nbsp;\n\n## 16.JavaScript操作表单\n\n**javascript-对表单的操作**\n\n```\n<form name=\"myform\" id=\"form1\" action=\"\" method=\"post\">\n   姓名:<input type=\"text\" name=\"names\" id=\"names\" value=\"zhangsan\"><br/>\n   年龄:<input type=\"text\" name=\"age\" value=\"19\"><br/>\n   性别：<input type=\"radio\" name=\"sex\" value=\"man\">男 <input type=\"radio\" name=\"sex\" value=\"woman\">女<br/>\n   爱好:<input type=\"checkbox\" name=\"like[]\" value=\"play\">玩  \n        <input type=\"checkbox\" name=\"like[]\" value=\"run\">跑步\n        <input type=\"checkbox\" name=\"like[]\" value=\"study\">学习<br/>\n    毕业院校:\n        <select name=\"school\">\n          <option>\n              北京大学\n          </option>\n                <option>\n              清华大学\n          </option>\n                <option>\n              南开大学\n          </option>\n        </select>\n        <br/>\n    简介：<textarea cols=\"60\" rows=\"10\" name=\"info\">测试</textarea><br/>\n    <input type=\"button\" value=\"提交\">\n</form>\n```\n\n&nbsp;\n\n**一、获得表单引用**<br />    　　1>通过直接定位的方式来获取<br />      　　document.getElementById();<br />      　　document.getElementsByName();<br />      　　document.getElementsByTagName();\n\n```\n      //var myform=document.getElementById(\"form1\");\n      //alert(myform.name);\n      //var myform=document.getElementsByName(\"myform\")[0];\n     // alert(myform.name);\n```\n\n　　2>通过集合的方式来获取引用\n\n 　　document.forms[下标]<br />      　　document.forms[\"name\"]<br />      　　document.forms.name\n\n```\n    //var myform=document.forms[0];\n    //var myform=document.forms[\"myform\"];\n    //var myform=document.forms.myform;\n    //alert(myform.name)\n```\n\n　　3>通过name直接获取&ldquo;（只适用于表单）\n\n　　document.name\n\n```\n    //var myform=document.myform;\n    //alert(myform.name);\n```\n\n&nbsp;\n\n**二、获得表单元素的引用**\n\n 　　1>直接获取<br />      　　document.getElementById();<br />      　　document.getElementsByName();<br />      　　document.getElementsByTagName();\n\n```\n     //直接获取\n      // var names=document.getElementById(\"names\").value;\n      // alert(names)\n\n      //var names=document.getElementsByName(\"names\")[0].value\n     // alert(names)\n```\n\n&nbsp;\n\n 　　2>通过集合来获取<br />       　　表单对象.elements  获得表单里面所有元素的集合<br />       　　表单对象.elements[下标]<br />       　　表单对象.elements[\"name\"]<br />       　　表单对象.elements.name\n\n```\n      //var eles=document.myform.elements.length;\n      //alert(eles)\n\n       //var names=document.myform.elements[0].value;\n         //var names=document.myform.elements[\"names\"].value;\n        // var names=document.myform.elements.names.value;\n       //alert(names)\n```\n\n&nbsp;\n\n 　　3>直接通过name的形式<br />    　　<br />      　　表单对象.name\n\n```\n         //var age=document.myform.age.value;\n         //alert(age);\n        // var info=document.myform.info.value;\n        // alert(info)\n\n```\n\n&nbsp;\n\n**三、表单元素共同的属性和方法**\n\n** 　　1>获取表单元素的值**<br />      　　表单元素对象.value   获取或是设置值\n\n```\n     // document.getElementById(\"fom1\").names\n     //  document.forms[0].elements[0]\n```\n\n&nbsp;\n\n** 　　2>属性**<br />      　　**disabled**  获取或设置表单控件是否禁用 true false\n\n```\n    //disabled \n     //var names=document.myform.names.disabled =true;\n     //var school=document.myform.school.disabled =true;\n```\n\n&nbsp;\n\n　　**form** 指向包含本元素的表单的引用\n\n```\n     //form\n      //var age=document.forms[0].elements.age.form\n      //alert(age.name);\n```\n\n&nbsp;\n\n** 　　3>方法**<br />      　　blur()失去焦点<br />      　　focus()  获得焦点\n\n```\n     //foucs\n       var name=document.forms.myform.elements.names.focus();\n```\n\n&nbsp;\n\n**四、文本域**<br />   　　<input type=\"text\" ><br />   　　***********************************<br />     　　操作文本域的值<br />     　　value 属性     设置或者获取值\n\n```\n   //文本域\n   //var names=document.forms[0].elements.names.value=\"lisi\";\n   //alert(names)\n```\n\n　　***********************************\n\n&nbsp;\n\n**五、单选按钮**\n\n 　　*******************************************<br />    　　checked  **返回或者设置单选的选中状态**<br />             　　true 选中     false 未选中\n\n 　　value 属性 获取选中的值，必须先判断选中状态。\n\n```\n       var sex=document.myform.sex;\n          sex[0].checked=true;\n         for (var i=0; i<sex.length; i++) {\n           if(sex[i].checked){\n             alert(sex[i].value)\n           }\n         }\n```\n\n　　*******************************************\n\n&nbsp;\n\n**六、多选按钮**\n\n 　　*******************************************<br />    　　checked  **返回或者设置单选的选中状态**<br />             　　true 选中     false 未选中\n\n 　　value 属性 获取选中的值，必须先判断选中状态。\n\n```\n      var likes=document.forms.myform[\"like[]\"];\n        likes[2].checked=true;\n      for (var i=0; i<likes.length; i++) {\n         if(likes[i].checked){\n           alert(likes[i].value);\n         }\n```\n\n　　*******************************************\n\n&nbsp;\n\n**七、下拉框**\n\n 　　************************************************<br />    　　selected  **设置或返回下拉框的选中状态**<br />                　　true 选中     false 未选中\n\n 　　selectedIndex 设置或返回下拉框被选中的索引号\n\n```\n       //var school=document.myform.school;\n      // var school=document.myform.school.options;\n       //alert(school.length)\n       //selected设置选中状态\n         //school[1].selected=true;\n       //selectedIndex设置选中状态\n          //school.selectedIndex=2;\n```\n\n　　************************************************\n\n&nbsp;\n\n**八、文本区域**\n\n ***************************************<br />   <textarea></textarea><br />    value 操作值\n\n```\n<form name=\"myform\">\n  <textarea name=\"info\" rows=\"7\" cols=\"60\"></textarea>\n</form>\n```\n\n```\nvar str=info.value;\n```\n\n***************************************\n\n<br />**九、验证表单**\n\n   1.事件<br />     onsubmit  当表单提交的时候触发的事件<br />     onblur<br />     onfocus<br />     onchange\n\n 2.return false; 阻止事件的默认行为(适用于所有事件)\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\" />\n<title>javascript-对表单的操作实例讲解(上)</title>\n<script>\n  function check (obj) {\n    if(obj.names.value==\"\"){\n      alert(\"姓名不能为空\");\n      return false;\n    }\n    if(obj.age.value==\"\"){\n     alert(\"年龄不能为空\");\n      return false;\n     }\n     var statu=0;\n     for (var i=0; i<obj.sex.length; i++) {\n      if(obj.sex[i].checked){\n        statu=1;\n      }\n     }\n     if(!statu){\n       alert(\"性别不能为空\");\n       return false;\n     }\n  }\n</script>\n</head>\n\n<body>\n<form name=\"myform\" id=\"form1\" action=\"\" method=\"post\" onsubmit=\" return check(this)\">\n   姓名:<input type=\"text\" name=\"names\" id=\"names\" ><br/>\n   年龄:<input type=\"text\" name=\"age\" ><br/>\n   性别：<input type=\"radio\" name=\"sex\" value=\"man\">男 <input type=\"radio\" name=\"sex\" value=\"woman\">女<br/>\n   爱好:<input type=\"checkbox\" name=\"like[]\" value=\"play\">玩  \n        <input type=\"checkbox\" name=\"like[]\" value=\"run\">跑步\n        <input type=\"checkbox\" name=\"like[]\" value=\"study\">学习<br/>\n    毕业院校:\n        <select name=\"school\">\n          <option>\n              北京大学\n          </option>\n                <option>\n              清华大学\n          </option>\n                <option>\n              南开大学\n          </option>\n        </select>\n        <br/>\n    简介：<textarea cols=\"60\" rows=\"10\" name=\"info\">测试</textarea><br/>\n    <input type=\"submit\" value=\"提交\">\n</form>\n\n</body>\n</html>\n```\n\n&nbsp;\n\n**十、提交方法**\n\n 表单的方法<br />   表单对象.submit()\n\n```\n<script>\n  window.onload=function  () {\n    var subs=document.myform.subs;\n     subs.onclick=function  () {\n      document.myform.action=\"1.html\";\n      document.myform.submit();\n    }\n\n    setTimeout(function  () {\n       document.myform.action=\"2.html\";\n      document.myform.submit();\n    },4000)\n  }\n</script>\n```\n\n&nbsp;\n\n&nbsp;\n\n## 17.JavaScript事件\n\n**javascript事件基础和事件绑定**\n\n**一、事件驱动**<br />    **1.事件**<br />      　　javascript侦测到的用户的操作或是页面的一些行为(怎么发生的)<br />    **2.事件源**<br />      　　引发事件的元素。(发生在谁的身上)<br />**3.事件处理程序**<br />      　　对事件处理的程序或是函数 (发生了什么事)\n\n&nbsp;\n\n**二、事件的分类**\n\n```\n<body>\n<input type=\"button\" value=\"改变\" id=\"one\" >\n\n</body>\n```\n\n**1.鼠标事件**\n\nonclick\n\n```\n    var one=document.getElementById(\"one\");\n    one.onclick=function  () {\n      alert(\"点击\");\n    }\n```\n\n```\n    var one=document.getElementById(\"one\");\n\none.onclick=aa;\nalert(aa)\nfunction aa () {\n  alert(\"点击\");\n}\n```\n\nondblclick\n\n onmousedowm<br />    onmouseup<br />    onmousemove<br />    onmouseover<br />    onmouseout\n\n** 2.键盘事件**<br />   onkeyup<br />   onkeydown<br />   onkeypress   鼠标按下或按住\n\n** 3.表单事件**<br />  onsubmit<br />  onblur<br />  onfoucs<br />  onchange\n\n** 4.页面事件**<br />  onload<br />  onunload<br />  onbeforeunload\n\n&nbsp;\n\n**三、如何绑定事件**\n\n  1.在脚本中绑定<br />  2.直接在HTML元素绑定<br />  3.<script for=\"two\" event=\"onclick\"><br />    alert(\"我是DIV2\");<br />     </script>\n\n<br />**四、同一个事件绑定多个事件处理程序**\n\n 1.自己写的<br />   2.IE:<br />     **对象.attachEvent(\"事件(on)\",\"处理程序\")** 　　 添加<br />     **对象.dettachEvent(\"事件(on)\",\"处理程序\")** 　　 删除\n\n```\n   one.attachEvent(\"onclick\",aa);\n   one.attachEvent(\"onclick\",bb);\n   function aa () {\n      alert(\"aa\");\n   }\n      function bb() {\n      alert(\"bb\");\n   }\n\n   one.detachEvent(\"onclick\",bb)\n   one.attachEvent(\"onclick\",function  () {\n    alert(\"cc\");\n   });\n      one.detachEvent(\"onclick\",function  () {\n    alert(\"cc\");\n   });\n```\n\n&nbsp;\n\n FF:<br />     **对象.addEventListener(\"事件\",\"处理程序\",布尔值)** 　　  添加<br />     **对象.removeEventListener(\"事件\",\"处理程序\",布尔值)** 　　 删除\n\n```\none.addEventListener(\"click\",bb,false)\n  one.addEventListener(\"click\",aa,false)\n   one.addEventListener(\"click\",function  () {\n     alert(\"cc\");\n   },false)\n    one.removeEventListener(\"click\",function  () {\n     alert(\"cc\");\n   },false)\n function aa () {\n      alert(\"aa\");\n   }\n     function bb() {\n      alert(\"bb\");\n   }\n```\n\n<br />     \n\n**Mozilla中：&nbsp;**<br /><br />**addEventListener的使用方式：&nbsp;**<br /><br />**target.addEventListener(type, listener, useCapture);&nbsp;**<br /><br />**target： 文档节点、document、window 或 XMLHttpRequest。&nbsp;**<br />**type： 字符串，事件名称，不含&ldquo;on&rdquo;，比如&ldquo;click&rdquo;、&ldquo;mouseover&rdquo;、&ldquo;keydown&rdquo;等。&nbsp;**<br />**listener ：实现了 EventListener 接口或者是 JavaScript 中的函数。&nbsp;**<br />**useCapture ：是否使用捕捉，一般用 false 。例如：document.getElementById(\"testText\").addEventListener(\"keydown\", function (event) { alert(event.keyCode); }, false);&nbsp;**<br /><br />**IE中：&nbsp;**<br /><br />**target.attachEvent(type, listener);&nbsp;**<br />**target： 文档节点、document、window 或 XMLHttpRequest。&nbsp;**<br />**type： 字符串，事件名称，含&ldquo;on&rdquo;，比如&ldquo;onclick&rdquo;、&ldquo;onmouseover&rdquo;、&ldquo;onkeydown&rdquo;等。&nbsp;**<br /><strong>listener\n ：实现了 EventListener 接口或者是 JavaScript 中的函数。 \n例如：document.getElementById(\"txt\").attachEvent(\"onclick\",function(event){alert(event.keyCode);});&nbsp;&nbsp;</strong>\n\n&nbsp;\n\n**javascript事件对象实例讲解**\n\n**一、事件对象**<br />    用来记录一些事件发生时的相关的信息的对象<br />    1.只有当事件发生的时候才产生，只能在处理函数内部访问\n\n    2.处理函数运行结束后自动销毁。\n\n&nbsp;\n\n**二、如何获取事件对象**\n\n    IE：window.event\n\n    FF:<br />      对象.on事件=function (e){}\n\n<br />   <br />**三、事件对象的属性**\n\n**    1.关于鼠标事件的事件对象**\n\n     相对于**浏览器**位置的<br />     clientX  当鼠标事件发生的时候，鼠标相对于**浏览器X轴**的位置<br />     clientY  当鼠标事件发生的时候，鼠标相对于**浏览器Y轴**的位置\n\n 相对于**屏幕**位置的<br />     screenX   当鼠标事件发生的时候，鼠标相对于**屏幕X轴**的位置<br />     screenY&nbsp;当鼠标事件发生的时候，鼠标相对于**屏幕Y轴**的位置\n\n```\n     document.onmousemove=function  (e) {\n       var ev=e||window.event;\n       var cx=ev.clientX;\n       var cy=ev.clientY;\n       var sx=ev.screenX;\n       var sy=ev.screenY;\n       div1.innerHTML=\"cx:\"+cx+\"--cy:\"+cy+\"<br/>sx:\"+sx+\"--sy:\"+sy;\n     }    \n```\n\n&nbsp;\n\n 相对于**事件源**的位置<br />     IE:<br />      offsetX   当鼠标事件发生的时候，鼠标相对于**事件源X轴**的位置<br />      offsetY\n\n FF:<br />     layerX   当鼠标事件发生的时候，鼠标相对于**事件源X轴**的位置<br />     laterY\n\n```\n    div1.onclick=function  (e) {\n      var ev=e||window.event;\n      var ox=ev.offsetX ||ev.layerX;\n      var oy=ev.offsetY ||ev.layerY;\n    div1.innerHTML=\"ox:\"+ox+\"--oy:\"+oy;\n    }\n```\n\n&nbsp;\n\n** 2.关于键盘事件的事件对象**<br />   <br />     **　　keyCode  获得键盘码**<br />     　　　　空格:32   回车13  左上右下：37 38 39 40\n\n<br />     **　　altKey   判断alt键是否被按下**  按下是true 反之是false   布尔值\n\n```\n     document.body.onkeydown=function  (e) {\n       var ev=e||window.event;\n       alert(ev.keyCode)\n       alert(ev.altKey)\n       alert(ev.type)\n     }\n```\n\n&nbsp;\n\n ctrlKey<br />     shiftKey<br />     **type   用来检测事件的类型   主要是用于多个事件通用一个事件处理程序的时候**<br />   \n\n&nbsp;\n\n**javascript事件流讲解和实例应用**\n\n 当页面元素触发事件的时候，该元素的容器以及整个页面都会按照特定<br />  顺序相应该元素的触发事件，事件传播的顺序叫做事件流程。\n\n&nbsp;\n\n**一、事件流的分类**\n\n**    　　1.冒泡型事件(所有的浏览器都支持)**<br />      　　　　由明确的事件源到最不确定的事件源依次向上触发。\n\n**    　　2.捕获型事件(IE不支持 w3c标准 火狐)**<br />      　　　　不确定的事件源到明确的事件源一次向下触发。<br />       　　　　addEventListener(事件，处理函数，false)<br />       　　　　addEventListener(事件，处理函数，true)\n\n&nbsp;\n\n**二、阻止事件流**<br />    　　IE:<br />    　　　　事件对象.cancelBubble=true;   <br />    　　FF:<br />     　　　　事件对象.stopPropagation();\n\n&nbsp;\n\n**三、目标事件源的对象**<br />    　　IE：事件对象.srcElement<br />    　　FF:事件对象.target<br />    \n\n    <br />      \n","tags":["前端"]},{"title":"Hive学习笔记——常用语法","url":"/Hive学习笔记——常用语法.html","content":"## 1.查看表的列表\n\n```\nshow tables\n\n```\n\n## 2.创建表\n\n多个字段的时候需要指定用什么来分隔\n\n```\ncreate table test(id int,name string)row format delimited fields terminated by '\\t';\ncreate table test(id int,name string)row format delimited fields terminated by ',';\n\n```\n\n## 3.插入数据\n\n```\ninsert into table test values (1,'row1'),(2,'row2');\n\n```\n\n也可以select任意一张空表来insert\n\n```\ninsert into table default.example_table (select 1L,'xiaodou',array(1L,2L,3L) from default.test limit 1);\n\n```\n\n如果遇到 Unable to create temp file for insert values Expression of type TOK_FUNCTION not supported in insert/values\n\n```\ninsert into table test partition(ds=\"2019-08-20\") select 1L,2,1S,1.111,\"test\",2Y,true,array(1,2,3),array(1Y,1Y,1Y),map('name','Xiao','age','20'),map(10L,false,20L,true),\"lin\",\"tong\";\n\n```\n\n## 4.加载数据\n\n也可以使用Hadoop fs -put命令直接上传文件,注意文件中的分隔符需要和创建表的时候指定分隔符保持一致\n\n```\nload data local inpath 'XXXX' into table XXXX;\n\n```\n\n如果是加载分区的数据的话\n\n```\nload data local inpath 'xxx/role_id=1' into table XXX PARTITION(role_id=1);\n\n```\n\n## 5.查询数据\n\n```\nselect * from XXXX;\n\n```\n\n## 6.删除表\n\n```\ndrop table XXXX;\n```\n\n## 7.hive优化\n\n参考：[hive调优](https://juejin.im/post/6883077122953314317)\n\n## 8.analyze table\n\n参考\n\n```\nhttps://www.jianshu.com/p/7a2bd40a6632\n\n```\n\n## 9.统计hive表大小行数\n\n```\nhttp://wimperio.tk/2019/02/21/Hive-Table-Statistics/\n\n```\n\n## 10.查看hive表分区的location\n\n```\ndescribe formatted xxx.xxx partition (date=\"2021-07-17\",hour=\"06\")\n\n```\n\n## 11.查看hive表分区信息\n\n```\n describe formatted xxx.xxx partition (pdate='2017-01-23'); \n\n```\n\n## 12.hive表加字段\n\n```\nALTER TABLE xxx.xxx ADD COLUMNS (xxx_count1 bigint);\n\n```\n\n## 13.hive表修改表结构\n\n对于hive1和hive2，如果是parquet表修改字段类型产生冲突的时候，可能会报<!--more-->\n&nbsp;Unable to alter table. The following columns have types incompatible with the existing columns in their respective positions\n\n对于struct类型字段增加字段，也是一样的，所以只能删掉重建，而且在parquet表中struct类型中字段的顺序是没有要求的\n\nhive3的话可能会有不同\n\n```\nalter table xxx.xxx replace columns(column_2 string);\n\n```\n\n## 14.hive表修改字段\n\n对于hive1和hive2，如果是parquet表修改字段类型产生冲突的时候，可能会报&nbsp;Unable to alter table. The following columns have types incompatible with the existing columns in their respective positions\n\n对于struct类型字段增加字段，也是一样的，所以只能删掉重建，而且在parquet表中struct类型中字段的顺序是没有要求的\n\nhive3的话可能会有不同\n\n```\nalter table xxx.xxx change aaaa bbbb string;\n\n```\n\n## 15.hive表修改location\n\n```\nalter table xxx.xxx set location 'hdfs://heracles/user/video-mvc/hive/warehouse/t_m_cc'\n\n```\n\n## 16.hive表修改分区location\n\n```\nALTER TABLE `xxx.xxxx` PARTITION (pdate='2021-09-22',phour='00') SET LOCATION \"s3a://xxxx/xxxx/2021-09-22/00\";\n\n```\n\n## 17.调整字段位置\n\n```\nALTER TABLE xxx.xxxx  CHANGE `xxxaaa` `xxxaaa` array<string>  after `xxxbbb`\n\n```\n\n## **18.HQL with语法**\n\n```\nwith a as (select * from test)　\nselect * from a;\n\n```\n\n## **19.HQL 解析json**\n\n```\nselect get_json_object(context, '$.aaa'),count(*) as num  from .xxx where pdate = \"2021-09-29\" group by get_json_object(context, '$.aaa') order by num\n\n```\n\n## **20.hive设置严格模式**\n\n设置hive.mapred.mode的值为strict\n\n参考：[踩过数据仓库hive的坑：hive设置严格模式](https://blog.csdn.net/lv_hulk/article/details/106745636)\n\n## **21.HQL COALESCE函数**\n\nCOALESCE是一个函数， (expression_1, expression_2, ...,expression_n)依次参考各参数表达式，遇到非null值即停止并返回该值。如果所有的表达式都是空值，最终将返回一个空值。\n\n## **22.修改hive tblproperties**\n\n```\nalter table xx.xx set tblproperties('property_name'='new_value');\n\n```\n\n## **23.删除hive tblproperties**\n\n```\nalter table xx.xx UNSET TBLPROPERTIES ('property_name');\n\n```\n\n## **24.hive的用户认证**\n\n```\nhttp://lxw1234.com/archives/2016/01/600.htm\n\n```\n\n## **24.调整map和reduce任务的数量**　　\n\n```\nset hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;\nset mapred.map.tasks=10;　\n```\n\n参考：[【Hive任务优化】&mdash;&mdash; Map、Reduce数量调整](https://blog.csdn.net/u013332124/article/details/97373278)\n\n## **25.hive时间戳转换**\n\n```\nselect from_unixtime(unix_timestamp(str, \"yyyy-MM-dd'T'HH:mm:ss\"), 'yyyy-MM-dd HH:mm:ss') as xx_time from xx.xx;\n\n```\n\n## 26.hive直接写数据到HDFS或者S3路径下\n\n```\nINSERT OVERWRITE DIRECTORY 's3://xx/xx/xx' \nROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \nSELECT * FROM xx.xx where pdate=\"2022-08-06\";\n\n```\n\n官方文档\n\n```\nhttps://sparkbyexamples.com/apache-hive/export-hive-table-into-csv-file-with-header/\n\n```\n\n如果要控制数据只有一个文件的话，可以设置reduce任务的数量为1，然后添加 distribute by rand() 强行触发reduce任务\n\n```\nset mapred.reduce.tasks=1;\n\nINSERT OVERWRITE DIRECTORY 's3://xx/xx/xx' \nROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \nSELECT user_id FROM xx.xx where pdate=\"2022-08-06\" \ndistribute by rand();\n\n```\n\n参考：[hive控制文件生成个数](https://blog.csdn.net/u010010664/article/details/58054085)\n\n## 27.如果hive的array数组判断不等于NULL不生效\n\n先运行如下命令\n\n```\nset hive.cbo.enable=false;\n\n```\n\n　　\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n","tags":["Hive"]},{"title":"CSS学习笔记——基本写法","url":"/CSS学习笔记——基本写法.html","content":"## **1.div+css网页标准布局**\n\n**1.div**\n\n　　1.DIV全称是division，意为&ldquo;**区块、分割**&rdquo;，DIV标签是一个无意义的容器标签，用于将页面划分出不同的区域\n\n　　2.通过DIV将复杂的页面进行细分块，可以将问题细分一个一个解决，所以通过DIV将页面分块是一个关键的工作，也是决定最终效果与质量的前提。\n\n**2.css**\n\n　　CSS (Cascading Style Sheet)，中文翻译为**层叠样式表**，是用于控制网页样式并允许将样式信息与网页内容分离的一种标记性语言。\n\n**3.div承载的是内容，而css承载的是样式**\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta name=\"keywords\"content=\"\"/>\n<meta name=\"description\" content=\"本篇网页的概述，一段话，对网站的进一步描述\"/>\n<meta name=\"author\"  content=\"网页作者的资料\">\n<meta name=\"robots\" content=\"\" />\n<meta http-equiv=\"Content－Type\" content=\"text/html; charset=gb2312\">\n<title>无标题文档</title>\n<style>\n/*\n     body{\n      color:blue;\n     }\n     div{\n       font-size:15px;background:red;\n     }\n     */\n</style>\n</head>\n\n<body>\n\n  我是div\n\n</body>\n</html><!--more-->\n&nbsp;\n```\n\n## 2.css写法\n\n### 1.嵌入式css写法\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>无标题文档</title>\n\n    <!-- 嵌入式css写法 -->\n    <style type=\"text/css\">/*选择器*/\n        p{\n            color:red;\n            font-size: 30px;\n        }\n        span{\n            color:green;\n            font-size: 40px;\n        }\n    </style>\n\n</head>\n\n<body>\n    \n    [百度]()\n    <br />\n\n    span标签\n\n    <p>今天是星期天</p>\n    <!-- 行内样式 -->\n    今天天气不错\n</body>\n</html>\n```\n\n### 2.引入样式\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>无标题文档</title>\n\n<!-- 引入样式 -->\n<link rel=\"stylesheet\" href=\"style.css\" />\n\n    \n</head>\n\n<body>\n    \n\n\n    <p>今天是星期天</p>\n    <p>今天是星期天</p>\n    <p>今天是星期天</p>\n    <p>今天是星期天</p>\n    <p>今天是星期天</p>\n\n</body>\n</html>\n```\n\nCSS部分\n\n```\n/*导入样式*/\n@import url('base.css');\n\np{\n            color:orange;\n            font-size: 50px;\n        }\n\n\nspan{\n            color:green;\n            font-size: 40px;\n        }\n```\n\n## **3.选择器<br />**\n\n**当我们定义一条样式规则时候，这条样式规则会作用于网页当中的某些元素，而我们的规定的这些元素的规则就叫做选择器。　　**\n\n**1.常用的选择器：**\n\n　　1、id选择器　　#idname\n\n　　2、类选择器　　.classname\n\n　　3、标签选择器　　tagname\n\n　　4、交叉选择器　　tagname.classname　　tagname#idname\n\n　　5、群组选择器　　多个选择器用&ldquo;，&rdquo;隔开\n\n　　6、后代选择器（包含选择器）　　父级和子级用空格隔开\n\n　　7、通用选择器　　* {}\n\n&nbsp;\n\n**2.CSS选择器优先级示意图****　　**\n\n所谓的优先级，指的就是哪条样式规则会最后作用于指定的元素，他只遵循一条规则，指定的越具体优先级越高\n\n**优先级由高到低：**\n\n**　　　　　　　　**行内样式\n\n　　　　　　　　交叉选择器\n\n　　　　　　　　id选择器\n\n　　　　　　　　类型选择器\n\n　　　　　　　　标签选择器\n\n　　　　　　　　*通配符\n\n　　　　　　　　浏览器对标记预定义的样式\n\n　　　　　　　　继承的样式\n\n**后代级别选择器**\n\n**<img src=\"/images/251930176565284.jpg\" alt=\"\" />**\n\n&nbsp;\n\n**同辈级别选择器**\n\n&nbsp;<img src=\"/images/251934281721886.jpg\" alt=\"\" />\n\n**伪类选择器**\n\n&nbsp;<img src=\"/images/251936060159243.jpg\" alt=\"\" />\n\n**属性选择器**\n\n&nbsp;<img src=\"/images/251936411099297.jpg\" alt=\"\" />\n\n**UI伪类选择器**\n\n**<img src=\"/images/251937142038007.jpg\" alt=\"\" />**\n\n&nbsp;\n\n```\n<link rel=\"stylesheet\" href=\"1.css\" media=\"screen and (min-width:1000px)\"\n```\n\n### **2.利用CSS3-Media Query实现响应式布局**\n\n**　　（2）在样式表中内嵌@media：**\n\n```\n<style>\n  @media  screen and (min-width: 600px) {\n     .one{\n        border:1px solid red;\n        height:100px;\n        width:100px;\n     }\n    \n  }\n</style><br /><br />\n```\n\n<img src=\"/images/261749509223304.jpg\" alt=\"\" />\n\n<img src=\"/images/261751315472878.jpg\" alt=\"\" />\n\n<img src=\"/images/261750394539195.jpg\" alt=\"\" />\n\n<img src=\"/images/261752307031850.jpg\" alt=\"\" /><br />\n\n&nbsp;\n\n&nbsp;\n\n特殊设备语法\n\n&nbsp;\n","tags":["前端"]},{"title":"Cassandra学习笔记——基本概念","url":"/Cassandra学习笔记——基本概念.html","content":"### 1.Cassandra介绍\n\nApache Cassandra是最流行的分布式宽表数据库，具有SQL的入口，最初由Facebook开发，后续贡献给Apache。参考：[\b认识Cassandra](https://www.jianshu.com/p/f1485d5151ba)\n\n<!--more-->\n&nbsp;\n\n### 2.Partition Key, Composite Key和Clustering Columns\n\n参考：[Apache Cassandra Composite Key\\Partition key\\Clustering key 介绍](https://developer.aliyun.com/article/699983)\n\n&nbsp;\n\n### 3.使用场景\n\n用户的画像信息，订单信息，Feed流，IOT车联网的高并发写入场景等，由于Cassandra是OLTP数据库，所以可以很方便的支持增删改查，增加扩展字段。和其功能类似的还有HBase，ScyllaDB等。\n\n参考：[Cassandra全球使用的公司及场景](https://developer.aliyun.com/article/718156)\n\n[Cassandra 的过去、现在、未来（三）](https://www.infoq.cn/article/etcdovq3lck7eiyjdlhp)\n\n&nbsp;\n\n### 4.性能\n\n在同类数据库中排名第一\n\n<img src=\"/images/517519-20220322221733842-1949340834.png\" width=\"600\" height=\"346\" loading=\"lazy\" />\n\n&nbsp;\n\n**理想的cassandra使用场景**\n\n事实证明，Cassandra对某些应用程序非常有用。理想的Cassandra应用程序具有以下特征：\n\n- 写入大幅度超出读。\n- 数据很少更新，并且在进行更新时它们是幂等的。\n- 通过主键查询，非二级索引。\n- 可以通过partitionKey均匀分区。\n- 不需要Join或聚合。\n\n我最推荐使用Cassandra的一些好场景是：\n\n- 交易日志：购买，测试分数，观看的电影等。\n- 存储时序数据（需要您自行聚合）。\n- 跟踪几乎任何事情，包括订单状态，包裹等。\n- 存储健康追踪数据。\n- 气象服务历史。\n- 物联网状态和事件历史。\n- 汽车的物联网数据。\n- 电子邮件\n\n参考：[Cassandra现在的应用前景怎么样？](https://www.zhihu.com/question/26410789)\n\n&nbsp;\n","tags":["cassandra"]},{"title":"Goland中config.go文件无法被正常识别","url":"/Goland中config.go文件无法被正常识别.html","content":"在使用Goland开发go项目的时候，突然有一次遇到有些包下面的类无法被引用的情况，排查下来发现是这些包下面的config.go文件无法被正常识别成go代码文件\n\n<img src=\"/images/517519-20240129234917468-501235886.png\" width=\"300\" height=\"318\" loading=\"lazy\" />\n\n从而导致下面无法引用其他包中的所有config.go文件中的类\n\nkratos无法正常引用\n\n<img src=\"/images/517519-20240129235120750-122847709.png\" width=\"300\" height=\"127\" loading=\"lazy\" />\n\ngorm的gen无法正常引用\n\n<img src=\"/images/517519-20240129235227683-1391514350.png\" width=\"300\" height=\"228\" loading=\"lazy\" />\n\n解决方法是去File Type下面的File type auto-detected by file content下面将config.go配置删除，就可以解决这个问题\n\n<img src=\"/images/517519-20240129235335354-189235061.png\" width=\"800\" height=\"476\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n","tags":["开发工具"]},{"title":"go学习笔记——配置","url":"/go学习笔记——配置.html","content":"## 1.读取命令行参数\n\n```\nfunc main() {\n    println(\"Hello \", os.Args[1])\n}\n\n```\n\n配置arg\n\n<img src=\"/images/517519-20240128153101073-1364009315.png\" width=\"500\" height=\"315\" loading=\"lazy\" />\n\n输出\n\n```\nHello World\n\n```\n\n注意arg[0]是go程序的运行目录\n\n## 2.使用flag加载环境变量\n\ngolang内置的标准库flag，可以用来读取配置文件的路径\n\n```\nfunc main() {\n\n\tvar configFilePath = flag.String(\"conf\", \"./\", \"config file path\") // flag也支持flag.Bool,flag.Int等用法\n\tflag.Parse()\n\tprintln(*configFilePath)\n}\n\n```\n\n配置\n\n<img src=\"/images/517519-20240128153846865-1143823151.png\" width=\"500\" height=\"288\" loading=\"lazy\" />\n\n输出\n\n```\n./configs\n\n```\n\n如果找不到-conf配置的话，才会输出 ./\n\n## 3.使用toml或者multiconfig加载toml配置文件\n\n### 1.使用toml\n\n```\ngo get github.com/BurntSushi/toml@latest\n\n```\n\n参考：[https://github.com/BurntSushi/toml](https://github.com/BurntSushi/toml)\n\nconfig.toml配置\n\n```\nAge = 25\nCats = [ \"Cauchy\", \"Plato\" ]\nPi = 3.14\nPerfection = [ 6, 28, 496, 8128 ]\nDOB = 1987-07-05T05:45:00Z\n\n```\n\n读取配置\n\n```\ntype Config struct {\n   Age        int\n   Cats       []string\n   Pi         float64\n   Perfection []int\n   DOB        time.Time // requires `import time`\n}\n\nfunc main() {\n\n\tbuf, err := os.ReadFile(\"./configs/config.toml\")\n\ttomlData := string(buf)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tvar conf Config\n\tif _, err := toml.Decode(tomlData, &amp;conf); err != nil {\n\t\tfmt.Println(err.Error())\n\t}\n\tfmt.Println(conf)\n\n}\n\n```\n\n输出\n\n```\n{25 [Cauchy Plato] 3.14 [6 28 496 8128] 1987-07-05 05:45:00 +0000 UTC}\n\n```\n\n### 2.使用multiconfig\n\n```\ngo get github.com/koding/multiconfig\n\n```\n\n参考：[https://github.com/koding/multiconfig](https://github.com/koding/multiconfig)\n\n读取配置\n\n```\nfunc main() {\n\n\tm := multiconfig.NewWithPath(\"./configs/config.toml\") // supports TOML and JSON\n\n\t// Get an empty struct for your configuration\n\tconf := new(Config)\n\n\t// Populated the serverConf struct\n\tm.MustLoad(conf) // Check for error\n\n\tfmt.Printf(\"%+v\\n\", conf)\n\n```\n\n输出\n\n```\n&amp;{Age:25 Cats:[Cauchy Plato] Pi:3.14 Perfection:[6 28 496 8128] DOB:1987-07-05 05:45:00 +0000 UTC}\n\n```\n\n## 4.使用viper加载配置文件\n\n安装viper\n\n```\ngo get github.com/spf13/viper\n\n```\n\n在config目录下添加viper.go文件\n\n<img src=\"/images/517519-20240128154302824-725057671.png\" width=\"200\" height=\"179\" loading=\"lazy\" />\n\n```\npackage config\n\nimport \"github.com/spf13/viper\"\n\n// 全局Viper变量\nvar Viper = viper.New()\n\nfunc Load(configFilePath string) error {\n\tViper.SetConfigName(\"config\")       // config file name without file type\n\tViper.SetConfigType(\"yaml\")         // config file type\n\tViper.AddConfigPath(configFilePath) // config file path\n\treturn Viper.ReadInConfig()\n}\n\n```\n\n配置文件\n\n```\ndb:\n  mysql:\n    dsn: root:123456@tcp(127.0.0.1:55000)/test?charset=utf8mb4&amp;parseTime=True\n\n```\n\n读取配置\n\n```\nfunc main() {\n\n\tvar configFilePath = flag.String(\"conf\", \"./\", \"config file path\")\n\tflag.Parse()\n\n\tif err := config.Load(*configFilePath); err != nil {\n\t\tpanic(err)\n\t}\n\n\tprintln(config.Viper.GetString(\"db.mysql.dsn\"))\n}\n\n```\n\n<!--more-->\n&nbsp;配置路径\n\n<img src=\"/images/517519-20240131004641640-922883998.png\" width=\"500\" height=\"61\" loading=\"lazy\" />\n\n参考：[Go语言微服务框架 - 2.实现加载静态配置文件](https://cloud.tencent.com/developer/article/1881959)\n\nviper还提供动态配置的功能，来实现热加载，可以使用go的协程来定时刷新配置\n\n```\npackage config\n\nimport (\n\t\"flag\"\n\t\"fmt\"\n\t\"github.com/fsnotify/fsnotify\"\n\t\"github.com/spf13/viper\"\n)\n\n// GlobalConfig 全局Viper变量\nvar GlobalConfig *viper.Viper\n\nfunc init() {\n\tinitConfig()\n\tgo dynamicConfig()\n}\n\nfunc initConfig() {\n\tvar configFilePath = flag.String(\"conf\", \"./\", \"config file path\")\n\tflag.Parse()\n\tGlobalConfig = viper.New()\n\tGlobalConfig.AddConfigPath(*configFilePath) // config file path\n\tGlobalConfig.SetConfigName(\"config\")        // config file name without file type\n\tGlobalConfig.SetConfigType(\"yaml\")          // config file type\n\terr := GlobalConfig.ReadInConfig()          // 读取配置文件\n\tif err != nil {                             // 可以按照这种写法，处理特定的找不到配置文件的情况\n\t\tif v, ok := err.(viper.ConfigFileNotFoundError); ok {\n\t\t\tfmt.Println(v)\n\t\t} else {\n\t\t\tpanic(fmt.Errorf(\"read config err:%s\\n\", err))\n\t\t}\n\t}\n}\n\n// viper支持应用程序在运行中实时读取配置文件的能力。确保在调用 WatchConfig()之前添加所有的configPaths。\nfunc dynamicConfig() {\n\tGlobalConfig.WatchConfig()\n\tGlobalConfig.OnConfigChange(func(event fsnotify.Event) {\n\t\tfmt.Printf(\"发现配置信息发生变化: %s\\n\", event.String())\n\t})\n}\n\n```\n\n参考：[go使用viper读取配置参数热加载](https://blog.csdn.net/qq_34252060/article/details/126860823)\n\n&nbsp;\n","tags":["golang"]},{"title":"sublime插件使用","url":"/sublime插件使用.html","content":"sublime软件支持安装插件来增强功能\n\nTool->Command Palette->install package\n\n## 1.sqlbeautifier SQL格式化\n\n<img src=\"/images/517519-20240206214104486-1814086758.png\" width=\"600\" height=\"78\" loading=\"lazy\" />\n\ncommand+K，然后command+F，格式化SQL\n\n## 2.添加行号或者递增的数字\n\n<img src=\"/images/517519-20240206214912607-1618013037.png\" width=\"600\" height=\"78\" loading=\"lazy\" />\n\n先选中对应的文本，然后按command+shift+L，并command+左箭头 把光标移动行首，如下\n\n<img src=\"/images/517519-20240206214453266-1673406463.png\" width=\"200\" height=\"120\" loading=\"lazy\" />\n\n再按command+option+N\n\n<img src=\"/images/517519-20240206214641840-414465523.png\" width=\"300\" height=\"228\" loading=\"lazy\" />\n\n1:1表示从1开始，每次递增+1，也可以修改成1:2\n\n<img src=\"/images/517519-20240206214804009-1538490863.png\" width=\"300\" height=\"246\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n","tags":["开发工具"]},{"title":"unblock netease music配置","url":"/unblock netease music配置.html","content":"yaml配置\n\n```\nport: 7890\nsocks-port: 7891\nallow-lan: false\nmode: Rule\nlog-level: silent\nexternal-controller: 127.0.0.1:9090\nsecret: \"\"\ndns:\n  enable: true\n  ipv6: false\n  nameserver:\n    - https://dns.rubyfish.cn/dns-query\n    - https://223.5.5.5/dns-query\n    - https://dns.pub/dns-query\n    - https://119.29.29.29/dns-query\n  fallback:\n    - https://1.0.0.1/dns-query\n    - https://public.dns.iij.jp/dns-query\n    - https://dns.twnic.tw/dns-query\n  fallback-filter:\n    geoip: true\n    ipcidr:\n      - 240.0.0.0/4\n      - 0.0.0.0/32\n      - 127.0.0.1/32\n    domain:\n      - +.google.com\n      - +.facebook.com\n      - +.youtube.com\n      - +.xn--ngstr-lra8j.com\n      - +.google.cn\n      - +.googleapis.cn\n      - +.gvt1.com\nproxies:\n    - name: \"UnblockMusic\"\n       type: http\n       server: localhost\n       port: xx\nproxy-groups:\n  - \n    name: \"Netease Music\"\n    type: select\n    proxies: \n      - UnblockMusic\n      - DIRECT\nrules:  \n  # Unblock Netease Music\n  - DOMAIN,api.iplay.163.com,Netease Music\n  - DOMAIN,apm3.music.163.com,Netease Music\n  - DOMAIN,apm.music.163.com,Netease Music\n  - DOMAIN,interface3.music.163.com,Netease Music\n  - DOMAIN,interface.music.163.com,Netease Music\n  - DOMAIN,music.163.com,Netease Music\n  - DOMAIN,interface.music.163.com.163jiasu.com,Netease Music\n  - DOMAIN,interface3.music.163.com.163jiasu.com,Netease Music\n  - DOMAIN,music.126.net,Netease Music\n  - DOMAIN-SUFFIX,163yun.com,Netease Music\n  - DOMAIN-SUFFIX,mam.netease.com,Netease Music\n  - DOMAIN-SUFFIX,hz.netease.com,Netease Music\n\n  # CIDR规则\n  - IP-CIDR,39.105.63.80/32,Netease Music\n  - IP-CIDR,45.254.48.1/32,Netease Music\n  - IP-CIDR,47.100.127.239/32,Netease Music\n  - IP-CIDR,59.111.160.195/32,Netease Music\n  - IP-CIDR,59.111.160.197/32,Netease Music\n  - IP-CIDR,59.111.181.35/32,Netease Music\n  - IP-CIDR,59.111.181.38/32,Netease Music\n  - IP-CIDR,59.111.181.60/32,Netease Music\n  - IP-CIDR,101.71.154.241/32,Netease Music\n  - IP-CIDR,103.126.92.132/32,Netease Music\n  - IP-CIDR,103.126.92.133/32,Netease Music\n  - IP-CIDR,112.13.119.17/32,Netease Music\n  - IP-CIDR,112.13.122.1/32,Netease Music\n  - IP-CIDR,115.236.118.33/32,Netease Music\n  - IP-CIDR,115.236.121.1/32,Netease Music\n  - IP-CIDR,118.24.63.156/32,Netease Music\n  - IP-CIDR,193.112.159.225/32,Netease Music\n  - IP-CIDR,223.252.199.66/32,Netease Music\n  - IP-CIDR,223.252.199.67/32,Netease Music\n  - IP-CIDR,59.111.21.14/31,Netease Music\n  - IP-CIDR,59.111.179.214/32,Netease Music\n  - IP-CIDR,59.111.238.29/32,Netease Music\n\n  # Advertising\n  - DOMAIN,admusicpic.music.126.net,REJECT\n  - DOMAIN,iadmat.nosdn.127.net,REJECT\n  - DOMAIN,iadmusicmat.music.126.net,REJECT\n  - DOMAIN,iadmusicmatvideo.music.126.net,REJECT\n\n  - DOMAIN-SUFFIX,local,DIRECT\n  - IP-CIDR,127.0.0.0/8,DIRECT\n  - IP-CIDR,172.16.0.0/12,DIRECT\n  - IP-CIDR,192.168.0.0/16,DIRECT\n  - IP-CIDR,10.0.0.0/8,DIRECT\n  - IP-CIDR,17.0.0.0/8,DIRECT\n  - IP-CIDR,100.64.0.0/10,DIRECT\n  - GEOIP,CN,DIRECT\n  - MATCH,DIRECT\n\n```\n\n如果使用yt-dlp音源需要额外安装\n","tags":["Linux"]},{"title":"R3300L电视盒子刷机","url":"/R3300L电视盒子刷机.html","content":"## 1.先线刷当贝安卓系统到EMMC\n\n1. 打开USB_Burning_Tool, 打开img文件, Erase flash 和 Erase bootloader 默认勾选. 等着\n1. R3300L断开电源, 开关关上(弹出状态), 顶住AV口内的小开关, 连上USB线,\n1. 等待几秒, 会看到USB_Burning_Tool界面显示Connect success, 然后点击Start\n1. 然后就等进度条一直写到100%\n1. 完成后点击Stop, 关闭USB_Burning_Tool\n\n拔掉USB线, 重新连上HDMI, 加电, 系统开机就是新的安卓系统了\n\n使用的系统是20180123-S905L-R3300L-V12C-root-qlzy\n\n参考：[R3300L运行CoreELEC, EmuELEC和Armbia](https://www.cnblogs.com/milton/p/11883811.html)\n\n## 2.再刷armbian系统到TF卡\n\n双系统不用**把armbian系统刷进emmc**，需要在安卓系统中安装Reboot to LibreELEC的apk来启动TF卡系统，装上TF卡就自动进行armbian系统\n\n参考：[魔百盒R3300L、CM101H以及中兴ZX10 B860AV1.2刷armbian跑甜糖](https://www.right.com.cn/forum/thread-4088276-1-1.html)\n\n使用的系统版本是Armbian_20.10_Arm-64_buster_current_5.9.0.img\n\n## 3.armbian换源\n\n```\ncp /etc/apt/sources.list /etc/apt/sources.list.bak\n\n```\n\n换成清华源\n\n```\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian buster main contrib non-free\n#deb-src https://mirrors.tuna.tsinghua.edu.cn/debian buster main contrib non-free\n\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian buster-updates main contrib non-free\n#deb-src https://mirrors.tuna.tsinghua.edu.cn/debian buster-updates main contrib non-free\n\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian buster-backports main contrib non-free\n#deb-src https://mirrors.tuna.tsinghua.edu.cn/debian buster-backports main contrib non-free\n\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian-security/ buster/updates main contrib non-free\n#deb-src https://mirrors.tuna.tsinghua.edu.cn/debian-security/ buster/updates main contrib non-free\n\n```\n\n然后\n\n```\napt-get update\n\n```\n\n如果遇到NO_PUBKEY的报错\n\n```\nErr:4 https://mirrors.tuna.tsinghua.edu.cn/debian buster-backports InRelease\n  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY XXXXXXXXX NO_PUBKEY XXXXXXXXX\n\n```\n\n执行下面命令\n\n```\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys XXXXXXX\n\n```\n\n## 4.安装docker\n\n```\narmbian-config\n\n```\n\n<img src=\"/images/517519-20240215135829342-28730660.png\" width=\"400\" height=\"247\" loading=\"lazy\" />\n\n<img src=\"/images/517519-20240215135902739-579065822.png\" width=\"400\" height=\"167\" loading=\"lazy\" />\n\n<img src=\"/images/517519-20240215135931851-960516431.png\" width=\"400\" height=\"385\" loading=\"lazy\" />\n\n```\nroot@arm-64:~# docker -v\nDocker version 25.0.3, build 4debf41\n\n```\n\n## 5.docker部署openwrt\n\n### 1.部署openwrt\n\n假设armbian的ip是192.168.0.xx\n\n创建network\n\n```\ndocker network create -d macvlan --subnet=192.168.0.0/24 --gateway=192.168.0.1 -o parent=eth0 macnet\n\n```\n\n启动pod，其中192.168.0.11是openwrt后面会使用的ip\n\n```\ndocker run \\\n  -d \\\n  --name=unifreq-openwrt-aarch64 \\\n  --restart=unless-stopped \\\n  --network=macnet \\\n  --privileged \\\n  --ip=192.168.0.11 \\\n  unifreq/openwrt-aarch64:latest\n\n```\n\n打开网卡混杂模式\n\n```\nsudo ip link set eth0 promisc on\n\n```\n\n进入pod查看/etc/config/network文件的option ipaddr，查看其值为192.168.0.1还是192.168.1.1\n\n接着将pod中/etc/config/network的192.168.0.1或者192.168.1.1修改成192.168.0.11\n\n```\ndocker exec unifreq-openwrt-aarch64 sed -e \"s/192.168.0.1/192.168.0.11/\" -i /etc/config/network\n\n```\n\n重启pod\n\n```\ndocker restart unifreq-openwrt-aarch64\n\n```\n\n参考：[M401a系列：armbian下docker安装openwrt做旁路由](https://blog.csdn.net/momomoer/article/details/128234891)\n\n### 2.登录openwrt\n\n登录192.168.0.11访问openwrt的管理页面，注意是http的浏览器访问的时候需要接受风险，默认账号密码是root和password\n\n<img src=\"/images/517519-20240215171810068-1342212521.png\" width=\"700\" height=\"256\" loading=\"lazy\" />\n\n<img src=\"/images/517519-20240215174250191-581901966.png\" width=\"700\" height=\"296\" loading=\"lazy\" />\n\n### 3.设置openwrt\n\n这时候openwrt需要进行一些设置\n\n#### 1.关闭 DHCP\n\n网络 => 接口 => LAN => DHCP 服务器 => 基本设置\n\n操作: 勾选忽略此接口\n\n<img src=\"/images/517519-20240215172159588-1662516936.png\" width=\"400\" height=\"130\" loading=\"lazy\" />\n\n#### 2.关闭ipv6\n\n网络 => 接口 => LAN => DHCP 服务器 => IPv6 设置\n\n操作: 禁用 `路由通告服务`, `DHCPv6 服务`, `NDP 代理`\n\n<img src=\"/images/517519-20240215172315576-1844403389.png\" width=\"500\" height=\"228\" loading=\"lazy\" />\n\n#### 3.配置 网关 和 DNS，这样在pod中才能访问网络\n\n网络 => 接口 => LAN => 一般设置 => 基本设置 <br />操作: \n\n1. `IPv4 网关` 改为 `192.168.0.1`<br />2. `IPv4 广播` 改为 `192.168.0.255`<br />3. `使用自定义的 DNS 服务器` 改为 `114.114.114.114`\n\n<img src=\"/images/517519-20240215172434461-903637524.png\" width=\"600\" height=\"342\" loading=\"lazy\" />\n\n这时候pod中就可以访问外网了\n\n<img src=\"/images/517519-20240215171234479-1607314713.png\" width=\"700\" height=\"289\" />\n\n#### 4.armbian访问openwrt\n\n其中192.168.0.116是armbian机器的ip，192.168.0.11是openwrt的ip\n\n```\nroot@arm-64:~# ip link add mynet link eth0 type macvlan mode bridge\nroot@arm-64:~# ip addr add 192.168.0.116 dev mynet\nroot@arm-64:~# ip link set mynet up\nroot@arm-64:~# ip route add 192.168.0.11 dev mynet\nroot@arm-64:~# ping 192.168.0.11\n\nPING 192.168.0.11 (192.168.0.11) 56(84) bytes of data.\n64 bytes from 192.168.0.11: icmp_seq=1 ttl=64 time=1.02 ms\n64 bytes from 192.168.0.11: icmp_seq=2 ttl=64 time=0.415 ms\n64 bytes from 192.168.0.11: icmp_seq=3 ttl=64 time=0.426 ms\n64 bytes from 192.168.0.11: icmp_seq=4 ttl=64 time=0.413 ms\n\n```\n\n参考：[N1 通过 Docker 安装 Openwrt 为旁路由](https://blog.isayme.org/posts/issues-65/)\n\n#### 5.设置软路由ss kx上网\n\n注意需要把软路由接到主路器上\n\n如果不想修改主路由的DNS地址，那么可以通过修改手机的路由器地址到openwrt的ip地址+修改DNS地址到openwrt的ip地址的方式\n\n参考：软路由（operwrt）通过ss Plus+ kx上网教程<br /><br />\n\n<!--more-->\n&nbsp;\n","tags":["openwrt"]},{"title":"各开放平台账号登录API对接文档","url":"/各开放平台账号登录API对接文档.html","content":"当我们开发的系统想要使用第三方账号系统（比如微信，微博，facebook）进行登录的时候，就需要使用第三方平台的登录能力，下面是各第三方平台的对接文档\n\n关于oauth和oauth2.0：[理解OAuth 2.0](https://www.ruanyifeng.com/blog/2014/05/oauth_2_0.html)\n\n[關於OAuth 2.0-以Facebook為例](https://medium.com/@justinlee_78563/%E9%97%9C%E6%96%BCoauth-2-0-%E4%BB%A5facebook%E7%82%BA%E4%BE%8B-6f78a4a55f52)\n\n## 1.微信\n\n<img src=\"/images/517519-20231223180235359-1505352506.png\" width=\"700\" height=\"307\" loading=\"lazy\" />\n\n[https://developers.weixin.qq.com/doc/oplatform/Website_App/WeChat_Login/Wechat_Login.html](https://developers.weixin.qq.com/doc/oplatform/Website_App/WeChat_Login/Wechat_Login.html)\n\n## 2.微信小程序\n\n<img src=\"/images/517519-20231223174152027-1530934469.png\" width=\"500\" height=\"507\" loading=\"lazy\" />\n\n[https://developers.weixin.qq.com/miniprogram/dev/framework/open-ability/login.html](https://developers.weixin.qq.com/miniprogram/dev/framework/open-ability/login.html)\n\n## 3.微博\n\nweb端\n\n<img src=\"/images/517519-20240415221334413-336064577.png\" width=\"400\" height=\"406\" loading=\"lazy\" />\n\napp移动端，可以使用refresh token\n\n<img src=\"/images/517519-20240415221403046-550225660.png\" width=\"400\" height=\"355\" loading=\"lazy\" />\n\n[https://open.weibo.com/wiki/%E6%8E%88%E6%9D%83%E6%9C%BA%E5%88%B6](https://open.weibo.com/wiki/%E6%8E%88%E6%9D%83%E6%9C%BA%E5%88%B6)\n\n## 4.哔哩哔哩\n\n<img src=\"/images/517519-20231223174220005-352668560.png\" width=\"400\" height=\"477\" loading=\"lazy\" />\n\n[https://openhome.bilibili.com/doc/4/eaf0e2b5-bde9-b9a0-9be1-019bb455701c](https://openhome.bilibili.com/doc/4/eaf0e2b5-bde9-b9a0-9be1-019bb455701c)\n\n<!--more-->\n&nbsp;\n","tags":["计算机网络"]},{"title":"Python爬虫——使用selenium和chrome爬取js动态加载的网页","url":"/Python爬虫——使用selenium和chrome爬取js动态加载的网页.html","content":"## 1.使用docker镜像运行selenium+chrome环境\n\n官方镜像仓库[selenium/standalone-chrome](https://hub.docker.com/r/selenium/standalone-chrome)，只支持amd64\n\n<img src=\"/images/517519-20240510221439592-929092042.png\" width=\"350\" height=\"442\" loading=\"lazy\" />\n\n拉取镜像\n\n```\ndocker pull selenium/standalone-chrome:120.0\n\n```\n\n启动\n\n```\ndocker run -d -p 4444:4444 -p 15900:5900 selenium/standalone-chrome:120.0\n\n```\n\n其他参数\n\n```\ndocker run -d -p 4444:4444 -p 15900:5900 -e SE_NODE_MAX_SESSIONS=5 --shm-size=2g selenium/standalone-chrome:120.0\n\n```\n\n参考：[https://hub.docker.com/r/selenium/standalone-chrome](https://hub.docker.com/r/selenium/standalone-chrome)\n\n访问 localhost:4444/ui 可以查看selenium的运行状态\n\n<img src=\"/images/517519-20240413110242672-198944174.png\" width=\"800\" height=\"328\" loading=\"lazy\" />\n\n可以使用mac自带的屏幕共享功能连接pod的vnc\n\n<img src=\"/images/517519-20240413111207907-2016482097.png\" width=\"400\" height=\"87\" loading=\"lazy\" />\n\n输入 vnc://localhost:15900，默认密码是secret\n\n<img src=\"/images/517519-20240413111339595-332456418.png\" width=\"400\" height=\"116\" loading=\"lazy\" />\n\n界面\n\n<img src=\"/images/517519-20240413113120555-1200000210.png\" width=\"600\" height=\"505\" loading=\"lazy\" />\n\n## 2.安装依赖\n\nubuntu/debian换源\n\n```\nsudo -s\nsed -i s@/archive.ubuntu.com/@/mirrors.aliyun.com/@g /etc/apt/sources.list\napt-get update\n\n```\n\n安装pip3\n\n```\napt-get install python3-pip\n\n```\n\n安装selenium和webdriver-manager\n\n```\npip3 install selenium\npip3 install webdriver-manager\n\n```\n\n## 3.运行selenium\n\n查看chrome和driver version\n\n```\nroot@ced974ac3394:/# google-chrome --version\nGoogle Chrome 120.0.6099.224 \nroot@ced974ac3394:/# chromedriver --version\nChromeDriver 120.0.6099.109 (3419140ab665596f21b385ce136419fde0924272-refs/branch-heads/6099@{#1483})\n\n```\n\n1.不启用chrome gui\n\n```\nroot@ced974ac3394:/# python3\nPython 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.chrome.service import Service\nfrom webdriver_manager.chrome import ChromeDriverManager\noptions = Options()\noptions.add_argument('--headless')\noptions.add_argument('--no-sandbox')\noptions.add_argument('--disable-dev-shm-usage')\ndriver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\ndriver.get(\"https://python.org\")\nprint(driver.title)\n\nWelcome to Python.org\n\nexit()\n\n```\n\n参考：[https://github.com/password123456/setup-selenium-with-chrome-driver-on-ubuntu_debian](https://github.com/password123456/setup-selenium-with-chrome-driver-on-ubuntu_debian)\n\n2.启用chrome gui\n\n```\nroot@fce0fc2def31:/# python3\nPython 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.chrome.service import Service\nfrom webdriver_manager.chrome import ChromeDriverManager\noptions = Options()\noptions.add_argument('--disable-dev-shm-usage')\noptions.add_argument(\"--remote-debugging-port=9222\") # 不加的话会报session not created: DevToolsActivePort file doesn't exist\noptions.add_argument('--no-sandbox') # 不加的话会报chrome not reachable\ndriver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n\n```\n\n此时vnc界面会弹出浏览器\n\n<img src=\"/images/517519-20240413113414554-305887693.png\" width=\"600\" height=\"504\" loading=\"lazy\" />\n\n```\n>>> driver.get(\"https://python.org\")\n>>> print(driver.title)\nWelcome to Python.org\n\n```\n\n打开了python的网页\n\n<img src=\"/images/517519-20240413113534962-963092156.png\" width=\"600\" height=\"503\" loading=\"lazy\" />\n\n## 4.cloudflare人机校验\n\n如果语言cloudflare的人机校验\n\n<img src=\"/images/517519-20240422221602756-567501559.png\" width=\"300\" height=\"108\" loading=\"lazy\" />\n\n### 1.可以尝试使用 **undetected_chromedriver** 这个包来代替selenium的webdriver\n\n安装 undetected_chromedriver\n\n```\npip3 install undetected_chromedriver\n\n```\n\n替换seleniuim的web driver\n\n```\nimport undetected_chromedriver as uc\n\ndriver = uc.Chrome(headless=False,use_subprocess=False)\ndriver.maximize_window()\ndriver.get('https://nowsecure.nl')\ndriver.save_screenshot('nowsecure.png')\n\n```\n\n参考：[https://github.com/ultrafunkamsterdam/undetected-chromedriver](https://github.com/ultrafunkamsterdam/undetected-chromedriver)\n\n如果报from session not created: This version of ChromeDriver only supports Chrome version 125\n\n可以在代码中指定chrome的版本，如下\n\n```\nundetecteddriver = uc.Chrome(options=options, use_subprocess=True, version_main=125)\n\n```\n\n或者可以尝试使用125.0版本的docker镜像\n\n```\ndocker run -d -p 4444:4444 -p 15900:5900 -e SE_NODE_MAX_SESSIONS=5 --shm-size=2g selenium/standalone-chrome:125.0\n\n```\n\n### 2.也可以尝试使用<!--more-->\n&nbsp;DrissionPage 这个包来代替selenium的webdriver\n\n安装DrissionPage\n\n```\npip3 install DrissionPage\n\n```\n\n使用\n\n```\n>>> from DrissionPage import ChromiumPage\n>>> page = ChromiumPage()\n>>> page.get('https://dev.epicgames.com/zh-CN/home')\nTrue\n\n```\n\n参考：[自动绕过 Cloudflare 验证码 - 两条相反的方法（选择最适合您的方法）](https://juejin.cn/post/7353543714151219239)\n\n不过无论是使用undetected_chromedriver还是DrissionPage，只能绕过cloudflare人机检验，无法绕过hcaptcha验证码，如下\n\n<img src=\"/images/517519-20240519114312295-1254688156.png\" width=\"300\" height=\"394\" loading=\"lazy\" />\n\n## 5.M1芯片环境运行selenium docker\n\n由于selenium/standalone-chrome镜像只支持amd64架构，如果是M1芯片的话，需要使用支持arm64架构的镜像 [seleniarm/standalone-chromium](https://hub.docker.com/r/seleniarm/standalone-chromium/tags)\n\n<img src=\"/images/517519-20240510222901312-459312493.png\" width=\"400\" height=\"314\" loading=\"lazy\" />\n\n拉取镜像\n\n```\ndocker pull seleniarm/standalone-chromium:120.0\n\n```\n\n启动，需要指定运行平台是linux/arm64\n\n```\ndocker run -d -p 4444:4444 -p 15900:5900 -e SE_NODE_MAX_SESSIONS=5 --shm-size=2g --platform linux/arm64 seleniarm/standalone-chrome:120.0\n\n```\n\n### 1.M1运行selenium webdriver\n\nseleniarm/standalone-chromium镜像中的浏览器是chromium，和chrome浏览器一些区别，需要指定webdriver的路径为/usr/bin/chromedriver\n\n```\nroot@fce0fc2def31:/# python3\nPython 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.chrome.service import Service\nfrom webdriver_manager.chrome import ChromeDriverManager\noptions = Options()\noptions.add_argument('--disable-dev-shm-usage')\noptions.add_argument(\"--remote-debugging-port=9222\") # 不加的话会报session not created: DevToolsActivePort file doesn't exist\noptions.add_argument('--no-sandbox') # 不加的话会报chrome not reachable\ndriver = webdriver.Chrome(service=Service(\"/usr/bin/chromedriver\"), options=options)\n\n```\n\n### 2.M1运行**undetected_chromedriver**\n\n如果使用m1芯片想 **undetected_chromedriver** 来绕过cloudflare的人机校验的话\n\n```\nimport undetected_chromedriver as uc\n\ndriver = uc.Chrome(driver_executable_path=\"/usr/bin/chromedriver\",headless=True,use_subprocess=False)\ndriver.maximize_window()\ndriver.get('https://nowsecure.nl')\ndriver.save_screenshot('nowsecure.png')\n\n```\n\n默认的下载路径在/root/Downloads\n\n## 6.验证码\n\n业界常用的验证码服务有hCaptcha和reCaptcha等\n\n### 1.hCaptcha\n\nhCaptcha根据面向用户不同可以分成普通版，隐形版，企业版等；根据难度不同可以分成Easy，Medium，Difficult版本等，具体的Demo可以参考 [http://www.52spider.com/captcha/hcaptcha-enterprise/](http://www.52spider.com/captcha/hcaptcha-enterprise/)\n\n要想通过hCaptcha验证码，如果不想使用验证码识别平台的话，可以使用其提供的视障账号\n\n#### 1.视障账号\n\nhCaptcha的**accessibility**，这是hCaptcha提供用于视障人员用于跳过验证码的功能\n\n```\nhttps://www.hcaptcha.com/accessibility\n\n```\n\n通过注册hCaptcha视障账号 ，可以在如下页面中通过点击set cookie按钮来设置一个cookie用于跳过验证码\n\n```\nhttps://dashboard.hcaptcha.com/welcome_accessibility\n\n```\n\n<img src=\"/images/517519-20240526163339303-1598914985.png\" width=\"500\" height=\"527\" loading=\"lazy\" />\n\n但是set cookie不是所有情况下都能成功的，如果你的环境被检测是机器人，或者ip被封，set cookie的时候则有可能报401，\n\n报错是：无法发布Cookie。如果此问题继续发生，请发送电子邮件至support@hcaptcha.com\n\n<img src=\"/images/517519-20240526164647650-1495843455.png\" width=\"500\" height=\"557\" loading=\"lazy\" />\n\n参考：[如何绕过烦人的 hCaptcha &amp; Cloudflare Captcha](https://blog.skk.moe/post/bypass-hcaptcha/)\n\n### 2.reCaptcha\n\n&nbsp;\n\n## 7.验证码识别平台\n\n可以尝试使用验证码识别平台来识别验证码，比如 **[2captcha](https://2captcha.com/)服务**，类似的服务还有**[yescaptcha](https://yescaptcha.com)**，**[NopeCHA](https://nopecha.com/)**。\n\n这类服务一般都提供API和浏览器插件2种方式用于识别验证码，API用于无头浏览器下获取验证码的captcha_response，填入后再点击检测后来提交验证码；而浏览器插件则可以在浏览器中实现自动点击验证码\n\n### 1.浏览器插件\n\n使用最为简单，只需要安装其提供的验证码识别插件即可，具体每个平台的识别准确率根据验证码难度的不同会不相同\n\n具体使用的时候，可以配合selenium一起使用\n\n#### 1.2captcha\n\n2captcha可以通过作为员工来点击验证码来赚取额度进行测试\n\n2captcha插件 for hCaptcha：[https://chromewebstore.google.com/detail/hcaptcha-solver-auto-capt/imgmoeegfjhhmljmphfkjeibkiffcdgl](https://chromewebstore.google.com/detail/hcaptcha-solver-auto-capt/imgmoeegfjhhmljmphfkjeibkiffcdgl)\n\n2captcha插件 for reCaptcha：[https://chromewebstore.google.com/detail/recaptcha-solver-auto-cap/infdcenbdoibcacogknkjleclhnjdmfh](https://chromewebstore.google.com/detail/recaptcha-solver-auto-cap/infdcenbdoibcacogknkjleclhnjdmfh)\n\n<img src=\"/images/517519-20240711221320053-1633565680.png\" width=\"300\" height=\"173\" loading=\"lazy\" />\n\n经测试2captcha for hCaptcha在某些网站无法工作，对于不同难度的验证码识别正确率暂时未知\n\n参考：[Selenium验证码和reCAPTCHA绕过](https://2captcha.com/p/selenium-captcha-solver)\n\n#### 2.yescaptcha\n\n貌似是国内开发运行的验证码识别平台，注册送免费额度\n\nyescaptcha插件：[https://chromewebstore.google.com/detail/yescaptcha-%E4%BA%BA%E6%9C%BA%E5%8A%A9%E6%89%8B/jiofmdifioeejeilfkpegipdjiopiekl?hl=zh-CN](https://chromewebstore.google.com/detail/yescaptcha-%E4%BA%BA%E6%9C%BA%E5%8A%A9%E6%89%8B/jiofmdifioeejeilfkpegipdjiopiekl?hl=zh-CN)\n\n<img src=\"/images/517519-20240711221835970-1051294041.png\" width=\"300\" height=\"429\" loading=\"lazy\" />\n\n经测试yescaptcha可以正常点击hCaptcha验证码，不过对于hCaptcha difficult难度的hCaptcha验证码有时能点击正确，但是也有较高概率点错\n\n#### 3.nopecaptcha\n\nnopecapcha验证码识别平台，每24小时刷新100免费额度\n\nnopecaptcha插件：[https://chromewebstore.google.com/detail/nopecha-captcha-solver/dknlfmjaanfblgfdfebhijalfmhmjjjo](https://chromewebstore.google.com/detail/nopecha-captcha-solver/dknlfmjaanfblgfdfebhijalfmhmjjjo)\n\n<img src=\"/images/517519-20240711222201996-110431206.png\" width=\"300\" height=\"518\" loading=\"lazy\" />\n\n经测试yescaptcha可以正常点击hCaptcha验证码，不过对于hCaptcha difficult难度的hCaptcha验证码错误率离谱，没见过能正确点对的时候\n\n#### 4.capsolver\n\n[https://www.capsolver.com/zh](https://www.capsolver.com/zh)\n\n通过公司邮箱注册的话可以领取免费额度，没进行测试，不知道效果如何\n\n### 2.API\n\n如果想要使用无头浏览器（比较节省系统资源），可以尝试使用验证码平台提供的API，比如2captcha其支持多种验证码类型，比如reCaptcha，hCaptcha等\n\nAPI文档可以参考：[https://2captcha.com/api-docs](https://2captcha.com/api-docs)\n\nSDK文档可以参考：[https://github.com/2captcha/2captcha-python](https://github.com/2captcha/2captcha-python)\n\n<img src=\"/images/517519-20240519160912753-918274556.png\" width=\"500\" height=\"561\" />\n\n比如我们要识别hCaptcha的验证码，可以查看2Captcha的API v2下面hCaptcha对应的文档 [https://2captcha.com/api-docs/hcaptcha](https://2captcha.com/api-docs/hcaptcha)\n\n提交的任务提交API将验证码提交上来 [https://api.2captcha.com/createTask](https://api.2captcha.com/createTask)\n\n注意websiteURL字段如果验证码是通过**iframe**弹出的话，需要填写iframe的URL\n\n```\n{\n    \"clientKey\":\"YOUR_API_KEY\",\n    \"task\": {\n        \"type\":\"HCaptchaTaskProxyless\",\n        \"websiteURL\":\"https://2captcha.com/demo/hcaptcha\",\n        \"websiteKey\":\"f7de0da3-3303-44e8-ab48-fa32ff8ccc7b\"\n    }\n}\n\n```\n\n如下\n\n<img src=\"/images/517519-20240519161355206-1341677745.png\" width=\"400\" height=\"503\" loading=\"lazy\" />\n\n然后去 https://2captcha.com/zh/enterpage 的 **上次提交的验证码** 页面查看验证码识别的进度\n\n<img src=\"/images/517519-20240519161544058-842759858.png\" width=\"300\" height=\"189\" loading=\"lazy\" />\n\n这个验证码识别成功\n\n<img src=\"/images/517519-20240519161713968-1099921196.png\" width=\"600\" height=\"296\" loading=\"lazy\" />\n\n返回的答案如下，其中token是验证码识别的结果，而respKey这个额外的返回值是部分网站需要的\n\n```\n{\n        \"token\": \"P1_eyJ0eXAiOiJKV...1LDq89KyJ5A\",\n        \"respKey\": \"E0_eyJ0eXAiOiJK...y2w5_YbP8PGuJBBo\",\n        \"userAgent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.5614.0 Safari/537.36\"\n    }\n\n```\n\n如果网站只需要token的话，，这里只需要找到data-hcaptcha-response的属性，将验证码的结尾添加到这个属性的value位置，最后点击check按钮来提交验证码\n\n如果网站同时需要token和respKey的话，这里有些网站会将token和respKey放到一个json中并将其转成jwt格式后调用login接口，一起提交上来\n\n参考：[HCaptcha 的模拟点击破解教程来了！](https://cuiqingcai.com/36060.html)\n\n[为了好玩而编写机器人 - 绕过 hCaptcha](https://devpress.csdn.net/python/62fb88b7c67703293080057e.html)\n\nhcapcha js混淆可以参考：[关于hcaptcha （vm wasm ob）三合一](https://cloud.tencent.com/developer/article/2208551)\n","tags":["Python"]},{"title":"MySQL学习笔记——多表连接和子查询","url":"/MySQL学习笔记——多表连接和子查询.html","content":"**多表连接查询**\n\n```\n# 返回的是两张表的乘积\nSELECT * FROM tb_emp,tb_dept\nSELECT COUNT(*) FROM tb_emp,tb_dept\n\n# 标准写法，每个数据库都能这么写\nSELECT * FROM tb_emp CROSS JOIN tb_dept\n\n# 内连接 只列出这些连接表中与连接条件相匹配的数据行\nSELECT * FROM tb_emp e,tb_dept d WHERE e.NAME = d.NAME\n\nSELECT * FROM tb_emp INNER JOIN tb_dept\nON tb_emp.NAME = tb_dept.NAME\n\n# 外链接 不仅列出与连接条件相匹配的行，还列出左表（左外连接），右表（右外连接）或两个表（全外连接）中所有符合where过滤条件的数据行\n# 左外连接 在外连接中，某些不满足条件的列也会显示出来，也就是说，只限制其中一个表的行，而不限制另一个表的行\n# 左边的表作为主表，左边的表会全部显示\nSELECT * FROM tb_emp LEFT JOIN tb_dept\nON tb_emp.NAME = tb_dept.NAME\n\n#oracle语法，左连接加号在左边\nSELECT * FROM tb_emp e,tb_dept d WHERE e.NAME=d.NAME(+)\n\n#右外连接\nSELECT * FROM tb_emp RIGHT JOIN tb_dept\nON tb_emp.NAME = tb_dept.NAME\n\n#全外连接 MySQL不支持 OUTER\n\n#自连接\nSELECT c.NAME '部门名字',c2.NAME '其他部门'\nFROM tb_dept c LEFT JOIN tb_dept c2\nON c.description=c2.id<br /><br /># rollup<br />SELECT  orderYear, productLine,  SUM(orderValue) totalOrderValue <br />FROM sales <br />GROUP BY  orderYear, productline <br />WITH ROLLUP; \n```\n\n参考：[https://www.begtut.com/mysql/mysql-rollup.html](https://www.begtut.com/mysql/mysql-rollup.html) \n\n<!--more-->\n&nbsp;\n\n**子查询**\n\n&nbsp;某些情况下，当运行查询的时候，需要的条件是另外一个select语句的结果，这个时候，就要用到子查询\n\n```\n#子查询\nSELECT * FROM tb_emp\n\n#查询年龄是22的人名\nSELECT age FROM tb_emp WHERE sex='男'\nSELECT * FROM tb_emp WHERE age>23\n\nSELECT * \nFROM tb_emp \nWHERE age> (\nSELECT age\nFROM tb_emp\nWHERE NAME='Tom'\n)\n\n#IN 与列表中的任一成员相等\nSELECT *\nFROM tb_emp\nWHERE age IN (\nSELECT age\nFROM tb_emp\nWHERE sex='男'\n)\n\n#ANY 与列表中的每一成员比较,小于最大的\nSELECT *\nFROM tb_emp\nWHERE age < ANY (\nSELECT age\nFROM tb_emp\nWHERE sex='男'\n)\n```\n\n&nbsp;\n","tags":["MySQL"]},{"title":"HTML学习笔记——基础知识","url":"/HTML学习笔记——基础知识.html","content":"## 1.标签\n\n### 1.title标签、网站关键词、网站描述、实现网页的跳转、单标签、对标签、p标签\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html>\n    <head>\n        <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n        \n        <!-- title标签 -->\n        <title>我是title</title>\n        \n        <!-- 网站关键词 -->\n        <meta name='keywords' content=\"切糕，卖切糕，卖切糕\"/>\n\n        <!-- 网站描述 -->\n        <meta name='description' content=\"本网站介绍，要有可读性，100-200字\"/>\n\n        <!-- 5秒钟后跳转到百度网 -->\n        <meta http-equiv='refresh' content=\"5;url='http://www.baidu.com'\"/>\n    </head>\n\n\n\n    <body>\n\n        <!-- 单标签 -->\n        <img src=\"/images///images//images/dog.jpg\" width=\"100\" height=\"150\" title=\"图片\"/>\n        \n        <!-- 对标签 -->\n        [百度](http://www.baidu.com)\n\n        **我是strong标签**\n        <p>我是p标签</p>\n\n    </body>\n</html>\n```\n\n### 2.h1标签、h2标签、font标签、加粗文字、下划线、嵌套\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>无标题文档</title>\n</head>\n\n<body>\n\n    <!-- h1文章标题/只能用一次 -->\n    <h1>我是p标签</h1>\n\n    <!-- h2文章标题/可以用多次/权重较低 -->\n    <h2>我是p标签</h2>\n\n    <!-- font/color可以是英文或00ff00/size从1到7 -->\n    <font color=\"red\" size=\"7\">我是font标签</font>\n    \n    <!-- 加粗文字 -->\n    **我是strong标签**\n\n    <!-- 嵌套 -->\n    <font color=\"red\"><u><del>红色文字加下划线</del></u></font>\n\n</body>\n</html>\n```\n\n<!--more-->\n&nbsp;<img src=\"/images/210700164265280.png\" alt=\"\" />\n\n### 3.pre标签、实体\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>无标题文档</title>\n</head>\n\n<body>\n\n    <!-- pre标签 -->\n    床前明月光\n    疑是地上霜\n    举头望明月\n    低头思故乡\n    <pre>\n    兰陵美酒郁金香\n    玉碗盛来琥珀光\n    但使主人能醉客\n    不知何处是他乡\n    </pre>\n\n    <!-- 实体 -->\n        ha标签的写法是这样的：&amp;lt;h1&amp;gt;h1实例&amp;lt;/h1&amp;gt;\n\n</body>\n</html>\n```\n\n## 2.锚链接\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>无标题文档</title>\n</head>\n    \n<body>\n    <!-- 从新的页面打开 -->\n    <!-- [百度首页](http://www.baidu.com) -->\n\n    <!-- 锚链接用法 -->\n    \n    <a href=\"#first\">第一章节</a>\n    <a href=\"#second\">第二章节</a>\n\n    <h2 id=\"first\">第一章节</h2>\n    <p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p>\n\n    <h2 id=\"second\">第二章节</h2>\n    <p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p><p>里面的没有内容</p>\n\n    <a href=\"#top\">回到顶部</a>\n</body>\n</html>\n```\n\n## 3.图片\n\n### 1.显示图片、用a标签实现点击图片跳转、地图标签/点击图片上固定区域跳转\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>无标题文档</title>\n</head>\n\n<body>\n\n    <!-- 图片 -->\n    <img src=\"/images///images//images/dog.jpg\" title=\"一只狗\" alt\"这是一只狗\" usemap=\"#dogmap\" />\n    \n    <!-- 用a标签实现点击图片跳转 -->\n    [<img src=\"/images///images//images/dog.jpg\" width=\"300\" height=\"500\" title=\"一只狗\" alt\"这是一只狗\"/>](http://www.baidu.com)\n\n    <!-- 地图标签/点击图片上固定区域跳转 -->\n    <map name=\"dogmap\">\n        <area shape=\"circle\" coords=\"185,198,69\" href=\"http://www.baidu.com\" target=\"_blank\" />\n        <area shape=\"circle\" coords=\"385,198,69\" href=\"http://www.sina.com\" target=\"_blank\" />\n        <area shape=\"rect\" coords=\"50,50,100,100\" href=\"http://www.sina.com\" target=\"_blank\" />\n        <!-- 依次写各个点的坐标 -->\n        <area shape=\"poly\" coords=\"0,0,0,50,50,0\" href=\"http://www.hao123.com\" />\n\n    </map>\n\n\n</body>\n</html>\n\n```\n\n### 2.index图片相对路径\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>无标题文档</title>\n</head>\n\n<body>\n\n    <!-- ../代表往上反一层 -->\n    <img src=\"/images//images/cat.jpg\" />\n\n    <!-- 相对路径 -->\n    <img src=\"images//images///images//images/dog.jpg\" />\n</body>\n</html>\n\n```\n\n## 4.列表和table\n\n### 1.有序列表、无序列表和自定义列表\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>无标题文档</title>\n</head>\n\n<body>\n\n    <!-- 有序列表 -->\n    <ol type=\"a\">\n        <li>有序列表1</li>\n        <li>有序列表2</li>\n        <li>有序列表3</li>\n        <li>有序列表4</li>\n        <li>有序列表5</li>\n    </ol>\n\n    <!-- 无序列表 -->\n    <ul type=\"circle\">\n        <li>无序列表1</li>\n        <li>无序列表2</li>\n        <li>无序列表3</li>\n        <li>无序列表4</li>\n        <li>无序列表5</li>\n    </ul>\n\n    <!-- 自定义列表 -->\n    <dl>\n        <dt>第一大段</dt>\n        <dd>第1小段</dd>\n        <dd>第2小段</dd>\n        <dd>第3小段</dd>\n        <dd>第4小段</dd>\n    </dl>\n</body>\n</html>\n```\n\n### 2.table\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>无标题文档</title>\n</head>\n\n<body>\n\n    <!-- border为边框的粗细 -->\n    <!-- tr为行，td为列 -->\n    <table border='1' width='600' bordercolor=\"blue\" bgcolor=\"grey\" align=\"center\">\n\n        <caption><font color='blue'>课程表</font></caption>\n\n        <tr width=\"600\" height=\"50\" align='center' valign='top'>\n            <td rowspan='3'>abc</td>\n            <td colspan='2'>123</td>\n            \n            <td>123</td>\n            <td>abc</td>\n        </tr>\n        <tr>\n            \n            <td>123</td>\n            <td>abc</td>\n            <td rowspan='2'>123</td>\n            <td>abc</td>\n        </tr>\n        <tr>\n            \n            <td>123</td>\n            <td>abc</td>\n            \n            <td>abc</td>\n        </tr>\n\n    </table>\n\n</body>\n</html>\n```\n\n## 5.post表单\n\n### 1.form1\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>无标题文档</title>\n</head>\n\n<body>\n\n    <form action=\"\">\n        <input type=\"text\" /><!-- 单行文本 -->    \n        <input type=\"password\" /><!-- 密码框 -->\n        <input type=\"radio\" /><!-- 单选框 -->\n        <input type=\"checkbox\" /><!-- 复选框 -->\n        <input type=\"hidden\" /><!-- 隐藏域 -->\n        <input type=\"submit\" /><!-- 提交按钮 -->\n        <input type=\"file\" /><!-- 文件域 -->\n        <input type=\"reset\" value=\"\" /><!-- 重置按钮 -->\n        <input type=\"button\" value=\"\" /><!-- 按钮 -->\n        <textarea></textarea><!-- 文本区域 -->\n        <select>\n            <option></option>\n        </select><!-- 下拉框 -->\n    </form>\n\n\n</body>\n</html>\n```\n\n### 2.form2\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>网站用户个人资料调查</title>\n</head>\n\n<body>\n    <!-- action用来指定接收信息的脚本 -->\n    <!-- method属性用来指定数据的传输方式 -->\n    <form action=\"post.php\" method='post'>\n        \n        <input type=\"hidden\" />\n\n        您当前的位置：<input type=\"text\" value='百度网' disabled='disabled'/>\n        <br /><br />\n\n        <label for='uname'>用户名：</label><input id='uname' type=\"text\" size='50' maxlength='10' name='uname' value='张三'/>\n        <br /><br />\n\n        密码：<input type=\"password\" name='pwd' />\n        <br /><br />\n\n        性别：男<input type=\"radio\" name='sex' value='1' checked='checked'/> 女<input type=\"radio\" name='sex' value='0'/>\n        <br /><br />\n\n        爱好：篮球<input type=\"checkbox\" name=\"hobby[]\" value='lanqiu' />\n            排球<input type=\"checkbox\" name=\"hobby[]\" value='paiqiu' />\n            足球<input type=\"checkbox\" name=\"hobby[]\" value='zuqiu' />\n            乒乓球<input type=\"checkbox\" name=\"hobby[]\" value='pinbgpangqiu' />\n        <br /><br />\n\n        来这里的目的：\n        <select name=\"target\" size='2' multiple='multiple'>\n            <option value=\"first\">第一</option>\n            <option value=\"second\" selected='selected'>第二</option>\n            <option value=\"third\">第三</option> \n            <option value=\"forth\">第四</option>\n        </select>\n        <br /><br />\n\n        个人介绍：<textarea name=\"jieshao\" rows='10' cols='40'></textarea>\n        <br /><br />\n\n        个人照片<input type=\"file\" />\n        <br /><br />\n\n        <input type=\"submit\" value=\"更新资料\" />\n        <input type=\"reset\" value=\"重置\"/>\n        <input type=\"button\" value=\"按钮\"/>\n        <br /><br />\n        \n\n    </form>\n</body>\n</html>\n```\n\n## 6.frameset和marquee\n\n### 1.frameset\n\n效果如百Google度\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>无标题文档</title>\n</head>\n\n<!-- <frameset cols='400,*'>\n    <frame src=\"http://www.baidu.com\"/>\n    <frame src=\"http://www.sina.com\"/>\n</frameset> -->\n\n<!-- <frameset rows='200,200,*'>\n    <frame src=\"http://www.baidu.com\"/>\n    <frame src=\"http://www.sina.com\"/>\n    <frame src=\"http://www.hao123.com\"/>\n</frameset> -->\n\n<frameset rows='200,*'>\n    <frame src='http://www.hao123.com'/>\n    <frameset cols='500,*'>\n        <frame src='http://www.baidu.com'/>\n        <frame src='http://www.sina.com'/>\n    </frameset>\n</frameset>\n\n</html>\n```\n\n### 2.marquee标签\n\n实现滚动效果\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>无标题文档</title>\n</head>\n\n<body>\n\n    <!-- 从右往左不停滚动 -->\n    <marquee>marquee标签</marquee>\n\n    <!-- 从左往右不停滚动/滚动次数为2次 -->\n    <marquee direction=\"right\" loop='2'>marquee标签</marquee>\n    \n    <!-- 从下往上不停滚动/背景颜色/高度宽度/鼠标停止 -->\n    <marquee direction=\"up\" bgcolor='grey' width='400' height='100' onmouseover='this.stop()' onmouseout='this.start()'>marquee标签</marquee>\n\n    <!-- 只滚动一次 -->\n    <marquee behavior=\"slide\">只滚动一次</marquee>\n\n    <!-- 来回滚动 -->\n    <marquee behavior=\"alternate\">来回滚动</marquee>\n\n    <!-- 10个像素/每1000毫秒 -->\n    <marquee behavior=\"scroll\" scrollamount='10' scrolldelay='1000'>滚动</marquee>\n\n</body>\n</html>\n```\n\n## 7.选择器\n\n### 1.ID选择器、交叉选择器、群组选择器、子代选择器\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>无标题文档</title>\n\n<style type=\"text/css\">\n    p{\n        color: purple;\n        font-size: 25px;\n    }\n    \n    li{\n        font-size: 25px;\n        line-height: 35px;\n    }    \n\n    #two{\n        color: red;\n    }\n\n/*    .lanse{\n        color: blue;\n    }*/\n    \n    /*交叉选择器*/\n    li.lanse{\n        color:blue; \n    }\n\n    /*群组选择器*/\n    #test,.seven{\n        color: orange;\n    }\n\n    /*子代选择器*/\n    li span{\n        color: pink;\n    }\n\n</style>\n\n</head>\n\n<body>\n    <!-- 下面是html的写法 -->\n    <font color='blue' size='3'>下面是html的写法</font>\n    <font color='blue' size='3'>下面是html的写法</font>\n    <font color='blue' size='3'>下面是html的写法</font>\n    <font color='blue' size='3'>下面是html的写法</font>\n    <font color='blue' size='3'>下面是html的写法</font>\n    <font color='blue' size='3'>下面是html的写法</font>\n\n    <!-- 下面是css的写法 -->\n    <p>下面是css的写法</p>\n    <p>下面是css的写法</p>\n    <p>下面是css的写法</p>\n    <p>下面是css的写法</p>\n    <p>下面是css的写法</p>\n\n    <span>我是ul外面的span标签\n\n    <ul>\n        <li>我是第1行li标签\n            <span>我是ul里面的span标签</li>\n        <li id=\"two\">我是第2行li标签</li>\n        <li class=\"lanse\">我是第3行li标签</li>\n        <li>我是第4行li标签</li>\n        <li class=\"lanse\">我是第5行li标签</li>\n        <li id=\"test\">我是第6行li标签</li>\n        <li class=\"seven\">我是第7行li标签</li>\n    </ul>\n\n    <p class=\"lanse\">我是class为lanse的p标签</p>\n\n</body>\n</html>\n```\n\n### 2.子选择器、相邻选择器、属性选择器、伪类选择器\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>无标题文档</title>\n<style type=\"text/css\">\n    /*子选择器*/\n    /*#box>p{\n        color:red;\n    }*/\n\n    /*相邻选择器*/\n    div+p{\n        color:blue;\n    }\n\n    /*属性选择器*/\n    p[name]{\n        color:red;\n    }\n\n    a{\n        font-size: 30px;\n        color:blue;\n    }\n    \n    /*伪类选择器*/\n    a:hover{\n        font-size: 50px;\n        color: red;\n    }\n\n    #word{\n        width: 300px;\n        border: 3px solid blue;\n        margin: 0 auto;\n    }\n\n    p#word:first-letter{\n        font-size: 50px;\n        color: red;\n    }\n\n    div[name]{\n        color: yellow;\n    }\n    \n</style>\n</head>\n\n<body>\n\n    我是name为box的div元素\n\n    <p id=\"word\">p标签p标签p标签p标签p标签p标签</p>\n\n    <a href=\"http://www.baidu.com\">百度</a>\n<br />\n<br />\n<br />\n    \n        \n            <p>我是son的p标签</p>\n        \n        <p>我是box的p标签</p>\n        <p name=\"test\">我是box的p标签</p>\n        <p>我是box的p标签</p>\n    \n</body>\n</html>\n```\n\n### 3.选择器的优先级\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>无标题文档</title>\n<style type=\"text/css\">\n    #box{\n        font-size: 30px;\n        color: red;    \n    }\n\n    h1{\n        font-size: 40px;\n        color: orange;\n    }\n    /*有优先级*/\n    #title{\n        color: blue;\n    }\n</style>\n</head>\n\n<body>\n    <h1 id=\"title\">今天是星期一</h1>\n    \n        <p>我是box里面的p标签</p>\n        <span>我是box里面的span标签\n    \n\n</body>\n</html>\n```\n\n## 8.块级标签、行级标签、图片标签\n\n### 1.块级标签、行级标签\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>无标题文档</title>\n<style type=\"text/css\">\n    p{\n        width: 200px;\n        height: 100px;\n        border: 3px solid red;\n    }\n\n    p span{\n        color: red;\n    }\n</style>\n</head>\n\n<body>\n\n    今天是星期一\n\n    <!-- 块级标签/能设置宽高度 -->\n    我是p标签\n    我是p标签\n    我是p标签\n    \n    <!-- 行级标签/能和其他标签待在一行 -->\n    我是span标签\n    我是span标签\n\n    [百度]()\n    [百度]()\n\n</body>\n</html>\n\n```\n\n### 2.图片标签\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>无标题文档</title>\n<style type=\"text/css\">\n    #dog{\n        width:300;\n        height: 400px;\n    }\n\n    #cat{\n        width:300;\n        height: 400px;\n    }\n\n    p{\n        color: blue;\n        font-size: 30px;\n        font-family: 微软雅黑；\n        width: 800px;\n        height: 200px;\n        border: 3px solid blue;\n        text-align:center;\n        line-height: 200px;\n    }\n</style>\n</head>\n\n<body>\n\n    <img src=\"/images///images//images/dog.jpg\" id=\"dog\" />\n    <img src=\"/images/cat.jpg\" id=\"cat\" />\n\n    今天是星期一\n</body>\n</html>\n\n```\n\n### 3.给标签设置格式\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>无标题文档</title>\n<style type=\"text/css\">\n/*空两个文字*/\n    p{\n        width: 300px;\n        border: 4px solid blue;\n        margin: 0 auto;\n        text-indent: 2em;\n        line-height: 1.5em;\n        /*加粗*/\n        font-weight: 700;\n        /*倾斜*/\n        font-style: italic;\n        /*大写*/\n        font-variant: small-caps;\n    }\n\n</style>\n</head>\n\n<body>\n    www.baidu.com今天是星期一今天是星期一今天是星期一今天是星期一今天是星期一今天是星期一今天是星期一\n</body>\n</html>\n\n```\n\n## 9.box\n\n### 1.HTML写法\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>无标题文档</title>\n<style type=\"text/css\">\n    #box{\n        width: 300px;\n        border: 5px solid blue;\n        margin: 0 auto; \n        /*内边距*/\n        /*padding: 0px;*/\n        /*padding-top: 20px;\n        padding-right: 40px;\n        padding-bottom: 30px;\n        padding-left: 50px;*/\n\n        /*一个参数代表四个方向*/\n        /*两个参数，第一个代表上下，第二个代表左右*/\n        /*三个参数，第一个代表上面，第二个代表左右，第三个代表下面*/\n        /*四个参数，代表上右下左*/\n        padding: 20px 30px 100px;\n    }\n</style>\n</head>\n\n<body>\n\n    \n        盒子模型盒子模型盒子模型盒子模型盒子模型盒子模型盒子模型盒子模型盒子模型盒子模型盒子模型盒子模型盒子模型盒子模型盒子模型盒子模型盒子模型盒子模型盒子模型盒子模型\n    \n</body>\n</html>\n```\n\n### 2.CSS写法\n\n```\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<title>无标题文档</title>\n<style type=\"text/css\">\n    *{\n        /*内边距/外边距*/\n        padding: 0px;\n        margin: 0px;\n    }\n\n    #title{\n        width: 500px;\n        height: 200px;\n        border: 4px solid blue;\n        padding: 100px;\n        background: #ccc;\n        /*margin-bottom: 50px;*/\n        float: left;\n        /*margin-right: 100px;*/\n    }\n\n    #box{\n        /*左右外边距和累加/上下外边距和合并*/\n        width: 400px;\n        height: 200px;\n        border: 4px solid orange;\n        background: #ccc;\n        /*margin-top: 50px;*/\n        float: left;\n        /*margin-left: */\n    }\n\n</style>\n</head>\n\n<body>\n    <h1 id=\"title\">今天是星期一    </h1>\n\n    \n</body>\n</html>\n```\n\n&nbsp;\n","tags":["前端"]},{"title":"chrome插件ModHeader使用","url":"/chrome插件ModHeader使用.html","content":"**ModHeader**全名modify header，这是一款可以对HTTP请求header进行修改的插件，其支持添加**模式Mod**和**过滤器Filter**。\n\n**Mod**可以支持对request的header，response的header进行修改，对请求进行重定向redirect等；\n\n**Filter**支持对特定的URL生效这面的这些Mod\n\n<img src=\"/images/517519-20231129230300747-1818943298.png\" width=\"500\" height=\"411\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n","tags":["开发工具"]},{"title":"MySQL学习笔记——函数","url":"/MySQL学习笔记——函数.html","content":"**常用函数**\n\n```\nALTER TABLE tb_emp\nADD diredate VARCHAR(20);\n\n#插入数据\nINSERT INTO tb_dept()\nVALUE(4,'市场部','负责市场工作');\n\n# concat 连接\nSELECT CONCAT(NAME,sex) FROM tb_emp;\n\n# UPPER 转换大写\nSELECT UPPER(NAME) FROM tb_emp WHERE dept_id=1;\n\n# 返回字符串长度\nSELECT LENGTH(NAME) FROM tb_emp WHERE dept_id=1;\n\n# 返回部分字符\nSELECT SUBSTR(NAME,2,2) FROM tb_emp WHERE dept_id=1;\n\n# 返回当前\nSELECT NOW();\n\n# 查询时间是1981年\nSELECT * FROM tb_emp\nWHERE YEAR(diredate) = 1981\nAND MONTH(diredate) = 1982;\n\n#插入时间\nINSERT INTO tb_emp(NAME,sex,age,address,email,dept_id,diredate)\nVALUES('ZHOU','男',33,'香港','ZHOU@163.com',2,'1988-09-09');\n\nINSERT INTO tb_emp(NAME,sex,age,address,email,dept_id,diredate)\nVALUES('CAI','女',30,'香港','CAI@163.com',2,NOW());\n\n# 条件判断语句\nSELECT NAME,sex,age '原来年龄'\n    CASE \n    WHEN age IS NULL THEN 100\n    ELSE age\n    END AS '年龄'\nFROM tb_emp;\n\n# IFNULL函数    如果字段不为NULL，则取第二个值，如果为空，择取第三个值\nSELECT NAME,IFNULL(age,age+100,100) AS age2 FROM tb_emp;\n\n# IFNULL函数    如果字段不为NULL，则直接去该值，如果为空，择取第二个值\nSELECT NAME,IFNULL(age,100) AS age2 FROM tb_emp;\n```\n\n<!--more-->\n&nbsp;\n\n&nbsp;\n\n**聚合函数**\n\n&nbsp;\n\n```\n# 聚合函数，也叫组合函数，忽略空值\nSELECT AVG(age) FROM tb_emp;\n\nSELECT SUM(age) FROM tb_emp;\n\nSELECT MAX(age) FROM tb_emp;\n\nSELECT MIN(age) FROM tb_emp;\n\nSELECT AVG(age) AS '平均年龄',SUM(age) AS '总年龄',MAX(age) AS '最高年龄',MIN(age) AS '最低年龄'\nFROM tb_emp WHERE dept_id=1;\n\n# COUNT不统计null，统计的是行数/记录数\nSELECT COUNT(*) FROM tb_emp\nSELECT COUNT(email) FROM tb_emp\n\n# 不统计重复记录\nSELECT COUNT(DISTINCT diredate) FROM tb_emp\n\n# 分组统计 GROUP BY\n# 每个部门的平均年龄\nSELECT dept_id,AVG(age) FROM tb_emp GROUP BY dept_id\n\nSELECT dept_id,AVG(age),address FROM tb_emp GROUP BY dept_id,address\n\n# 限定查询结果 HAVING 不能使用where,where子句中不可以使用函数\nSELECT dept_id,AVG(age) FROM tb_emp GROUP BY dept_id\nHAVING AVG(age)>23\nORDER BY AVG(age) DESC;\n\n# LIMIT 常用来分页\nSELECT * FROM tb_emp LIMIT 5; #查询前5个记录\nSELECT * FROM tb_emp LIMIT 5,10; #查询前6-10个记录\n```\n\n&nbsp;\n","tags":["MySQL"]},{"title":"MySQL学习笔记——增删改查","url":"/MySQL学习笔记——增删改查.html","content":"**有关数据库的DML操作**\n\n　　-insert into\n\n　　-delete、truncate\n\n　　-update\n\n　　-select\n\n　　　　-条件查询\n\n　　　　-查询排序\n\n　　　　-聚合函数\n\n　　　　-分组查询\n\n<!--more-->\n&nbsp;\n\n**DROP、TRUNCATE、DELETE**\n\n-DELETE删除数据，保留表结构，可以回滚，如果数据量大，很慢，回滚就是因为备份删除的数据\n\n-TRUNCATE删除所有数据，保留表结构，不可以回滚，一次全部删除所有数据，速度相对很快\n\n-DROP删除数据和表结构，删除数据最快（直接从内存抹去这一块数据）\n\n```\n#1.指明字段进行插入，注意字段和值的数量和类型都需要匹配\nINSERT INTO tb_dept (NAME,loc,description) VALUES('开发部','广州','负责软件开发工作');\n\n#2.如果插入的values是所有字段，可以不同显式写插入的字段名，不推荐\nINSERT INTO tb_dept VALUES(3,'财务部','广州','负责财务工作');\n\n#auto_increment会记住曾经生成的值\n\n#3.一次插入多条记录 mysql特有\nINSERT INTO tb_dept (NAME,loc,description)\nVALUES('开发部','广州','负责软件开发工作'),\n('财务部','广州','负责财务工作'),\n('市场部','广州','负责采购工作');\n\n#4.可以从一张表中插入数据\n#创建一张表和tb_dept表的结构一样，通过这种方式建表只是复制表结构，不复制约束\nCREATE TABLE tb_dept2\nSELECT * FROM tb_dept\n#where id = 99\n\n#先建表再插入\nINSERT INTO tb_dept2(id,NAME,loc,description)\nSELECT id,NAME,loc,description FROM tb_dept\n\nINSERT INTO tb_emp(id,NAME,sex,age,address,email,dept_id)\nVALUES(1,'Tony','男',26,'广州','Tony@163.com',1);\n\n#更新 UPDATE table SET column = value [,column = value] [WHERE condition]\n#where建议使用主键或者唯一键，建议是主键\nUPDATE tb_emp SET age=23 WHERE id = 1;\nUPDATE tb_emp SET age=23,sex='女' WHERE id = 2;\n\n#删除 DELETE [FROM] table [WHERE condition];\nDELETE FROM tb_emp;    #删除表所有数据\nDELETE FROM tb_emp WHERE id=2;\n\n#阶段，DDL语句 TRUNCATE语句 作用是完全清空一个表\nTRUNCATE TABLE tb_emp;\n```\n\n&nbsp;\n\n**最简单的SELECT语句**\n\n```\n#查找 字段、字段。。。从 表 *表示所有的列\nSELECT NAME,loc,description FROM tb_dept\nSELECT * FROM tb_dept \nSELECT NAME FROM tb_dept\n\n#SELECT语句中的算数表达式\nSELECT NAME,age,age*2\nFROM tb_emp;\n\n#NULL和0还有空字符不是一个概念\nSELECT * FROM tb_emp;\nSELECT * FROM tb_emp WHERE age = 0;\nSELECT * FROM tb_emp WHERE age IS NULL;\n\n#改变列的标题头，别名\nSELECT NAME '姓名',age AS '年龄',age*2 '年龄乘2'\nFROM tb_emp;\n\n#重复记录\n#缺省情况下查询显示所有行，包括重复行\nSELECT dept_id\nFROM tb_emp;\n\n#使用DISTINCT关键字可从查询结果中清楚重复行\nSELECT DISTINCT dept_id\nFROM tb_emp;\n\n#DISTINCT作用的范围是后面字段的组合\nSELECT DISTINCT dept_id,age\nFROM tb_emp WHERE dept_id=1;\n\n#使用WHERE子句限定返回的记录\nSELECT *\nFROM tb_emp\nWHERE age=22;\n\n#字符串和日期要用单引号括起来\nSELECT *\nFROM tb_emp\nWHERE NAME = 'Tom';\n\n#比较运算符>  <  >=  <=  =  <>\nSELECT NAME,age\nFROM tb_emp\nWHERE age>=24;\n\nSELECT NAME,age\nFROM tb_emp\nWHERE age>=24 AND age<27;\n\n#BETWEEN AND 包含最小值和最大值\nSELECT NAME,age\nFROM tb_emp\nWHERE age BETWEEN 24 AND 27;\n\n#使用IN运算符\nSELECT NAME,age\nFROM tb_emp\nWHERE age IN (22,26);\n\nSELECT NAME,age\nFROM tb_emp\nWHERE age NOT IN (22,26);\n\n#使用LIKE运算符进行模糊查询 _代表一个字符  %代表一个或者多个字符\nSELECT NAME\nFROM tb_emp\nWHERE NAME LIKE '_a%';\n\n#IS NULL\nSELECT NAME,age\nFROM tb_emp\nWHERE age IS NULL;\n\n#对结果进行排序 ORDER BY 从高到低\nSELECT * \nFROM tb_emp\nORDER BY age DESC;\n\n#默认从低到高或者ASC\nSELECT * \nFROM tb_emp\nORDER BY age;\n```\n\n&nbsp;\n\n　\n\n　　　　\n","tags":["MySQL"]},{"title":"MySQL学习笔记——约束","url":"/MySQL学习笔记——约束.html","content":"## 1.数据库约束简介\n\n1.约束是在表上强制执行的数据检验规则，约束主要用于保证数据库的完整性。\n\n2.当表中数据有相互依赖性时，可以保护相关的数据不被删除。\n\n3.大部分数据库支持下面五类完整性约束：\n\n　　- NOT NULL非空\n\n　　- UNIQUE Key唯一值\n\n　　- PRIMARY KEY主键\n\n　　- FOREIGN KEY外键\n\n　　- CHECK检查\n\n4.约束作为数据库对象，存放在系统表中，也有自己的名字\n\n5.创建约束的时机：\n\n　　-在建表的同时创建\n\n　　-建表后创建（修改表）\n\n6.有单列约束和多列约束\n\n## 2.列级约束和表级约束\n\n列级约束直接跟在列后面定义，不再需要指定列名，与列定义之间用空格分开\n\n表级约束通常放在所有的列定义之后定义，要显式指定对哪些列建立列级约束，与列定义之间采用英语逗号，隔开\n\n如果是对多列建联合约束，只能使用表级约束语法\n\n### 1.非空约束(NOT NULL)<br />\n\n<!--more-->\n&nbsp;\n\n```\nNAME VARCHAR(18) NOT NULL\n\n```\n\n&nbsp;\n\n列级约束：只能使用列级约束语法定义；确保字段值不允许为空；只能在字段级定义；　　　　\n\nNULL值：所有数据类型的值都可以是NULL；空字符串不等于NULL；0也不等于NULL；\n\n### 2.唯一约束(UNIQUE)\n\n&nbsp;\n\n```\nNAME VARCHAR(18) UNIQUE NOT NULL\n\n```\n\n唯一性约束条件确保所在的字段或者字段组合不出现重复值；<br />\n\n　　　　　　唯一性约束条件的字段允许出现多个NULL；\n\n　　　　　　同一张表内可建多个唯一约束；\n\n　　　　　　唯一约束可由多列组合而成；\n\n　　　　　　建唯一约束时MySQL会为止建立对应的索引；\n\n　　　　　　如果不给唯一约束起名，该唯一约束默认与列名相同；\n\n　　　　主键约束(PRIMARY KEY)　　栗子：id INT PRIMARY KEY AUTO INCREMENT\n\n　　　　　　主键从功能上看相当于非空且唯一\n\n　　　　　　一个表中之允许一个主键\n\n　　　　　　主键是表中唯一确定一行数据的字段\n\n　　　　　　主键字段可以是单字段或者是多字段组合\n\n　　　　　　当建立主键约束时，MySQL为主键创建对应的索引\n\n　　　　　　主键约束名总为PRIMARY KEY\n\n### 3.外键约束(FOREIGN KEY&nbsp;REFERENCE)　　\n\n1. dept_id INT REFERENCE tb_dept(id)&nbsp;\n\n2. 表内 &nbsp;CONSTRAINT&nbsp;FOREIGN KEY 外键名（建议是表名_约束名） （外键）&nbsp;REFERENCE &nbsp;主表 （字段）\n\n&nbsp;\n\n```\nCONSTRAINT FOREIGN KEY tb_eployee_fk (dept_id) REFERENCE tb_dept(dept_id)\n\n```\n\n3>表外 &nbsp;ALTER TABLE 表名 ADD [CONSTRAINT constraint] type (column);\n\n&nbsp;\n\n```\nALTER TABLE tb_emp ADD CONSTRAINT FOREIGN KEY tb_eployee_fk (dept_id) REFERENCE tb_dept(dept_id)\n\n```\n\n　　　　　　外键是构建一个表的两个字段或者两个表的两个字段之间的关系\n\n　　　　　　外键确保了相关的两个字段的两个关系\n\n　　　　　　子（从）表外键列的值必须在主表参照列值的范围内，或者为空（也可以加非空约束，强制不允许为空）\n\n　　　　　　当主表的记录被子表参照时，主表记录不允许被删除\n\n　　　　　　外键参照的只能是主表主键或者唯一键，保证子表记录可以准确定位到被参照的记录\n\n　　　　　　格式FOREIGN KEY（外键列名） REFERENCE 主表（参照列）\n\n### 4.CHECK约束\n\n&nbsp;\n\n```\nage INT CHECK(age>18 AND age<60) 或者 sex VARCHAR(2) (sex='男' OR sex='女') check约束在MySQL中不起作用\n\n```\n\n　　　　　　既可作为列级约束，也可作为表级约束\n\n　　　　　　定义在字段上的每一记录都要满足的条件<br />\n\n　　　　　　在CHECK中定义检查的条件表达式，数据需要符合设置的条件\n\n　　　　　　条件表达式不允许使用\n\n　　　　　　参照其他记录的值\n\n## 3.增加约束\n\ni>　在建表之后，即表外　ALTER TABLE table ADD [CONSTRAINT constraint] type (column);&nbsp;\n\nii>　在建表的时候，即表内　CONSTRAINT&nbsp;FOREIGN KEY tb_eployee_fk (dept_id)&nbsp;REFERENCE tb_dept(dept_id)\n\n　　　　可增加或删除约束，但不能直接修改\n\n　　　　可使约束启用和禁用\n\n　　　　非空约束必须使用MODIFY子句增加\n\n　　　　只要是可以使用列级约束语法来定义的约束，都可以通过modify来增加该约束\n\n插入数据测试，这两句执行一次添加一行数据\n\n　　　　INSERT INTO tb_emp(NAME,sex,age,addresss,email,dept_id)\n\n　　　　VALUE('a','男','23','gz','123@163.com',1);\n\n## 4.删除约束\n\n　　　　约束可被删除，删除约束不会对数据产生影响\n\n　　　　当删除被外键参照的主键时候，应该采用CASCADE关键字来级联删除外键，否则无法删除主键\n\n　　　　语法如下：ALTER TABLE 表名 DROP CONSTRAINT 约束名\n\n```\n#删除约束\n\n#删除NOT NULL约束\nALTER TABLE tb_emp MODIFY NAME VARCHAR(18)\n\n#删除UNIQUE约束\nALTER TABLE tb_emp DROP INDEX email\n\n#删除PRIMAEY KEY约束 自动增长不能删除\nALTER TABLE tb_emp MODIFY id INT\nALTER TABLE tb_emp DROP PRIMARY KEY\n\n#删除外键约束\nALTER TABLE tb_emp DROP FOREIGN KEY tb_emp_ibfk_1\n```\n\n&nbsp;11.自动增长和默认值\n\n　　auto_increment只是MYSQL特有的，其他数据库里面没有，只能放在主键后面\n\n　　sex VARCHAR(2) DEFAULT '男',\n","tags":["MySQL"]},{"title":"MySQL学习笔记——基本语法","url":"/MySQL学习笔记——基本语法.html","content":"SQL&mdash;&mdash;结构化查询语言（Structured Query Language）\n\n## **1. 字符集和<strong>大小写**<br /></strong>\n\n**SQL语言不区分大小写，建议关键字用大写，但是字符串常量区分大小写**\n\n**字符集**\n\n```\ncharacter_set_client：服务器将系统变量character_set_client作为客户端发送语句时使用的字符集。\ncharacter_set_connection：用于没有字符集介绍器指定的字面量和用于数字到字符串转换的字符集。\ncharacter_set_database：默认数据库使用的字符集。每当默认数据库更改时，服务器都会设置此变量。\ncharacter_set_filesystem：文件系统字符集。\ncharacter_set_results：用于向客户端返回查询结果的字符集。这包括结果数据(如列值)、结果元数据(如列名)和错误消息。\ncharacter_set_server：服务器的默认字符集。如果你设置了这个变量，你还应该设置collation_server来指定字符集的排序规则。\ncharacter_set_system：服务器用于存储标识符的字符集。\ncharacter_sets_dir：安装字符集的目录。\n\n```\n\nMySQL5.7的默认字符集character和排序字符集collation\n\n```\nmysql> show variables like '%character%';\n+--------------------------+--------------------------------------------------------------+\n| Variable_name            | Value                                                        |\n+--------------------------+--------------------------------------------------------------+\n| character_set_client     | latin1                                                       |\n| character_set_connection | latin1                                                       |\n| character_set_database   | latin1                                                       |\n| character_set_filesystem | binary                                                       |\n| character_set_results    | latin1                                                       |\n| character_set_server     | latin1                                                       |\n| character_set_system     | utf8                                                         |\n| character_sets_dir       | /opt/rh/rh-mysql57/root/usr/share/rh-mysql57-mysql/charsets/ |\n+--------------------------+--------------------------------------------------------------+\n8 rows in set (0.00 sec)\n\nmysql> show variables like '%collation%';\n+----------------------+-------------------+\n| Variable_name        | Value             |\n+----------------------+-------------------+\n| collation_connection | latin1_swedish_ci |\n| collation_database   | latin1_swedish_ci |\n| collation_server     | latin1_swedish_ci |\n+----------------------+-------------------+\n3 rows in set (0.00 sec)\n\n```\n\nMySQL8.0的默认字符集\n\n```\nmysql> show variables like '%character%';\n+--------------------------+--------------------------------+\n| Variable_name            | Value                          |\n+--------------------------+--------------------------------+\n| character_set_client     | latin1                         |\n| character_set_connection | latin1                         |\n| character_set_database   | utf8mb4                        |\n| character_set_filesystem | binary                         |\n| character_set_results    | latin1                         |\n| character_set_server     | utf8mb4                        |\n| character_set_system     | utf8mb3                        |\n| character_sets_dir       | /usr/share/mysql-8.0/charsets/ |\n+--------------------------+--------------------------------+\n8 rows in set (0.00 sec)\n\nmysql> show variables like '%collation%';\n+-------------------------------+--------------------+\n| Variable_name                 | Value              |\n+-------------------------------+--------------------+\n| collation_connection          | latin1_swedish_ci  |\n| collation_database            | utf8mb4_0900_ai_ci |\n| collation_server              | utf8mb4_0900_ai_ci |\n| default_collation_for_utf8mb4 | utf8mb4_0900_ai_ci |\n+-------------------------------+--------------------+\n4 rows in set (0.00 sec)\n\n```\n\n查看mysql table字段的字符集\n\nMySQL5.7\n\n```\nmysql> create table user\n(\n    id bigint unsigned auto_increment comment '主键' primary key,\n    username varchar(128) not null comment '用户名',\n    email varchar(128) not null comment '邮箱'\n)\ncomment '用户表' charset=utf8mb4;\n\nmysql> SELECT COLUMN_NAME, CHARACTER_SET_NAME, COLLATION_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = 'default' and TABLE_NAME = 'user';\n+-------------+--------------------+--------------------+\n| COLUMN_NAME | CHARACTER_SET_NAME | COLLATION_NAME     |\n+-------------+--------------------+--------------------+\n| id          | NULL               | NULL               |\n| username    | utf8mb4            | utf8mb4_general_ci |\n| email       | utf8mb4            | utf8mb4_general_ci |\n+-------------+--------------------+--------------------+\n3 rows in set (0.00 sec)\n\n```\n\n或者\n\n```\nmysql > CREATE TABLE IF NOT EXISTS `t_user` (`username` varchar(64) NOT NULL,`password` varchar(11) NOT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\nmysql > SELECT COLUMN_NAME, CHARACTER_SET_NAME, COLLATION_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = 'default' and TABLE_NAME = 't_user';\n+-------------+--------------------+-----------------+\n| COLUMN_NAME | CHARACTER_SET_NAME | COLLATION_NAME  |\n+-------------+--------------------+-----------------+\n| username    | utf8               | utf8_general_ci |\n| password    | utf8               | utf8_general_ci |\n+-------------+--------------------+-----------------+\n2 rows in set (0.00 sec)\n\n```\n\nMySQL8.0\n\n```\nmysql> CREATE TABLE IF NOT EXISTS `t_user123` (`username` varchar(64) NOT NULL,`password` varchar(11) NOT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8;\nQuery OK, 0 rows affected, 1 warning (0.02 sec)\n\nmysql> SELECT COLUMN_NAME, CHARACTER_SET_NAME, COLLATION_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = 'test' and TABLE_NAME = 't_user123';\n+-------------+--------------------+-----------------+\n| COLUMN_NAME | CHARACTER_SET_NAME | COLLATION_NAME  |\n+-------------+--------------------+-----------------+\n| password    | utf8mb3            | utf8_general_ci |\n| username    | utf8mb3            | utf8_general_ci |\n+-------------+--------------------+-----------------+\n\n```\n\n**utf8**（别名utf8mb3）utf8 是 MySQL 早期版本使用的字符集，它实际上是 UTF-8 的一种实现，只支持最多 3 字节来存储一个字符。utf8 是 utf8mb3 的别名。从 MySQL 8.0.28 版本开始，utf8 已经被明确标识为 utf8mb3，但在此之前，它们是同义的。utf8适用于大多数情况，但不能用于需要存储 4 字节 UTF-8 字符的场景（如表情符号）。\n\n**utf8mb4** 是 MySQL 完整实现的 UTF-8 字符集，支持最多 4 字节来存储一个字符。支持所有 Unicode 字符，包括所有表情符号和扩展字符。\n\n**utf8mb4_general_ci** 的 ci 表示 **case-insensitive**，即**不区分大小写**。\n\n**utf8mb4_bin** 的 bin 表示 **binary**，这种排序规则是**区分大小写**的。\n\n将字段类型修改成可区别大小写\n\n```\nCREATE TABLE your_table_name (column_name VARCHAR(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin);\n\n```\n\n在创建表的时候指定可区别大小写\n\n```\nCREATE TABLE user (\n    id BIGINT UNSIGNED AUTO_INCREMENT COMMENT '主键' PRIMARY KEY,\n    username VARCHAR(128) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '用户名',\n    email VARCHAR(128) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '邮箱'\n) \nCOMMENT '用户表' CHARSET=utf8mb4;\n\n```\n\n如果想区分case insensitive的字段，也可以使用binary函数将其转换成大小写敏感的，如下\n\n```\nselect * from user where binary username = 'mlinnockax'\n\n```\n\n## **2. Database**\n\n### 1.列出MySQL服务器主机上的数据库\n\n```\nSHOW DATABASES[LIKE wild];\n\n```\n\n### 2. 用给定的名字创建一个数据库\n\n语法：CREATE DATABASE[IF NO EXISTS]<!--more-->\n&nbsp;数据库名字　　创建之后要刷新才能在列表中看见新建立的数据库\n\n```\ncreate database spring_user default character set utf8 collate utf8_general_ci;\n\n```\n\n### 3.&nbsp;删除数据库中的所有表和数据\n\n```\nDROP DATABASE[IF NO EXISTS] 数据库名字\n\n```\n\n### 4.&nbsp;指定数据库\n\n把指定数据库作为默认（当前）**数据库使用**，用于后续语句\n\n```\nUSE 数据库名字 \n\n```\n\n### 5.&nbsp;数据库对象的命名规则\n\n```\n1.必须以字母开头\n2.可包括数字和三个特殊字符（# _ $）　\n3.不要使用MySQL的保留字\n4.同一个schema下的对象不能同名\n\n```\n\n## 3. Table\n\n### 1.&nbsp;建表语句\n\nCREATE TABLE [schema] 表的名字 (column datatype[DEFAULT expr],...) ENGINE = 存储机制\n\n数据表的每行称为一条记录(record)，每一列称为一个字段(field)，主键列：唯一能够识别每条记录的列&nbsp;\n\n```\nCREATE TABLE IF NOT EXISTS `t_user` (`username` varchar(64) NOT NULL,`password` varchar(11) NOT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\n```\n\n或者\n\n```\nCREATE TABLE 表的名字（\n　　　　　　　　列名 列类型，\n　　　　　　　　列名 列类型\n　　　　）；\n\nCREATE TABLE tb_dept( #创建一个部门表叫做tb_dept\n　id INT PRIMARY KEY AUTO INCREMENT, #部门编号id,整型，primary key 主键\n　NAME VARCHAR(18), #部门名称 name,字符，最多只能储存18个字符\n　description VARCHAR(100) #描述description,字符，最多只能存储100个字符\n);\n\n```\n\n### 2.&nbsp;MySQL支持的列类型\n\n```\ni>数值类型\nii>日期/时间类型\niii>字符串（字符）类型　\n整数：int或者integer\n浮点：double\n字符：char、varchar、blob、text\n\n```\n\n<img src=\"/images/517519-20160321095115995-1235227035.png\" alt=\"\" />\n\n<img src=\"/images/517519-20160321095133261-1419919399.png\" alt=\"\" />\n\n### 3.&nbsp;显示当前数据库中已有的数据表的信息\n\n```\nSHOW TABLES [FROM 数据库名字][LIKE wild]\n\n```\n\n### 4.&nbsp;查看数据表中各列的信息\n\ndescription或者DESC 表名[列名]\n\n```\nmysql> desc t_user;\n\n```\n\n### 5.&nbsp;用ALTER TABLE语句修改表的结构\n\n修改列类型　　ALTER TABLE 表名 MODIFY 列名 列类型　　注意：不是任何情况都可以修改，例如名字是char不能改成int\n\n增加列　　ALTER TABLE 表名 ADD 列名 列类型　　　　注意：使用ADD子句增加字段，新的字段只能被加到整个表的最后\n\n```\nALTER TABLE employees ADD gender CHAR(1);\n\n```\n\n删除列　　ALTER TABLE 表名 DROP 列名 列类型\n\n```\nALTER TABLE employees DROP gender CHAR(1);　　#mysql特有\n\n```\n\n```\nALTER TABLE employees DROP COLUMN gender CHAR(1);　#oracle\n\n```\n\n列改名　　ALTER TABLE 表名 CHANGE 旧列名 新列名 列类型\n\n更改表名　　ALTER TABLE 表名 RENAME 新表名\n\nRENAME TABLE 表名 TO 新表名\n\n### 6.&nbsp;删除表\n\n```\nDROP TABLE 表的名字\n\n```\n\n### 7. 不存在插入，存在时更新\n\n可以使用replace into语法或者on duplicate key update语法来实现\n\nreplace into语法\n\n```\nreplace into xx_table (f1, f2, f3) values(v1, v2, v3),(v4, v5, v6);\n\n```\n\non duplicate key update语法\n\n```\ninsert into xx_table (id, f1, f2) values(1, 'test', 123) on duplicate key update f1 = 'test1', f2 = 456;\n\n```\n\n区别：当主键或者唯一索引不重复的时候，两者都是直接insert；当主键或者唯一索引重复的时候，replace into会先删除数据再insert，on duplicate key update是执行update语句\n\n## 4. 视图\n\n### 1.创建视图\n\n视图的好处：可以限制对数据的访问、可以是复杂的查询变得简单、提供了数据的独立性、提供了对相同数据的不同显式\n\n```\n# 创建视图\nCREATE VIEW emo_v_10 AS\nSELECT NAME AS '名字',sex '性别',age '年龄'\nFROM tb_emp\nWHERE dept_id=2;\n\n# 使用视图\nSELECT * FROM emo_v_10\n```\n\n## 5. 其他\n\n### 1.&nbsp;SQL注释\n\n```\n/**/多行注释\n--单行注释\nMySQL注释：#\n\n```\n\n### 2.查看MySQL版本\n\n```\nmysql> select version();\n+-----------+\n| version() |\n+-----------+\n| 8.0.29    |\n+-----------+\n1 row in set (0.00 sec)\n\n```\n\n### 3.查看当前连接MySQL的客户端数量\n\n```\nmysql> show processlist;\n+----+-----------------+------------------+------+---------+--------+------------------------+------------------+\n| Id | User            | Host             | db   | Command | Time   | State                  | Info             |\n+----+-----------------+------------------+------+---------+--------+------------------------+------------------+\n|  5 | event_scheduler | localhost        | NULL | Daemon  | 292527 | Waiting on empty queue | NULL             |\n| 22 | root            | localhost        | NULL | Query   |      0 | init                   | show processlist |\n| 23 | root            | 172.17.0.1:60036 | test | Sleep   |     14 |                        | NULL             |\n+----+-----------------+------------------+------+---------+--------+------------------------+------------------+\n3 rows in set (0.00 sec)\n\n```\n\nCommand中的Daemon表示这是一个守护进程，Sleep表示当前连接处于空闲状态，Query表示当前连接正在执行一个查询\n\n连接默认的最大空闲时间由 `wait_timeout `参数控制，默认为8小时（28800秒）\n\n```\nmysql> show variables like 'wait_timeout';\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| wait_timeout  | 28800 |\n+---------------+-------+\n1 row in set (0.01 sec)\n\n```\n\n可以使用kill命令手动杀死这个连接\n\n```\nmysql> kill connection +23;\nQuery OK, 0 rows affected (0.00 sec)\n\n```\n\n一个连接被服务端主动断开的时候，客户端下次连接的时候会先抛出下面的错误，然后再次尝试连接\n\n```\nERROR 2013 (HY000): Lost connection to MySQL server during query\nNo connection. Trying to reconnect...\nConnection id:    23\nCurrent database: test\n\n```\n\nMySQL的最大连接数量由 `max_connections` 参数控制\n\n```\nmysql> show variables like 'max_connections';\n+-----------------+-------+\n| Variable_name   | Value |\n+-----------------+-------+\n| max_connections | 151   |\n+-----------------+-------+\n1 row in set (0.00 sec)\n\n```\n\n### 4.&nbsp;修改MySQL的密码\n\n```\nSET PASSWORD FOR 'root'@'localhost' = PASSWORD('newpass');\n\n```\n\n### 5.&nbsp;MySQL怎么取消错误的命令\n\n```\n1.没办法的办法，ctrl+c，完全退出\n2.可以输入\\c 废弃本次语句，若输入后没效果是因为未保持当前输入语句完整匹配\n\n```\n\n### 6. 清空表并将自增id归1\n\n```\nTRUNCATE TABLE baike_pages;\n\n```\n\n### 7. 使用timestamp，并在修改的时候自动更新\n\n**将默认值设置为CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP，并为NOT NULL**\n\n### 8. **MySQL数据导出**，使用管道命令\n\n```\nmysql -uroot -pXXXX -e\"select * from music.tencent_music\" > /home/mi/下载/dump\n\n```\n\n也可以使用mysqldump\n\n```\nmysqldump -hlocalhost -uroot -p xxx_table > ./xxx_table_2020-04-20.bak\n\n```\n\n### 9.MySQL导入大量测试数据\n\n生成测试数据可以使用如下网站：[https://mockaroo.com/](https://mockaroo.com/)，生成后保存成csv文件\n\n然后使用MySQL的load命令将数据导入到MySQL表中\n\n```\nmysql> LOAD DATA INFILE '/var/opt/rh/rh-mysql57/lib/mysql-files/MOCK_DATA.csv'\n    -> INTO TABLE `default`.user\n    -> FIELDS TERMINATED BY ','\n    -> ENCLOSED BY '\"'\n    -> LINES TERMINATED BY '\\n'\n    -> IGNORE 1 LINES;\nQuery OK, 1000 rows affected (0.01 sec)\nRecords: 1000  Deleted: 0  Skipped: 0  Warnings: 0\n\n```\n\n如果遇到 ERROR 1290 (HY000): The MySQL server is running with the --secure-file-priv option so it cannot execute this statement 的报错，可以使用如下命令查看允许导入数据的目录\n\n```\nmysql> SHOW VARIABLES LIKE \"secure_file_priv\";\n+------------------+-----------------------------------------+\n| Variable_name    | Value                                   |\n+------------------+-----------------------------------------+\n| secure_file_priv | /var/opt/rh/rh-mysql57/lib/mysql-files/ |\n+------------------+-----------------------------------------+\n1 row in set (0.01 sec)\n\n```\n\n### 10.MySQL查看表数据和索引所占的存储大小\n\n```\nselect concat(round(sum(data_length/1024/1024),2),'MB') as data_length_MB,\n    concat(round(sum(index_length/1024/1024),2),'MB') as index_length_MB\n    from information_schema.tables where\n    table_schema='xx_db'\n    and table_name = 'xx_table';\n\n```\n\n结果\n\n<img src=\"/images/517519-20230302160723730-125737714.png\" alt=\"\" loading=\"lazy\" />\n\n### 11.使用explain查看SQL的执行计划\n\n查看查询SQL的执行计划，其中username字段添加了索引\n\n```\nexplain select * from user where username = 'test';\n\n```\n\n如果数据量很少的话，其执行计划如下\n\n```\nmysql> explain select * from user where username = 'test';\n+----+-------------+-------+------------+------+---------------------+------+---------+------+------+----------+-------------+\n| id | select_type | table | partitions | type | possible_keys       | key  | key_len | ref  | rows | filtered | Extra       |\n+----+-------------+-------+------------+------+---------------------+------+---------+------+------+----------+-------------+\n|  1 | SIMPLE      | user  | NULL       | ALL  | user_username_index | NULL | NULL    | NULL |   18 |    88.89 | Using where |\n+----+-------------+-------+------------+------+---------------------+------+---------+------+------+----------+-------------+\n1 row in set, 1 warning (0.00 sec)\n\n```\n\n添加了1000条随机数据后，其执行计划如下\n\n```\nmysql> explain select * from user where username = 'test';\n+----+-------------+-------+------------+------+---------------------+---------------------+---------+-------+------+----------+-------+\n| id | select_type | table | partitions | type | possible_keys       | key                 | key_len | ref   | rows | filtered | Extra |\n+----+-------------+-------+------------+------+---------------------+---------------------+---------+-------+------+----------+-------+\n|  1 | SIMPLE      | user  | NULL       | ref  | user_username_index | user_username_index | 514     | const |   16 |   100.00 | NULL  |\n+----+-------------+-------+------------+------+---------------------+---------------------+---------+-------+------+----------+-------+\n1 row in set, 1 warning (0.00 sec)\n\n```\n\nexplain输出信息的含义，参考：[https://dev.mysql.com/doc/refman/5.7/en/explain-output.html](https://dev.mysql.com/doc/refman/5.7/en/explain-output.html)\n\n\n\n|字段|含义\n| ---- | ---- \n|id|`id` 字段是查询中每个 `SELECT` 语句的标识符。在一个查询中，每个 `SELECT` 语句会有一个唯一的 `id`。\n|select_type|`select_type` 字段显示 `SELECT` 语句的类型。- `SIMPLE`：简单查询，不包含子查询或联合查询。- `PRIMARY`：最外层的 `SELECT` 查询。- `UNION`：联合查询中的第二个或后续 `SELECT`。- `DEPENDENT UNION`：依赖于外部查询的 `UNION`。- `UNION RESULT`：`UNION` 的结果。- `SUBQUERY`：子查询。- `DEPENDENT SUBQUERY`：依赖于外部查询的子查询。- `DERIVED`：派生表（子查询中使用的临时表）。\n|table|`table` 字段表示正在访问的表的名称或派生表的别名。\n|partitions|`partitions` 字段显示查询匹配的分区信息。如果表是分区表，该字段显示哪些分区被扫描。\n|type|`type` 字段表示 MySQL 选择的查询类型或访问类型，指示表的访问方式。常见值（按效率排序）：- `system`：表只有一行（等于 `const` 表）。- `const`：表最多只有一个匹配行（常量索引）。- `eq_ref`：对于每个来自前一个表的行组合，最多有一个匹配行。- `ref`：对于每个来自前一个表的行组合，有可能有多个匹配行。- `fulltext`：使用全文索引。- `ref_or_null`：与 `ref` 类似，但会查找带有 `NULL` 的行。- `index_merge`：使用索引合并优化方法。- `unique_subquery`：用于优化在 `IN` 子查询中使用的唯一索引。- `index_subquery`：用于优化 `IN` 子查询，而不使用唯一索引。- `range`：仅检索给定范围的行，使用一个索引来选择行。常见于 '<', '<=', '>', '>=', 'between' 等操作符已经like 'xx'或者like 'xx%'的模式匹配查询。- `index`：全表扫描，但是扫描的只是索引，比如覆盖索引或order by排序查询。- `ALL`：全表扫描。\n|possible_keys|`possible_keys` 字段显示 MySQL 考虑在查询中使用的索引。显示优化器在查询分析阶段的可能索引。可以帮助判断是否存在未被使用的索引。\n|key|`key` 字段表示 MySQL 实际使用的索引名称。显示查询优化器选择的实际索引。若为空，表示未使用索引。\n|key_len|`key_len` 字段表示 MySQL 使用的索引的字节长度。指示优化器在查询过程中使用的索引部分的长度（单位为字节）。有助于理解索引的使用范围。\n|ref|`ref` 字段显示使用的列或常量，来与索引列进行比较。指示查询中与索引列进行比较的列或常量。显示的是匹配条件，如 `const`，表示索引列与常量比较。\n|rows|`rows` 字段是 MySQL 估计的要读取和检查的行数。提供有关查询性能的估计值，显示需要访问的行数来执行查询。\n|filters|`filtered 字段显示查询条件过滤掉的行的百分比。`较高的过滤率（接近 100%）表示大多数行都匹配了 WHERE 条件，低过滤率可能表明查询性能不佳。\n|Extra|`Extra` 字段提供了查询执行过程中的其他详细信息。常见值：- `Using index`：查询中使用了覆盖索引（只从索引树中读取数据）。- `Using where`：查询过程中使用了 WHERE 子句来过滤行。- `Using temporary`：MySQL 需要使用临时表来存储结果。- `Using filesort`：MySQL 需要额外的排序操作，而不是从索引中读取数据。\n\n可以对比一下下面2个SQL中where和having的效率\n\n```\nmysql> explain select username, count(1) as cnt from `default`.user group by username having cnt > 1;\n+----+-------------+-------+------------+-------+---------------------+---------------------+---------+------+------+----------+-------------+\n| id | select_type | table | partitions | type  | possible_keys       | key                 | key_len | ref  | rows | filtered | Extra       |\n+----+-------------+-------+------------+-------+---------------------+---------------------+---------+------+------+----------+-------------+\n|  1 | SIMPLE      | user  | NULL       | index | user_username_index | user_username_index | 514     | NULL | 1018 |   100.00 | Using index |\n+----+-------------+-------+------------+-------+---------------------+---------------------+---------+------+------+----------+-------------+\n1 row in set, 1 warning (0.00 sec)\n\nmysql> explain select * from (select username, count(1) as cnt from `default`.user group by username) t1 where t1.cnt > 1;\n+----+-------------+------------+------------+-------+---------------------+---------------------+---------+------+------+----------+-------------+\n| id | select_type | table      | partitions | type  | possible_keys       | key                 | key_len | ref  | rows | filtered | Extra       |\n+----+-------------+------------+------------+-------+---------------------+---------------------+---------+------+------+----------+-------------+\n|  1 | PRIMARY     | <derived2> | NULL       | ALL   | NULL                | NULL                | NULL    | NULL | 1018 |    33.33 | Using where |\n|  2 | DERIVED     | user       | NULL       | index | user_username_index | user_username_index | 514     | NULL | 1018 |   100.00 | Using index |\n+----+-------------+------------+------------+-------+---------------------+---------------------+---------+------+------+----------+-------------+\n2 rows in set, 1 warning (0.00 sec)\n\n```\n\n可以对比一下下面2个SQL中给username字段建立索引和email字段没有索引的效率\n\n```\nmysql> explain select username as cnt from `default`.user where username = 'test';\n+----+-------------+-------+------------+------+---------------------+---------------------+---------+-------+------+----------+-------------+\n| id | select_type | table | partitions | type | possible_keys       | key                 | key_len | ref   | rows | filtered | Extra       |\n+----+-------------+-------+------------+------+---------------------+---------------------+---------+-------+------+----------+-------------+\n|  1 | SIMPLE      | user  | NULL       | ref  | user_username_index | user_username_index | 514     | const |   16 |   100.00 | Using index |\n+----+-------------+-------+------------+------+---------------------+---------------------+---------+-------+------+----------+-------------+\n1 row in set, 1 warning (0.00 sec)\n\nmysql> explain select email as cnt from `default`.user where email = 'test@test';\n+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+\n| id | select_type | table | partitions | type | possible_keys | key  | key_len | ref  | rows | filtered | Extra       |\n+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+\n|  1 | SIMPLE      | user  | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 1018 |    10.00 | Using where |\n+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+\n1 row in set, 1 warning (0.00 sec)\n\nmysql> explain select email as cnt from `default`.user where username = 'test' and email = 'test@test';\n+----+-------------+-------+------------+------+---------------------+---------------------+---------+-------+------+----------+-------------+\n| id | select_type | table | partitions | type | possible_keys       | key                 | key_len | ref   | rows | filtered | Extra       |\n+----+-------------+-------+------------+------+---------------------+---------------------+---------+-------+------+----------+-------------+\n|  1 | SIMPLE      | user  | NULL       | ref  | user_username_index | user_username_index | 514     | const |   16 |    10.00 | Using where |\n+----+-------------+-------+------------+------+---------------------+---------------------+---------+-------+------+----------+-------------+\n1 row in set, 1 warning (0.00 sec)\n\n```\n\n### 12.count性能排序\n\n```\nCOUNT(*) &asymp; COUNT(1) > COUNT(主键字段) > COUNT(非主键字段)\n\n```\n\n参考：[https://xiaolincoding.com/mysql/index/count.html](https://xiaolincoding.com/mysql/index/count.html)\n","tags":["MySQL"]},{"title":"使用confluent schema registry将protobuf schema转换成avro schema","url":"/使用confluent schema registry将protobuf schema转换成avro schema.html","content":"confleunt提供了一些方法，可以将protobuf schema转换成avro schema，用于支持将kafka protobuf序列化的message落盘成avro格式的文件\n\n## 1.引入依赖\n\n```\n    <repositories>\n        <repository>\n            <id>cloudera</id>\n            <url>https://repository.cloudera.com/artifactory/cloudera-repos/</url>\n        </repository>\n        <repository>\n            <id>confluent</id>\n            <url>https://packages.confluent.io/maven/</url>\n        </repository>\n    </repositories>\n\n    <dependencies>\n        <!--pb-->\n        <dependency>\n            <groupId>com.google.protobuf</groupId>\n            <artifactId>protobuf-java</artifactId>\n            <version>3.21.7</version>\n        </dependency>\n        <!--confluent-->\n        <dependency>\n            <groupId>io.confluent</groupId>\n            <artifactId>kafka-schema-registry</artifactId>\n            <version>7.1.1</version>\n        </dependency>\n        <dependency>\n            <groupId>io.confluent</groupId>\n            <artifactId>kafka-protobuf-provider</artifactId>\n            <version>7.1.1</version>\n        </dependency>\n        <dependency>\n            <groupId>io.confluent</groupId>\n            <artifactId>kafka-connect-avro-data</artifactId>\n            <version>7.1.1</version>\n        </dependency>\n        <dependency>\n            <groupId>io.confluent</groupId>\n            <artifactId>kafka-connect-protobuf-converter</artifactId>\n            <version>7.1.1</version>\n        </dependency>\n        <dependency>\n            <groupId>io.confluent</groupId>\n            <artifactId>kafka-connect-avro-data</artifactId>\n            <version>7.1.1</version>\n        </dependency>\n        <!--kafka-->\n        <dependency>\n            <groupId>org.apache.kafka</groupId>\n            <artifactId>connect-api</artifactId>\n            <version>1.1.0</version>\n        </dependency>\n    </dependencies>\n\n```\n\n## 2.定义protobuf schema\n\n定义一个protobuf schema\n\n```\nsyntax = \"proto3\";\npackage com.acme;\n\nmessage MyRecord {\n  string f1 = 1;\n  OtherRecord f2 = 2;\n}\n\nmessage OtherRecord {\n  int32 other_id = 1;\n}\n\n```\n\n编译java代码\n\n```\nprotoc -I=./ --java_out=./src/main/java ./src/main/proto/other.proto\n\n```\n\n得到schema的java代码\n\n<img src=\"/images/517519-20231026122002799-1052709931.png\" width=\"300\" height=\"81\" loading=\"lazy\" />\n\n## 3.将protobuf schema转换成avro schema\n\nconfluent schema registry在处理处理protobuf，avro，json格式的数据的时候，会统一先将其转换成connect schema格式的数据，然后再将其写成parquet，avro等具体的文件格式\n\n```\nimport com.acme.Other;\nimport io.confluent.connect.avro.AvroData;\nimport io.confluent.connect.avro.AvroDataConfig;\nimport io.confluent.connect.protobuf.ProtobufData;\nimport io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\nimport io.confluent.kafka.schemaregistry.protobuf.ProtobufSchemaUtils;\nimport org.apache.kafka.connect.data.SchemaAndValue;\n\npublic class ProtobufToAvro {\n\n    public static void main(String[] args) {\n        // 初始化protobuf定义的类\n        Other.MyRecord obj = Other.MyRecord.newBuilder().build();\n        // 获取pb schema\n        ProtobufSchema pbSchema = ProtobufSchemaUtils.getSchema(obj);\n        ProtobufData protobufData = new ProtobufData();\n//        SchemaAndValue result = protobufData.toConnectData(pbSchema, obj);\n//        System.out.println(result);\n\n        AvroDataConfig avroDataConfig = new AvroDataConfig.Builder()\n                .with(AvroDataConfig.SCHEMAS_CACHE_SIZE_CONFIG, 1)\n                .with(AvroDataConfig.CONNECT_META_DATA_CONFIG, false)\n                .with(AvroDataConfig.ENHANCED_AVRO_SCHEMA_SUPPORT_CONFIG, true)\n                .build();\n        AvroData avroData = new AvroData(avroDataConfig);\n        // 先将protobuf schema转换成connect schema，然后再转换成avro schema\n        org.apache.avro.Schema avroSchema = avroData.fromConnectSchema(protobufData.toConnectSchema(pbSchema));\n        System.out.println(avroSchema);\n    }\n\n}\n\n```\n\n转换的avro schema输出如下\n\n```\n{\n   \"type\":\"record\",\n   \"name\":\"MyRecord\",\n   \"fields\":[\n      {\n         \"name\":\"f1\",\n         \"type\":[\n            \"null\",\n            \"string\"\n         ],\n         \"default\":null\n      },\n      {\n         \"name\":\"f2\",\n         \"type\":[\n            \"null\",\n            {\n               \"type\":\"record\",\n               \"name\":\"OtherRecord\",\n               \"fields\":[\n                  {\n                     \"name\":\"other_id\",\n                     \"type\":[\n                        \"null\",\n                        \"int\"\n                     ],\n                     \"default\":null\n                  }\n               ]\n            }\n         ],\n         \"default\":null\n      }\n   ]\n}\n\n```\n\n注意：confluent在具体实现的时候，比较严谨，在protobuf的uint32（0 到 2^32 -1）的时候，会统一转换成long（-2^63 ~ 2^63-1）,不会出现越界的情况，参考源码\n\n```\nhttps://github.com/confluentinc/schema-registry/blob/v7.1.1/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java#L1485\n\n```\n\n转换实现参考源码\n\n```\nhttps://github.com/confluentinc/schema-registry/blob/v7.1.1/avro-data/src/test/java/io/confluent/connect/avro/AdditionalAvroDataTest.java\n\n```\n\n<!--more-->\n&nbsp;\n","tags":["avro","confluent"]},{"title":"Hive学习笔记——安装hive客户端","url":"/Hive学习笔记——安装hive客户端.html","content":"hive client安装文档\n\n```\nhttps://cwiki.apache.org/confluence/display/Hive/GettingStarted\n\n```\n\nhive 配置官方文档\n\n```\nhttps://cwiki.apache.org/confluence/display/hive/configuration+properties\n\n```\n\nhive 配置中文文档\n\n```\nhttps://www.docs4dev.com/docs/zh/apache-hive/3.1.1/reference/Configuration_Properties.html\n\n```\n\n　　\n","tags":["Hive"]},{"title":"Hive学习笔记——执行计划","url":"/Hive学习笔记——执行计划.html","content":"Hive学习笔记&mdash;&mdash;执行计划\n","tags":["Hive"]},{"title":"avro学习笔记——实现序列化和反序列化","url":"/avro学习笔记——实现序列化和反序列化.html","content":"## 1.使用java语言来实现avro序列化和反序列化\n\n使用DatumWriter和DatumReader对avro进行序列化和反序列化\n\n```\n    public static <T> byte[] binarySerializable(T t) {\n        ByteArrayOutputStream out = new ByteArrayOutputStream();\n        BinaryEncoder binaryEncoder = EncoderFactory.get().binaryEncoder(out, null);\n        DatumWriter<T> writer = new SpecificDatumWriter<T>((Class<T>) t.getClass());\n        try {\n            writer.write(t, binaryEncoder);\n            binaryEncoder.flush();\n            out.flush();\n        } catch (IOException e) {\n            LOGGER.error(\"binary serializable error\");\n            e.printStackTrace();\n        }\n        LOGGER.debug(\"ByteArrayOutputStream = {}\", new String(out.toByteArray()));\n        return out.toByteArray();\n    }\n\n\n    public static <T> T binaryDeserialize(byte[] bytes, Schema schema) {\n        try {\n            BinaryDecoder binaryDecoder = DecoderFactory.get().binaryDecoder(bytes, null);\n            DatumReader<T> datumReader = new SpecificDatumReader<T>(schema);\n            T read = datumReader.read(null, binaryDecoder);\n            return read;\n        } catch (IOException e) {\n            LOGGER.error(\"binary deserialize error\");\n            e.printStackTrace();\n        }\n        return null;\n    }\n\n```\n\n使用Twitter 的 Bijection 类库来实现avro对象的序列化和反序列化\n\n参考：[Kafka 中使用 Avro 序列化框架(二)：使用 Twitter 的 Bijection 类库实现 avro 的序列化与反序列化](https://www.jianshu.com/p/a70950bab06d)\n\n<!--more-->\n&nbsp;\n\n## 2.使用python语言来实现avro序列化和反序列化\n\n&nbsp;\n\n## 3.使用go语言来实现avro序列化和反序列化\n\n&nbsp;\n\n**&nbsp;**\n","tags":["avro"]},{"title":"JavaWeb学习笔记——ServletConfig和ServletContext","url":"/JavaWeb学习笔记——ServletConfig和ServletContext.html","content":"1.只有在第一次请求服务器产生实例的时候才会调用init()方法，有一种办法能在服务器一启动的时候就加载init()方法。\n\n即服务器启动即加载Servlet，且按数字大小顺序实例化Servlet。\n\n方法：\n\n　　创建一个TestObject.java\n\n　　在web.xml中的<servlet></servlet>写<load-on-startup>1</load-on-startup>，数字小的Servlet将先启动。\n\n　　再创建一个TestObject2.java，然后在web.xml中写<load-on-startup>2</load-on-startup>\n\n　　\n\n　　　　1>在Servlet的配置文件中，可以使用一个或多个<init-param>标签为servlet配置一些初始化参数。（配置在某个servlet标签或者整个web-app下）\n\n<!--more-->\n&nbsp; &nbsp; &nbsp;　　&nbsp;2>当servlet配置了初始化参数后，web容器在创建servlet实例对象时，会自动将这些初始化参数封装到ServletConfig对象中，\n\n&nbsp; &nbsp; &nbsp; 并在调用servlet的init方法时，将ServletConfig对象传递给servlet。进而，程序员通过ServletConfig对象就可以得到当前servlet的初始化参数信息。\n\n&nbsp;\n\n2.ServletConfig：\n\n　　　　1>在`**[init](http://www.cnblogs.com/)**([ServletConfig](http://www.cnblogs.com/)&nbsp;config)中的ServletConfig参数是一个接口，这个ServletConfig对象代表一个Servlet`\n\n　　　　一个WebApp可以有多个Servlet，每个Servlet对应一个ServletConfig对象,从而每个Servlet的实例是不一样的，即this是不一样的，this指的就是当前这个Servlet。\n\n　　　　而一个WebApp只有一个ServletContext对象，所以所有ServletConfig对象共享一个ServletContext对象。<br />\n\n　　　　　　总而言之：一个Servlet有一个对应的Config对象，这个对象也是由容器创建给我们的，代表每个Servlet的信息对象（Config就是信息的意思）\n\n　　　　　　　　　　　每个Servlet　　<1>可以获取初始化参数和ServletContext对象\n\n　　　　　　　　　　　　　　　　　　 <2>不能获取其他Servlet里面定义的init-param的值（因为跨出了范围）\n\n　　　　　　　　　　　　　　　　　　&nbsp;<3>不能获取Context里面定义的init-param的值（因为跨出了范围）\n\n　　　　方法：在web.xml中的<servlet></servlet>写\n\n```\n    <!-- 设置参数 -->\n    <init-param>\n        <param-name>coursename</param-name>\n        <param-value>JAVAWEB</param-value>\n    </init-param>　　　\n```\n\n　　　　　　　再建一个TestObject.hava后，在web.xml中加入\n\n```\n<!-- 设置参数 -->\n    <init-param>\n        <param-name>filename</param-name>\n        <param-value>CONFIG</param-value>\n    </init-param>\n    \n    <load-on-startup>2</load-on-startup>\n```\n\n&nbsp;\n\n3.ServletContext：　　　　\n\n　　　　每一个webapp有且只有一个对应的Context对象（也是一个接口），这个对象也是容器创建给我们的，代表每个web应用的上下文环境\n\n　　　　1>可以用来获取context-param初始化参数\n\n　　　　2>不能获取config里面定义的初始化参数\n\n　　　　3>每个Servlet共享同一个context对象（在context里面定义的参数所有Servlet都可以通过context对象去获取）\n\n　　　　4>Context是一个重量级的对象，是Servlet里面最大的缓存，存储在里面的信息可以被所有的Servlet共享\n\n　　　　　　我们可以在Context看成一个很大的缓存（内存）\n\n　　　　　　其实可以把Context看成一个容器（集合）\n\n　　　　　　重要的方法：\n\n　　　　　　　　1.`**[setAttribute](http://www.cnblogs.com/)**(java.lang.String&nbsp;name, java.lang.Object&nbsp;object)`\n\n　　　　　　　　2.`**[getAttribute](http://www.cnblogs.com/)**(java.lang.String&nbsp;name)`\n\n　　　　　　　　3.`**[removeAttribute](http://www.cnblogs.com/)**(java.lang.String&nbsp;name)`\n\n　　　　　　　　可以通过上面方法存储对象，使得多有Servlet共享，但是如果不是所有Servlet都需要使用的不要放在Context缓存里面，浪费资源。\n\n&nbsp;\n\n4.页面导航（跳转）\n\n　　//步骤一.获取转发对象，参数就是调整的url，示例：成功就跳转到success.do<br />\t\t\t　　RequestDispatcher rd = request.getRequestDispatcher(\"success.do\");\n\n　　问题：\n\n　　　　1>跳转是使用get方式还是post方式？\n\n　　　　　　从doGet跳转过来就是doGet，从doPost跳转过来就是doPost\n\n　　　　2>如果想在下一个页面获取前面页面传递的数据？\n\n　　　　　　a>可以通过ServletContext对象传递数据，\n\n　　　　　　弊端:所有Servlet里面都有username这个属性值了，因为Context对象对所有Servlet都是共享的。类似于静态共享\n\n　　　　　　b>String username = pro.getProperty(\"username\");\n\n　　　　　　可以保存到request对象当中，request属于请求对象，只在请求访问内有效\n\n　　　　　　request属于请求对象，只在请求访问内有效，比Context小多了\n\n　　　　3>rd.forward(request, response);\n\n　　　　　　forward的时候请求链没有断开，可以从请求中获取request对象里面存储的数据\n\n　　　　　　forward可以使用&ldquo;/&rdquo;，也可以使用相对路径\n\n　　　　　　　　\n\n　　　　　　RequestDispatcher rd = this.getServletContext().getRequestDispatcher(\"success.do\");\n\n　　　　　　RequestDispatcher对象也可以从ServletContext对象当中获取\n\n　　　　　　而从ServletContext当中获取RequestDispatcher只可以使用&ldquo;/&rdquo;，建议从request中取，而不从context中取\n\n&nbsp;　　\n\n　　　　　　RequestDispatcher rd = this.getServletContext().getRequestDispatcher(\"login/success.do\");　　　　　　\n\n　　　　　　RequestDispatcher rd = this.getServletContext().getRequestDispatcher(\"/user/test.do\");从根目录开始找\n\n　　　　　　成功，从根目录开始找可以找到\n\n　　　　　　RequestDispatcher rd = this.getServletContext().getRequestDispatcher(\"login/success.do\");　　　　　　\n\n　　　　　　RequestDispatcher rd = this.getServletContext().getRequestDispatcher(\"user/test.do\");从login目录开始找user找不到\n\n　　　　　　失败\n\n　　　　　　\n\n　　　　　　response.sendRedirect(\"fail.do\"); 跳转的时候请求链会断开，不可以从请求中获取request对象里面存储的数据\n\n　　　　　　如果是超链接的话则请求链断开，不可以从请求中获取request对象里面存储的数据\n\n　　　　　　forward的时候地址栏显示的是不确定的地址；sendRedirect的时候地址栏显示的是实际跳转的地址\n\n　　　　　　sendRedirect有一个功能是forward完成不了的，forward只能在本webapp内跳转，但是sendRedirect实现的是和在地址栏里面输入地址一样的操作\n\n　　　　可以将重定向到任一的url上，而不是同一应用程序的url（可以实现单点登陆功能：财务系统、OA系统、仓储系统，即不同的web应用互相访问的时候，只有这一种方式）\n\n　　　　4>rd.include(request, response);\n\n　　　　　　将下一个页面的结果包含到当前页面，用的少\n\n　　　　　　\n\n&nbsp;\n","tags":["JavaWeb"]},{"title":"如何使用Hexo搭建静态博客","url":"/如何使用Hexo搭建静态博客.html","content":"#### 介绍如何使用Hexo和Github搭建静态博客<br/>使用的主题为Bootstrap<br/>搭建的环境是ubuntu 14.04 LTS\n\n### <font color=#ff0000><1>什么是Hexo</font>\n\n<font face=\"微软雅黑\">\nHexo 是一款基于node 的静态博客网站生成器，作者：tommy351是一个台湾的在校大学生。\n相比其他的静态网页生成器而言有着，生成静态网页最快，插件丰富（已经移植了大量Octopress插件）。\n同其他很多轻量级博客如jekyll、octopress、jekyllbootstrap等一样，也是使用Markdown语法进行编辑博文，即编辑.md后缀的文件。</br>\n</font>\n\nhexo的github主页地址: [Hexo_Github](https://github.com/tommy351/hexo)\nMarkdown的中文语法: [Hexo_Docs](http://wowubuntu.com/markdown/#list)\n\n<!--more-->\n&nbsp;\n\n### <font color=#ff0000><2>注册Github并创建个人站点</font>\n\n<font face=\"微软雅黑\">\nGit是一个分布式的版本控制系统，功能强大而且免费\n要想在Github上搭建一个基于Hexo的博客，首先要注册一个Github帐号，并在上面创建个人站点\n这里我参考的是博客园一篇博文的前几个步骤，即创建Github Pages</br>\n</font>\n\nGithub主页: [Github](https://github.com/)\n参考的博客园博文地址: [创建Github Pages参考](http://www.cnblogs.com/purediy/archive/2013/03/07/2948892.html)\n\n\n### <font color=#ff0000><3>安装Node.js和Git</font>\n\n<font face=\"微软雅黑\">\n要想使用Hexo，必须安装Node.js和Git</br>\n</font>\n\n#### **安装Git**\n\n<font face=\"微软雅黑\">\n由于用的是ubuntu，安装git直接在终端下运行命令apt-get就行了</br>\n</font>\n\n``` bash\n$ sudo apt-get install git-core\n```\n\n#### **安装Node.js**\n\n<font face=\"微软雅黑\">\n参考网上一篇CSDN文章安装的Node.js，使用的是源码进行编译安装\n安装的版本是v0.10.29</br>\n</font>\n\n``` bash\ncd /usr/local/src\nwget http://nodejs.org/dist/v0.10.29/node-v0.10.29.tar.gz\ntar zxvf node-v0.10.29.tar.gz\ncd node-v0.10.29\n./configure\nmake\nmake install\n```\n\n``` bash\nnode -v\nv0.10.29\n```\n\n参考CSDN文章地址: [ubuntu下安装Node.js参考](http://blog.csdn.net/nsrainbow/article/details/33740915)\n\n#### **安装NPM**\n<font face=\"微软雅黑\">\nNPM的全称是Node Package Manager，是Nodejs的包管理器，要安装Hexo的话，就是通过NPM来进行的\n安装NPM的命令\n这里详解一下这句命令的意思，curl http://npmjs.org/install.sh 是通过curl命令获取这个安装shell脚本，按后通过管道符| 将获取的脚本交由sh命令来执行。这里如果没有权限会安装不成功，需要加上sudo来确保权限</br>\n</font>\n\n``` bash\ncurl http://npmjs.org/install.sh | sudo sh\n```\n\n<font face=\"微软雅黑\">\n上面的网址不行的话换成www.npmjs.com,安装成功后执行npm命令，会得到一下的提示:</br>\n</font>\n\n``` bash\nUsage: npm <command>\nwhere <command> is one of:\nadduser, apihelp, author, bin, bugs, c, cache, completion,\nconfig, deprecate, docs, edit, explore, faq, find, get,\nhelp, help-search, home, i, info, init, install, la, link,\nlist, ll, ln, ls, outdated, owner, pack, prefix, prune,\npublish, r, rb, rebuild, remove, restart, rm, root,\nrun-script, s, se, search, set, show, star, start, stop,\nsubmodule, tag, test, un, uninstall, unlink, unpublish,\nunstar, up, update, version, view, whoami\n```\n\n<font face=\"微软雅黑\">\n另外，由于某些原因，国内npm速度比较慢，甚至打不开，建议使用淘宝源：http://npm.taobao.org/ (本条有可能导致出错)</br>\n</font>\n\n``` bash　　\nnpm install -g cnpm --registry=https://registry.npm.taobao.org\n```\n\n其他文章地址: [ubuntu下安装NPM参考](http://www.bubuko.com/infodetail-835851.html)\n可能出现错误的解决方法: [npm ERR!](http://cnodejs.org/topic/5222d4e5bee8d3cb1265ad60)\n\n### <font color=#ff0000><4>安装Hexo</font>\n\n<font face=\"微软雅黑\">\nNode, npm和Git都安装成功, 开始安装Hexo,要求安装G++和Python</br>\n</font>\n\n``` bash\nnpm install hexo -g  #-g表示全局安装, npm默认为当前项目安装\n```\n\n<font face=\"微软雅黑\">\nHexo使用命令:</br>\n</font>\n\n``` bash\nhexo init <folder>  #执行init命令初始化hexo到你指定的目录\nhexo g       #自动根据当前目录下文件,生成静态网页\nhexo s         #运行本地服务\n```\n\n<font face=\"微软雅黑\">\n浏览器输入http://localhost:4000 就可以看到效果</br>\n</font>\n\n#### **设置Hexo全局配置**\n\n<font face=\"微软雅黑\">\n打开站点配置文件_config.yml，只需要修改部分配置，大部分保持默认即可\n也可以下载Bootstrap主题，然后把Hexo的_config.yml的theme里面改成Bootstrap</br>\n</font>\n\n``` bash\n# Hexo Configuration\n## Docs: https://hexo.io/docs/configuration.html\n## Source: https://github.com/hexojs/hexo/\n\n# Site\ntitle: Common的个人主页\nsubtitle:\ndescription:\nauthor: Common\nlanguage: zh-CN\ntimezone:\n\n# URL\n## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'\nurl: http://lintong-common.github.io\nroot: /\npermalink: :year/:month/:day/:title/\npermalink_defaults:\n\n# Directory\nsource_dir: source\npublic_dir: public\ntag_dir: tags\narchive_dir: archives\ncategory_dir: categories\ncode_dir: downloads/code\ni18n_dir: :lang\nskip_render:\n\n# Writing\nnew_post_name: :title.md # File name of new posts\ndefault_layout: post\ntitlecase: false # Transform title into titlecase\nexternal_link: true # Open external links in new tab\nfilename_case: 0\nrender_drafts: false\npost_asset_folder: false\nrelative_link: false\nfuture: true\nhighlight:\n  enable: true\n  line_number: true\n  auto_detect: false\n  tab_replace:\n\n# Category & Tag\ndefault_category: uncategorized\ncategory_map:\ntag_map:\n\n# Date / Time format\n## Hexo uses Moment.js to parse and display date\n## You can customize the date format as defined in\n## http://momentjs.com/docs/#/displaying/format/\ndate_format: YYYY-MM-DD\ntime_format: HH:mm:ss\n\n# Pagination\n## Set per_page to 0 to disable pagination\nper_page: 10\npagination_dir: page\n\n# Extensions\n## Plugins: https://hexo.io/plugins/\n## Themes: https://hexo.io/themes/\ntheme: bootstrap-blog\n\n# Deployment\n## Docs: https://hexo.io/docs/deployment.html\ndeploy:\n  type: git #部署工具的类型\n  repository: git@github.com:Lintong-common/lintong-common.github.com.git  #SSH链接\n  branch: master #分支\n```\n\n参考的博客地址: [Hexo(一)：在GitHub上搭建静态博客](http://blog.hjtxxx.com/2015/08/13/Hexo-%E4%B8%80-%EF%BC%9A%E5%9C%A8GitHub%E4%B8%8A%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2/)\n\n### <font color=#ff0000><5>下载Bootstrap主题</font>\n\n<font face=\"微软雅黑\">\n在下面给的网址中下载主题，放在hexo/themes/目录下，然后修改Hexo目录下的._config.yml文件中的theme参数</br>\n</font>\n\nHexo主题库: [Hexo主题库](https://hexo.io/themes/)\n\n<font face=\"微软雅黑\">\n以下是我修改后的themes/bootstrap-blog目录下的._config.yml文件</br>\n</font>\n\n``` bash\n# Header\nnavbar_brand: false\nmenu:\n  主页: index.html\n  所有文章: archives/\n  其他页面: page/Hello.html\nrss: /atom.xml\n\n# Content\nexcerpt_link: 全文 >>\nfancybox: true\n\n# Sidebar\nwidgets:\n- about         # See also: `about_content`\n- category\n- tag\n- tagcloud\n- archive\n- recent_posts\n\nabout_widget_content:\n  <div style=\"text-align:center;\"><img src=\"/images/he.jpg\" width=\"150\" height=\"150\" style=\"vertical-align:middle;\"/></div>    #About 关于我\n\n# widget behavior\narchive_type: 'monthly'\nshow_count: true\n\n# Miscellaneous\ngoogle_analytics:\nfavicon: http://o6g92sjqd.bkt.clouddn.com/3232.ico\ntwitter_id:\ngoogle_plus:\nfb_admins:\nfb_app_id:\n```\n\n### <font color=#ff0000><6>部署到Github上</font>\n\n<font face=\"微软雅黑\">\n在下面给的网址中下载主题，放在hexo/themes/目录下，然后修改Hexo目录下的._config.yml文件中的theme参数</br>\n</font>\n\n参考该博客的博文的如何部署到Github: [如何部署到Github](http://www.jianshu.com/p/740411136715)\n\n添加可跳转的目录，参考： [Hexo 博客踩坑](https://convivae.top/posts/hexo-bo-ke-cai-keng)\n\n### <font color=#ff0000>写在之后：搭建的过程中遇到了各种问题，也是google各种解决方法，参考了很多博文，总结的难免有披露，同时也感谢网上大神们的分享</font>\n","tags":["hexo"]},{"title":"Chrome使用笔记","url":"/Chrome使用笔记.html","content":"## 1.开启debug模式\n\n可以通过在启动命令中添加\n\n```\n--remote-debugging-port=9222\n\n```\n\n来开启chrome的debug端口，这样就可以通过这个端口来获取或者操作chrome，如下\n\nmacos\n\n```\n/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --remote-debugging-port=9222\n\n```\n\nlinux\n\n```\ngoogle-chrome --remote-debugging-port=9222\n\n```\n\nwindows\n\n```\n右键点击 Chrome 的快捷方式图标，选择属性\n在目标一栏，最后加上--remote-debugging-port=9222 注意要用空格隔开\n\n```\n\n可以打开另一个chrome浏览器B来监控开启了debug端口的chrome浏览器A\n\n在Chrome浏览器B中打开\n\n```\nchrome://inspect/#devices\n\n```\n\n将其中的端口设置成开启了debug模式的chrome浏览器B的端口\n\n<img src=\"/images/517519-20240727153246511-1980815495.png\" width=\"600\" height=\"237\" loading=\"lazy\" />\n\n这样就可以在浏览器A中看到浏览器B的浏览记录\n\n```\nchrome://inspect/#pages\n\n```\n\n<img src=\"/images/517519-20240727153722253-1610190142.png\" width=\"500\" height=\"161\" loading=\"lazy\" />\n\n参考：[Google Chrome远程调试的简单利用](https://www.cnblogs.com/--kisaragi--/p/15241080.html)\n\n也可以使用selenium来连接开启了debug端口的chrome\n\n参考：[Python爬虫&mdash;&mdash;使用selenium和chrome爬取js动态加载的网页](https://www.cnblogs.com/tonglin0325/p/4663307.html)\n\n如果想获取chrome用户的cookie的话，需要指定chrome的用户目录，如下\n\n```\n/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --remote-debugging-port=9222 --user-data-dir=/Users/xxx/Library/Application\\ Support/Google/Chrome\n\n```\n\nvscode设置chrome远程调试\n\n参考：[设置 Chrome 远程调试端口](https://blog.csdn.net/crisschan/article/details/79970813)\n\n## 2.浏览器调整动画播放速度\n\n<!--more-->\n&nbsp;\n","tags":["开发工具"]},{"title":"go学习笔记——go-redis","url":"/go学习笔记——go-redis.html","content":"官方文档\n\n[https://pkg.go.dev/github.com/go-redis/redis/v8#section-readme](https://pkg.go.dev/github.com/go-redis/redis/v8#section-readme)\n\n添加依赖\n\n```\ngo get github.com/go-redis/redis/v8\ngo get github.com/go-redis/redis/extra/redisotel/v8\n\n```\n\n初始化client\n\n```\nclient := redis.NewClient(&amp;redis.Options{\n    Addr:     \"localhost:6379\",\n    Password: \"\", // no password set\n    DB: 0,          // use default DB\n})\n\nclient.AddHook(&amp;redisotel.TracingHook{})\nif err := client.Ping(context.Background()).Err(); err != nil {\n    logger.Error(\"redis connect failed, err:\", zap.Any(\"err\", err))\n    panic(\"failed to connect redis\")\n}\n\n```\n\nset key\n\n```\nerr = rdb.Set(ctx, \"key\", 10, time.Hour).Err()\nif err != nil {\n    fmt.Println(err)\n}\n\n```\n\nget key\n\n```\nresult := client.Get(ctx, \"key\")\nstr, err := result.Result()\nif err != nil {\n\tfmt.Println(err)\n}\nfmt.Println(str)\n\n```\n\n参考：[Go语言操作Redis](https://www.liwenzhou.com/posts/Go/redis/)\n\n<!--more-->\n&nbsp;\n","tags":["golang"]},{"title":"JavaWeb学习笔记——HelloWord Servlet","url":"/JavaWeb学习笔记——HelloWord Servlet.html","content":"## 1.实现servlet打印日志\n\n开发一个动态web资源，即开发一个Java程序向浏览器输出数据，需要完成以下2个步骤：\n\n### 1.编写一个Java类，实现Servlet接口\n\n开发一个动态web资源必须实现javax.servlet.Servlet接口，Servlet接口定义了Servlet引擎与Servlet程序之间通信的协议约定\n\n以下是MyServlet.java文件中的代码（写的这个类的名字叫做MyServlet）：\n\n```\npackage org.MyServlet.MyServlet;\n\nimport java.io.IOException;\n\nimport javax.servlet.Servlet;\nimport javax.servlet.ServletConfig;\nimport javax.servlet.ServletException;\nimport javax.servlet.ServletRequest;\nimport javax.servlet.ServletResponse;\n\n//开发一个动态web资源必须实现javax.servlet.Servlet接口\n//Servlet接口定义了Servlet引擎与Servlet程序之间通信的协议约定\n\n//Q:MyServlet完成了一个动态网页程序，或者说是一个功能，如何让客户端能否准确得找到我们得这个Servlet服务\n//A:服务器需要预先为我们预留出扩展接口，我们只需要按照一定的规则去提供相应的扩展功能\n\n//Q:如何和服务器进行通讯\n//A:web.xml就是服务器提供给我们的完成功能的地方\npublic class MyServlet implements Servlet{\n\n    @Override\n    public void destroy() {\n        // TODO Auto-generated method stub\n        \n    }\n\n    @Override\n    public ServletConfig getServletConfig() {\n        // TODO Auto-generated method stub\n        return null;\n    }\n\n    @Override\n    public String getServletInfo() {\n        // TODO Auto-generated method stub\n        return null;\n    }\n\n    @Override\n    public void init(ServletConfig arg0) throws ServletException {\n        // TODO Auto-generated method stub\n        \n    }\n\n    //所有客户端请求会自动调用Service方法进行处理\n    //ServletRequest    是一个对象，封装所有HTTP请求信息\n    //ServletResponse    是一个对象，封装所有HTTP响应信息\n    //这两个对象是Tomcat服务器给我们的\n    @Override\n    public void service(ServletRequest arg0, ServletResponse arg1)\n            throws ServletException, IOException {\n        // TODO Auto-generated method stub\n        \n        System.out.println(\"执行 MyServlet 的 service() 方法。。。。。\");\n    }\n\n}\n```\n\n关于其中的Service方法的一些Tip：　　\n\n```\n//所有客户端请求会自动调用Service方法进行处理\n//ServletRequest 是一个对象，封装所有HTTP请求信息\n//ServletResponse 是一个对象，封装所有HTTP响应信息\n//这两个对象是Tomcat服务器给我们的\n\n此外，如果是只实现service方法，则称为适配器模式\n\n```\n\n以下是web.xml文件中的代码：\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?><br /><web-app\n xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \nxmlns=\"http://java.sun.com/xml/ns/javaee\" \nxsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee \nhttp://java.sun.com/xml/ns/javaee/web-app_3_0.xsd\" id=\"WebApp_ID\" \nversion=\"3.0\"><br />  <display-name>MyServlet</display-name><br />  <welcome-file-list><br />    <welcome-file>index.html</welcome-file><br />    <welcome-file>index.htm</welcome-file><br />    <welcome-file>index.jsp</welcome-file><br />    <welcome-file>default.html</welcome-file><br />    <welcome-file>default.htm</welcome-file><br />    <welcome-file>default.jsp</welcome-file><br />  </welcome-file-list><br />  <br />  //定义一个Servlet服务<br />  //Servlet服务的名字叫做aaa<br />  <servlet><br />  \t<servlet-name>aaa</servlet-name><br />  \t<servlet-class>org.MyServlet.MyServlet.MyServlet</servlet-class><br />  </servlet><br />  <br />  //定义一个Servlet服务的映射关系<br />  //Servlet服务的名字叫做aaa<br />  //请求的路径是/myServlet.do<br />  <br />  //1.服务器启动模式加载webapps下面所有的应用，加载web应用的时候会读取每个应用的web。xml文件<br />  //2.客户单发送请求http://127.0.0.1:8080/MyServlet/myServlet.do<br />  //3.请求就找到http://127.0.0.1:8080，找到MyServlet(Context)<br />  //去mapping里面查找/myServlet.do,如果找到，定位到<servlet-name>aaa</servlet-name><br />  //4.去Servlet的定义里面查找Servlet-name是aaa的Servlet服务<br />  //然后定位到org.MyServlet.MyServlet.MyServlet,执行该class的service方法<br />  <servlet-mapping><br />  \t<servlet-name>aaa</servlet-name><br />  \t<url-pattern>/myServlet.do</url-pattern><br />  </servlet-mapping><br /></web-app>\n\n然后在浏览器中输入\n\n```\nhttp://127.0.0.1:8080/MyServlet/myServlet.do\n\n```\n\n便可以在Tomcat中看到\n\n```\n执行 MyServlet 的 service() 方法。。。。。\n\n```\n\n### 2.把开发好的Java类部署到web服务器中\n\n<!--more-->\n&nbsp;\n\n## 2.实现servlet打印HelloWorld\n\n该工程的功能是在页面上输出一段话\n\n首先在src里面新建一个class,在interface里面添加javax.servlet.Servlet\n\n以下是HelloServlet.java中的代码：\n\n```\npackage org.common.Servlet;\n\nimport java.io.IOException;\nimport java.io.PrintWriter;\n\nimport javax.servlet.Servlet;\nimport javax.servlet.ServletConfig;\nimport javax.servlet.ServletException;\nimport javax.servlet.ServletRequest;\nimport javax.servlet.ServletResponse;\n\npublic class HelloServlet implements Servlet {\n\n    @Override\n    public void destroy() {\n        // TODO Auto-generated method stub\n\n    }\n\n    @Override\n    public ServletConfig getServletConfig() {\n        // TODO Auto-generated method stub\n        return null;\n    }\n\n    @Override\n    public String getServletInfo() {\n        // TODO Auto-generated method stub\n        return null;\n    }\n\n    @Override\n    public void init(ServletConfig arg0) throws ServletException {\n        // TODO Auto-generated method stub\n\n    }\n    \n    //开发完成之后要告诉服务器我的存在，就要在web。xml里面继续写代码\n    @Override\n    public void service(ServletRequest arg0, ServletResponse arg1)\n            throws ServletException, IOException {\n        // TODO Auto-generated method stub\n        //获取一个输出流，就可以对页面写出内容\n        PrintWriter pw = arg1.getWriter();\n        pw.println(\"终于成功了！\");\n        pw.close();\n    }\n\n}\n```\n\n以下是web.xml文件中的代码：\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<web-app xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://java.sun.com/xml/ns/javaee\" xsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd\" id=\"WebApp_ID\" version=\"3.0\">\n  <display-name>HelloApp</display-name>\n  <welcome-file-list>\n    <welcome-file>index.html</welcome-file>\n    <welcome-file>index.htm</welcome-file>\n    <welcome-file>index.jsp</welcome-file>\n    <welcome-file>default.html</welcome-file>\n    <welcome-file>default.htm</welcome-file>\n    <welcome-file>default.jsp</welcome-file>\n  </welcome-file-list>\n  \n  <!-- 定义Servlet服务 -->\n  <servlet>\n      <servlet-name>hello</servlet-name>    <!--随便取-->    \n      <servlet-class>org.common.Servlet.HelloServlet</servlet-class>    <!--取包名。类名-->\n  </servlet>\n  \n  <!-- 定义映射关系 -->\n  <servlet-mapping>\n      <!-- 注意：mapping里面的servlet-name在Servlet的定义里面一定要有匹配的，否则启动报错 -->\n      <servlet-name>hello</servlet-name>\n      <url-pattern>/hello</url-pattern>\n  </servlet-mapping>\n</web-app>&nbsp;\n```\n\n访问的网址是\n\n```\nhttp://127.0.0.1:8080/Helloapp/hello\n\n```\n\n其中Helloapp是Tomcat里面的文件夹的名字，hello是url-pattern里面写的名字\n\n&nbsp;\n\n## 3.实现servlet打印当前时间\n\n该工程的功能是实现在页面中显示当前的时间\n\n### 1.编写一个Java类，实现Servlet接口\n\n以下的代码是HelloServlet.java中的代码\n\npackage helloapp2;\n\nimport java.io.IOException;<br />import java.io.PrintWriter;<br />import java.util.Date;\n\nimport javax.servlet.GenericServlet;<br />import javax.servlet.ServletException;<br />import javax.servlet.ServletRequest;<br />import javax.servlet.ServletResponse;\n\npublic class HelloServlet extends GenericServlet {\n\n\t@Override<br />\tpublic void service(ServletRequest arg0, ServletResponse arg1)<br />\t\t\tthrows ServletException, IOException {<br />\t\t// TODO Auto-generated method stub\n\n\t\tPrintWriter pw = arg1.getWriter();<br />\t\tpw.println(\"Hello\");<br />\t\tpw.println(new Date().toLocaleString());<br />\t\tpw.close();<br />\t}\n\n}\n\n以下的代码是web.xml中的代码\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?><br /><web-app\n xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \nxmlns=\"http://java.sun.com/xml/ns/javaee\" \nxsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee \nhttp://java.sun.com/xml/ns/javaee/web-app_2_5.xsd\" id=\"WebApp_ID\" \nversion=\"2.5\"><br />  <display-name>helloapp2</display-name><br />  <welcome-file-list><br />    <welcome-file>index.html</welcome-file><br />    <welcome-file>index.htm</welcome-file><br />    <welcome-file>index.jsp</welcome-file><br />    <welcome-file>default.html</welcome-file><br />    <welcome-file>default.htm</welcome-file><br />    <welcome-file>default.jsp</welcome-file><br />  </welcome-file-list><br />  <br />  \t<servlet><br />  \t\t<servlet-name>Hello</servlet-name><br />  \t\t<servlet-class>helloapp2.HelloServlet</servlet-class><br />  \t</servlet><br />  \t<br />  \t<servlet-mapping><br />  \t\t<servlet-name>Hello</servlet-name><br />  \t\t<url-pattern>/Hello</url-pattern><br />  \t</servlet-mapping><br /></web-app>\n\n### 2.把开发好的Java类部署到web服务器中\n\n使用myeclipse自动部署的方法(使用myeclipse 2015自动部署有问题，待解决)(换成2014的便可以自动部署)：\n\n　　1.Window->preferences->Myeclipse->Servers->Runtime Ecvironment->add->Tomcat 6.0，要选中添加local的那个选项\n\n　　2.在manage deployment的module里面选择要部署的工程\n\n　　3.重新安装Tomcat出现的问题解决，但是还是要手动部署\n\n&nbsp;\n\n自动部署设置，参考：[Eclipse中的Web项目自动部署到Tomcat](https://www.cnblogs.com/ywl925/p/3815173.html)\n\n自动部署后要把网页的文件放在根目录下\n\n&nbsp;\n\n该工程的名称是testhttp，功能是在页面中表格打印浏览过程中的相关头信息。\n\n&nbsp;　　新建一个工程，然后在这个工程里面新建一个servlet，这样便可以省去编写web.xml的过程\n\n&nbsp;\n\n## 3.实现servlet打印header\n\n以下是TestHttpServlet.java中的代码\n\n```\npackage org.common.servlet;\n\nimport java.io.IOException;\nimport java.io.PrintWriter;\nimport java.util.Enumeration;\n\nimport javax.servlet.ServletException;\nimport javax.servlet.http.HttpServlet;\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\n\npublic class TestHttpServlet extends HttpServlet {\n\n    /**\n     * Constructor of the object.\n     */\n    public TestHttpServlet() {\n        super();\n    }\n\n    /**\n     * Destruction of the servlet. <br>\n     */\n    public void destroy() {\n        super.destroy(); // Just puts \"destroy\" string in log\n        // Put your code here\n    }\n\n    /**\n     * The doGet method of the servlet. <br>\n     *\n     * This method is called when a form has its tag value method equals to get.\n     * \n     * @param request the request send by the client to the server\n     * @param response the response send by the server to the client\n     * @throws ServletException if an error occurred\n     * @throws IOException if an error occurred\n     */\n    \n    //Servlet接口的参数service(ServletRequest arg0, ServletResponse arg1)\n    \n    //HttpServletRequest封装了所有的请求信息，其实就是Tomcat将请求信息按照JAVA EE的Servlet的规范\n    //封装好给我们\n    \n    public void doGet(HttpServletRequest request, HttpServletResponse response)\n            throws ServletException, IOException {\n        \n        //设置返回类型\n        //response.setContentType(\"text/html;charset=GBK\");\n        response.setContentType(\"text/html\");\n        response.setCharacterEncoding(\"GBK\");\n        \n        //获取输出流\n        PrintWriter out = response.getWriter();\n        out.println(\"<!DOCTYPE HTML PUBLIC \\\"-//W3C//DTD HTML 4.01 Transitional//EN\\\">\");\n        out.println(\"<HTML>\");\n        out.println(\"  <HEAD><TITLE>testhttp Servlet</TITLE></HEAD>\");\n        out.println(\"  <BODY>\");\n        \n        out.print(\"<center>\");\n        out.print(\" <h2>请求头信息列表<h2> \");\n        \n        out.print(\" <table border=1> \");\n        out.print(\" |名字|值\n \");\n        //返回头消息名字集合，返回的是一个枚举\n        Enumeration enums = request.getHeaderNames();\n        //遍历获取所有头信息名和值\n        while(enums.hasMoreElements()){\n            //获取每一个头消息的名字\n            String headName = (String)enums.nextElement();\n            out.println(\" <tr> \");\n            out.println(\" | \" + headName + \"  \");\n            //getHeader(java.lang.String name) \n            //Returns the value of the specified request header as a String.\n            //返回这个名字的头信息的值\n            out.println(\" | \" + request.getHeaders(headName) + \"  \");\n            out.println(\" </tr> \");\n        }\n        out.println(\" </table> \");\n        \n        out.println(\" <hr> \");\n        //测试HttpServletRequest的方法\n        out.println(\"Method： \" + request.getMethod() + \"<br>\");\n        out.println(\"Request URI： \" + request.getRequestURI() + \"<br>\");\n        out.println(\"Protocol： \" + request.getProtocol() + \"<br>\");\n        out.println(\"PathInfo： \" + request.getPathInfo() + \"<br>\");\n        out.println(\"Remote Address： \" + request.getRemoteAddr() + \"<br>\");\n        out.println(\"ContextPath： \" + request.getContextPath() + \"<br>\");\n        out.println(\"getScheme： \" + request.getScheme() + \"<br>\");\n        out.println(\"getServerName： \" + request.getServerName() + \"<br>\");\n        out.println(\"getServerPort： \" + request.getServerPort() + \"<br>\");\n        out.println(\"getRequestURI： \" + request.getRequestURI() + \"<br>\");\n        String path = request.getContextPath();\n        //请求全路径\n        String basePath\n            = request.getScheme() + \"://\" + request.getServerName() + \":\"\n            + request.getServerPort() + request.getRequestURI();\n        out.println(\" path: \" + path + \"<br>\");\n        out.println(\" basePath: \" + basePath + \"<br>\");\n        \n        //out.print(this.getClass());\n        //out.println(\", using the GET method\");\n        \n        out.print(\"</center>\");\n        out.println(\"  </BODY>\");\n        out.println(\"</HTML>\");\n        out.flush();\n        out.close();\n    }\n\n    /**\n     * The doPost method of the servlet. <br>\n     *\n     * This method is called when a form has its tag value method equals to post.\n     * \n     * @param request the request send by the client to the server\n     * @param response the response send by the server to the client\n     * @throws ServletException if an error occurred\n     * @throws IOException if an error occurred\n     */\n    public void doPost(HttpServletRequest request, HttpServletResponse response)\n            throws ServletException, IOException {\n        \n        //调用doGet方法\n        this.doGet(request, response);\n    }\n\n    /**\n     * Initialization of the servlet. <br>\n     *\n     * @throws ServletException if an error occurs\n     */\n    public void init() throws ServletException {\n        // Put your code here\n    }\n\n}\n```\n\n## 请求头信息列表<h2> \");\n        \n        out.print(\" <table border=1> \");\n        out.print(\" |名字|值\n \");\n        //返回头消息名字集合，返回的是一个枚举\n        Enumeration enums = request.getHeaderNames();\n        //遍历获取所有头信息名和值\n        while(enums.hasMoreElements()){\n            //获取每一个头消息的名字\n            String headName = (String)enums.nextElement();\n            out.println(\" <tr> \");\n            out.println(\" | \" + headName + \"  \");\n            //getHeader(java.lang.String name) \n            //Returns the value of the specified request header as a String.\n            //返回这个名字的头信息的值\n            out.println(\" | \" + request.getHeaders(headName) + \"  \");\n            out.println(\" </tr> \");\n        }\n        out.println(\" </table> \");\n        \n        out.println(\" <hr> \");\n        //测试HttpServletRequest的方法\n        out.println(\"Method： \" + request.getMethod() + \"<br>\");\n        out.println(\"Request URI： \" + request.getRequestURI() + \"<br>\");\n        out.println(\"Protocol： \" + request.getProtocol() + \"<br>\");\n        out.println(\"PathInfo： \" + request.getPathInfo() + \"<br>\");\n        out.println(\"Remote Address： \" + request.getRemoteAddr() + \"<br>\");\n        out.println(\"ContextPath： \" + request.getContextPath() + \"<br>\");\n        out.println(\"getScheme： \" + request.getScheme() + \"<br>\");\n        out.println(\"getServerName： \" + request.getServerName() + \"<br>\");\n        out.println(\"getServerPort： \" + request.getServerPort() + \"<br>\");\n        out.println(\"getRequestURI： \" + request.getRequestURI() + \"<br>\");\n        String path = request.getContextPath();\n        //请求全路径\n        String basePath\n            = request.getScheme() + \"://\" + request.getServerName() + \":\"\n            + request.getServerPort() + request.getRequestURI();\n        out.println(\" path: \" + path + \"<br>\");\n        out.println(\" basePath: \" + basePath + \"<br>\");\n        \n        //out.print(this.getClass());\n        //out.println(\", using the GET method\");\n        \n        out.print(\"</center>\");\n        out.println(\"  </BODY>\");\n        out.println(\"</HTML>\");\n        out.flush();\n        out.close();\n    }\n\n    /**\n     * The doPost method of the servlet. <br>\n     *\n     * This method is called when a form has its tag value method equals to post.\n     * \n     * @param request the request send by the client to the server\n     * @param response the response send by the server to the client\n     * @throws ServletException if an error occurred\n     * @throws IOException if an error occurred\n     */\n    public void doPost(HttpServletRequest request, HttpServletResponse response)\n            throws ServletException, IOException {\n        \n        //调用doGet方法\n        this.doGet(request, response);\n    }\n\n    /**\n     * Initialization of the servlet. <br>\n     *\n     * @throws ServletException if an error occurs\n     */\n    public void init() throws ServletException {\n        // Put your code here\n    }\n\n}</code></pre>\n\n然后部署并启动Tomcat服务器，在浏览器中输入\n\n<pre class=\"brush:bash;gutter:true;\"><code>http://127.0.0.1:8080/testhttp/servlet/TestHttpServlet\n</code></pre>\n\n<h2>4.使用servlet实现简单的login\n\n本工程的功能是实现Javaweb的servlet身份验证\n\n&nbsp;\n\n一下是login.html文件中的代码\n\n```\n<!DOCTYPE html>\n<html>\n  <head>\n    <title>login.html</title>\n    \n    <meta name=\"keywords\" content=\"keyword1,keyword2,keyword3\">\n    <meta name=\"description\" content=\"this is my page\">\n    <meta name=\"content-type\" content=\"text/html; charset=GBK\">\n    \n    <!--<link rel=\"stylesheet\" type=\"text/css\" href=\"./styles.css\">-->\n\n    <script type=\"text/javascript\">\n        function check(){\n            //获取控件内容\n            var loginname = document.getElementById(\"loginname\").value;\n            if(loginname == \"\"){\n                alert(\"用户名不能为空\");\n                document.getElementById(\"loginname\").focus();//获取焦点\n                return false;\n            }\n            \n            var password = document.getElementById(\"password\").value;\n            if(password == \"\"){\n                alert(\"密码不能为空\");\n                document.getElementById(\"password\").focus();\n                return false;\n            }  \n            \n            //验证成功\n            document.loginform.submit();\n        }\n    </script>\n\n  </head>\n  \n  <body>\n    <center>\n        <h2>登陆页面</h2>\n        <br>\n        <!-- html数据由两种传输方式 1.get 从地址栏传递 2.form表单传输 \n            form代表表单\n                --action属性代表提交的url\n                    action=\"login.do\",那么在web.xml里面定义<servlet-mapping>的<url-pattern>\n                    的时候也是login.do\n                --method属性代表提交表单的方式，http里面重点是get和post，默认get方式提交\n                --name属性给表单其名字\n                --id属性代表唯一标示该表单的名字，主要是javascript脚本使用\n        -->\n        <form action=\"login.do\" method=\"get\" name=\"loginform\" id=\"loginform\">\n            <table>\n                <tr>\n                    <td>登录名：</td>\n                    <td><input type=\"text\" name=\"loginname\" id=\"loginname\"/></td>\n                </tr>\n                <tr>\n                    <td>密码：</td>\n                    <td><input type=\"password\" name=\"password\" id=\"password\"/></td>\n                </tr>\n            </table>\n            <table>\n                <tr>\n                    <td><input type=\"button\" value=\"提交\" onclick=\"check();\"></td>\n                    &amp;nbsp;&amp;nbsp;\n                    <td><input type=\"reset\" value=\"重置\"></td>\n                </tr>\n            </table>\n        </form>\n        \n    </center>\n  </body>\n</html>\n```\n\n&nbsp;\n\n以下代码是LoginServlet.java中的代码\n\n```\npackage org.common.servlet;\n\nimport java.io.IOException;\nimport java.io.PrintWriter;\nimport java.util.Enumeration;\nimport java.util.Properties;\n\nimport javax.servlet.ServletException;\nimport javax.servlet.http.HttpServlet;\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\n\npublic class LoginServlet extends HttpServlet {\n\n    /**\n     * Constructor of the object.\n     */\n    public LoginServlet() {\n        super();\n    }\n\n    /**\n     * Destruction of the servlet. <br>\n     */\n    public void destroy() {\n        super.destroy(); // Just puts \"destroy\" string in log\n        // Put your code here\n    }\n\n    /**\n     * The doGet method of the servlet. <br>\n     *\n     * This method is called when a form has its tag value method equals to get.\n     * \n     * @param request the request send by the client to the server\n     * @param response the response send by the server to the client\n     * @throws ServletException if an error occurred\n     * @throws IOException if an error occurred\n     */\n    public void doGet(HttpServletRequest request, HttpServletResponse response)\n            throws ServletException, IOException {\n        \n        System.out.println(\"执行 doGet 方法...\");\n//        //1.接收前台传递过来的参数\n//        Enumeration enums = request.getParameterNames();\n//        while(enums.hasMoreElements()){\n//            System.out.println(enums.nextElement());\n//            \n//        }\n        \n        //转换编码的第2种方式，配合doPost()方法使用\n        request.setCharacterEncoding(\"GBK\");\n        \n        //提交的name可以在后台使用request.getParameter(\"loginname\")获取值\n        String loginname = request.getParameter(\"loginname\");\n        System.out.println(\"转换前loginname:\" + loginname);\n        //String password = request.getParameter(\"password\");\n        \n        //把loginname这个字符串转成GBK，前提你要确定编码\n        loginname = new String(loginname.getBytes(\"iso-8859-1\"),\"GBK\");\n        System.out.println(\"转换后loginname:\" + loginname);\n        String password = request.getParameter(\"password\");\n        \n        //properties文件是java的默认配置文件，以key-value的形式存储数据\n        //增加了一个user.properties文件存储用户名密码\n        Properties pro = new Properties();\n        //load方法从输入流中读取属性列表（键和元素对）\n        pro.load(this.getClass().getResourceAsStream(\"/user.properties\"));\n        //System.out.print(pro);\n        \n        response.setContentType(\"text/html;charset=GBK\");\n        PrintWriter out = response.getWriter();\n        out.println(\"<!DOCTYPE HTML PUBLIC \\\"-//W3C//DTD HTML 4.01 Transitional//EN\\\">\");\n        out.println(\"<HTML>\");\n        out.println(\"  <HEAD><TITLE>A Servlet</TITLE></HEAD>\");\n        out.println(\"  <BODY>\");\n        \n        //out.print(\" loginname: \" + loginname);\n        //out.print(\" password: \" + password);\n        if(loginname.equals(pro.getProperty(\"loginname\")) \n                &amp;&amp; password.equals(pro.getProperty(\"password\"))){\n            out.println(\" 欢迎[\"+pro.getProperty(\"username\")+\"]登陆\");\n        }else{\n            out.println(\"用户名密码错误\");\n        }\n        \n        out.println(\"  </BODY>\");\n        out.println(\"</HTML>\");\n        out.flush();\n        out.close();\n    }\n\n    /**\n     * The doPost method of the servlet. <br>\n     *\n     * This method is called when a form has its tag value method equals to post.\n     * \n     * @param request the request send by the client to the server\n     * @param response the response send by the server to the client\n     * @throws ServletException if an error occurred\n     * @throws IOException if an error occurred\n     */\n    public void doPost(HttpServletRequest request, HttpServletResponse response)\n            throws ServletException, IOException {\n\n        this.doGet(request, response);\n    }\n\n    /**\n     * Initialization of the servlet. <br>\n     *\n     * @throws ServletException if an error occurs\n     */\n    public void init() throws ServletException {\n        // Put your code here\n    }\n\n}\n```\n\n&nbsp;\n\ndoGet()方法不安全，所以尽量使用doPost()方法\n","tags":["JavaWeb"]},{"title":"代理前端请求到本地服务","url":"/代理前端请求到本地服务.html","content":"在开发过程中，有时候我们需要将前端的请求（当然也可以是部分请求）代理到我们的本地开发环境的服务中进行调试，下面借助whistle+<a class=\"ng-binding\" title=\"关于\">SwitchyOmega</a>来实现这个功能\n\n## 1.安装whistle\n\n```\nnpm install whistle -g --registry=https://registry.npmmirror.com\n\nadded 130 packages in 8s\nnpm notice\nnpm notice New major version of npm available! 9.1.2 -> 10.5.0\nnpm notice Changelog: https://github.com/npm/cli/releases/tag/v10.5.0\nnpm notice Run npm install -g npm@10.5.0 to update!\nnpm notice\n\n```\n\n启动\n\n```\nw2 start\n\n[i] whistle@2.9.66 started\n[i] 1. use your device to visit the following URL list, gets the IP of the URL you can access:\n       http://127.0.0.1:8899/\n       http://192.168.8.188:8899/\n       Note: If all the above URLs are unable to access, check the firewall settings\n             For help see https://github.com/avwo/whistle\n[i] 2. set the HTTP proxy on your device with the above IP &amp; PORT(8899)\n[i] 3. use Chrome to visit http://local.whistlejs.com/ to get started\n\n```\n\n启动后访问http://127.0.0.1:8899/界面如下\n\n<img src=\"/images/517519-20240313221025474-414526809.png\" width=\"800\" height=\"272\" loading=\"lazy\" />\n\n## 2.浏览器安装<a class=\"ng-binding\" title=\"关于\">SwitchyOmega插件</a>\n\n将proxy设置成代理到whistle的8899端口，并将127.0.0.1的本地代理限制删掉\n\n<img src=\"/images/517519-20240313221854637-338839970.png\" width=\"800\" height=\"336\" loading=\"lazy\" />\n\n并将改proxy启动\n\n## 3.代理前端请求\n\n比如我们要代理https://xx.com/api/abc的接口代理到我们本地的http://127.0.0.1/api/v2/abc的服务上，这时我们应该如下配置\n\n```\nhttps://xx.com/api/abc 127.0.0.1/api/v2/abc/\n\n```\n\n<img src=\"/images/517519-20240313222753412-1215445591.png\" width=\"800\" height=\"247\" loading=\"lazy\" />\n\n如果是http协议，在启动点击OFF来启动whistle就可以实现代理功能了\n\n如果是https协议，所以我们还需要点击HTTPS来添加HTTPS证书和勾选相关选项\n\n<img src=\"/images/517519-20240313223045341-1782547161.png\" width=\"300\" height=\"383\" loading=\"lazy\" /> \n\n参考：[whistle+Proxy SwitchyOmega代理前端页面接口到本地开发服务](https://blog.csdn.net/qq_37160346/article/details/129824561)\n\n[https://wproxy.org/whistle/](https://wproxy.org/whistle/)\n","tags":["开发工具"]},{"title":"MySQL学习笔记——SQL注入","url":"/MySQL学习笔记——SQL注入.html","content":"## 1.常见SQL注入的方法\n\n假设我们使用goland的GORM框架写了以下面SQL\n\n```\nerr := u.data.db.Raw(\"select id, username, email from user where username = '\" + s + \"'\").Scan(&amp;user).Error\nif err != nil {\n\tu.log.Error(fmt.Sprintf(\"find user by id username fail, error: %v\", err))\n\treturn nil, err\n}\n\n```\n\n如果正常的查询参数的值为test123，请求如下接口传入该值\n\n```\nhttp://localhost:8080/api/v1/user?username=test123\n\n```\n\n接口输出的结果为\n\n```\n{\n    \"Code\": 200,\n    \"Msg\": \"find user by username success\",\n    \"Data\": [\n        {\n            \"id\": 18,\n            \"username\": \"test123\",\n            \"email\": \"test@test123\"\n        }\n    ]\n}\n\n```\n\n但是使用字符串拼接来实现查询逻辑的话，很容易被人使用SQL注入的方法进行攻击\n\n### 1.Error-based\n\n基于错误的SQL注入主要是用于获得数据的相关信息，方便进行后序的攻击，比如输入单引号 '\n\n```\nhttp://localhost:8080/api/v1/user?username='\n\n```\n\n此时从接口的返回值中就可以知道使用的是MySQL数据库\n\n```\n{\n    \"Code\": 500,\n    \"Msg\": \"find user by username fail\",\n    \"Data\": {\n        \"Number\": 1064,\n        \"SQLState\": [\n            52,\n            50,\n            48,\n            48,\n            48\n        ],\n        \"Message\": \"You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ''''' at line 1\"\n    }\n}\n\n```\n\n### 2.数字型\n\n比如使用数组id来请求用户信息的接口\n\n```\nhttp://localhost:8080/api/v1/user/1\n\n```\n\n如果使用3-2可以查询到id=1的用户信息，表示可以使用数字型SQL注入\n\n```\nhttp://localhost:8080/api/v1/user/3-2\n\n```\n\n参考：[【第九天 - 數字型 SQL注入】](https://ithelp.ithome.com.tw/articles/10270691)\n\n### 3.布尔型\n\n可以通过布尔表达式来判断猜测的数据是否最正确\n\n```\n<原SQL語法> and length(database())>=1--\n\n```\n\n参考：[【第十一天 - 布林SQL盲注】](https://ithelp.ithome.com.tw/articles/10272103)\n\n### 4.Union-based\n\n基于union的SQL注入可以通过拼接上UNION语句来实现SQL注入，比如输入' union all select 123,system_user(),user()%23，其中%23是#\n\n```\nhttp://localhost:8080/api/v1/user?username=' union all select 123,system_user(),user()%23\n\n```\n\n最终执行的SQL是\n\n```\nselect id, username, email from user where username = '' union all select 123,system_user(),user()#'\n\n```\n\n此时从接口的返回值中可以查询到数据库的用户名\n\n```\n{\n    \"Code\": 200,\n    \"Msg\": \"find user by username success\",\n    \"Data\": [\n        {\n            \"id\": 19,\n            \"username\": \"\",\n            \"email\": \"\"\n        },\n        {\n            \"id\": 123,\n            \"username\": \"root@172.17.0.1\",\n            \"email\": \"root@172.17.0.1\"\n        }\n    ]\n}\n\n```\n\n### **5.Time-based**\n\n基于sleep函数的SQL注入可以通过拼接上sleep函数来实现sql注入，配合上IF语句可以通过sleep函数是否执行来判断IF条件是否正确（例如在没有权限使用database()等函数的情况下猜测库名表名等），\n\n比如输入test123' and sleep(5)%23，其中%23是#\n\n```\nhttp://localhost:8080/api/v1/user?username=test123' and sleep(5)%23\n\n```\n\n最终执行的SQL是\n\n```\nselect id, username, email from user where username = 'test123' and sleep(5)#'\n\n```\n\n注入成功的话，请求会延时5秒之后再返回\n\n<img src=\"/images/517519-20240316204314792-358820179.png\" width=\"300\" height=\"120\" loading=\"lazy\" />\n\n参考：[SQL注入-时间盲注整理](https://sueisok.github.io/blog/sqli-time-based/)\n\n### 6.堆叠型\n\n```\n<原SQL語法>;DROP DATABASE 資料庫名\n\n```\n\n参考：[【第十四天 - 堆疊型 SQL注入】](https://ithelp.ithome.com.tw/articles/10274184)\n\n## 2.防范SQL注入的方法\n\n### 1.使用参数化\n\n不使用字符串拼接的方式\n\n```\nerr := dao.User.Where(dao.User.Username.Eq(s)).Scan(&amp;user)\nif err != nil {\n\tu.log.Error(fmt.Sprintf(\"find user by id username fail, error: %v\", err))\n\treturn nil, err\n}\n\n```\n\n### 2.输入过滤\n\n通过检查SQL中有某些特殊意思的字符来防止SQL注入，比如\n\n```\n\\\n;\n'\n\"\n`\n-- \n#\n/* */\n\n```\n\n参考：[秒懂 SQL Injection](https://tech-blog.cymetrics.io/posts/nick/sqli/)\n","tags":["MySQL"]},{"title":"JavaWeb学习笔记——Tomcat相关","url":"/JavaWeb学习笔记——Tomcat相关.html","content":"**Tomcat目录分析**\n\n　　1.bin　　　　　　存放启动和关闭Tomcat的脚本文件\n\n　　2.conf　　　　　<!--more-->\n&nbsp;存放Tomcat服务器的各种配置文件\n\n　　3.lib　　　　　　&nbsp;存放Tomcat服务器的支持jar包\n\n　　4.logs　　　　　&nbsp;存放Tomcat的日志文件\n\n　　5.temp　　　　 &nbsp;存放Tomcat运行时产生的临时文件\n\n　　6.webapps　　 &nbsp; web应用所在目录，即供外界访问的web资源的存放目录\n\n　　7.work　　　　　Tomcat的工作目录\n\n&nbsp;\n\n**关于端口冲突**\n\n　　1.HTTP的8080端口冲突，需要修改server.xml中端口的值\n\n　　2.启动多个Tomcat时端口冲突：Address in use: JVM_Bind\n\n　　3.用netstat　-p　tcp命令查看TCP监听端口\n\n　　4.用netstat　-a　显示所有的连接和监听端口\n\n&nbsp;\n\n**设置WEB站的根目录**\n\n**　　1.一个WEB站点必须有且只有一个虚拟根目录(子目录)**\n\n　　　　<Host name=\"localhost\" &nbsp;appBase=\"webapps\"&nbsp;/>\n\n　　　一个Host元素代表一个web站点，上面这个web站点虚拟路径是localhost(127.0.0.1)\n\n　　　<Host>元素的appBase属性所设置的应用程序基准目录中的ROOT子目录，也就是说名字是localhost的web站点的虚拟根目录是webapps/ROOT，一个WEB站点默认初始页面或者叫做欢迎页面是index.html\n\n**　　2.使用server.xml文件中的<Context>元素(虚拟目录)**\n\n　　　　&mdash;&mdash;<Context path=\"\"docBase=\"d:\\test\"\n\n　　　　　debug=\"0\"/>\n\n　　　　&mdash;&mdash;一个Host元素代表一个web站点，每一个Context元素代表站点下面的一个虚拟目录，当path属性设置成\"\",代表这个虚拟目录就是这个web站点的根目录。\n\n　　　　　　即<Context path=\"\" docBase=\"d:\\javaweb\" />\n\n&nbsp;\n\n<img src=\"/images/517519-20160401163431160-1658795146.png\" alt=\"\" />\n","tags":["JavaWeb"]},{"title":"Mybatis学习笔记——mybatis-generator","url":"/Mybatis学习笔记——mybatis-generator.html","content":"通用mabatis-generator可以由mysql表自动生成model类，mapper映射文件和mapper接口，参考：[MyBatis通用Mapper和PageHelper](https://mrbird.cc/MyBatis%20common%20Mapper%20PageHelper.html)\n\n1.依赖\n\n```\n<!-- mybatis -->\n<dependency>\n    <groupId>tk.mybatis</groupId>\n    <artifactId>mapper-spring-boot-starter</artifactId>\n    <version>2.1.5</version>\n</dependency>\n<dependency>\n    <groupId>tk.mybatis</groupId>\n    <artifactId>mapper</artifactId>\n    <version>3.3.9</version>\n</dependency>\n\n```\n\n2.插件\n\n```\n<!-- mybatis-generator -->\n<plugin>\n<groupId>org.mybatis.generator</groupId>\n<artifactId>mybatis-generator-maven-plugin</artifactId>\n<version>1.3.5</version>\n<configuration>\n    <configurationFile>src/main/resources/generator-config.xml</configurationFile>\n    <overwrite>true</overwrite>\n    <verbose>true</verbose>\n</configuration>\n<dependencies>\n    <dependency>\n        <groupId>mysql</groupId>\n        <artifactId>mysql-connector-java</artifactId>\n        <version>5.1.47</version>\n    </dependency>\n    <dependency>\n        <groupId>tk.mybatis</groupId>\n        <artifactId>mapper</artifactId>\n        <version>3.3.9</version>\n    </dependency>\n</dependencies>\n</plugin>\n\n```\n\n3.在application.properties中配置数据库相关参数\n\n```\n# mysql\nspring.datasource.url=jdbc:mysql://localhost:3306/demo?useUnicode=true&amp;characterEncoding=utf-8&amp;useLegacyDatetimeCode=false&amp;serverTimezone=Hongkong&amp;zeroDateTimeBehavior=convertToNull\nspring.datasource.username=root\nspring.datasource.password=xxxx\nspring.datasource.driver=com.mysql.jdbc.Driver\n\n```\n\n4.配置generator-config.xml，其中通用MyMapper参考：[Mybatis学习笔记&mdash;&mdash;通用mapper](https://www.cnblogs.com/tonglin0325/p/5267526.html)\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE generatorConfiguration\n        PUBLIC \"-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN\"\n        \"http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd\">\n\n<generatorConfiguration>\n    <!-- 引用外部配置文件-->\n    <properties resource=\"application.properties\" />\n\n    <context id=\"context\" targetRuntime=\"MyBatis3Simple\">\n        <!-- MapperPlugin -->\n        <plugin type=\"tk.mybatis.mapper.generator.MapperPlugin\">\n            <!-- 该配置会使生产的Mapper自动继承MyMapper -->\n            <property name=\"mappers\" value=\"com.example.demo.core.mapper.MyMapper\" />\n            <!-- caseSensitive默认false，当数据库表名区分大小写时，可以将该属性设置为true -->\n            <property name=\"caseSensitive\" value=\"false\"/>\n        </plugin>\n        <!-- 去掉生成出来的代码的注解 -->\n        <commentGenerator>\n            <property name=\"suppressAllComments\" value=\"true\" />\n            <property name=\"suppressDate\" value=\"true\" />\n        </commentGenerator>\n        <!-- 数据库信息 -->\n        <jdbcConnection driverClass=\"com.mysql.jdbc.Driver\"\n                        connectionURL=\"${spring.datasource.url}\"\n                        userId=\"root\"\n                        password=\"xxxx\">\n        </jdbcConnection>\n        <!-- 生成Model类的包名和位置 -->\n        <javaModelGenerator targetPackage=\"com.example.demo.model\" targetProject=\"src/main/java\">\n            <property name=\"enableSubPackages\" value=\"true\" />\n            <property name=\"trimStrings\" value=\"true\" />\n        </javaModelGenerator>\n        <!-- 生成Mapper映射文件的包名和位置 -->\n        <sqlMapGenerator targetPackage=\"mapper\" targetProject=\"src/main/resources\">\n            <property name=\"enableSubPackages\" value=\"true\" />\n        </sqlMapGenerator>\n        <!-- 生成Mapper接口的包名和位置 -->\n        <javaClientGenerator type=\"XMLMAPPER\" targetPackage=\"com.example.demo.mapper\" targetProject=\"src/main/java\">\n            <property name=\"enableSubPackages\" value=\"true\" />\n        </javaClientGenerator>\n        <!-- 要生成代码的表 tableName=%时为所有表生成 表名字段名都有默认规则生成 -->\n        <table tableName=\"%\">\n            <!-- 指定生成的主键属性名 生成SQL语句的类型 -->\n            <generatedKey column=\"id\" sqlStatement=\"MySql\"/>\n        </table>\n    </context>\n\n</generatorConfiguration>\n\n```\n\n5.生成，点击plugin中的mabatis-generator:generate\n\n<img src=\"/images/517519-20210621172700488-974594860.png\" alt=\"\" loading=\"lazy\" />\n\n6.自动生成model，mapper和xml\n\n<img src=\"/images/517519-20210621172736771-746432427.png\" alt=\"\" loading=\"lazy\" />\n\n7.添加<!--more-->\n&nbsp;@MapperScan(basePackages = \"com.example.demo.mapper\") 注解\n\n```\n@SpringBootApplication\n@MapperScan(basePackages = \"com.example.demo.mapper\")\npublic class DemoApplication {\n\n\tpublic static void main(String[] args) {\n\t\tSpringApplication.run(DemoApplication.class, args);\n\t}\n\n}\n\n```\n\n8. 测试\n\n```\n@RunWith(SpringJUnit4ClassRunner.class)\n@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.DEFINED_PORT)\npublic class UserMapperTest {\n\n    @Resource\n    private UserMapper mapper;\n\n    @Test\n    public void UserMapper() {\n        User user = mapper.selectByPrimaryKey(1L);\n        System.out.println(user.getId());\n    }\n\n}\n\n```\n\n如果在需要添加除了自定义SQL到xml文件中的话，需要在application.properties中添加如下配置\n\n```\n# mybatis\nmybatis.type-aliases-package=com.example.demo.entity\nmybatis.mapper-locations=classpath:mapper/**/*.xml\n\n```\n\n否则可能会报如下错误\n\n```\norg.apache.ibatis.binding.BindingException: Invalid bound statement (not found)\n\n```\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n","tags":["mybatis"]},{"title":"SpringBoot学习笔记——kaptcha","url":"/SpringBoot学习笔记——kaptcha.html","content":"kaptcha是一个java验证码生成框架，可以和spring集成用于验证码服务\n\n和spring集成的官方文档\n\n```\nhttps://code.google.com/archive/p/kaptcha/wikis/SpringUsage.wiki\n\n```\n\n1.依赖\n\n```\n<dependency>\n    <groupId>com.github.penggle</groupId>\n    <artifactId>kaptcha</artifactId>\n    <version>2.3.2</version>\n</dependency>\n\n```\n\n2.kaptcha生成验证码的配置类\n\n```\nimport com.google.code.kaptcha.Constants;\nimport com.google.code.kaptcha.impl.DefaultKaptcha;\nimport com.google.code.kaptcha.util.Config;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\nimport java.util.Properties;\n\n@Configuration\npublic class KaptchaConfig {\n\n    /**\n     * 验证码生成器参数\n     */\n    @Bean\n    public DefaultKaptcha captchaProducer() {\n        DefaultKaptcha captchaProducer = new DefaultKaptcha();\n        Properties properties = new Properties();\n        properties.setProperty(Constants.KAPTCHA_IMAGE_WIDTH, \"100\");\n        properties.setProperty(Constants.KAPTCHA_IMAGE_HEIGHT, \"30\");\n        properties.setProperty(Constants.KAPTCHA_TEXTPRODUCER_FONT_SIZE, \"22\");\n        properties.setProperty(Constants.KAPTCHA_TEXTPRODUCER_CHAR_LENGTH, \"4\");\n        properties.setProperty(Constants.KAPTCHA_TEXTPRODUCER_CHAR_SPACE, \"6\");\n        properties.setProperty(Constants.KAPTCHA_TEXTPRODUCER_FONT_COLOR, \"black\");\n        properties.setProperty(Constants.KAPTCHA_BORDER_COLOR, \"LIGHT_GRAY\");\n        properties.setProperty(Constants.KAPTCHA_BACKGROUND_CLR_FROM, \"WHITE\");\n        properties.setProperty(Constants.KAPTCHA_NOISE_IMPL, \"com.google.code.kaptcha.impl.NoNoise\");\n        properties.setProperty(Constants.KAPTCHA_OBSCURIFICATOR_IMPL, \"com.google.code.kaptcha.impl.ShadowGimpy\");\n        properties.setProperty(Constants.KAPTCHA_TEXTPRODUCER_CHAR_STRING, \"0123456789\");\n        properties.setProperty(Constants.KAPTCHA_SESSION_CONFIG_KEY, \"checkCode\");\n        Config config = new Config(properties);\n        captchaProducer.setConfig(config);\n        return captchaProducer;\n    }\n\n}\n\n```\n\n配置的含义参考\n\n```\nhttps://code.google.com/archive/p/kaptcha/wikis/ConfigParameters.wiki\n```\n\n验证码图片生成接口\n\n```\nimport com.google.code.kaptcha.Constants;\nimport com.google.code.kaptcha.Producer;\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.stereotype.Controller;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.servlet.ModelAndView;\n\nimport javax.imageio.ImageIO;\nimport javax.servlet.ServletOutputStream;\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\nimport java.awt.image.BufferedImage;\n\n@Slf4j\n@Controller\n@RequestMapping(\"/kaptcha\")\npublic class KaptchaController {\n\n    @Autowired\n    private Producer captchaProducer;\n\n    @RequestMapping(\"/getCode\")\n    public ModelAndView getKaptchaImage(HttpServletRequest request, HttpServletResponse response) throws Exception {\n        response.setDateHeader(\"Expires\", 0);\n        // Set standard HTTP/1.1 no-cache headers.\n        response.setHeader(\"Cache-Control\", \"no-store, no-cache, must-revalidate\");\n        // Set IE extended HTTP/1.1 no-cache headers (use addHeader).\n        response.addHeader(\"Cache-Control\", \"post-check=0, pre-check=0\");\n        // Set standard HTTP/1.0 no-cache header.\n        response.setHeader(\"Pragma\", \"no-cache\");\n        response.setContentType(\"image/jpeg\");\n        // create the text for the image\n        String capText = captchaProducer.createText();\n        log.info(\"******************验证码是: \" + capText + \"******************\");\n        // store the text in the session\n        request.getSession().setAttribute(Constants.KAPTCHA_SESSION_KEY, capText);\n        // create the image with the text\n        BufferedImage bi = captchaProducer.createImage(capText);\n        ServletOutputStream out = response.getOutputStream();\n        // write the data out\n        ImageIO.write(bi, \"jpg\", out);\n        try {\n            out.flush();\n        } finally {\n            out.close();\n        }\n        return null;\n    }\n}\n\n```\n\n验证码校验和参数含义参考：[Google-kaptcha验证码使用步骤(基于springboot/使用redis存储)](https://juejin.cn/post/6844903894661890055)\n\n```\n kaptcha.border                     \t| Border around kaptcha. Legal values are yes or no.                     \t\t\t| yes \n kaptcha.border.color                   | Color of the border. Legal values are r,g,b (and optional alpha) or white,black,blue.         | black \n kaptcha.border.thickness               | Thickness of the border around kaptcha. Legal values are > 0.                   \t        | 1 \n kaptcha.image.width                    | Width in pixels of the kaptcha image.                    \t\t\t\t        | 200 \n kaptcha.image.height                   | Height in pixels of the kaptcha image.                   \t\t\t\t        | 50 \n kaptcha.producer.impl                  | The image producer.                       \t\t\t\t\t\t        | com.google.code.kaptcha.impl.DefaultKaptcha \n kaptcha.textproducer.impl              | The text producer.                    \t\t\t\t\t\t        | com.google.code.kaptcha.text.impl.DefaultTextCreator \n kaptcha.textproducer.char.string       | The characters that will create the kaptcha.                  \t\t\t        | abcde2345678gfynmnpwx \n kaptcha.textproducer.char.length       | The number of characters to display.                    \t\t\t\t        | 5 \n kaptcha.textproducer.font.names        | A list of comma separated font names.                  \t\t\t\t        | Arial, Courier \n kaptcha.textproducer.font.size         | The size of the font to use.                    \t\t\t\t\t        | 40px. \n kaptcha.textproducer.font.color        | The color to use for the font. Legal values are r,g,b.              \t\t\t        | black \n kaptcha.textproducer.char.space        | The space between the characters                \t\t\t\t\t        | 2 \n kaptcha.noise.impl                     | The noise producer.                  \t\t\t\t\t\t\t        | com.google.code.kaptcha.impl.DefaultNoise \n kaptcha.noise.color                    | The noise color. Legal values are r,g,b.          \t\t\t\t                | black \n kaptcha.obscurificator.impl            | The obscurificator implementation.                  \t\t\t\t\t        | com.google.code.kaptcha.impl.WaterRipple \n kaptcha.background.impl                | The background implementation.                    \t\t\t\t\t        | com.google.code.kaptcha.impl.DefaultBackground \n kaptcha.background.clear.from          | Starting background color. Legal values are r,g,b.                    \t\t        | light grey \n kaptcha.background.clear.to            | Ending background color. Legal values are r,g,b.                   \t\t\t        | white \n kaptcha.word.impl                      | The word renderer implementation.                     \t\t\t\t\t| com.google.code.kaptcha.text.impl.DefaultWordRenderer \n kaptcha.session.key                    | The value for the kaptcha is generated and is put into the HttpSession. \t\t\t| KAPTCHA_SESSION_KEY \n\t\t\t\t\t  This is the key value for that item in the session.                     \n kaptcha.session.date                   | The date the kaptcha is generated is put into the HttpSession. \t\t\t\t| KAPTCHA_SESSION_DATE\n\t\t\t\t\t  This is the key value for that item in the session. \n\n```\n\n效果\n\n<img src=\"/images/517519-20210602113139797-1742849243.png\" alt=\"\" loading=\"lazy\" />\n\n校验的原理：\n\n生成验证码的时候返回一个uuid给浏览器，同时将uuid作为key，验证码作为value存在服务器session当中，比如redis，前端提交验证码的时候将uuid带上，查询redis进行验证\n\n<!--more-->\n&nbsp;\n\n　\n\n&nbsp;\n","tags":["SpringBoot"]},{"title":"数据仓库建模的一些理论","url":"/数据仓库建模的一些理论.html","content":"## 1.数据分层\n\n数据明细层：DWD（Data Warehouse Detail）\n\n数据中间层：DWM（Data WareHouse Middle）\n\n数据服务层：DWS（Data WareHouse Servce）\n\n数据应用层：ADS（Application Data Service）\n\n## 2.数仓建模方法\n\n在数据仓库模型中，星型模型和雪花型模型是两个常用的设计模式。参考：[数据仓库系列：星型模型和雪花型模型](https://zhuanlan.zhihu.com/p/139656253)\n\n### 1.星型模型\n\n星型模型是一种简单的数据仓库模型，也是最常见的模型之一。在星型模型中，中心表（称为业务事实表）连接到几个维度表（称为业务维度表）。维度表中包含了业务的各个特征，如时间、区域、产品等。\n\n<img src=\"/images/517519-20230822201256717-978542871.png\" width=\"500\" height=\"350\" loading=\"lazy\" />\n\n在 SQL 中，我们可以使用以下语句来创建一个星型模型：\n\n```\nCREATE TABLE fact_sales ( # 都是key\n  sales_id INT PRIMARY KEY,\n  date_key INT,\n  product_key INT,\n  store_key INT,\n  sales_amount DECIMAL(15,2)\n);\n\nCREATE TABLE dim_date (\n  date_key INT PRIMARY KEY,\n  date_full DATE,\n  year INT,\n  quarter INT,\n  month INT,\n  day_of_week CHAR(9),\n  holiday VARCHAR(32)\n);\n\nCREATE TABLE dim_product (\n  product_key INT PRIMARY KEY,\n  product_name VARCHAR(128),\n  category VARCHAR(32),\n  subcategory VARCHAR(32)\n);\n\nCREATE TABLE dim_store (\n  store_key INT PRIMARY KEY,\n  store_name VARCHAR(128),\n  city VARCHAR(32),\n  state VARCHAR(2),\n  country VARCHAR(64)\n);\n\n```\n\n### 2.雪花型模型\n\n雪花型模型是在星型模型基础上的扩展，因其形似雪花而得名。这种模型在星型模型的基础上，将维度表拆分成更小的表形式，形成多层表的结构。\n\n雪花型模型：当有一个或多个维表没有直接连接到事实表上，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。雪花模型是对星型模型的扩展。它对星型模型的维表进一步层次化，原有的各维表可能被扩展为小的事实表，形成一些局部的 \"层次 \" 区域，这些被分解的表都连接到主维度表而不是事实表。\n\n雪花型模型通过最大限度地减少数据存储量以及联合较小的维表来改善查询性能。雪花型结构去除了数据冗余。\n\n<img src=\"/images/517519-20230822201142197-724057740.png\" width=\"600\" height=\"352\" loading=\"lazy\" />\n\n### 星型模型对比雪花模型\n\n1、查询性能角度来看\n\n在OLAP（Hive）建议用星型模型，Hive表通过大量的冗余来提升查询效率，星型模型对OLAP的分析引擎支持比较友好；而雪花型要做多个表联接，性能会低于星型架构\n\n在OLTP（MySQL）建议用雪花模型，由于雪花型架构更有利于度量值的聚合，因此性能要高于星型架构，所以雪花模型在关系型数据库中如MySQL，Oracle中非常常见\n\n2、模型复杂度角度\n\n星型架构更简单方便处理\n\n3、层次结构角度\n\n雪花型架构更加贴近OLTP系统的结构，比较符合业务逻辑，层次比较清晰。\n\n4、存储角度\n\n雪花型架构具有关系数据模型的所有优点，不会产生冗余数据，而相比之下星型架构会产生数据冗余。\n\n### 3.Data Vault 模型\n\nData Vault 是另一种数据仓库建模方法，是 Dan Linstedt 在 20 世纪 90 年代提出的，主要用于**企业级的数据仓库建模**。\n\nData Vault 模型由中心表（Hub）、链接表（Link）、附属表（Satellite）三个主要组成部分。其中，中心表是核心，用于存储业务主键，链接表记录业务关系，附属表记录业务描述。\n\n（1）中心表\n\n中心表用来存储企业每个业务实体的业务主键，业务主键唯一标识某个业务实体。中心表和源系统是相互独立的，即无论业务主键是否用于多个业务系统，它在 Data Vault 中只保留一份，其他的组件都链接到这一个业务主键上。\n\n出于设计上的考虑，中心表一般由主键、业务主键、装载时间戳、数据来源系统四个字段组成。其中主键是系统生成的代理键，仅供内部使用。\n\n（2）链接表\n\n链接表是不同中心表的链接。一个链接表一般在两个或多个中心表之间有关联。一个链接表通常是一个外键，表示一种业务关系，比如：交易表、客户关联账户等。\n\n链接表主要包括主键、外键1、&hellip;&hellip;、外键n、装载时间戳、数据来源系统等字段构成，其中主键对应多个外键的唯一组合，一般是与业务无关的序列数值。\n\n（3）附属表\n\n附属表用来保存中心表和链接表的描述属性，包含所有历史变化数据，附属表有且仅有一个唯一外键关联到中心表或链接表。\n\n附属表主要包括主键、外键、属性1、&hellip;&hellip;、属性n、装载时间、失效时间、数据来源系统，主键用于唯一标识附属表中的一行记录，一般是与业务无关的序列数值。\n\n参考：[数据仓库进阶之路](https://zhuanlan.zhihu.com/p/338218985)\n","tags":["Hive"]},{"title":"MySQL学习笔记——索引原理","url":"/MySQL学习笔记——索引原理.html","content":"## **1.索引（index）**\n\n可以通过在数据库中创建index来加速对表的查询，index可以避免对表的一个全面扫描。对于**主键和唯一键**，会自动在上面创建索引。\n\n- 通过使用快速路径访问方法快速定位数据，减少了磁盘的I/O\n- 与表独立存放，但不能独立存在，必须属于某个表\n- 由数据库自动维护，表被删除时，该表上的索引自动被删除\n\n索引的原理：当以某个字段建立一个索引的时候，数据库就会生成一个索引页，索引页不单单保存索引的数据，还保存了索引在数据库的具体的物理地址。\n\n```\n# 手动创建索引\nCREATE INDEX index_tb_dept_name\nON tb_dept(NAME);\n\n# 使用索引，在where之后加上索引，提高查询效率\nSELECT * FROM tb_dept WHERE NAME='Tom'\n\n# 重建索引\ndrop index index_name;\ncreate index index_name on table(column);\n\n```\n\n注意：如果表的列很少，不适合建立索引。当执行过很多次的insert、delete、update后，会出现索引碎片。影响查询速度，我们应该对索引进行重组。\n\n**索引列最好设置为 `NOT NULL`**，这通常可以提升查询效率和简化索引操作。\n\n**索引失效**的场景：\n\n- 当我们使用左或者左右模糊匹配的时候，也就是 `like %xx` 或者 `like %xx%`这两种方式都会造成索引失效；\n- 当我们在查询条件中对索引列做了计算、函数、类型转换操作，这些情况下都会造成索引失效；\n- 联合索引要能正确使用需要遵循最左匹配原则，也就是按照最左优先的方式进行索引的匹配，否则就会导致索引失效。\n- 在 WHERE 子句中，如果在 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列，那么索引会失效。\n\n参考：[https://xiaolincoding.com/mysql/index/index_interview.html#有什么优化索引的方法？](https://xiaolincoding.com/mysql/index/index_interview.html#%E6%9C%89%E4%BB%80%E4%B9%88%E4%BC%98%E5%8C%96%E7%B4%A2%E5%BC%95%E7%9A%84%E6%96%B9%E6%B3%95)\n\n[https://xiaolincoding.com/mysql/index/index_lose.html#索引失效有哪些？](https://xiaolincoding.com/mysql/index/index_lose.html#%E7%B4%A2%E5%BC%95%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84%E9%95%BF%E4%BB%80%E4%B9%88%E6%A0%B7)\n\n## **2.常见索引的种类**\n\n主键索引 PRIMARY、唯一索引 UNIQUE、普通索引 INDEX（多字段为组合索引）、全文索引 FULLTEXT、空间索引<!--more-->\n&nbsp;SPATIAL\n\n参考：[深入理解MySQL索引原理和实现&mdash;&mdash;为什么索引可以加速查询？](https://blog.csdn.net/tongdanping/article/details/79878302)\n\n### **1、主键索引**\n\n即主索引，根据主键pk_clolum（length）建立索引，不允许重复，不允许空值；\n\n```\nALTER TABLE 'table_name' ADD PRIMARY KEY pk_index('col')；\n\n```\n\n比如user的id上的主键索引，以PRIMARY命名\n\n<img src=\"/images/517519-20210305161510802-1307185327.png\" alt=\"\" />\n\n主键也可以是复合主键，即有多个字段，比如\n\n<img src=\"/images/517519-20210305170205431-382287210.png\" alt=\"\" />\n\n主键索引最好是自增的，如果使用的是非自增主键，可能导致页分裂。参考：[https://xiaolincoding.com/mysql/index/index_interview.html](https://xiaolincoding.com/mysql/index/index_interview.html)\n\n### **2、唯一索引**\n\n用来建立索引的列的值必须是唯一的，**允许多个重复的空值**，因为根据 SQL 标准，`NULL` 值之间被认为是不相等的\n\n```\nALTER TABLE `table_name` ADD UNIQUE index_name(`col`);\n\n```\n\n比如urn和modified_date字段需要唯一，命名为uix_表名_字段1_字段2\n\n<img src=\"/images/517519-20210305171650963-1233649960.png\" alt=\"\" />\n\n### **3、普通索引**\n\n用表中的普通列构建的索引，没有任何限制\n\n```\nALTER TABLE `table_name` ADD INDEX index_name(`col`);\n\n```\n\n#### **1.单列索引（Column Indexes）**\n\n最常见的索引类型涉及单列，它将该列值的副本存储在数据结构中，允许快速查找具有相应列值的行。B-tree数据结构允许索引快速找到一个特定的值、一组值或一个值范围，对应于WHERE子句中的=、>、&le;、BETWEEN、IN等操作符。\n\n参考：[https://dev.mysql.com/doc/refman/5.7/en/column-indexes.html](https://dev.mysql.com/doc/refman/5.7/en/column-indexes.html)\n\n比如user表的username上加普通索引，命名为idx_表名_字段名\n\n<img src=\"/images/517519-20210305161643348-1540822036.png\" alt=\"\" />\n\n#### **2.组合索引（联合索引）**\n\nMySQL可以创建复合索引(即在多个列上创建索引)。一个索引最多可以由16列组成。\n\n```\nCREATE TABLE test (\n    id         INT NOT NULL,\n    last_name  CHAR(30) NOT NULL,\n    first_name CHAR(30) NOT NULL,\n    PRIMARY KEY (id),\n    INDEX name (last_name,first_name)\n);\n\n```\n\n参考：[https://dev.mysql.com/doc/refman/5.7/en/multiple-column-indexes.html](https://dev.mysql.com/doc/refman/5.7/en/multiple-column-indexes.html)\n\n如果表已经存在，可以使用以下SQL来添加联合索引，在构建联合索引时，组合的多个列中允许**有空值（`NULL`）**。\n\n```\nALTER TABLE `table_name` ADD INDEX index_name(`col1`,`col2`,`col3`);\n\n```\n\n组合索引需要注意字段的顺序，遵循**最左匹配原则**，参考：[Mysql联合索引最左匹配原则](https://segmentfault.com/a/1190000015416513)\n\n<img src=\"/images/517519-20210305172046498-1870850348.png\" alt=\"\" />\n\n### **4、前缀索引**\n\nMySQL的前缀索引（Prefix Index）是一种对部分列数据进行索引的方式，通常用于字符串类型的列（如 `VARCHAR`、`TEXT`、`BLOB`），而不是对整个列值创建索引。前缀索引可以在提高查询性能的同时，减少索引的大小，尤其在索引非常长的字符串列时，效果显著。\n\n前缀索引的作用：对于较长的字符串列，建立索引时会占用大量的存储空间，前缀索引通过只对字符串的前一部分建立索引来减少存储开销。虽然前缀索引并不适用于所有查询场景，但对于某些情况下，可以有效地平衡空间和查询性能。\n\n假设有一个 `users` 表，其中有一个名为 `email` 的列，通常来说 `email` 字段可能会很长，如果你想只对前 10 个字符建立索引\n\n```\nCREATE INDEX idx_email_prefix ON users (email(10));\n\n```\n\n前缀索引的优点：\n\n- **节省存储空间**：相比对整个列创建索引，前缀索引可以显著减少索引的存储空间，特别是在字符串较长时效果更明显。\n- **提高查询性能**：对于某些查询，前缀索引可以帮助加速查询，因为索引数据量减少了。\n\n前缀索引的缺点：\n\n- **选择性较差**：前缀索引的选择性（区分不同记录的能力）通常不如完整索引。当多个记录的前缀相同时，查询可能无法充分利用索引。\n- **不能用于某些查询**：前缀索引不能用于 ORDER BY 或 GROUP BY 中涉及的列，也不能用于基于列的全值比较。\n\n前缀索引的适用场景：前缀索引适合用于字符串较长且分布比较分散的列，例如：\n\n- **Email 地址**：通常前几个字符就能有效区分不同的邮箱地址。\n- **URL**：长 URL 的前缀部分往往能很好地区分不同的记录。\n- **某些大文本字段**：如文章标题、内容的摘要等。\n\n### **5、全文索引**\n\n用大文本对象的列构建的索引\n\n```\nALTER TABLE 'table_name' ADD FULLTEXT INDEX ft_index('col')；\n\n```\n\n比如，命名为fti_表名_all\n\n<img src=\"/images/517519-20210305171008200-1301899267.png\" alt=\"\" />\n\n### **6、空间索引&nbsp;SPATIAL（只存在于MyISAM存储引擎）**\n\n空间索引是对空间数据类型的字段建立的索引，MYSQL中的空间数据类型有4种，分别是GEOMETRY、POINT、LINESTRING、POLYGON。\n\nMYSQL使用SPATIAL关键字进行扩展，使得能够用于创建正规索引类型的语法创建空间索引。\n\n创建空间索引的列，必须将其声明为NOT NULL，空间索引只能在存储引擎为MyISAM的表中创建。\n\n参考：[详细介绍mysql索引类型：FULLTEXT、NORMAL、SPATIAL、UNIQUE](https://blog.csdn.net/guo_qiangqiang/article/details/88794971)\n\n## **3.不同存储引擎的索引类型**\n\nMySQL支持诸多存储引擎，而各种存储引擎对索引的支持也各不相同，因此MySQL数据库支持多种索引类型，如BTree索引（实际上是B+Tree索引），哈希索引，全文索引等等。参考：[https://dev.mysql.com/doc/refman/5.7/en/create-index.html](https://dev.mysql.com/doc/refman/5.7/en/create-index.html)\n\n<img src=\"/images/517519-20210305110438653-757965480.png\" alt=\"\" />\n\n索引记录存储在b树或r树数据结构的叶页中。索引页的默认大小是16KB。页面大小由MySQL实例初始化时的innodb_page_size设置决定。\n\n参考：[https://dev.mysql.com/doc/refman/5.7/en/innodb-physical-structure.html](https://dev.mysql.com/doc/refman/5.7/en/innodb-physical-structure.html)\n\n### 1.InnoDB\n\nInnoDB索引都是b-tree数据结构。实际上InnoDB也可以额外开启hash index，hash索引的创建由InnoDB存储引擎引擎自动优化创建。\n\n参考：[mysql InnoDB引擎支持hash索引吗](https://blog.csdn.net/doctor_who2004/article/details/77414742)\n\n**<img src=\"/images/517519-20210305111449826-752557055.png\" width=\"900\" height=\"551\" />**\n\n参考：**[https://dev.mysql.com/doc/refman/5.7/en/innodb-introduction.html](https://dev.mysql.com/doc/refman/5.7/en/innodb-introduction.html)**\n\n### 2.MyISAM\n\nMyISAM的空间索引使用r-tree，这是一种专门用于索引多维数据的数据结构。\n\n## 4.Innodb存储引擎使用的是B+树\n\n[为什么MySQL索引要用B+树，而不是B树？](https://database.51cto.com/art/201909/603430.htm)\n\n### 1.B树\n\nBTree是平衡搜索多叉树，一棵 <img src=\"/images/517519-20240908161227896-1616905804.gif\" title=\"m\" />m阶（M阶表示这个树的每一个节点最多可以拥有的子节点个数）的 B树的特性：\n\n- 每个节点最多有 m 个子节点\n- 每一个非叶子节点（除根节点）最少有 [m/2] 个子节点\n- 如果根节点不是叶子节点，那么它至少有两个子节点\n- 有 k 个子节点的非叶子节点拥有 k-1 个键，且升序排列，满足 k[i] < k[i+1]\n- 所有的叶子节点都在同一层\n\n<img src=\"/images/517519-20240908154502496-946717089.png\" width=\"500\" height=\"139\" loading=\"lazy\" />\n\n在BTree的结构下，就可以使用二分查找的查找方式，查找复杂度为h*log(n)，一般来说树的高度是很小的，一般为3左右，因此BTree是一个非常高效的查找结构。\n\n参考：[https://oi-wiki.org/ds/b-tree/](https://oi-wiki.org/ds/b-tree/)\n\n### 2.B+树\n\nB+Tree是BTree的一个变种，在B+树中的节点通常被表示为一组有序的元素和子指针。\n\n- B+树是一棵**m阶树**，即每个节点最多有 `m` 个子节点，最少有 `&lceil;m/2&rceil;` 个子节点。\n- 每个非叶子节点至多有 `m-1` 个键值（也称为分支或索引），用于引导查找过程。\n- 所有的数据都存储在叶子节点\n- 根节点可以有少于 `&lceil;m/2&rceil;` 的子节点（允许少于最小数量的子节点）。\n\n m &minus; 1 <img src=\"/images/ecbbd201e0d8f1ccc91cb46362c4b72fa1bbe6c2\" alt=\"{\\displaystyle m-1}\" class=\"mwe-math-fallback-image-inline mw-invert skin-invert\" style=\"vertical-align: -0.505ex; width: 6.043ex; height: 2.343ex;\" /> 个元素，对于任意的结点有最多 m 个子指针 &lfloor; m / 2 &rfloor; <img src=\"/images/87be5eefdb8fa8b05d4e77a49222798c08e66318\" alt=\"{\\displaystyle \\lfloor m/2\\rfloor }\" class=\"mwe-math-fallback-image-inline mw-invert skin-invert\" style=\"vertical-align: -0.838ex; width: 6.43ex; height: 2.843ex;\" /> 个元素最多  m &minus; 1 <img src=\"/images/ecbbd201e0d8f1ccc91cb46362c4b72fa1bbe6c2\" alt=\"{\\displaystyle m-1}\" class=\"mwe-math-fallback-image-inline mw-invert skin-invert\" style=\"vertical-align: -0.505ex; width: 6.043ex; height: 2.343ex;\" /> 个元素，对于任意的结点有最多 m 个子指针\n\n<img src=\"/images/517519-20240908155510005-301249702.png\" width=\"500\" height=\"230\" loading=\"lazy\" />\n\n参考：[https://oi-wiki.org/ds/bplus-tree/](https://oi-wiki.org/ds/bplus-tree/)\n\n### **3.B+Tree对比BTree的优点**\n\n#### 1、磁盘读写代价更低\n\n一般来说B+Tree比BTree更适合实现外存的索引结构，因为存储引擎的设计专家巧妙的利用了外存（磁盘）的存储结构，即磁盘的最小存储单位是扇区（sector），而操作系统的块（block）通常是整数倍的sector，操作系统以页（page）为单位管理内存，一页（page）通常默认为4K，数据库的页通常设置为操作系统页的整数倍，因此索引结构的节点被设计为一个页的大小，然后利用外存的&ldquo;预读取&rdquo;原则，每次读取的时候，把整个节点的数据读取到内存中，然后在内存中查找，已知内存的读取速度是外存读取I/O速度的几百倍，那么提升查找速度的关键就在于尽可能少的磁盘I/O，由于 B+ 树的 **非叶子节点** 不存储实际数据，能够容纳更多的索引键和指针，从而使每个节点的存储效率更高。也就是说，B+ 树的非叶子节点可以容纳更多的子节点，**B+树的 扇出系数 更大**，导致树的层级深度更低，那么树的高度越小，需要I/O的次数越少，因此一般来说B+Tree比BTree更快。\n\n#### 2、查询速度更稳定\n\n由于 **B+Tree 非叶子节点不存储数据**（data），因此所有的数据都要查询至叶子节点，而叶子节点的高度都是相同的，因此所有数据的查询速度都是一样的。而B树查询时候的波动就会比较大，因为数据可能在非叶子节点，也可能在叶子节点。\n\n#### 3、适合范围查询\n\nB+Tree **叶子节点采用的是双链表连接**，适合 MySQL 中常见的基于范围的顺序查找，而 B 树无法做到这一点。\n\n## **5.聚簇索引**和**非聚簇索引**\n\nMySQL中最常见的两种存储引擎分别是MyISAM和InnoDB，**MyISAM**采用非聚簇索引，而**InnoDB**主键索引是聚簇索引，而二级索引是非聚簇索引。\n\n如下图，Primary key是主键索引，Secondary key是二级索引\n\nInnoDB 存储引擎：B+ 树索引的叶子节点保存数据本身；\n\nMyISAM 存储引擎：B+ 树索引的叶子节点保存数据的物理地址；\n\n<img src=\"/images/517519-20210308155731828-860798107.png\" width=\"500\" height=\"429\" />\n\n## **5.聚簇索引**和**非聚簇索引**\n\n### 1.聚簇索引\n\n每个InnoDB表都有一个特殊的索引，称为**聚簇索引(clustered index)**，用于存储行数据。 聚簇索引的 B+ 树的叶子节点存储了整行数据。聚簇索引的顺序就是数据的物理存储顺序。\n\n- 当你在表上定义一个**主键**时，InnoDB使用它作为聚簇索引。应该为每个表定义一个主键。如果没有逻辑唯一且非空的列或一组列可以使用主键，则添加一个自动递增列。自动递增的列值是唯一的，在插入新行时自动添加。\n- 如果你没有为表定义一个主键，InnoDB会使用**第一个唯一的索引**作为聚簇索引，所有的键列都定义为not NULL。\n- 如果一张表没有主键或合适的唯一索引，InnoDB会在一个包含行ID值的合成列上生成一个名为GEN_CLUST_INDEX的**隐藏聚簇索引**。这些行是根据InnoDB分配的行ID来排序的。row ID是一个6字节的字段，随着新行插入单调递增。因此，按行ID排序的行在物理上是按插入顺序排列的。\n\n参考：[https://dev.mysql.com/doc/refman/5.7/en/innodb-index-types.html](https://dev.mysql.com/doc/refman/5.7/en/innodb-index-types.html)\n\n### 2.非聚簇索引\n\nInnoDB 的**二级索引**是**非聚簇索引**，也是基于 B+ 树的，但是其叶子节点存储的并不是完整的行数据，而是**主键值和索引列的值**。通过二级索引查找到主键值后，InnoDB 会使用主键查找来获取完整的行数据（这就是所谓的\"**回表**\"操作）。索引顺序与数据物理排列顺序无关。\n\n**回表** 是指 MySQL 在使用非聚簇索引（也称为辅助索引或二级索引）查询时，无法通过索引本身获取所需的全部数据，必须回到主表（聚簇索引）中获取完整的行数据的过程。\n\n回表通常会在以下情况下发生：\n\n- **使用非聚簇索引**：查询条件使用了非聚簇索引，且查询需要的列不完全包含在索引中。\n- **查询所需数据不在索引中**：查询需要的数据列没有在索引中全部覆盖。\n\n具体参考：[深入理解MySQL索引原理和实现&mdash;&mdash;为什么索引可以加速查询？](https://blog.csdn.net/tongdanping/article/details/79878302)\n\n什么时候不需要回表：\n\n- 在查询执行时，MySQL 可以**完全从索引中获取所有需要的数据**，而不需要回表（即访问实际的表数据）。这叫做**覆盖索引（Covering Index）**，覆盖索引就是**包含了查询所需的所有列**的索引。\n\n如果使用了覆盖索引，在explain的时候Extra的值为Using index\n","tags":["MySQL"]},{"title":"Android学习笔记——SQLite","url":"/Android学习笔记——SQLite.html","content":"该工程的功能是实现关于数据库的操作，即creat、update、insert、query、delete\n\n调试的时候请用模拟器，用真机调试的时候进入cmd-adb shell，再进入cd data/data的时候会显示permission denied\n\n<!--more-->\n&nbsp;\n\n以下的代码是MainActivity.java中的代码\n\n```\npackage com.example.sqlite;\n\nimport com.example.sqlite.db.DataBaseHelper;\n\nimport android.app.Activity;\nimport android.content.ContentValues;\nimport android.database.Cursor;\nimport android.database.sqlite.SQLiteDatabase;\nimport android.os.Bundle;\nimport android.view.Menu;\nimport android.view.MenuItem;\nimport android.view.View;\nimport android.view.View.OnClickListener;\nimport android.widget.Button;\nimport android.widget.EditText;\nimport android.widget.TextView;\n\npublic class MainActivity extends Activity {\n    \n    private Button createDBButton ;\n    private Button insertDBButton ;\n    private Button updateDBButton ;\n    private Button queryDBButton ;\n    private Button deleteDBButton ;\n    private EditText input_id ;\n    private EditText input_name ;\n    private TextView test ;\n    \n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n        //此处为创建数据库， 通过按钮监听器来实现\n        createDBButton = (Button) findViewById(R.id.creatDB);\n        createDBButton.setOnClickListener(new CreateDBListener());\n        //此处为插入数据到数据库中\n        insertDBButton = (Button) findViewById(R.id.insert);\n        insertDBButton.setOnClickListener(new InsertListener());\n        //此处为更新数据表\n        updateDBButton = (Button) findViewById(R.id.update);\n        updateDBButton.setOnClickListener(new updateListener());\n        //此处为更新数据表\n        queryDBButton = (Button) findViewById(R.id.query);\n        queryDBButton.setOnClickListener(new queryListener());\n        //此处为删除数据内容\n        deleteDBButton = (Button) findViewById(R.id.delete);\n        deleteDBButton.setOnClickListener(new deleteListener());\n        //此处为显示查询结果\n        test = (TextView) findViewById(R.id.result);\n        test.setText(\"name\") ;\n        //此处为添加数据的选项\n        //input_id = (EditText) findViewById(R.id.input_id);\n        //input_name = (EditText) findViewById(R.id.input_name);\n    }\n    \n    class CreateDBListener implements OnClickListener{\n        @Override\n        public void onClick(View v) {\n            // 此处为调用另外一个类中的方法来创建数据库， 或者直接来创建数据库\n\n            String db_name = \"test_mars_db_one\" ;\n            System.out.println(\"Create\");\n            DataBaseHelper dbHelper = new DataBaseHelper(MainActivity.this, db_name) ;\n            SQLiteDatabase db = dbHelper.getReadableDatabase() ;\n        } \n    }\n    \n    class InsertListener implements OnClickListener{\n\n        @Override\n        public void onClick(View v) {\n            //生成一个ContentValues对象\n            ContentValues values = new ContentValues() ;\n            //想该对象当中插入键值对，其中键是列名，值是希望插入到这列的值，值必须\n            values.put(\"id\", 1) ;\n            values.put(\"name\", \"zhangsan\") ;\n            System.out.println(\"Insert\");\n            DataBaseHelper dbHelper = new DataBaseHelper(MainActivity.this, \"test_mars_db_one\") ;\n            SQLiteDatabase db = dbHelper.getWritableDatabase() ;\n            //调用insert方法， 就可以将数据插入到数据库中\n            db.insert(\"user\", null, values) ;\n        }\n        \n    }\n\n    class updateListener implements OnClickListener{\n        @Override\n        public void onClick(View arg0) {\n            // 此处为更新数据内容   \n            System.out.println(\"Update\");\n            DataBaseHelper dbHelper = new DataBaseHelper(MainActivity.this, \"test_mars_db_one\") ;\n            SQLiteDatabase db = dbHelper.getWritableDatabase() ;\n            ContentValues values = new ContentValues() ;\n            values.put(\"name\", \"zhangsanfeng\") ;\n            //第一个参数为要更新的表名\n            //第二个参数为一个ContentValues对象\n            //第三个参数是where语句\n            db.update(\"user\", values, \"id=?\", new String[]{\"1\"}) ;\n        }\n    }\n    \n    \n    class queryListener implements OnClickListener{\n        @Override\n        public void onClick(View v) {\n            // 此处为查询数据内容， 并用到cursor来实现\n            System.out.println(\"query\");\n            DataBaseHelper dbHelper = new DataBaseHelper(MainActivity.this, \"test_mars_db_one\") ;\n            SQLiteDatabase db = dbHelper.getWritableDatabase() ;\n            Cursor cursor = db.query(\"user\", new String[]{\"id\",\"name\"}, \"id=?\", new String[]{\"2\"},null,null,null,null) ;\n            while(cursor.moveToNext()){\n                String name = cursor.getString(cursor.getColumnIndex(\"name\")) ;\n                System.out.print(\"query---> \" + name) ;\n                //name += name ;\n                test.setText(name) ;\n            }\n        }\n    }\n    \n    class deleteListener implements OnClickListener{\n        public void onClick(View v) {\n\n         //此处为实现删除数据\n            System.out.println(\"delete\");\n            DataBaseHelper dbHelper = new DataBaseHelper(MainActivity.this, \"test_mars_db_one\") ;\n            SQLiteDatabase db = dbHelper.getWritableDatabase() ;\n\n            //删除特定条件的数据\n            //db.delete(\"user\",\"id=?\",new String[]{\"2\"});\n            //删除所有数据\n            db.delete(\"user\",null,null);\n\n        }\n    }\n\n}\n    \n    \n    \n```\n\n&nbsp;\n\n以下的代码是DataBaseHelper.java中的代码\n\n```\npackage com.example.sqlite.db;\n\nimport android.content.Context;\nimport android.database.sqlite.SQLiteDatabase;\nimport android.database.sqlite.SQLiteDatabase.CursorFactory;\nimport android.database.sqlite.SQLiteOpenHelper;\n\n//DatabaseHelper作为一个访问SQLite的助手类，提供两个方面的功能\n//第一，getReadableDatabase(),getWriteableDatabase()可以获得SQLiteDatabase对象，通过该对象可以对数据库进行操作\n//第二，提供了onCreate()和onUpgrade()两个回调函数，允许我们在创建和升级数据库时，进行自己的操作\n\npublic class DataBaseHelper extends SQLiteOpenHelper {\n    \n    private static final int VERSION = 1;\n    //在SQLiteOpenHelper的子类当中，必须有该构造函数\n    public DataBaseHelper(Context context, String name, CursorFactory factory,\n            int version) {\n        //必须通过super调用父类当中的构造函数\n        super(context, name, factory, version);\n        // TODO Auto-generated constructor stub\n    }\n    \n    public DataBaseHelper(Context context,String name){\n        this(context,name,VERSION);\n    }\n    \n    public DataBaseHelper(Context context,String name,int version){\n        this(context,name,null,version);\n    }\n    \n    //该函数是在第一次创建数据库的时候执行，实际上是在第一次得到SQLiteDatabase对象的时候，才会调用该方法\n    @Override\n    public void onCreate(SQLiteDatabase db) {\n        // TODO Auto-generated method stub\n        System.out.println(\"Create a Database\");\n        //execSQL函数用于执行SQL语句\n        db.execSQL(\"create table user(id int,name varchar(20))\");\n        System.out.println(\"Create a Database successful\");\n    }\n\n    @Override\n    public void onUpgrade(SQLiteDatabase db, int oldVersion, int newVersion) {\n        // TODO Auto-generated method stub\n        System.out.println(\"update a DataBase\");\n    }\n\n}\n```\n\n&nbsp;\n\n以下的代码是activity_main.xml中的代码\n\n```\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:id=\"@+id/LinearLayout1\"\n    android:layout_width=\"fill_parent\"\n    android:layout_height=\"fill_parent\"\n    android:orientation=\"vertical\"\n    tools:context=\"${relativePackage}.${activityClass}\" >\n    \n    \n    <Button\n        android:id=\"@+id/creatDB\"\n        android:text=\"creatDatabase\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"    \n        />\n    \n    <Button\n        android:id=\"@+id/update\"\n        android:text=\"updateDatabase\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"    \n        />\n    \n    <Button\n        android:id=\"@+id/insert\"\n        android:text=\"Insert\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"    \n        />\n    \n    <Button\n        android:id=\"@+id/query\"\n        android:text=\"query\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"    \n        />\n    \n    <Button\n        android:id=\"@+id/delete\"\n        android:text=\"delete\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"    \n        />\n    \n    \n    <TextView\n        android:id=\"@+id/result\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"\n        android:text=\"@string/hello_world\" />\n\n</LinearLayout>\n```\n\n&nbsp;\n\n**利用adb shell管理虚拟机的数据库**\n\nGoogle Android操作系统内核基于Linux，其数据库采用了sqlite。sqlite是一个开源的小巧的大小小于500KB的微型数据库系统。\n\nAndroid Debug Bridge(adb)是Android下的一个让你管理手机或者手机虚拟机的多功能工具。\n\n本文主要介绍如何在虚拟机上操作sqlite数据库。\n\nGoogle的Android的开发包内有个虚拟机,启动虚拟机后，在window命令行窗口内输入**adb shell**即可登入设备，我们就拥有了一个Linux环境。\n\n&nbsp;\n\n图1：Android虚拟机\n\n&nbsp;\n\n图2：adbshell登入Linux系统\n\nAndroid把数据都存放在data/data目录下。\n\n我们使用cd命令转到data/data目录下：\n\n**cd** /data/data\n\nls显示所有数据\n\n&nbsp;\n\n你会看到像我目录一样有个mars.sqlite3文件夹。\n\n**cd** mars.sqlite3/databases进入mars.sqlite3/databases目录\n\n**ls**命令你会看到test_mars_db,这就是我们的创建的测试数据库文件\n\n在#提示符下输入以下命令**sqlite3** test_mars_db;登入数据库\n\n&nbsp;\n\n图3：登入sqlite3数据库\n\n这个命令会打开test_mars_db数据库，若test_mars_db数据库不存在，则会新建一个名为\n\ntest_mars_db的数据库。(注意数据库名大小写有区别)\n\n在sqlite>提示符下输入\n\n**.help**这个命令让我们看到许多命令\n\n**.tables**查看所有表，例如我的系统里有android_metadata和user两个表\n\n查询表user数据内容，我们输入\n\nSelect * from user;我们查出三条数据。\n\n&nbsp;\n\n图4：显示查询结果\n\n这个数据库的显示方式让我们习惯了Oralce，看得很不舒服，输入\n\n以下两个命令，让我们改变这种显示模式。\n\n**.mode** column\n\n**.header** on\n\n再输入select * from user;我们看着舒服多了。\n\n&nbsp;\n\n图5：改变数据显示模式\n\n**.exit**命令退出sqlite，返回到#提示符。\n\n在#提示符下输入exit退出Linux。\n\n&nbsp;\n","tags":["Android"]},{"title":"Android学习笔记——Layout","url":"/Android学习笔记——Layout.html","content":"下面列举了Android中Layout，Table，Menu，Checkbox，Listview，Button，Bundle的基本使用和demo\n\n## 1.Layout：该工程的功能是实现LinearLayout\n\nAndroid Layout有多种，比如：Layout，MixLayout，TableLayout等\n\n以下的代码是MainActivity.java中的代码\n\n```\npackage com.example.linearlayout;\n\nimport android.app.Activity;\nimport android.os.Bundle;\nimport android.view.Menu;\nimport android.view.MenuItem;\nimport android.widget.TextView;\n\n\npublic class MainActivity extends Activity {\n\n    private TextView firstText;\n    private TextView secondText;\n    \n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n        \n        firstText = (TextView)findViewById(R.id.firstText);\n        secondText = (TextView)findViewById(R.id.secondText);\n        \n        firstText.setText(R.string.firstText);\n        secondText.setText(R.string.secondText);\n    }\n}\n```\n\n<!--more-->\n&nbsp;\n\n以下的代码是activity_main.xml中的代码\n\n```\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:id=\"@+id/LinearLayout1\"\n    android:layout_width=\"fill_parent\"\n    android:layout_height=\"match_parent\"\n    android:orientation=\"vertical\"\n    tools:context=\"${relativePackage}.${activityClass}\" >\n\n<!-- \n         android:id            为控件制定相应的ID\n        android:text        指定控件当中显示的文字\n        android:gravity        指定控件的基本位置/居中居右等\n        android:textSize    指定控件的基本位置/居中居右等 \n        android:background    指定该控件所使用的背景色，RGB命名法\n        android:width        指定控件的宽度\n        android:height        指定控件的高度\n        android:padding        指定控件的内边距\n        android:weight        数字为相应的比例\n        android:singleLine    设定true为同一行显示\n         -->\n        \n  \n   \n    <TextView\n        android:id=\"@+id/firstText\"\n        android:gravity=\"center_vertical\"\n        android:textSize=\"20pt\"\n        android:background=\"#0000ff\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"0.0dip\"\n        android:paddingLeft=\"10dip\"\n        android:paddingTop=\"20dip\"\n        android:paddingRight=\"30dip\"\n        android:paddingBottom=\"40dip\"\n        android:layout_weight=\"1\"\n        android:singleLine=\"true\"\n        />\n    \n    <TextView\n        android:id=\"@+id/secondText\"\n        android:gravity=\"center_vertical\"\n        android:textSize=\"15pt\"\n        android:background=\"#00ff00\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"0.0dip\"\n        android:paddingLeft=\"10dip\"\n        android:paddingTop=\"20dip\"\n        android:paddingRight=\"30dip\"\n        android:paddingBottom=\"40dip\"\n        android:layout_weight=\"2\"\n        android:singleLine=\"true\"\n        />\n\n</LinearLayout>\n```\n\n&nbsp;\n\n以下的代码是string.xml中的代码\n\n```\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<resources>\n\n    <string name=\"app_name\">LinearLayout</string>\n    <string name=\"hello_world\">Hello world!</string>\n    <string name=\"firstText\">第一行</string>\n    <string name=\"secondText\">第二行</string>\n</resources>\n```\n\n&nbsp;\n\n## 2.Layout：该工程的功能是实现LinearLayout+TableLayout\n\n以下代码是MainActivity.java中的代码\n\n```\npackage com.example.mixlayout;\n\nimport android.app.Activity;\nimport android.os.Bundle;\nimport android.view.Menu;\nimport android.view.MenuItem;\nimport android.widget.TextView;\n\npublic class MainActivity extends Activity {\n\n    private TextView firstText; \n    \n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n    }\n}\n```\n\n&nbsp;\n\n以下代码是activity_main.xml中的代码\n\n```\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:id=\"@+id/LinearLayout1\"\n    android:layout_width=\"fill_parent\"\n    android:layout_height=\"fill_parent\"\n    android:orientation=\"vertical\"\n    tools:context=\"${relativePackage}.${activityClass}\" >\n\n    <LinearLayout\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"fill_parent\"\n        android:layout_weight=\"1\"\n        android:orientation=\"horizontal\" >\n\n        <TextView\n            android:layout_width=\"wrap_content\"\n            android:layout_height=\"fill_parent\"\n            android:layout_weight=\"1\"\n            android:background=\"#aa0000\"\n            android:gravity=\"center_horizontal\"\n            android:text=\"red\" />\n\n        <TextView\n            android:layout_width=\"wrap_content\"\n            android:layout_height=\"fill_parent\"\n            android:layout_weight=\"1\"\n            android:background=\"#00aa00\"\n            android:gravity=\"center_horizontal\"\n            android:text=\"green\" />\n\n        <TextView\n            android:layout_width=\"wrap_content\"\n            android:layout_height=\"fill_parent\"\n            android:layout_weight=\"1\"\n            android:background=\"#0000aa\"\n            android:gravity=\"center_horizontal\"\n            android:text=\"blue\" />\n    </LinearLayout>\n\n    <LinearLayout\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"fill_parent\"\n        android:layout_weight=\"1\"\n        android:orientation=\"horizontal\" >\n\n        <TableLayout \n            xmlns:android=\"http://schemas.android.com/apk/res/android\" \n            android:layout_width=\"fill_parent\"\n            android:layout_height=\"fill_parent\"\n            android:stretchColumns=\"1\">\n            <TableRow>\n\n                <TextView\n                    android:padding=\"3dip\"\n                    android:text=\"row1_column1\" />\n\n                <TextView\n                    android:gravity=\"center_horizontal\"\n                    android:padding=\"3dip\"\n                    android:text=\"row1_column2\" />\n\n                <TextView\n                    android:gravity=\"right\"\n                    android:padding=\"3dip\"\n                    android:text=\"row1_column3\" />\n            </TableRow>\n\n            <TableRow>\n\n                <TextView\n                    android:padding=\"3dip\"\n                    android:text=\"row2_column1\" />\n\n                <TextView\n                    android:gravity=\"center_horizontal\"\n                    android:padding=\"3dip\"\n                    android:text=\"row2_column2\" />\n\n                <TextView\n                    android:gravity=\"right\"\n                    android:padding=\"3dip\"\n                    android:text=\"row2_column3\" />\n            </TableRow>\n        </TableLayout>\n    </LinearLayout>\n\n</LinearLayout>\n```\n\n&nbsp;\n\n## 3.Table：该工程的功能是实现在一个activity中显示一个表格\n\n以下代码是MainActivity.java中的代码\n\n```\npackage com.example.tablelayout;\n\nimport android.app.Activity;\nimport android.os.Bundle;\nimport android.view.Menu;\nimport android.view.MenuItem;\n\npublic class MainActivity extends Activity {\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n    }\n}\n```\n\n&nbsp;\n\n以下的代码是activity_main.xml中的代码\n\n```\n<TableLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:id=\"@+id/TableLayout1\"\n    android:layout_width=\"fill_parent\"\n    android:layout_height=\"fill_parent\"\n    android:stretchColumns=\"1\"\n    tools:context=\"${relativePackage}.${activityClass}\" >\n    \n    <!-- 由于fill_parent不能填满，所以stretchColumns指定列拉伸 -->\n    \n    <TableRow>\n        <TextView\n            android:text=\"@string/hello_world\" \n            android:padding=\"3dip\"\n            android:background=\"#aa0000\" />\n        <TextView\n            android:text=\"@string/hello_world\" \n            android:padding=\"3dip\"\n            android:background=\"#00aa00\"\n            android:gravity=\"center_horizontal\" />\n        <TextView\n            android:text=\"@string/hello_world\" \n            android:padding=\"3dip\"\n            android:background=\"#0000aa\"\n            android:gravity=\"right\" />\n                     \n    </TableRow>\n        \n    <TableRow>\n        <TextView\n            android:text=\"@string/hello_world\" \n            android:padding=\"3dip\" />\n             \n        <TextView\n            android:text=\"@string/hello_world\" \n            android:padding=\"3dip\"\n            android:gravity=\"right\" />\n                   \n    </TableRow>\n\n</TableLayout>\n```\n\n## 4.Menu：该工程的功能是实现两个数相乘，并在另外一个Activity中显示计算的结果\n\n以下的代码是MainActivity.java中的代码\n\n```\npackage com.example.menu;\n\nimport android.app.Activity;\nimport android.content.Intent;\nimport android.os.Bundle;\nimport android.view.Menu;\nimport android.view.MenuItem;\nimport android.view.View;\nimport android.view.View.OnClickListener;\nimport android.widget.Button;\nimport android.widget.EditText;\nimport android.widget.TextView;\n\n\npublic class MainActivity extends Activity {\n    \n    private EditText factorOne;\n    private EditText factorTwo;\n    private TextView symbol;\n    private Button calculate;\n    \n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n        \n        factorOne = (EditText)findViewById(R.id.factorOne);\n        factorTwo = (EditText)findViewById(R.id.factorTwo);\n        symbol = (TextView)findViewById(R.id.symbol);\n        calculate = (Button)findViewById(R.id.calculate);\n        \n        //symbol.setText(\"乘以\");\n        //calculate.setText(\"计算\");\n        \n        //为symbol和calculate设置显示的值\n        symbol.setText(R.string.symbol);\n        calculate.setText(R.string.calculate);\n        //将监听器的对象绑定到按钮对象上面\n        calculate.setOnClickListener(new CalculateListener());\n    }\n    \n    \n    @Override\n    public boolean onCreateOptionsMenu(Menu menu) {\n        // TODO Auto-generated method stub\n        menu.add(0,1,1,R.string.exit);\n        menu.add(0,2,2,R.string.about);\n        return super.onCreateOptionsMenu(menu);\n    }\n\n    //当客户点击MENU按钮的时候，调用该方法\n    @Override\n    public boolean onOptionsItemSelected(MenuItem item) {\n        // TODO Auto-generated method stub\n        if(item.getItemId() == 1){\n            finish();\n        }\n        return super.onOptionsItemSelected(item);\n    }\n\n\n    class CalculateListener implements OnClickListener{\n\n        @Override\n        public void onClick(View v) {\n            // TODO Auto-generated method stub\n            //取得两个EditText控件的值\n            String factorOneStr = factorOne.getText().toString();\n            String factorTwoStr = factorTwo.getText().toString();\n            //将这两个值存放到Intent对象当中\n            Intent intent = new Intent();\n            intent.putExtra(\"one\",factorOneStr);\n            intent.putExtra(\"two\",factorTwoStr);\n            intent.setClass(MainActivity.this,ResultActivity.class );\n            //使用这个Intent对象来启动ResultActivity\n            MainActivity.this.startActivity(intent);\n        }\n        \n        \n    }\n}\n```\n\n&nbsp;\n\n以下的代码是ResultActivity.java中的代码\n\n```\npackage com.example.menu;\n\nimport android.app.Activity;\nimport android.content.Intent;\nimport android.os.Bundle;\nimport android.widget.TextView;\n\npublic class ResultActivity extends Activity{\n    \n    private TextView resultView;\n    \n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_result);\n        resultView = (TextView)findViewById(R.id.result);\n        //得到Intent对象当中的值\n        Intent intent = getIntent();\n        String factorOneStr = intent.getStringExtra(\"one\");\n        String factorTwoStr = intent.getStringExtra(\"two\");\n        int factorOneInt = Integer.parseInt(factorOneStr);\n        int factorTwoInt = Integer.parseInt(factorTwoStr);\n        //计算两个值的积\n        int result = factorOneInt * factorTwoInt;\n        resultView.setText(result + \"\");\n    }\n}\n```\n\n&nbsp;\n\n以下的代码是activity_main.xml中的代码\n\n```\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:layout_width=\"fill_parent\"\n    android:layout_height=\"fill_parent\"\n    android:orientation=\"vertical\" >\n\n    <EditText\n        android:id=\"@+id/factorOne\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"\n      />  \n        \n    <TextView\n        android:id=\"@+id/symbol\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"\n        />\n\n     <EditText\n        android:id=\"@+id/factorTwo\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"\n      /> \n      \n     <Button\n         android:id=\"@+id/calculate\"\n         android:layout_width=\"fill_parent\"\n         android:layout_height=\"wrap_content\"\n         />\n    \n</LinearLayout>\n```\n\n&nbsp;\n\n以下的代码是activity_result.xml中的代码\n\n```\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    \n    android:orientation=\"vertical\"\n    android:layout_width=\"fill_parent\"\n    android:layout_height=\"fill_parent\"\n    >\n\n    <TextView\n        android:id=\"@+id/result\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"\n        />\n        \n\n</LinearLayout>\n```\n\n&nbsp;\n\n以下的代码是string.xml中的代码\n\n```\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<resources>\n\n    <string name=\"app_name\">menu</string>\n    <string name=\"hello_world\">Hello world!</string>\n    <string name=\"resultLabel\">result</string>\n    <string name=\"symbol\">乘法</string>\n    <string name=\"calculate\">计算</string>\n    <string name=\"exit\">退出</string>\n    <string name=\"about\">关于</string>\n</resources>\n```\n\n&nbsp;\n\n以下的代码是AndroidManifest.xml中的代码\n\n```\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    package=\"com.example.menu\"\n    android:versionCode=\"1\"\n    android:versionName=\"1.0\" >\n\n    <uses-sdk\n        android:minSdkVersion=\"8\"\n        android:targetSdkVersion=\"21\" />\n\n    <application\n        android:allowBackup=\"true\"\n        android:icon=\"@drawable/ic_launcher\"\n        android:label=\"@string/app_name\"\n        android:theme=\"@style/AppTheme\" >\n        <activity\n            android:name=\".MainActivity\"\n            android:label=\"@string/app_name\" >\n            <intent-filter>\n                <action android:name=\"android.intent.action.MAIN\" />\n\n                <category android:name=\"android.intent.category.LAUNCHER\" />\n            </intent-filter>\n        </activity>\n        <activity\n            android:name = \".ResultActivity\">\n        </activity>\n    </application>\n\n</manifest>\n```\n\n## 5.checkbox：该工程的功能实现在一个activity中显示一个单选框和一个多选框\n\n以下代码是MainActivity.java文件中的代码\n\n```\npackage com.example.checkbox;\n\nimport android.app.Activity;\nimport android.os.Bundle;\nimport android.view.Menu;\nimport android.view.MenuItem;\nimport android.widget.CheckBox;\nimport android.widget.CompoundButton;\nimport android.widget.RadioButton;\nimport android.widget.RadioGroup;\nimport android.widget.Toast;\n\npublic class MainActivity extends Activity {\n    //对控件对象进行声明\n    private RadioGroup gendergroup = null;\n    private RadioButton femaleButton = null;\n    private RadioButton maleButton = null;\n    private CheckBox swimBox = null;\n    private CheckBox runBox = null;\n    private CheckBox readBox = null;\n    \n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n        //通过控件的ID来得到代表控件的对象\n        gendergroup = (RadioGroup)findViewById(R.id.genderGroup);\n        femaleButton = (RadioButton)findViewById(R.id.femaleButton);\n        maleButton = (RadioButton)findViewById(R.id.maleButton);\n        swimBox = (CheckBox)findViewById(R.id.swim);\n        runBox = (CheckBox)findViewById(R.id.run);\n        readBox = (CheckBox)findViewById(R.id.read);\n        //设置监听器\n        gendergroup.setOnCheckedChangeListener(new RadioGroup.OnCheckedChangeListener() {\n            \n            @Override\n            public void onCheckedChanged(RadioGroup group, int checkedId) {\n                // TODO Auto-generated method stub\n                if(femaleButton.getId() == checkedId){\n                    System.out.println(\"female\");\n                    Toast.makeText(MainActivity.this, \"female\", Toast.LENGTH_SHORT).show();\n                }\n                else if(maleButton.getId() == checkedId)\n                {\n                    System.out.println(\"male\");\n                    Toast.makeText(MainActivity.this, \"male\", Toast.LENGTH_SHORT).show();\n                }    \n            }\n        });\n        //为多选按钮添加监听器\n        swimBox.setOnCheckedChangeListener(new CompoundButton.OnCheckedChangeListener() {\n            \n            @Override\n            public void onCheckedChanged(CompoundButton buttonView, boolean isChecked) {\n                // TODO Auto-generated method stub\n                if(isChecked)\n                {\n                    System.out.println(\"swim is checked\");\n                }\n                else\n                {\n                    System.out.println(\"swim is unchecked\");\n                }\n            }\n        });\n        runBox.setOnCheckedChangeListener(new CompoundButton.OnCheckedChangeListener() {\n            \n            @Override\n            public void onCheckedChanged(CompoundButton buttonView, boolean isChecked) {\n                // TODO Auto-generated method stub\n                if(isChecked)\n                {\n                    System.out.println(\"run is checked\");\n                }\n                else\n                {\n                    System.out.println(\"run is unchecked\");\n                }\n            }\n        });\n        readBox.setOnCheckedChangeListener(new CompoundButton.OnCheckedChangeListener() {\n            \n            @Override\n            public void onCheckedChanged(CompoundButton buttonView, boolean isChecked) {\n                // TODO Auto-generated method stub\n                if(isChecked)\n                {\n                    System.out.println(\"read is checked\");\n                }\n                else\n                {\n                    System.out.println(\"read is unchecked\");\n                }\n            }\n        });\n    }\n}\n```\n\n&nbsp;\n\n以下代码是activity_main.xml文件中的代码\n\n```\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:id=\"@+id/LinearLayout1\"\n    android:layout_width=\"fill_parent\"\n    android:layout_height=\"fill_parent\"\n    android:orientation=\"vertical\"\n    tools:context=\"${relativePackage}.${activityClass}\" >\n\n    <TextView\n        android:id=\"@+id/textView1\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"\n        android:text=\"@string/hello_world\" />\n\n    <RadioGroup\n        android:id=\"@+id/genderGroup\"\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        android:orientation=\"vertical\"\n        >\n        \n        <RadioButton\n            android:id=\"@+id/femaleButton\"\n            android:layout_width=\"wrap_content\"\n            android:layout_height=\"wrap_content\"\n            android:text=\"@string/female\"\n            />\n    \n        <RadioButton\n            android:id=\"@+id/maleButton\"\n            android:layout_width=\"wrap_content\"\n            android:layout_height=\"wrap_content\"\n            android:text=\"@string/male\"\n            />\n    </RadioGroup>\n    \n    <CheckBox\n        android:id=\"@+id/swim\"\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        android:text=\"@string/swim\"\n        />\n    <CheckBox\n        android:id=\"@+id/run\"\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        android:text=\"@string/run\"\n        />\n    <CheckBox\n        android:id=\"@+id/read\"\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        android:text=\"@string/read\"\n        />\n    \n</LinearLayout>\n```\n\n&nbsp;\n\n以下代码是string.xml文件中的代码\n\n```\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<resources>\n\n    <string name=\"app_name\">CheckBox</string>\n    <string name=\"hello_world\">Hello world!</string>\n    <string name=\"female\">女</string>\n    <string name=\"male\">男</string>\n    <string name=\"swim\">游泳</string>\n    <string name=\"run\">跑步</string>\n    <string name=\"read\">读书</string>\n</resources>\n```\n\n## 6.listview：该工程的功能是实现在一个activity中显示一个列表\n\n以下代码是MainActivity.java中的代码\n\n```\npackage com.example.listview;\n\nimport java.util.ArrayList;\nimport java.util.HashMap;\n\n\nimport android.app.ListActivity;\nimport android.os.Bundle;\nimport android.view.Menu;\nimport android.view.MenuItem;\nimport android.widget.SimpleAdapter;\n\npublic class MainActivity extends ListActivity {\n\n    @Override\n    public void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n        \n        \n        ArrayList<HashMap<String, String>> list = \n                new ArrayList<HashMap<String, String>>(); \n        HashMap<String, String> map1 = new HashMap<String,String>();\n        HashMap<String, String> map2 = new HashMap<String,String>();\n        HashMap<String, String> map3 = new HashMap<String,String>();\n        \n        map1.put(\"user_name\",\"zhangsan\");\n        map1.put(\"user_ip\",\"192.168.0.1\");\n        map2.put(\"user_name\",\"lisi\");\n        map2.put(\"user_ip\",\"192.168.0.2\");\n        map3.put(\"user_name\",\"wangwu\");\n        map3.put(\"user_ip\",\"192.168.0.3\");\n        \n        list.add(map1);\n        list.add(map2);\n        list.add(map3);\n        \n        SimpleAdapter listAdapter = new SimpleAdapter(this, list, \n                R.layout.activity_user, new String[] {\"user_name\", \"user_ip\"},\n                new int[] {R.id.user_name, R.id.user_ip});\n        setListAdapter(listAdapter);\n    }\n}\n```\n\n&nbsp;\n\n以下的代码是activity_main.xml中的代码\n\n```\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:id=\"@+id/LinearLayout1\"\n    android:layout_width=\"fill_parent\"\n    android:layout_height=\"fill_parent\"\n    android:orientation=\"vertical\"\n    tools:context=\"${relativePackage}.${activityClass}\" >\n\n    <LinearLayout\n        android:id=\"@+id/listLinearLayout\"\n        android:orientation=\"vertical\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"\n        >\n        <ListView\n            android:id=\"@id/android:list\"\n            android:scrollbars=\"vertical\"\n            android:layout_width=\"fill_parent\"\n            android:layout_height=\"wrap_content\"\n            android:drawSelectorOnTop=\"false\"\n            />\n    </LinearLayout>\n\n</LinearLayout>\n```\n\n&nbsp;\n\n以下代码是activity_user.xml中的代码\n\n```\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:id=\"@+id/LinearLayout2\"\n    android:layout_width=\"fill_parent\"\n    android:layout_height=\"fill_parent\"\n    android:orientation=\"horizontal\"\n    android:paddingLeft=\"10dip\"\n    android:paddingRight=\"10dip\"\n    android:paddingTop=\"1dip\"\n    android:paddingBottom=\"1dip\"\n    tools:context=\"${relativePackage}.${activityClass}\" >\n\n    <TextView\n        android:id=\"@+id/user_name\"\n        android:layout_width=\"180dip\"\n        android:layout_height=\"30dip\"\n        android:textSize=\"10pt\"\n        android:singleLine=\"true\"\n        />\n    \n    <TextView\n        android:id=\"@+id/user_ip\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"fill_parent\"\n        android:gravity=\"right\"\n        android:textSize=\"10pt\"\n        />\n\n</LinearLayout>&nbsp;\n```\n\n## 7.Button：工程的功能是实现在一个acticity上点击按钮，切换到另外一个activity\n\n以下代码为MainActivity.java中的代码\n\n```\npackage com.example.button_activity;\n\nimport android.app.Activity;\nimport android.content.Intent;\nimport android.os.Bundle;\nimport android.view.Menu;\nimport android.view.MenuItem;\nimport android.view.View;\nimport android.view.View.OnClickListener;\nimport android.widget.Button;\n\npublic class MainActivity extends Activity {\n\n    private Button myButton = null; \n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n        myButton = (Button)findViewById(R.id.myButton);\n        myButton.setOnClickListener(new MyButtonListener());  \n    }\n    \n    class MyButtonListener implements OnClickListener{\n\n        @Override\n        public void onClick(View v) {\n            // TODO Auto-generated method stub\n        Intent intent = new Intent();\n        intent.setClass(MainActivity.this, OtherActivity.class);\n        MainActivity.this.startActivity(intent);\n        }\n        \n        \n    }\n        \n}\n```\n\n&nbsp;\n\n以下代码为OtherActivity.java中的代码\n\n```\npackage com.example.button_activity;\n\nimport android.app.Activity;\nimport android.os.Bundle;\nimport android.widget.TextView;\n\npublic class OtherActivity extends Activity{\n\n    private TextView myTextView = null; \n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        // TODO Auto-generated method stub\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_other);\n        myTextView = (TextView)findViewById(R.id.myTextView);\n        myTextView.setText(R.string.other);\n    }\n    \n\n}\n```\n\n&nbsp;\n\n以下代码为activity_main.xml中的代码\n\n```\n<RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    tools:context=\"${relativePackage}.${activityClass}\" >\n\n    <TextView\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        android:text=\"@string/hello_world\" />\n    \n    <Button\n        android:id=\"@+id/myButton\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"\n        \n        />\n\n</RelativeLayout>\n```\n\n&nbsp;\n\n以下代码为activity_other.xml中的代码\n\n```\n<RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    tools:context=\"${relativePackage}.${activityClass}\" >\n\n    <TextView\n        android:id=\"@+id/myTextView\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"\n        android:text=\"@string/hello_world\" />\n    \n\n</RelativeLayout>\n```\n\n&nbsp;\n\n以下代码为string.xml中的代码\n\n```\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<resources>\n\n    <string name=\"app_name\">button_activity</string>\n    <string name=\"hello_world\">Hello world!</string>\n    <string name=\"other\">OtherActivity</string>\n</resources>\n```\n\n&nbsp;\n\n以下代码为AndroidManifest.xml中的代码&nbsp;\n\n注意修改package的名称\n\n```\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    package=\"com.example.button_activity\"\n    android:versionCode=\"1\"\n    android:versionName=\"1.0\" >\n\n    <uses-sdk\n        android:minSdkVersion=\"8\"\n        android:targetSdkVersion=\"21\" />\n\n    <application\n        android:allowBackup=\"true\"\n        android:icon=\"@drawable/ic_launcher\"\n        android:label=\"@string/app_name\"\n        android:theme=\"@style/AppTheme\" >\n        <activity\n            android:name=\".MainActivity\"\n            android:label=\"@string/app_name\" >\n            <intent-filter>\n                <action android:name=\"android.intent.action.MAIN\" />\n\n                <category android:name=\"android.intent.category.LAUNCHER\" />\n            </intent-filter>\n        </activity>\n        <activity\n            android:name=\".OtherActivity\"\n            android:label=\"@string/other\" >\n        </activity>     \n    </application>\n\n</manifest>\n```\n\n&nbsp;\n\n如果不能运行请kill-adb和start-adb并重新启动eclipse\n\n&nbsp;\n\n## 8.Button：该工程的功能是实现在activity中显示一个TextView和一个Button\n\n以下代码是MainActivity中的代码\n\n```\npackage com.example.button;\n\nimport android.app.Activity;\nimport android.os.Bundle;\nimport android.view.Menu;\nimport android.view.MenuItem;\nimport android.widget.Button;\nimport android.widget.TextView;\n\npublic class MainActivity extends Activity {\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n        TextView myTextView = (TextView)findViewById(R.id.myTextView);\n        Button myButton = (Button)findViewById(R.id.myButton);\n        myTextView.setText(\"我的第一个TextView\");\n        myButton.setText(\"我的第一个Button\");\n        \n    }\n}\n```\n\n&nbsp;\n\n以下代码是activity_main中的代码\n\n```\n<RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    tools:context=\"${relativePackage}.${activityClass}\" >\n\n    <TextView\n        android:id=\"@+id/myTextView\"\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        android:text=\"@string/hello_world\" />\n    \n    <Button\n        android:id=\"@+id/myButton\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"\n        />\n\n</RelativeLayout>\n```\n\n&nbsp;\n\n该工程的功能是实现进度条的显示，按以下按钮进度条增加10%\n\n&nbsp;\n\n以下代码是MainActivity.java中的代码\n\n```\npackage com.example.progressbar;\n\nimport android.app.Activity;\n\nimport android.os.Bundle;\nimport android.view.Menu;\nimport android.view.MenuItem;\nimport android.view.View;\nimport android.view.View.OnClickListener;\nimport android.widget.Button;\nimport android.widget.ProgressBar;\n\npublic class MainActivity extends Activity {\n    \n    //申明限量\n    private ProgressBar firstBar = null;\n    private ProgressBar secondBar = null;\n    private Button myButton = null;\n    private int i = 0;\n    \n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n        //根据控件的ID来取得代表控件的对象\n        firstBar = (ProgressBar)findViewById(R.id.firstBar);\n        secondBar = (ProgressBar)findViewById(R.id.secondBar);\n        myButton = (Button)findViewById(R.id.myButton);\n        myButton.setOnClickListener(new ButtonListener());\n        System.out.print(firstBar.getMax());\n    }\n    \n    class ButtonListener implements OnClickListener{\n\n        @Override\n        public void onClick(View v) {\n            // TODO Auto-generated method stub\n            if(i == 0)\n            {    \n                //设置进度条处于可见的状态\n                firstBar.setVisibility(View.VISIBLE);\n                secondBar.setVisibility(View.VISIBLE);\n            }\n            else if(i < firstBar.getMax())\n            {    \n                //设置主进度条的当前值\n                firstBar.setProgress(i);\n                //设置第二进度条的当前值\n                firstBar.setSecondaryProgress(i + 10);\n                //因为默认的进度条无法显示进行的状态\n                //secondBar.setProgress(i);\n            }\n            else\n            {    \n                //设置进度条处于不可见状态\n                firstBar.setVisibility(View.GONE);\n                secondBar.setVisibility(View.GONE);\n            }\n            i = i + 10;\n        }\n\n    }\n}\n```\n\n&nbsp;\n\n以下代码是activity_main.xml中的代码\n\n```\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:id=\"@+id/LinearLayout1\"\n    android:layout_width=\"fill_parent\"\n    android:layout_height=\"fill_parent\"\n    android:orientation=\"vertical\"\n    tools:context=\"${relativePackage}.${activityClass}\" >\n\n    <TextView\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"\n        android:text=\"@string/hello_world\" />\n    \n    <ProgressBar\n        android:id=\"@+id/firstBar\"\n        style=\"?android:attr/progressBarStyleHorizontal\"\n        android:layout_width=\"200dp\"\n        android:layout_height=\"wrap_content\"\n        android:visibility=\"gone\"\n        />\n    \n    <ProgressBar\n        android:id=\"@+id/secondBar\"\n        style=\"?android:attr/progressBarStyle\"\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        android:visibility=\"gone\"\n        android:max=\"200\"\n        />\n\n    <Button\n        android:id=\"@+id/myButton\"\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        android:text=\"begin\"\n        />\n    \n    \n</LinearLayout>\n```\n\n&nbsp;\n\n## 9.Bundle：该工程的功能是实现不同线程之间数据的传递\n\n以下代码是MainActivity.java中的代码\n\n```\npackage com.example.bundle;\n\nimport android.app.Activity;\nimport android.os.Bundle;\nimport android.os.Handler;\nimport android.os.HandlerThread;\nimport android.os.Looper;\nimport android.os.Message;\nimport android.view.Menu;\nimport android.view.MenuItem;\n\npublic class MainActivity extends Activity {\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n        //打印了当前线程的ID\n        System.out.println(\"Activity-->\"  + Thread.currentThread().getId());\n        //生成一个HandleThread对象，实现了使用Looper来处理消息队列的功能，这个类由android程序框架提供\n        HandlerThread handlerThread = new HandlerThread(\"handler_thread\");\n        //在使用HandlerThread的getLooper（）方法之前，必须先调用该类的start（）\n        handlerThread.start();\n        MyHandler myHandler = new MyHandler(handlerThread.getLooper());\n        Message msg = myHandler.obtainMessage();\n        //msg.obj=\"abc\";\n        //将msg发送到目标对象，所谓的目标对象，就是生成该msg对象的handler对象\n        Bundle b = new Bundle();\n        b.putInt(\"age\", 20);\n        b.putString(\"name\", \"John\");\n        msg.setData(b);\n        msg.sendToTarget();\n    }\n    \n    class MyHandler extends Handler{\n        public MyHandler(){\n            \n        }\n        public MyHandler(Looper looper){\n            //super调用父类\n            super(looper);\n        }\n        public void handleMessage(Message msg){\n            //String s = (String)msg.obj;\n            Bundle b = msg.getData();\n            int age = b.getInt(\"age\");\n            String name = b.getString(\"name\");\n            System.out.println(\"age is \" + age +\", name is \" + name);\n            System.out.println(\"Handler-->\" + Thread.currentThread().getId());\n            System.out.println(\"handlerMessage\");\n        }\n    }\n}\n```\n\n<br />以下代码是activity_main.xml中的代码\n\n```\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:id=\"@+id/LinearLayout1\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    android:orientation=\"vertical\"\n    tools:context=\"${relativePackage}.${activityClass}\" >\n\n    <TextView\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        android:text=\"@string/hello_world\" />\n\n</LinearLayout>\n```\n\n&nbsp;\n\n## 10.ProcessBar：该工程的功能是实现点击按钮进度条按10%递增，使用的方式是Handler\n\n以下的代码是MainActivity.java中的代码\n\n```\npackage com.example.progressbarhandler;\n\nimport android.app.Activity;\nimport android.os.Bundle;\nimport android.os.Handler;\nimport android.os.Message;\nimport android.view.View;\nimport android.view.View.OnClickListener;\nimport android.widget.Button;\nimport android.widget.ProgressBar;\n\n\npublic class MainActivity extends Activity {\n    //申明变量\n    ProgressBar bar = null;\n    Button startButton = null;\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n        //根据空间的ID得到代表控件的对象，并未按钮去设置监听器\n        bar = (ProgressBar)findViewById(R.id.bar);\n        startButton = (Button)findViewById(R.id.startButton);\n        startButton.setOnClickListener(new ButtonListener());\n    }\n    \n    class ButtonListener implements OnClickListener{\n\n        @Override\n        public void onClick(View v) {\n            // TODO Auto-generated method stub\n            bar.setVisibility(View.VISIBLE);\n            updateBarHandler.post(updateThread);\n        }\n    }\n    //使用匿名内部类来复写Handler当中的handMessage方法\n    Handler updateBarHandler = new Handler(){\n        public void handleMessage(Message msg){\n            bar.setProgress(msg.arg1);\n            //线程队列\n            updateBarHandler.post(updateThread);\n        }    \n    };\n    //线程类，该类使用匿名内部类的方式进行声明\n    Runnable updateThread = new Runnable(){\n        int i = 0;\n        public void run(){\n            System.out.println(\"Begin Thread\");\n            i = i + 10;\n            //得到一个消息对象，message类是由android操作系统提供\n            Message msg = updateBarHandler.obtainMessage();\n            //将msg对象的arg1参数的值设置为i，用arg1和arg2这两个成员传递消息\n            msg.arg1 = i;\n            try{\n                //设置当前显示睡眠1秒\n                Thread.sleep(1000);\n            }\n            catch (InterruptedException e){\n                e.printStackTrace();\n            }\n            //将msg对象加入到消息队列当中\n            updateBarHandler.sendMessage(msg);\n            if(i == 100){\n                //如果当i的值为100时，就将线程对象从handle当中移除\n                updateBarHandler.removeCallbacks(updateThread);\n            }\n        }\n    };\n}\n```\n\n&nbsp;\n\n以下的代码是activity_main.xml中的代码\n\n```\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:id=\"@+id/LinearLayout1\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    android:orientation=\"vertical\"\n    tools:context=\"${relativePackage}.${activityClass}\" >\n\n    <ProgressBar\n        android:id=\"@+id/bar\"\n        style=\"?android:attr/progressBarStyleHorizontal\"\n        android:layout_width=\"200dp\"\n        android:layout_height=\"wrap_content\"\n        android:visibility=\"gone\"\n        />\n\n    <Button\n        android:id=\"@+id/startButton\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"\n        android:text=\"start\"\n        />\n</LinearLayout>\n```\n\n&nbsp;\n\n## 11.Handler：线程管理\n\n步骤：\n\n```\n1. 申请一个Handler对象\nHandler handler = new Handler();\n\n2. 创建一个线程\n{继承Thread类或者实现Runnable这个接口}\n使用Runnable创建一个内部匿名类对象updateThread（要复写run方法）\n\n3. 使用handler的post方法将线程加入到线程队列中\nhandler.post(updateThread);\n\n4. 使用handler的removeCallbacks方法移出updateThread线程\n注意：如果线程从线程队列中出来被执行后，则队列中就不在有线程\n因此如果线程在被执行后没有方法将其再次加入到队列中，则无需使用removeCallbacks\n\n线程走出线程队列有两种情况：\n一种是被执行，此时要执行run方法\n一种是使用removeCallbacks方法，此时线程不被执行，因此不调用run\n\n5. 使用handler的postDelayed方法延时将线程加入到队列中\nhandler.postDelayed(updateThread,3000)\n\n```\n\n&nbsp;\n\n以下代码是MainActivity.java中的代码\n\n```\npackage com.example.handler;\n\nimport android.app.Activity;\nimport android.os.Bundle;\nimport android.os.Handler;\nimport android.view.View;\nimport android.view.View.OnClickListener;\nimport android.widget.Button;\n\npublic class MainActivity extends Activity {\n    //声明两个按钮控件\n    private Button StartButton = null;\n    private Button EndButton = null;\n    \n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n        //根据控件的ID得到代表控件的对象，并为这两个按钮设置相应的监听器\n        StartButton = (Button)findViewById(R.id.StartButton);\n        StartButton.setOnClickListener(new  StartButotnListener());\n        EndButton = (Button)findViewById(R.id.EndButton);\n        EndButton.setOnClickListener(new  EndButtonListener());\n    }\n    \n    class StartButotnListener implements OnClickListener{\n\n        @Override\n        public void onClick(View v) {\n            // TODO Auto-generated method stub\n            handler.post(updateThread);\n        }\n    }\n    \n    class EndButtonListener implements OnClickListener{\n\n        @Override\n        public void onClick(View v) {\n            // TODO Auto-generated method stub\n            handler.removeCallbacks(updateThread);\n        }\n    }\n    //创建一个Handler对象\n    Handler handler = new Handler();\n    //将要执行的操作下载线程对象的run方法当中\n    Runnable updateThread = new Runnable(){\n        \n        public void run(){\n            System.out.println(\"UpdateThread\");\n            //在run方法内部，执行postDelayed或者是post方法\n            handler.postDelayed(updateThread,3000);\n        }\n    };\n}\n```\n\n&nbsp;\n\n以下代码是activity_main.xml中的代码\n\n```\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:id=\"@+id/LinearLayout1\"\n    android:layout_width=\"fill_parent\"\n    android:layout_height=\"fill_parent\"\n    android:orientation=\"vertical\"\n    tools:context=\"${relativePackage}.${activityClass}\" >\n\n    \n    \n    <Button\n        android:id=\"@+id/StartButton\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"\n        android:text=\"Start\"\n        />\n    \n    <Button\n        android:id=\"@+id/EndButton\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"\n        android:text=\"End\"\n        />\n\n</LinearLayout>\n```\n\n&nbsp;\n\n以下代码是MainActivity.java中的代码\n\n```\npackage com.example.handlertest;\n\nimport android.app.Activity;\nimport android.os.Bundle;\nimport android.os.Handler;\n\n\npublic class MainActivity extends Activity {\n    \n    private Handler handler = new Handler();\n    \n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        \n        setContentView(R.layout.activity_main);\n        handler.post(r);\n        //Thread t = new Thread(r);\n        //t.start();\n        System.out.println(\"activity--->\"+ Thread.currentThread().getId());\n        System.out.println(\"activityname--->\"+ Thread.currentThread().getName());\n    }\n    \n    Runnable r = new Runnable(){\n        public void run() {\n            System.out.println(\"handler--->\"+ Thread.currentThread().getId());\n            System.out.println(\"handlername--->\"+ Thread.currentThread().getName());\n            try{\n                Thread.sleep(10000);\n            }\n            catch (InterruptedException e)\n            {\n                e.printStackTrace();\n            }\n        }\n    };\n}\n```\n\n&nbsp;\n\n以下代码是activity_main.xml中的代码\n\n```\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:id=\"@+id/LinearLayout1\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    android:orientation=\"vertical\"\n    tools:context=\"${relativePackage}.${activityClass}\" >\n\n    <TextView\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        android:text=\"@string/hello_world\" />\n\n</LinearLayout>\n```\n\n&nbsp;\n","tags":["Android"]},{"title":"Flink学习笔记——Broadcast State","url":"/Flink学习笔记——Broadcast State.html","content":"使用broadcast state实现动态配置更新，即双流：一个数据流，一个配置流\n\n```\nhttps://flink.apache.org/2019/06/26/broadcast-state.html\n\n```\n\n可以参考官方文档\n\n```\nhttps://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/datastream/fault-tolerance/broadcast_state/\n\n```\n\n例子：[Flink使用Broadcast State实现流处理配置实时更新](http://shiyanjun.cn/archives/1857.html)\n\n<!--more-->\n&nbsp;\n","tags":["Flink"]},{"title":"Hadoop学习笔记——namenode","url":"/Hadoop学习笔记——namenode.html","content":"fsimage和edit log，参考：[查看hdfs的fsimage和editlog](https://blog.csdn.net/lxf20054658/article/details/80565670)\n\n[浅谈HDFS的fsimage、edit log与SecondaryNameNode](https://www.jianshu.com/p/75b37c14fb92)\n\n[HDFS- High Availability](https://www.cnblogs.com/sodawoods-blogs/p/8874231.html)\n","tags":["Hadoop"]},{"title":"Mac下安装minikube","url":"/Mac下安装minikube.html","content":"## 1.安装kubectl命令\n\n```\nbrew install kubectl\n\n```\n\n如果想安装指定版本的kubectl\n\n```\ncurl -LO \"https://dl.k8s.io/release/v1.20.0/bin/darwin/amd64/kubectl\"\nchmod +x ./kubectl\nsudo mv ./kubectl /usr/local/bin/kubectl\nsudo chown root: /usr/local/bin/kubectl\n\n```\n\n验证版本，会打印出client端和server端的版本，官方建议2个版本直接版本相差不要大于+/-1\n\n```\nkubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.0\", GitCommit:\"af46c47ce925f4c4ad5cc8d1fca46c7b77d13b38\", GitTreeState:\"clean\", BuildDate:\"2020-12-08T17:59:43Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.7\", GitCommit:\"132a687512d7fb058d0f5890f07d4121b3f0a2e2\", GitTreeState:\"clean\", BuildDate:\"2021-05-12T12:32:49Z\", GoVersion:\"go1.15.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n\n```\n\n　　\n\n## 2.下载和安装minikube\n\n```\n➜  /Users/lintong/Downloads $ curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 68.6M  100 68.6M    0     0  9365k      0  0:00:07  0:00:07 --:--:-- 11.0M\n➜  /Users/lintong/Downloads $ sudo install minikube-darwin-amd64 /usr/local/bin/minikube\n\nPassword:\n\n```\n\n参考\n\n```\nhttps://minikube.sigs.k8s.io/docs/start/\n\n```\n\n如果要安装**指定版本**的话，请到下面的地址下载\n\n```\nhttps://github.com/kubernetes/minikube/releases\n\n```\n\n比如安装v1.21.0版本\n\n```\ncurl -LO https://github.com/kubernetes/minikube/releases/download/v1.21.0/minikube-darwin-amd64\nsudo install minikube-darwin-amd64 /usr/local/bin/minikube\n\n```\n\n卸载minikube\n\n```\nminikube delete\nrm -rf ~/.minikube\n\n```\n\n　　\n\n## 3.安装virtualbox\n\n```\nbrew install virtualbox\n\n```\n\n　　\n\n## 4.使用virtualbox来启动minikube集群\n\n参考：[在Mac上基于virtualbox安装minikube](https://blog.csdn.net/weixin_42072289/article/details/110450042)\n\n<!--more-->\n&nbsp;\n\n```\nminikube start --memory=4096 --driver=virtualbox\n\n```\n\n如果报&nbsp;[Vagrant up failing for VirtualBox provider with E_ACCESSDENIED on host-only network](https://stackoverflow.com/questions/69722254/vagrant-up-failing-for-virtualbox-provider-with-e-accessdenied-on-host-only-netw)，则添加&nbsp;/etc/vbox/networks.conf 文件，如下\n\n```\n192.0.0.0/8\n\n```\n\n且保证在mac的&nbsp;System Preferences -> Security &amp; Privacy -> Allow -> Then allow the software corporation (in this case Oracle)\n\n<img src=\"/images/517519-20220713105929280-675049004.png\" width=\"500\" height=\"166\" loading=\"lazy\" />\n\n&nbsp;\n\n5.使用请参考：[ubuntu16.04安装minikube](https://www.cnblogs.com/tonglin0325/p/5284283.html)\n\n&nbsp;\n","tags":["k8s"]},{"title":"Mac下安装mongodb","url":"/Mac下安装mongodb.html","content":"## 1.添加mongo的仓库\n\n```\nbrew tap mongodb/brew\n```\n\n## 2.安装mongodb\n\n```\nbrew install mongodb-community@4.4\n```\n\n参考：[Mac OSX 平台安装 MongoDB](https://www.runoob.com/mongodb/mongodb-osx-install.html)\n\n安装成功\n\n<img src=\"/images/517519-20230605142917566-1092537610.png\" width=\"600\" height=\"184\" loading=\"lazy\" />\n\n## 3.打开mongo shell\n\n```\n/usr/local/opt/mongodb-community@4.4/bin/mongo xxx:27017/your_db\n\n```\n\n如果是mongo 6.0的话，是没有mongo命令的，需要额外安装mongosh，下载地址\n\n```\nhttps://www.mongodb.com/try/download/shell\n\n```\n\n## 4.查询数据\n\n查看db\n\n```\nshow dbs\n\n```\n\n如果遇到not master and slaveOk=false的报错，可以执行如下命令\n\n```\nrs.slaveOk()\n或者\nrs.secondaryOk()\n\n```\n\n如果遇到Error: Authentication failed的报错，可能是缺少了--authenticationDatabase admin参数\n\n```\n/Users/lintong/Downloads/mongosh-1.9.1-darwin-x64/bin/mongosh mongodb://xxx:27017/test -u xxx --authenticationDatabase admin -p\n\n```\n\n其他参数参考：[https://www.mongodb.com/docs/mongodb-shell/reference/options/](https://www.mongodb.com/docs/mongodb-shell/reference/options/)\n\n查询数据\n\n```\ndb.getCollection(\"your_collection\").find()\n\n```\n\n查询一条数据\n\n```\ndb.getCollection(\"your_collection\").findOne()\n\n```\n\n　　\n\n<!--more-->\n&nbsp;\n","tags":["mongo"]},{"title":"Spark学习笔记——rdd,dataframe和dataset转换","url":"/Spark学习笔记——rdd,dataframe和dataset转换.html","content":"**1.生成RDD**\n\n```\nval rdd: RDD[(String, Int)] = sc.parallelize(Seq((\"cat\", 30), (\"fox\", 40)))\n\n```\n\n**2.生成case class RDD**\n\n```\ncase class WordCount(word: String, count: Long)\nval rdd: RDD[WordCount] = sc.parallelize(Seq(WordCount(\"dog\", 50), WordCount(\"cow\", 60)))\n\n```\n\n**3.rdd转df**，注意这里需要隐式转换\n\n```\nimport spark.implicits._\n\nval df = rdd.toDF()\n\n```\n\n**4.rdd转ds**，注意<!--more-->\n&nbsp;WordCount 需要写在主函数之外\n\n```\nimport spark.implicits._\n\nval ds: Dataset[WordCount]= rdd.toDS()\n\n```\n\n**5.df转ds**\n\n```\nval ds: Dataset[WordCount]= df.as[WordCount]\n\n```\n\n**6.thrift class rdd转df**\n\n```\nval df = spark.createDataFrame(rdd, classOf[MyBean])\n\n```\n\n**7.thrift class df转ds**，需要为thrift class指定encoder\n\n```\nval ds = df.as[MyBean](Encoders.bean(classOf[MyBean]))\n\n```\n\n或者\n\n```\nimplicit val mapEncoder = Encoders.bean(classOf[MyBean])\n\nval ds = df.as[MyBean]\n\n```\n\n**8.avro class rdd转df**，参考\n\n```\nhttps://stackoverflow.com/questions/47264701/how-to-convert-rddgenericrecord-to-dataframe-in-scala\n\n```\n\n使用databricks的spark-avro包\n\n```\n<dependency>\n    <groupId>com.databricks</groupId>\n    <artifactId>spark-avro_2.11</artifactId>\n    <version>3.2.0</version>\n</dependency>\n\n```\n\n工具类，注意包名必须写成 com.databricks.spark.avro\n\n```\npackage com.databricks.spark.avro\n\nimport com.databricks.spark.avro.SchemaConverters.createConverterToSQL\nimport org.apache.avro.Schema\nimport org.apache.spark.sql.types.StructType\n\nobject SchemaConverterUtils {\n\n  def converterSql(schema: Schema, sqlType: StructType) = {\n    createConverterToSQL(schema, sqlType)\n  }\n\n}\n\n```\n\n代码\n\n```\nval sqlType = SchemaConverters.toSqlType(test_serializer.SCHEMA$).dataType.asInstanceOf[StructType]\nval converter = SchemaConverterUtils.converterSql(test_serializer.SCHEMA$, sqlType)\nval rowRdd = rdd.flatMap(record => {\n  Try(converter(record).asInstanceOf[Row]).toOption\n})\nval df = sparkSession.createDataFrame(rowRdd, sqlType)\ndf.show()\n\n```\n\n输出<br /><img src=\"/images/517519-20211225204921345-986265448.png\" width=\"1000\" height=\"120\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["Spark"]},{"title":"使用ajax-interceptor mock数据","url":"/使用ajax-interceptor mock数据.html","content":"ajax-interceptor chrome安装地址\n\n[https://chromewebstore.google.com/detail/ajax-modifier/nhpjggchkhnlbgdfcbgpdpkifemomkpg?hl=zh-CN](https://chromewebstore.google.com/detail/ajax-modifier/nhpjggchkhnlbgdfcbgpdpkifemomkpg?hl=zh-CN)\n\n<!--more-->\n&nbsp;\n\n参考：[前端请装上这个Chrome插件](https://juejin.cn/post/7049211255181017102)\n","tags":["开发工具"]},{"title":"使用tweak插件修改HTTP请求","url":"/使用tweak插件修改HTTP请求.html","content":"**tweak**是一款可以对request请求的response进行修改的浏览器插件，区别于ModHeader只能对header进行修改，tweak可以对请求的request payload（收费）和response payload（免费）进行拦截和修改。\n\n<img src=\"/images/517519-20240327002249517-1643874648.png\" width=\"500\" height=\"424\" loading=\"lazy\" />\n\n下图是tweak的界面\n\n<img src=\"/images/517519-20240326004110052-1646498984.png\" width=\"600\" height=\"278\" loading=\"lazy\" />\n\n同类产品还有**requestly**，**tweak**的修改HTTP请求response功能是免费的**，**而**requestly**修改API response和request body都是需要收费版本才提供支持\n\n参考：[requestly 代理插件](https://blog.csdn.net/zhq426/article/details/127284730)\n\n<img src=\"/images/517519-20240326003745996-1452148544.png\" width=\"400\" height=\"542\" loading=\"lazy\" />\n\nFirefox tweak插件安装地址\n\n[https://addons.mozilla.org/en-US/firefox/addon/tweak-extension/](https://addons.mozilla.org/en-US/firefox/addon/tweak-extension/)\n\nChrome tweak插件安装地址\n\n[https://chromewebstore.google.com/detail/feahianecghpnipmhphmfgmpdodhcapi](https://chromewebstore.google.com/detail/feahianecghpnipmhphmfgmpdodhcapi)\n\ntweak的使用分成**拦截请求**和**修改请求**2个步骤\n\n### 1.拦截请求\n\n点击图中按钮\n\n<img src=\"/images/517519-20240326102407856-871195610.png\" width=\"600\" height=\"237\" loading=\"lazy\" />\n\n在浏览器中请求对应的接口\n\n接下来在URL中输入接口的相关关键字，tweak就能找到这个接口\n\n<img src=\"/images/517519-20240326102704096-1497591470.png\" width=\"600\" height=\"423\" loading=\"lazy\" />\n\n### 2.**修改请求**\n\n修改HTTP请求的response，比如我们将返回json中的技术修改成技术xxxx，产品修改成产品yyyy\n\n然后取消request autocomplete，勾选extension\n\n<img src=\"/images/517519-20240327002602783-1601840038.png\" width=\"600\" height=\"415\" loading=\"lazy\" />\n\n再次刷新页面发起请求，这时我们会发生页面中的信息已经被拦截和修改\n\n<img src=\"/images/517519-20240327002845424-1174517526.png\" width=\"600\" height=\"265\" loading=\"lazy\" />\n\n参考：[方便智能的chrome数据mock插件](https://juejin.cn/post/7069040602385498142)\n","tags":["开发工具"]},{"title":"autox.js使用","url":"/autox.js使用.html","content":"autox.js下载软件地址\n\n[https://github.com/kkevsekk1/AutoX/releases](https://github.com/kkevsekk1/AutoX/releases)\n\n<!--more-->\n&nbsp;\n\n参考：[Autox.js 脚本开发环境搭建，从案例到打包apk（详细流程）](https://juejin.cn/post/7287817398316859433)\n","tags":["开发工具"]},{"title":"Android学习笔记——download","url":"/Android学习笔记——download.html","content":"1.该工程的功能是实现从网上的链接下载一个lrc文件和一个mp3文件\n\n以下代码是MainActivity.java中的代码\n\n```\npackage com.example.download;\n\n\nimport com.example.utils.HttpDownloader;\n\nimport android.app.Activity;\nimport android.os.Bundle;\nimport android.view.View;\nimport android.view.View.OnClickListener;\nimport android.widget.Button;\n\npublic class MainActivity extends Activity {\n    /** Called when the activity is first created. */\n    private Button downloadTxtButton;\n    private Button downloadMp3Button;\n    @Override\n    public void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n        downloadTxtButton = (Button)findViewById(R.id.downloadTxt);\n        downloadTxtButton.setOnClickListener(new DownloadTxtListener());\n        downloadMp3Button = (Button)findViewById(R.id.downloadMp3);\n        downloadMp3Button.setOnClickListener(new DownloadMp3Listener());\n    }\n    \n    class DownloadTxtListener implements OnClickListener{\n        public void onClick(View v) {\n        HttpDownloader httpDownloader = new HttpDownloader();\n        int lrc = httpDownloader.downFile(\"http://play.baidu.com/data2/lrc/121017633/121017633.lrc\", \"voa/\", \"1201250291414036861128.lrc\");\n        System.out.println(lrc);\n        } \n    }\n    \n    class DownloadMp3Listener implements OnClickListener{\n        public void onClick(View v) {\n        // TODO Auto-generated method stub\n        HttpDownloader httpDownloader = new HttpDownloader();\n        int result = httpDownloader.downFile(\"http://cdn.y.baidu.com/yinyueren/532bfdb43a336a584533ff61a7289503.mp3\", \"voa/\", \"江南.mp3\");\n        System.out.println(result);\n        }\n    }\n    \n}\n```\n\n<!--more-->\n&nbsp;\n\n以下代码是FileUtils.java中的代码\n\n```\npackage com.example.utils;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport android.os.Environment;\n\npublic class FileUtils {\n    private String SDPATH;\n    public String getSDPATH() {\n    return SDPATH;\n    }\n    \n    public FileUtils() {\n     //得到当前外部存储设备的目录\n     // /SDCARD\n     SDPATH = Environment.getExternalStorageDirectory() + \"/\";\n    }\n    \n    /**\n     * 在SD卡上创建文件\n     * \n     * @throws IOException\n     */\n    public File creatSDFile(String fileName) throws IOException {\n        File file = new File(SDPATH + fileName);\n        file.createNewFile();\n        return file;\n    }\n \n    /**\n     * 在SD卡上创建目录\n     * \n     * @param dirName\n     */\n    public File creatSDDir(String dirName) {\n        File dir = new File(SDPATH + dirName);\n        dir.mkdirs();\n        return dir;\n    }\n    \n    /**\n     * 判断SD卡上的文件夹是否存在\n     */\n    public boolean isFileExist(String fileName){\n        File file = new File(SDPATH + fileName);\n        return file.exists();\n    }\n \n    /**\n     * 将一个InputStream里面的数据写入到SD卡中\n     */\n    public File write2SDFromInput(String path,String fileName,InputStream input){\n        File file = null;\n        OutputStream output = null;\n        try{\n            creatSDDir(path);\n            file = creatSDFile(path + fileName);\n            output = new FileOutputStream(file);\n            byte buffer [] = new byte[4 * 1024];\n            int len=-1;\n            while((len=input.read(buffer)) != -1){\n                //在这里使用另一个重载，防止流写入的问题.\n                output.write(buffer,0,len);\n            }\n            output.flush();\n        }\n        catch(Exception e){\n            e.printStackTrace();\n        }\n        finally{\n                try{\n                    output.close();\n                }    \n                catch(Exception e){\n                    e.printStackTrace();\n                }\n        }\n        return file;\n    }\n}\n```\n\n&nbsp;\n\n以下代码是HttpDownloader.java中的代码\n\n```\npackage com.example.utils;\n\nimport java.io.BufferedReader;\nimport java.io.File;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.net.HttpURLConnection;\nimport java.net.MalformedURLException;\nimport java.net.URL;\n\npublic class HttpDownloader {\n    private URL url = null;\n    /**\n     * 根据URL下载文件，前提是这个文件当中的内容是文本，函数的返回值就是文件当中的内容\n     * 1.创建一个URL对象\n     * 2.通过URL对象，创建一个HttpURLConnection对象\n     * 3.得到InputStram\n     * 4.从InputStream当中读取数据\n     * @param urlStr\n     * @return\n     */\n    \n    public String download(String urlStr) {\n        StringBuffer sb = new StringBuffer();\n        String line = null;\n        BufferedReader buffer = null;\n        try {\n            // 创建一个URL对象\n            url = new URL(urlStr);\n            // 创建一个Http连接\n            HttpURLConnection urlConn = (HttpURLConnection) url\n                    .openConnection();\n            // 使用IO流读取数据\n            buffer = new BufferedReader(new InputStreamReader(urlConn\n                    .getInputStream()));\n            while ((line = buffer.readLine()) != null) {\n                sb.append(line);\n            }\n        } \n        catch (Exception e) {\n            e.printStackTrace();\n        } \n        finally {\n            try {\n                buffer.close();\n            } \n            catch (Exception e) {\n                e.printStackTrace();\n            }\n        }\n        return sb.toString();\n    }\n \n    /**\n     * 该函数返回整形 -1：代表下载文件出错 0：代表下载文件成功 1：代表文件已经存在\n     */\n    public int downFile(String urlStr, String path, String fileName) {\n        InputStream inputStream = null;\n        try {\n            FileUtils fileUtils = new FileUtils();\n   \n            if (fileUtils.isFileExist(path + fileName)) {\n                return 1;\n            } \n            else {\n                inputStream = getInputStreamFromUrl(urlStr);\n                File resultFile = fileUtils.write2SDFromInput(path,fileName, inputStream);\n                if (resultFile == null) {\n                    return -1;\n                }\n            }\n        } \n        catch (Exception e) {\n            e.printStackTrace();\n            return -1;\n        } \n        finally {\n            try {\n                inputStream.close();\n            } \n            catch (Exception e) {\n                e.printStackTrace();\n            }\n        }\n        return 0;\n    }\n \n    /**\n     * 根据URL得到输入流\n     * \n     * @param urlStr\n     * @return\n     * @throws MalformedURLException\n     * @throws IOException\n     */\n    public InputStream getInputStreamFromUrl(String urlStr)\n        throws MalformedURLException, IOException {\n        url = new URL(urlStr);\n        HttpURLConnection urlConn = (HttpURLConnection) url.openConnection();\n        InputStream inputStream = urlConn.getInputStream();\n        return inputStream;\n    }\n}\n```\n\n&nbsp;\n\n以下代码是activity_main.xml中的代码\n\n```\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:id=\"@+id/LinearLayout1\"\n    android:layout_width=\"fill_parent\"\n    android:layout_height=\"fill_parent\"\n    android:orientation=\"vertical\"\n    tools:context=\"${relativePackage}.${activityClass}\" >\n\n    <TextView\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"\n        android:text=\"@string/hello_world\" />\n    \n    <Button\n        android:id=\"@+id/downloadTxt\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"\n        android:text=\"downloadTxt\"\n        />\n    \n    <Button\n        android:id=\"@+id/downloadMp3\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"\n        android:text=\"downloadMp3\"\n        />\n\n</LinearLayout>\n```\n\n&nbsp;\n\n以下代码是AndroidManifest.xml中的代码\n\n```\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    package=\"com.example.download\"\n    android:versionCode=\"1\"\n    android:versionName=\"1.0\" >\n\n    <uses-sdk\n        android:minSdkVersion=\"8\"\n        android:targetSdkVersion=\"21\" />\n\n    <application\n        android:allowBackup=\"true\"\n        android:icon=\"@drawable/ic_launcher\"\n        android:label=\"@string/app_name\"\n        android:theme=\"@style/AppTheme\" >\n        <activity\n            android:name=\".MainActivity\"\n            android:label=\"@string/app_name\" >\n            <intent-filter>\n                <action android:name=\"android.intent.action.MAIN\" />\n\n                <category android:name=\"android.intent.category.LAUNCHER\" />\n            </intent-filter>\n        </activity>\n    </application>\n    <uses-sdk android:minSdkVersion=\"8\" />  \n    <!-- 访问网络和操作SD卡 加入的两个权限配置-->  \n    <uses-permission android:name=\"android.permission.INTERNET\"/>\n    <uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\"/>\n    \n</manifest>\n```\n\n&nbsp;\n\n2.该工程实现下载一个xml文件，并解析\n\n转自[http://blog.csdn.net/sam_zhang1984](http://blog.csdn.net/sam_zhang1984)\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;XML&nbsp;解析主要需要进行下列步骤：\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;创建事件处理程序\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;创建&nbsp;SAX&nbsp;解析器\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将事件处理程序分配给解析器\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对文档进行解析，将每个事件发送给处理程序。\n\n<img src=\"/images/191958568736466.jpg\" alt=\"\" width=\"580\" height=\"435\" />\n\n&nbsp;\n\n以下的代码是MainActivity.java中的代码\n\n```\npackage com.example.xml;\n\nimport java.io.StringReader;\n\nimport javax.xml.parsers.SAXParserFactory;\n\nimport com.example.utils.HttpDownloader;\n\nimport org.xml.sax.InputSource;\nimport org.xml.sax.XMLReader;\n\nimport android.app.Activity;\nimport android.os.Bundle;\nimport android.view.View;\nimport android.view.View.OnClickListener;\nimport android.widget.Button;\n\npublic class MainActivity extends Activity {\n    /** Called when the activity is first created. */\n    private Button parseButton ;\n    @Override\n    public void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n        parseButton = (Button)findViewById(R.id.parseButton);\n        parseButton.setOnClickListener(new ParseButtonListener());\n    }\n    \n    class ParseButtonListener implements OnClickListener{\n\n        public void onClick(View v) {\n            HttpDownloader hd = new HttpDownloader();\n            String resultStr = hd.download(\"http://192.168.1.107:8081/voa1500/test.xml\");\n            System.out.println(resultStr);\n            try{\n                //创建一个SAXParserFactory\n                SAXParserFactory factory = SAXParserFactory.newInstance();\n                XMLReader reader = factory.newSAXParser().getXMLReader();\n                //为XMLReader设置内容处理器\n                reader.setContentHandler(new MyContentHandler());\n                //开始解析文件\n                reader.parse(new InputSource(new StringReader(resultStr)));\n            }\n            catch(Exception e){\n                e.printStackTrace();\n            }\n        }\n        \n    }\n}\n```\n\n&nbsp;\n\n以下的代码是MyContentHandler.java中的代码\n\n```\npackage com.example.xml;\n\nimport org.xml.sax.Attributes;\nimport org.xml.sax.SAXException;\nimport org.xml.sax.helpers.DefaultHandler;\n\npublic class MyContentHandler extends DefaultHandler {\n    String hisname, address, money, sex, status;\n    String tagName;\n\n    public void startDocument() throws SAXException {\n        System.out.println(\"````````begin````````\");\n    }\n\n    public void endDocument() throws SAXException {\n        System.out.println(\"````````end````````\");\n    }\n\n    public void startElement(String namespaceURI, String localName,\n            String qName, Attributes attr) throws SAXException {\n        tagName = localName;\n        if (localName.equals(\"worker\")) {\n            //获取标签的全部属性\n            for (int i = 0; i < attr.getLength(); i++) {\n                System.out.println(attr.getLocalName(i) + \"=\" + attr.getValue(i));\n            }\n        }\n    }\n\n    public void endElement(String namespaceURI, String localName, String qName)\n            throws SAXException {\n        //在workr标签解析完之后，会打印出所有得到的数据\n        tagName = \"\";\n        if (localName.equals(\"worker\")) {\n            this.printout();\n        }\n    }\n    public void characters(char[] ch, int start, int length)\n            throws SAXException {\n        if (tagName.equals(\"name\"))\n            hisname = new String(ch, start, length);\n        else if (tagName.equals(\"sex\"))\n            sex = new String(ch, start, length);\n        else if (tagName.equals(\"status\"))\n            status = new String(ch, start, length);\n        else if (tagName.equals(\"address\"))\n            address = new String(ch, start, length);\n        else if (tagName.equals(\"money\"))\n            money = new String(ch, start, length);\n    }\n\n    private void printout() {\n        System.out.print(\"name: \");\n        System.out.println(hisname);\n        System.out.print(\"sex: \");\n        System.out.println(sex);\n        System.out.print(\"status: \");\n        System.out.println(status);\n        System.out.print(\"address: \");\n        System.out.println(address);\n        System.out.print(\"money: \");\n        System.out.println(money);\n        System.out.println();\n    }\n\n}\n```\n\n&nbsp;\n\n以下的代码是FileUtils.java中的代码\n\n```\npackage com.example.utils;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport android.os.Environment;\n\npublic class FileUtils {\n    private String SDPATH;\n    public String getSDPATH() {\n    return SDPATH;\n    }\n    \n    public FileUtils() {\n     //得到当前外部存储设备的目录\n     // /SDCARD\n     SDPATH = Environment.getExternalStorageDirectory() + \"/\";\n    }\n    \n    /**\n     * 在SD卡上创建文件\n     * \n     * @throws IOException\n     */\n    public File creatSDFile(String fileName) throws IOException {\n        File file = new File(SDPATH + fileName);\n        file.createNewFile();\n        return file;\n    }\n \n    /**\n     * 在SD卡上创建目录\n     * \n     * @param dirName\n     */\n    public File creatSDDir(String dirName) {\n        File dir = new File(SDPATH + dirName);\n        dir.mkdirs();\n        return dir;\n    }\n    \n    /**\n     * 判断SD卡上的文件夹是否存在\n     */\n    public boolean isFileExist(String fileName){\n        File file = new File(SDPATH + fileName);\n        return file.exists();\n    }\n \n    /**\n     * 将一个InputStream里面的数据写入到SD卡中\n     */\n    public File write2SDFromInput(String path,String fileName,InputStream input){\n        File file = null;\n        OutputStream output = null;\n        try{\n            creatSDDir(path);\n            file = creatSDFile(path + fileName);\n            output = new FileOutputStream(file);\n            byte buffer [] = new byte[4 * 1024];\n            int len=-1;\n            while((len=input.read(buffer)) != -1){\n                //在这里使用另一个重载，防止流写入的问题.\n                output.write(buffer,0,len);\n            }\n            output.flush();\n        }\n        catch(Exception e){\n            e.printStackTrace();\n        }\n        finally{\n                try{\n                    output.close();\n                }    \n                catch(Exception e){\n                    e.printStackTrace();\n                }\n        }\n        return file;\n    }\n}\n```\n\n&nbsp;\n\n以下的代码是HttpDownloader.java中的代码\n\n```\npackage com.example.utils;\n\nimport java.io.BufferedReader;\nimport java.io.File;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.net.HttpURLConnection;\nimport java.net.MalformedURLException;\nimport java.net.URL;\n\npublic class HttpDownloader {\n    private URL url = null;\n    /**\n     * 根据URL下载文件，前提是这个文件当中的内容是文本，函数的返回值就是文件当中的内容\n     * 1.创建一个URL对象\n     * 2.通过URL对象，创建一个HttpURLConnection对象\n     * 3.得到InputStram\n     * 4.从InputStream当中读取数据\n     * @param urlStr\n     * @return\n     */\n    \n    public String download(String urlStr) {\n        StringBuffer sb = new StringBuffer();\n        String line = null;\n        BufferedReader buffer = null;\n        try {\n            // 创建一个URL对象\n            url = new URL(urlStr);\n            // 创建一个Http连接\n            HttpURLConnection urlConn = (HttpURLConnection) url\n                    .openConnection();\n            // 使用IO流读取数据\n            buffer = new BufferedReader(new InputStreamReader(urlConn\n                    .getInputStream()));\n            while ((line = buffer.readLine()) != null) {\n                sb.append(line);\n            }\n        } \n        catch (Exception e) {\n            e.printStackTrace();\n        } \n        finally {\n            try {\n                buffer.close();\n            } \n            catch (Exception e) {\n                e.printStackTrace();\n            }\n        }\n        return sb.toString();\n    }\n \n    /**\n     * 该函数返回整形 -1：代表下载文件出错 0：代表下载文件成功 1：代表文件已经存在\n     */\n    public int downFile(String urlStr, String path, String fileName) {\n        InputStream inputStream = null;\n        try {\n            FileUtils fileUtils = new FileUtils();\n   \n            if (fileUtils.isFileExist(path + fileName)) {\n                return 1;\n            } \n            else {\n                inputStream = getInputStreamFromUrl(urlStr);\n                File resultFile = fileUtils.write2SDFromInput(path,fileName, inputStream);\n                if (resultFile == null) {\n                    return -1;\n                }\n            }\n        } \n        catch (Exception e) {\n            e.printStackTrace();\n            return -1;\n        } \n        finally {\n            try {\n                inputStream.close();\n            } \n            catch (Exception e) {\n                e.printStackTrace();\n            }\n        }\n        return 0;\n    }\n \n    /**\n     * 根据URL得到输入流\n     * \n     * @param urlStr\n     * @return\n     * @throws MalformedURLException\n     * @throws IOException\n     */\n    public InputStream getInputStreamFromUrl(String urlStr)\n        throws MalformedURLException, IOException {\n        url = new URL(urlStr);\n        HttpURLConnection urlConn = (HttpURLConnection) url.openConnection();\n        InputStream inputStream = urlConn.getInputStream();\n        return inputStream;\n    }\n}\n```\n\n&nbsp;\n\n以下的代码是activity_main.xml中的代码\n\n```\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:orientation=\"vertical\"\n    android:layout_width=\"fill_parent\"\n    android:layout_height=\"fill_parent\"\n    >\n    \n    <TextView\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"wrap_content\"\n        android:text=\"@string/hello_world\" />\n\n    <Button\n         android:id=\"@+id/parseButton\"\n        android:layout_width=\"fill_parent\" \n        android:layout_height=\"wrap_content\" \n        android:text=\"开始解析XML\"/>\n</LinearLayout>\n```\n\n&nbsp;\n","tags":["Android"]},{"title":"CDH5.16安装flink1.10.0","url":"/CDH5.16安装flink1.10.0.html","content":"1.采用parcels包的方式来安装flink\n\n编译parcels请参考：[制作Flink的Parcel包和csd文件](https://blog.csdn.net/weixin_43215250/article/details/105578244)\n\n[CDH5.16.1 集成 Flink-1.10.0](https://blog.csdn.net/haozhuxuan/article/details/109540372)\n\n[cdh5.15.1集成flink说明](https://www.jianshu.com/p/8f2f0234d9e9)\n\n2.将parcels包拷贝到对应目录\n\n```\nlintong@master:/opt/cloudera/parcel-repo$ ls | grep FLINK\nFLINK-1.10.0-BIN-SCALA_2.11-xenial.parcel\nFLINK-1.10.0-BIN-SCALA_2.11-xenial.parcel.sha\n\n```\n\ncsd包\n\n```\nlintong@master:/opt/cloudera/csd$ ls\nFLINK_ON_YARN-1.10.0.jar\n\n```\n\n3.安装\n\n<img src=\"/images/517519-20211114193124313-1296617760.png\" width=\"600\" height=\"154\" loading=\"lazy\" />\n\n配置主机\n\n<img src=\"/images/517519-20211114193252475-463401496.png\" width=\"600\" height=\"195\" loading=\"lazy\" />\n\n配置flink集群参数\n\n<img src=\"/images/517519-20211114193424573-822503791.png\" width=\"400\" height=\"597\" loading=\"lazy\" />\n\n<img src=\"/images/517519-20211114193519659-1452153686.png\" width=\"400\" height=\"499\" loading=\"lazy\" />\n\n部署成功\n\n<img src=\"/images/517519-20211114193629523-959854831.png\" width=\"700\" height=\"233\" loading=\"lazy\" />\n\nflink集群部署后会在YARN上启动一个以flink用户运行的flink session，注意此时在此flink session中是不能认证成其他用户的，只能是flink用户，可以参考：[Flink on Yarn with Kerberos](https://zhuanlan.zhihu.com/p/144557388)\n\n比如使用flink-scala-shell就会运行在此flink session中，而在里面认证是不会生效\n\n<img src=\"/images/517519-20211114193915111-881483852.png\" width=\"800\" height=\"174\" loading=\"lazy\" />\n\n部署成功\n\n<img src=\"/images/517519-20211114214807025-1985449779.png\" width=\"400\" height=\"190\" loading=\"lazy\" />\n\n参考：[Cloudera Manager中安装部署Flink服务](https://www.jianshu.com/p/bcfd2c62f6bf)\n\n<!--more-->\n&nbsp;\n\n4.遇到的一些问题：\n\n其中kerberos相关的参数如果集群没有启用kerberos的话可以不填写\n\n如果有开启kerberos的话，需要在首先在os上添加flink用户\n\n```\nsudo groupadd flink\nsudo useradd flink -g flink -r --no-log-init -d /var/lib/flink\nsudo mkdir /var/lib/flink\nsudo chown flink:flink /var/lib/flink\n\n```\n\n然后创建生成flink用户的keytab\n\n如果集成了ldap，需要在ldap中创建flink Group和People\n\n<img src=\"/images/517519-20211114201213597-1096949061.png\" width=\"200\" height=\"374\" loading=\"lazy\" /><img src=\"/images/517519-20211114201318041-222171918.png\" width=\"200\" height=\"251\" loading=\"lazy\" />\n\n如果重启flink集群的时候遇到\n\n```\nsecurity.UserGroupInformation: PriviledgedActionException as:flink (auth:KERBEROS) cause:javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n\n```\n\n需要先使用flink用户进行kinit认证\n\n```\nsudo -u flink kinit -kt /etc/keytab/flink.keytab flink/master@HADOOP.COM\n\n```\n\n启动flink-scala-shell验证\n\n```\nlintong@master:/opt/cloudera/parcels/FLINK/lib/flink/bin$ ./start-scala-shell.sh yarn\n\n```\n\n如果遇到\n\n```\nException in thread \"main\" java.lang.NoSuchMethodError: jline.console.completer.CandidateListCompletionHandler.setPrintSpaceAfterFullCompletion(Z\n\n```\n\n请下载 jline-2.14.3.jar包 到flink的lib目录\n\n```\nlintong@master:/opt/cloudera/parcels/FLINK/lib/flink/lib$ wget https://repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar\n\n```\n\n如果遇到\n\n```\nError: A JNI error has occurred, please check your installation and try again\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/hadoop/yarn/exceptions/YarnException\n    at java.lang.Class.getDeclaredMethods0(Native Method)\n    at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)\n    at java.lang.Class.privateGetMethodRecursive(Class.java:3048)\n    at java.lang.Class.getMethod0(Class.java:3018)\n    at java.lang.Class.getMethod(Class.java:1784)\n    at sun.launcher.LauncherHelper.validateMainClass(LauncherHelper.java:544)\n    at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:526)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.exceptions.YarnException\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n    ... 7 more\n\n```\n\n请下载 flink-shaded-hadoop-2-uber-2.7.5-7.0.jar包，放到flink的lib目录下\n\n```\nlintong@master:/opt/cloudera/parcels/FLINK/lib/flink/lib$ wget https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.7.5-7.0/flink-shaded-hadoop-2-uber-2.7.5-7.0.jar\n\n```\n\n读取文件并打印\n\n```\nscala> val dataSet = benv.readTextFile(\"hdfs:///user/hive/warehouse/test/000000_0\")\nscala> dataSet.print()\n\n```\n\n&nbsp;<img src=\"/images/517519-20211114212659369-112734716.png\" width=\"600\" height=\"155\" loading=\"lazy\" />\n\n&nbsp;\n","tags":["CDH"]},{"title":"毕业","url":"/毕业.html","content":"　想想终于要离开武汉了\n","tags":["杂谈"]},{"title":"Datagrip添加big data tools","url":"/Datagrip添加big data tools.html","content":"1.datagrip插件中安装Big Data Tools\n\n<img src=\"/images/517519-20240413143416790-27660139.png\" width=\"800\" height=\"310\" loading=\"lazy\" />\n\n2.big data tool支持amazon s3，alibaba oss(需要datagrip高版本)\n\n<img src=\"/images/517519-20240413144808769-145425643.png\" width=\"400\" height=\"263\" loading=\"lazy\" />\n\n添加connection\n\n<img src=\"/images/517519-20240413145236130-352387875.png\" width=\"400\" height=\"415\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n","tags":["开发工具"]},{"title":"Android常用命令","url":"/Android常用命令.html","content":"## 1.查看Android手机的CPU架构\n\n```\n$ getprop ro.product.cpu.abi\narm64-v8a\n\n```\n\ncpu架构表格\n\n\n\n|架构|Tag|说明\n| ---- | ---- | ---- \n|armeabi|linux/arm/v5或者linux/arm/v6|第5代第6代的ARM处理器，早期手机用的比较多。比如ARM9，ARM11系列**（08年左右的手机，如HTC Hero，使用骁龙MSM7200A处理器，基于 ARM11 架构）**等\n|armeabi-v7a|linux/arm/v7|第七代及以上的ARM处理器。比如骁龙400系列（12年左右的手机）。\n|arm64-v8a|linux/arm64|第8代64位ARM处理器。`比如 ``2016年` 之后中高端的手机，比如 `骁龙8系列`、`麒麟9系列`、`联发科1000+` 等。\n|x86|linux/386|32位的桌面PC电脑CPU。`平板`、`模拟器`用得比较多。\n|x86_64|linux/amd64|64位桌面PC电脑CPU。`64` 位的平板。\n\n<!--more-->\n&nbsp;\n\n## 2.termux更换源\n\n```\ntermux-change-repo\n\n```\n\n参考：[https://mirrors.tuna.tsinghua.edu.cn/help/termux/](https://mirrors.tuna.tsinghua.edu.cn/help/termux/)\n","tags":["Android"]},{"title":"Python爬虫——selenium IDE","url":"/Python爬虫——selenium IDE.html","content":"## 1.在Chrome浏览器中安装selenium IDE插件\n\n[https://chromewebstore.google.com/detail/selenium-ide/mooikfkahbdckldjjndioackbalphokd](https://chromewebstore.google.com/detail/selenium-ide/mooikfkahbdckldjjndioackbalphokd)\n\n## 2.录制步骤\n\n创建项目\n\n<img src=\"/images/517519-20240413145955021-2114551555.png\" width=\"400\" height=\"504\" loading=\"lazy\" />\n\n输入网址\n\n<img src=\"/images/517519-20240413150100487-1941447417.png\" width=\"400\" height=\"499\" loading=\"lazy\" />\n\n开始点击页面，停止并保存\n\n<img src=\"/images/517519-20240413150254637-102462791.png\" width=\"800\" height=\"330\" loading=\"lazy\" />\n\n可以使用export将其导出成python等语言的代码\n\n<img src=\"/images/517519-20240413151046086-891145290.png\" width=\"800\" height=\"444\" loading=\"lazy\" />\n\n注意在Tests下是可以export的，executing是不可以的\n\n<img src=\"/images/517519-20240413150919676-15359832.png\" width=\"200\" height=\"213\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n","tags":["Python"]},{"title":"YARN学习笔记——调度器","url":"/YARN学习笔记——调度器.html","content":"YARN有3种调度器：FIFO Scheduler，Capacity Scheduler(容器调度器) 和 FairS cheduler(公平调度器)。cloudera官方推荐使用Capacity Scheduler，而且在cloudera的CDP中，只保留了Capacity Scheduler这一种调度器。\n\n**1.FIFO Scheduler**：FIFO Scheduler把应用按提交的顺序排成一个队列，这是一个先进先出队列，在进行资源分配的时候，先给队列中最头上的应用进行分配资源，待最头上的应用需求满足后再给下一个分配，以此类推。在FIFO 调度器中，小任务会被大任务阻塞。\n\n<img src=\"/images/517519-20230928000638981-1103889244.png\" width=\"300\" height=\"234\" loading=\"lazy\" />\n\n**<strong>2.Capacity Scheduler（容量调度器）**：</strong>而对于Capacity调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO调度器时的时间。\n\n1. 多队列：每个队列可配置一定的资源量，每个队列采用FIFO调度策略。\n1. 容量保证：管理员可为每个队列设置资源最低保证和资源使用上限。\n1. 灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用 程序提交，则其他队列借调的资源会归还给该队列。\n1. 多租户：支持多用户共享集群(如图中queueC，配置租户ss、cls)和多应用程序同时运行；为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定。\n\n参考：[hadoop3 Yarn容量(Capacity Scheduler)调度器和公平(Fair Scheduler)调度器配置](https://cloud.tencent.com/developer/article/2022577)\n\n<!--more-->\n&nbsp;\n\n<img src=\"/images/517519-20230928001008684-1731295468.png\" width=\"300\" height=\"263\" loading=\"lazy\" />\n\n&nbsp;\n\n**3.Fair Scheduler（公平调度器）**：\n\n在Fair调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。\n\n如下图所示，当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。\n\n需要注意的是，在下图Fair调度器中，从第二个任务提交到获得资源会有一定的延迟，因为它需要等待第一个任务释放占用的Container。小任务执行完成之后也会释放自己占用的资源，大任务又获得了全部的系统资源。最终的效果就是Fair调度器即得到了高的资源利用率又能保证小任务及时完成。\n\n<img src=\"/images/517519-20230928001038074-965456463.png\" width=\"300\" height=\"235\" loading=\"lazy\" />\n\n参考：[Yarn队列调度介绍](https://www.jianshu.com/p/baebf26c4ed5)\n","tags":["YARN"]},{"title":"mac挂载linux的磁盘","url":"/mac挂载linux的磁盘.html","content":"到了2022年，已经不能使用brew安装sshfs来挂载linux的磁盘了，但是可以通过安装包的方式来安装，会报错\n\n```\nerror: sshfs has been disabled because it requires closed-source macFUSE!\n\n```\n\n首先需要安装sshfs，版本是SSHFS 2.5.0\n\n```\nhttps://osxfuse.github.io/2014/02/03/SSHFS-2.5.0.html\n\n```\n\n其次需要安装macFUSE\n\n```\nhttps://osxfuse.github.io/2022/08/19/macFUSE-4.4.1.html\n\n```\n\n然后就能挂载linux的硬盘了\n\n首先创建目录\n\n```\nmkdir /Users/xxx/data01\n\n```\n\n挂载linux的/data01目录到刚刚创建的目录\n\n```\nsshfs -C -o reconnect master:/data01 /Users/xxx/data01\n\n```\n\n然后就能在Finder中查看挂载的linux目录\n\n<img src=\"/images/517519-20221117230400132-1018731066.png\" width=\"100\" height=\"89\" loading=\"lazy\" />\n\n也可以固定到侧边栏\n\n<img src=\"/images/517519-20221117230509479-1640396521.png\" width=\"200\" height=\"283\" loading=\"lazy\" />\n\n要卸载可以使用命令或者直接右键\n\n```\numount /Users/xxx/data01\n\n```\n\n<img src=\"/images/517519-20221117230605890-2093385990.png\" width=\"300\" height=\"158\" loading=\"lazy\" />\n\n<!--more-->\n&nbsp;\n","tags":["mac"]},{"title":"Linux学习笔记","url":"/Linux学习笔记.html","content":"## **1.重启或者关机**\n\n```\nshutdown -h now　　#立刻进行关机\nshutdown -r now或者reboor　　#现在重新启动计算机\n```\n\n## **2.查看本机的IP地址**\n\n```\nifconfig -a\n\n```\n\n**ubuntu修改IP地址和网关**\n\n参考：[ubuntu修改IP地址和网关的方法](https://blog.csdn.net/shenzhen_zsw/article/details/74025066)\n\n## **3.查看内存磁盘CPU**\n\n### **查看内存**\n\n```\nfree -m\n\n```\n\n查看内存及cpu使用情况的命令\n\n```\ntop\n\n```\n\n也可以安装htop工具，这样更直观，安装命令如下\n\n```\nsudo apt-get install htop\n\n```\n\n安装完后，直接输入命令：htop，就可以看到内存或cpu的使用情况了。\n\n### 查看磁盘容量\n\n查看磁盘使用情况\n\n```\ndf -hT\n\n```\n\n查看linux系统分区具体情况　　　　\n\n```\nfdisk -l\n```\n\n### 查看磁盘iotop\n\n```\nsudo iotop -oPa<!--more-->\n&nbsp;\n```\n\n### 清理buffer/caches\n\n```\necho 3 > /proc/sys/vm/drop_caches\n\n```\n\n### 查看机器物理CPU个数\n\n```\ncat /proc/cpuinfo| grep \"physical id\"| sort| uniq| wc -l\n\n```\n\n### 查看单个物理CPU中core的个数(即核数) \n\n```\ncat /proc/cpuinfo| grep \"cpu cores\"| uniq\n\n```\n\n### 查看逻辑CPU的个数\n\n```\ncat /proc/cpuinfo| grep \"processor\"| wc -l\n\n```\n\n### 排查cpu 100%\n\n参考：[线上服务 CPU 100%？一键定位 so easy！](https://cloud.tencent.com/developer/article/1044019)\n\n## 4.查看服务是否是开机自动启动\n\n```\nsystemctl is-enabled nginx\nenabled\n\n```\n\n&nbsp;其他相关命令\n\n```\nsystemctl enable *.service #开机运行服务\nsystemctl disable *.service #取消开机运行\nsystemctl start *.service #启动服务\nsystemctl stop *.service #停止服务\nsystemctl restart *.service #重启服务\nsystemctl reload *.service #重新加载服务配置文件\nsystemctl status *.service #查询服务运行状态\n\n```\n\n### **配置开机自启动**\n\n```\n# 添加开机自启动服务\nchkconfig --add haproxy\nchkconfig haproxy on\n\n```\n\n参考\n\n```\nhttps://docs.cloudera.com/documentation/enterprise/5-15-x/topics/admin_ha_hiveserver2.html\n\n```\n\n## **5.Linux文件系统**\n\n### **常用目录结构**\n\n<img src=\"/images/130105090030541.jpg\" width=\"500\" height=\"244\" />\n\n```\n/bin　　存放常用命令\n/boot　 存放启动程序\n/dev     存放设备文件\n/etc      存放启动，关闭，配置程序与文件\n/home  用户工作根目录\n/lib       存放共享链接库\n/root    超级用户的工作目录\n/sbin    系统管理员的常用管理程序\n/tmp    存放临时文件\n/lost+found  系统出现异常时，用于保存部分资料\n\n```\n\n### **Linux账号与用户组**\n\n**用户识别：UID和GID**\n\n**/etc/passwd记录这用户的账号**\n\n```\ncat /etc/passwd    #查看用户\n\nroot:x:0:0:root:/root:/bin/bash    #UID是0表示这个账号是&rdquo;系统管理员&ldquo;，1~499保留给系统使用的ID，500~65535给一般用户使用\n...\ncommon:x:1000:1000:common,,,:/home/common:/bin/bash\n\n```\n\n/etc/shadow保存的是用户的密码\n\n**关于用户组：有效与初始用户组、groups、newgrp**\n\n```\ncat /etc/group    #输出用户组名称和支持的账号名称（有多个）\n\n```\n\n&nbsp;每个用户可以**拥有多个支持的用户组**，在/etc/passwd里的GID就是&rdquo;**初始用户组（initial group）**&ldquo;，当用户登录系统，立刻就拥有这个用户组的相关权限\n\n```\ngroups    #输出当前登录的用户所在的群组，可以有多个，且第一个是有效用户组\nadm cdrom sudo dip plugdev lpadmin sambashare common wireshark lantern\n\nnewgrp adm    #切换当前用户的有效用户组为adm\n\n```\n\n**新增与删除用户**\n\n-u：接UID　　-g：接初始用户组　　-G：接这个账号还可以支持的用户组　　-Mm：强制不要（要）建立home　　-c：说明内容　　\n\n-d：制定某个目录成为home目录　　-r：建立一个系统账号，这个账号的UID会有限制　　-s：后面接一个shell，默认是/bin/bash\n\n```\nuseradd [-u UID] [-g initial_group] [-G other_group] [-Mm] [-c 说明栏] [-d home] [-s shell] username\n\n```\n\n**passwd命令**\n\n设置密码命令\n\n```\nsudo passwd common    #使用root帮其他用户修改密码\npasswd    #某个用户自己修改密码\n\n```\n\n**usermod命令**\n\n进行账号相关数据的修改\n\n**userdel命令**&nbsp;\n\n删除用户\n\n```\nuserdel -r common    #连同用户的home目录一起删除\n\n```\n\n**finger命令/id命令/w命令/who命令**\n\n查看用户的信息\n\n**新增与删除用户组**\n\ngroupadd命令\n\n```\ngroupadd [-g GID] [-r]    #没有参数建立用户组，-g后面接特定的GID，-r建立系统用户组\n\n```\n\ngroupmod命令\n\n```\ngroupmod [-g GID] [-n group_name] group    #把group用户组的名字改成group_name，id改成GID\n\n```\n\ngroupdel命令\n\n```\ngroupdel [groupname]    #删除用户组\n\n```\n\ngpasswd命令\n\n```\ngpasswd [-ad] user groupname    #在group中加入/删除user\n\n```\n\nnewgrp命令\n\n```\nnewgrp common    #把当前用户的有效用户组修改成common\n\n```\n\n### **修改linux文件夹权限**\n\n如果要将drwxrwxrwx的目录改成drwxrwxr-x\n\n```\nchmod 755 -R ./collections/\nchmod g+w -R ./collections/\n\n```\n\n## **6.配置机器的DNS服务器**\n\n**配置文件地址为 /etc/resolv.conf**\n\n```\nvim /etc/resolv.conf\n\n```\n\n## 7.**防火墙**\n\n### **1.查询防火墙状态**\n\n```\nsudo service ufw status # 防火墙服务进程\nsudo ufw status\n\n```\n\n输出\n\n```\n状态： 激活\n\n至                          动作          来自\n-                          --          --\n389                        ALLOW       Anywhere                  \n22/tcp                     ALLOW       Anywhere                  \n389 (v6)                   ALLOW       Anywhere (v6)             \n22/tcp (v6)                ALLOW       Anywhere (v6)\n\n```\n\n### **2****.关闭防火墙**\n\n```\nsudo service ufw stop\n\n```\n\n### **3.开启防火墙**\n\n**运行以下两条命令后，开启了防火墙，并在系统启动时自动开启**\n\n```\nsudo ufw enable  # 启动防火墙服务进程后，启用防火墙\nsudo ufw default deny # 关闭所有外部对本机的访问,但本机访问外部正常\n\n```\n\n### **4.配置防火墙**\n\n```\n#开启和禁用\nsudo ufw allow|deny [service]\n\n#打开或关闭某个端口,例如:\nsudo ufw allow smtp　　　#允许所有的外部IP访问本机的25/tcp (smtp)端口\nsudo ufw allow 22/tcp 　　#允许所有的外部IP访问本机的22/tcp (ssh)端口\nsudo ufw allow 53 　　#允许外部访问53端口(tcp/udp)\nsudo ufw allow 11200:11299/tcp\nsudo ufw allow from 192.168.1.100 　　#允许此IP访问所有的本机端口\nsudo ufw allow proto udp 192.168.0.1 port 53 to 192.168.0.2 port 53\nsudo ufw deny smtp 　　#禁止外部访问smtp服务\nsudo ufw delete allow smtp 　　#删除上面建立的某条规则\n\n#允许某特定 IP\nsudo ufw allow from xxx.xxx.xx.xxx\n\n# 允许192.168.1.1 到192.168.1.254\nsudo ufw allow from 192.168.1.0/24\n\n#删除 smtp 端口的许可\nsudo ufw delete allow smtp\n\n```\n\nufw日志在/var/log/ufw.log，可以看到端口被扫描的信息\n\n```\nJun 29 10:10:10 xxxxxxx kernel: [3276816.402874] [UFW BLOCK] IN=enp3s0 OUT= MAC=xxxxxxxxxxx SRC=xx.xx.xx.xx DST=xx.xx.xx.xx LEN=228 TOS=0x00 PREC=0x00 TTL=128 ID=1579 PROTO=UDP SPT=61418 DPT=18323 LEN=208\n\n```\n\n参数含义\n\n```\nTOS, for Type of service,\nDST is destination ip,\nSRC is source ip\nTTL is time to live, a small counter decremented each time a packet is passed through another router (so if there is a loop, the package destroy itself once to 0)\nDF is \"don't fragment\" bit, asking to packet to not be fragmented when sent\nPROTO is the protocol (mostly TCP and UDP)\nSPT is the source port\nDPT is the destination port\n\n```\n\n## **8.init命令**\n\n**<img src=\"/images/517519-20160402170210223-2110772998.png\" width=\"300\" height=\"162\" />**\n\n## 9.history命令\n\n显示最近使用的命令\n\n　　history　　history 5\n\n　　!编号　　执行编号480的命令\n\n## 10.tcp/ip命令\n\n<img src=\"/images/517519-20160403222854879-1034896743.png\" width=\"200\" height=\"202\" />\n\n数据包：帧头 ip头 tcp头 app头 数据 帧尾\n\n### ping命令\n\n```\nping www.baidu.com　　#退出shift+ctrl+c\n\n```\n\nping命令一般用于检测网络通与不通，也叫时延，其值越大，速度越慢PING(PacketInternetGrope)，因特网包探索器，用于测试网络连接量的程序。\n\nping发送一个ICMP回声请求消息给目的地并报告是否收到所希望的ICMP回声应答。它是用来检查网络是否通畅或者网络连接速度的命令。\n\n原理：网络上的机器都有唯一确定的IP地址，我们给目标IP地址发送一个数据包，对方就要返回一个同样大小的数据包，根据返回的数据包我们可以确定目标主机的存在，可以初步判断目标主机的操作系统等。&nbsp;\n\n### traceroute命令\n\n追踪路由命令，查看与连接的ip地址之间经历了多少个路由\n\n```\ntraceroute www.baidu.com/ip\n\n```\n\n### netstat命令**<br />**\n\n显示网络统计信息\n\n```\nnetstat -an | more\n\n```\n\n显示进程号　　　　　　　　　　\n\n```\nnetstat -anp | more　　#查看的时候ip地址的foreign address，进程号是最前面的那个\n\n```\n\n查看端口的TCP UDP的连接\n\n```\nsudo netstat -antup\n```\n\n查看已经连接的服务端口（ESTABLISHED）**<br />**\n\n```\nnetstat -a\n\n```\n\n查看所有的服务端口（LISTEN，ESTABLISHED）**<br />**\n\n```\nnetstat -ap\n\n```\n\n查看8080端口，则可以结合grep命令：\n\n```\nnetstat -ap | grep 8080\n\n```\n\n从大到小排序查看进程的tcp连接数\n\n```\nsudo netstat -antp | awk '{print $(7)}' | sort | uniq -c | sort -rn\n\n```\n\n<img src=\"/images/517519-20201209162545708-1157894696.png\" alt=\"\" />\n\n### lsof命令\n\n查看8888端口，则在终端中输入：**<br />**\n\n```\nlsof -i :8888\n\n```\n\n查看用户占用的连接\n\n```\nsudo lsof -u xxxx | wc -l\n\n```\n\n### route命令\n\n查看本机路由表\n\n### mtr命令\n\n查看网络链路连通性，需要sudo\n\n```\nsudo mtr www.baidu.com\n```\n\n### dig命令\n\n查看DNS解析是否正常，查看ANSWER SECTION，如果没有则不正常\n\n```\ndig baidu.com\n\n; <<>> DiG 9.10.6 <<>> baidu.com\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 52394\n;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 512\n;; QUESTION SECTION:\n;baidu.com.\t\t\tIN\tA\n\n;; ANSWER SECTION:\nbaidu.com.\t\t585\tIN\tA\t220.181.38.148\nbaidu.com.\t\t585\tIN\tA\t220.181.38.251\n\n;; Query time: 46 msec\n;; SERVER: 10.0.31.20#53(10.0.31.20)\n;; WHEN: Wed Jun 15 14:47:43 CST 2022\n;; MSG SIZE  rcvd: 70\n\n```\n\n### wget命令\n\n[使用wget整站下载](http://blog.jqian.net/post/wget-download-website.html)\n\n### **检测tcp和udp端口**\n\n```\nnc -uz 127.0.0.1 8080 #udp\nnc -vz 127.0.0.1 8080 # tcp\n\n```\n\n## **11.<strong>任务调度命令**crontab</strong>\n\n<img src=\"/images/517519-20160404120616781-2095208290.png\" width=\"400\" height=\"261\" />\n\n4.&nbsp;<img src=\"/images/517519-20160404121741312-1242741004.png\" alt=\"\" width=\"265\" height=\"61\" />\n\n如果crontab -e进入的是nano，使用下面命令进行切换\n\n```\nexport EDITOR=vim\n\n```\n\n## **12.Linux进程管理**\n\n### **子程序与父程序**\n\n**PID**是进程的ID，**PPID**是其父进程的ID\n\n登录bash之后，就是获取了一个名为bash的PID，在这个环境上所执行的其他命令，就是其子程序\n\n```\ncommon@common-Aspire-4750:~$ ps -l\nF S   UID   PID  PPID  C PRI  NI ADDR SZ WCHAN  TTY          TIME CMD\n0 S  1000  5190  5182  0  80   0 -  2153 wait   pts/1    00:00:00 bash\n0 R  1000  5203  5190  0  80   0 -  1606 -      pts/1    00:00:00 ps\n```\n\n### **ps -aux命令**\n\n**参数：-A：**所有的进程均显示出来，与-e具有同样的作用\n\n**　　　-a：**与终端无关的所有进程\n\n**　　　-u：**有效用户的相关进程\n\n**　　　x：**通常与a参数一起使用，可列出较完整的信息\n\n**输出格式：-l：**较长、较详细地显示该PID的信息\n\n**　　　　　j：**作业的格式，**带有PID、PPID等信息**\n\n**　　　　　-f：**更为完整的输出，**进程树格式显示**\n\n```\nps -l    #将当前属于自己这次登录的PID与相关信息显示出来，以长格式显示\nps aux    #列出当前所有正在内存中的进程\nps -lA    #显示出所有的进程，以长格式显示\nps -axjf    #以进程树的方式显示进程\nps -aux | grep 'lantern'　　#找出和lantern这个服务有关的PID号码，下面是ps命令本身\ncommon    6929  0.0  0.0   6136  2048 pts/1    S+   13:42   0:00 grep --color=auto lantern\n\n```\n\n如果某个进程的CMD后面还有<defunct>，就表示该进程是僵尸进程\n\n造成僵尸进程的原因是，该进程应该已执行完毕，或者是因故应该终止了，但是该进程的父进程却无法完整地结束该进程，而造成那个进程一直存在于内存中\n\n### top命令\n\n**top命令**可以持续检测整个系统的进程工作状态\n\n```\ntop [-d] [-bnp]\n\n```\n\n参数：-d，间隔多少秒输出\n\n　　　-b，将批处理的结果输出到文件\n\n　　　-n，与-b搭配，需要进行几次top的输出结果\n\n　　　-p，指定某些个PID来进行观察\n\n```\ntop -d 2    #每隔两秒输出\ntop -b -d 2 > a.txt    #以批处理的方式输出到文件，每隔两秒，必须加上-b不然会乱码\ntop -d 2 -p6559    #6559是已经存在的PID，每隔两秒查询一次\n\n```\n\ntop主要分为两个画面，上面的画面为整个系统的资源使用状态，有6行：\n\n<img src=\"/images/517519-20161231142909836-70048126.png\" alt=\"\" />\n\n第一行：显示系统时间，上线人数，整体负载（分别表示1、5、10分钟的平均负载，一般不会超过1）\n\n第二行：显示当前观察的进程数量，注意最后的僵尸进程数量\n\n第三行：显示CPU的总负载，id的数值接近于100，表示系统资源使用的很少\n\n第四、五行：表示当前物理内存与虚拟内存（Men和Swap）的使用情况\n\n第六行：输入命令显示状态的地方\n\n**<img src=\"/images/517519-20161231143544898-1949451125.png\" alt=\"\" width=\"749\" height=\"81\" />**\n\n### **pstree命令**\n\n**pstree查看进程的关联性**\n\n**参数：-A：各进程树之间以ASCII字符来连接**\n\n**　　　-p：同时列出每个进程PID**\n\n**　　　-u：同时列出每个进程的所属账号名称**\n\n```\npstree -Aup    #列出当前系统上所有进程树的关联性，并显示PID和用户\n\n```\n\n### centos清理僵尸进程\n\n```\nps -A -o stat,ppid,pid,cmd | grep -e '^[Zz]'\nkill -9 XXX\n\n```\n\n## 13.作业管理\n\n### **&amp;**命令\n\n**&amp; **可以直接将命令放到后台执行\n\n输入命令后，在该命令的最后加上一个&amp;，表示将该命令放到后台，此时会返回一个&ldquo;**作业号**&rdquo;[1]，还有一个PID。\n\n```\ncommon@common-Aspire-4750:~/下载$ tar -cvzf temp.tar.gz temp/ &amp;    \n[1] 5225\ncommon@common-Aspire-4750:~/下载$ temp/\ntemp/b.txt\ntemp/a_copy.txt\ntemp/a.txt\ntemp/a.tar.gz\ntemp/b (复件).txt\n回车\n[1]+  已完成               tar -cvzf temp.tar.gz temp/\n\n```\n\n&nbsp;在后台中执行的命令，如果有stdout和stderr的时候，它的数据依然是输出到屏幕上的，所以要使用**数据流重导向**，将输出传至某个文件中\n\n```\ntar -cvzf temp.tar.gz temp/ >> a.txt &amp;\n\n```\n\n### **Ctrl-z**\n\n**Ctrl-z **可以将&ldquo;当前&rdquo;作业放到后台&ldquo;**暂停**&rdquo;\n\n```\ncommon@common-Aspire-4750:~/下载$ vi a.txt\n\n请按 ENTER 或其它命令继续[1]   已完成               tar -cvzf temp.tar.gz temp/ >> a.txt\n\n按ctrl+z　　#暂停\n\n[2]+  已停止               vi a.txt\ncommon@common-Aspire-4750:~/下载$ \n\n```\n\n### jobs命令\n\n**jobs **观察当前后台作业状态\n\n　　-l：除了列出作业号之外，同时列出PID\n\n　　-r：仅列出正在后台运行（run）的作业\n\n　　-s：仅列出正在后台暂停（stop）的作业\n\n```\ncommon@common-Aspire-4750:~/下载$ jobs -l\n[2]+  5310 停止                  vi a.txt　　#+-符号，+表示默认的作业，在仅输入fg的时候，+的作业会被拿到前台处理\n\n```\n\n### **<strong>fg**命令</strong>\n\n**fg** 将后台作业拿到前台处理\n\n```\ncommon@common-Aspire-4750:~/下载$ fg %2    # %后面跟的是作业号，默认取出+的作业\nvi a.txt\n\n```\n\n### **<strong>bg**命令</strong>\n\n** <strong>bg**</strong> 让作业在后台运行\n\n```\ncommon@common-Aspire-4750:~/下载$ bg %2    \n[2]+ vi a.txt &amp;　　#多了一个&amp;符号\n\n```\n\n### 管理后台作业命令\n\n**kill**命令\n\n参数\n\n```\nHUP     1    终端断线\nINT       2    中断（同 Ctrl + C）\nQUIT    3    退出（同 Ctrl + \\）\nTERM    15    终止\nKILL      9    强制终止\nCONT   18    继续（与STOP相反， fg/bg命令）\nSTOP    19    暂停（同 Ctrl + Z）\n\n```\n\n参考：[Linux命令kill和signal](https://www.cnblogs.com/itech/archive/2012/03/05/2380794.html)\n\n　　-l：列出当前kill能够使用的信号（signal），signal表示指示，使用man 7 signal可以知道\n\n　　-1：重新读取一次参数的设置文件，（类似reload）\n\n　　-2：表示与由键盘输入ctrl-c同样的动作\n\n　　-9：立刻强制删除一个作业\n\n```\nkill -9　进程号　#强力杀死\n\n```\n\n&nbsp;　　-15：以正常的程序方式终止一项作业\n\n**killall**命令\n\n强制终止所有以httpd启动的进程\n\n```\nkillall -9 httpd\n\n```\n\n**xkill**命令\n\n杀死一个窗口程序，在QQ后台无法呼出的时候使用　　&nbsp;\n\n## **14.用w查看有哪些用户登陆**\n\n```\nUSER     TTY      FROM        \nhmy      tty7     :0                   \nhmy      pts/0    :0.0               \nhmy      pts/1    :0.0              \n\ntty7是你的图形界面。\npts/0, pts/1表示你开了两个terminal\n\n```\n\n### 切换控制台\n\nctrl+alt+F1-F7可以切换到7个不同的控制台，F1-F6是字符界面，F7是图形化的界面。\n\n## 15.tar命令\n\n**tar命令**\n\n**-c：创建压缩文件**\n\n**-x：解开压缩文件**\n\n**-t：查看tarfile里的文件，（c/x/t只能有一个，不能同时存在）**\n\n```\ntar -cvf a.tar home/common/下载/temp/    #把文件夹打包，名字是a.tar，不压缩\ntar -cvzf a.tar.gz home/common/下载/temp/    #用gzip压缩,名字是a.tar.gz\ntar -cvjf a.tar.bz2 temp/    　　　　　　　　#用bzip2压缩，文件名是a.tar.bz2\ntar -zxvf XXXXXXX    #解压缩gzip文件\n\n```\n\n**-j：是否需要用bzip2压缩**\n\n**-v：在压缩的过程中显示文件**\n\n**-f：使用文件名，在f之后要立即接文件名，不要再加参数**\n\n**-p：使用源文件的原来属性**\n\n**-P：可以使用绝对路径来压缩**\n\n**-N：比后面接的日期（yyyy/mm/dd）还要新的才会被打包进新建的文件中**\n\n**--exclude FILE：在压缩的过程中，不要将FILE打包**\n\n```\n2.3G Sep 16 15:12 test1.log\ntar -cvzf test1.tar.gz ./test1.log\n23M Sep 16 16:50 test1.tar.gz\n\n```\n\n## **16.linux下进程和线程**\n\n进程：就是正在执行的程序\n\n线程：（1）轻量级的进程\n\n　　　（2）进程有独立的地址空间，线程没有\n\n　　　（3）线程不能独立存储，它是由线程创建的，（fork） thread\n\n　　　&nbsp; (4）线程耗费CPU和内存小于进程\n\n**&nbsp;**\n\n17.　进入mysql的bin目录下，使用./mysqldump\n\n<img src=\"/images/517519-20160404144802031-214013289.png\" alt=\"\" width=\"440\" height=\"72\" />\n\n## 17.重定向命令\n\n```\nls -l > a.txt　　#列表中的内容写入文件a.txt中（覆盖写）\nls -al >> a.txt　　#列表中的内容追加到文件aa.txt的末尾\n\ncat > a.txt    #从键盘输入，重写a.txt\ncat >> a.txt    #从键盘输入，在a.txt的内容后面追加\n\ncat > a.txt < b.txt    #将b.txt的内容写入到a.txt中\ncat >> a.txt < b.txt    #将b.txt的内容追加到a.txt后面\n\ncat >> a.txt << end    #当输入end的时候，该次输入就结束了，否则需要使用ctrl+D\n> start\n> end\n\n```\n\n## 18.more命令\n\nmore命令　　　　显示文件内容，带分页　　空格往下一页翻，pageup往上一页翻\n\nless命令　　　　　显示文件内容带分页\n\ngrep命令　　　　　在文本中查询内容\n\n管道　　　　　　比如　　ls -l /etc/　|　　more　　　　把ls命令返回的目录结果给more命令，使其分页显示\n\n<img src=\"/images/517519-20160404183944531-370316014.png\" alt=\"\" width=\"405\" height=\"254\" />\n\n## 19.搜索命令\n\n**which命令**，寻找&ldquo;执行文件&rdquo;\n\n　　参数：-a，将所有可找到的命令均列出，而不仅仅列出第一个找到的命令名称\n\n```\ncommon@common-Aspire-4750:~$ which passwd\n/usr/bin/passwd\n\n```\n\n**whereis命令**，寻找特定文件\n\n　　参数：-b：只找到二进制文件\n\n　　　　　-m：只找在帮助文件manual路径下的文件\n\n　　　　　-s：只找源文件\n\n　　　　　-u：没有帮助文件的文件\n\n```\ncommon@common-Aspire-4750:~$ whereis passwd    #任何和passwd有关的文件名都会被列出来\npasswd: /usr/bin/passwd /etc/passwd /usr/bin/X11/passwd /usr/share/man/man5/passwd.5.gz /usr/share/man/man1/passwd.1.gz /usr/share/man/man1/passwd.1ssl.gz\n\n```\n\n**locate命令**，直接在后面输入&ldquo;文件的部分名称&rdquo;，就能够得到结果\n\n　　locate查找的速度很快，是根据已经有的数据库/var/lib/slocate/里面的数据所搜索的\n\n```\ncommon@common-Aspire-4750:~$ locate passwd | more\n/etc/passwd\n/etc/passwd-\n\n```\n\n**find命令**，find [PATH] [option] [action]\n\n```\nfind / -name a.java　　　　#从根目录开始以名字来查找\n\n```\n\n<img src=\"/images/517519-20160404184339500-162958223.png\" alt=\"\" width=\"422\" height=\"234\" />\n\n## 20.修改环境变量命令\n\n<img src=\"/images/517519-20160404200418422-666056447.png\" alt=\"\" width=\"423\" height=\"211\" />\n\n## 21.**alias用法**\n\n```\nalias llh='ls -l /home'    #ls -l /home取别名\n```\n\n## 22.在文件夹下面获得root权限\n\n```\nsudo nautilus\n\n```\n\n## 23.挂载磁盘或者ISO文件\n\n```\nmount /mnt/cdrom 光驱的默认挂载路径，也可以是其他路径\nunmount\n\n```\n\nubuntu下安装ISO的Latex\n\n　　在linux下默认的ISO挂载点位于\"/media\"目录，下面是挂载一个ISO文件的方法：<br />　　　　sudo mkdir /media/cdimage&nbsp;&nbsp;＃建立一个文件夹作为ISO挂载点<br />　　　　sudo mount -o loop myfile.iso /media/cdimage&nbsp;&nbsp; ＃挂载ISO文件，使用参数 -o loop<br /><br />　　使用你想挂载的iso文件代替myfile.iso。<br /><br />　　挂载一个镜像文件使之能被写入，使用下面的命令：<br />　　　　sudo mkdir /media/cdimage<br />　　　　sudo mount -o rw,loop myfile.iso /media/cdimage\n\n　　\n\n　　然后<br />　　　　perl install-tl -gui<br />　　\n\n　　卸载镜像文件：<br />　　　　sudo umount /media/cdimage<br />　　　　rmdir /media/cdimage\n\n&nbsp;\n\n　　安装的路径在usr/local/下\n\n## 24.创建软链接\n\nLinux链接分两种，一种被称为**硬链接（Hard Link）**，另一种被称为**符号链接（Symbolic Link）**。默认情况下，ln命令产生硬链接。\n\n【**硬连接**】<br />硬连接指通过索引节点来进行连接。在Linux的文件系统中，保存在磁盘分区中的文件不管是什么类型都给它分配一个编号，称为**索引节点号(Inode Index)**。在Linux中，**多个文件名指向同一索引节点是存在的**。一般这种连接就是硬连接。硬连接的作用是允许一个文件拥有多个有效路径名，这样用户就可以建立硬连接到重要文件，以防止&ldquo;误删&rdquo;的功能。其原因如上所述，因为对应该目录的索引节点有一个以上的连接。只删除一个连接并不影响索引节点本身和其它的连接，只有当最后一个连接被删除后，文件的数据块及目录的连接才会被释放。也就是说，**文件真正删除的条件是与之相关的所有硬连接文件均被删除**。\n\n【**软连接**】<br />另外一种连接称之为**符号连接（Symbolic Link）**，也叫软连接。**软链接文件有类似于Windows的快捷方式**。它实际上是一个特殊的文件。在符号连接中，文件实际上是一个文本文件，其中包含的有另一文件的位置信息。\n\n```\nsudo ln -s /XXX/bin /usr/bin\n\n```\n\n## 25.端口转发\n\n```\nssh -L8090:remote_ip:remote_port remote_host\n```\n\n## 26.linux命令并发执行\n\n先将要执行的命令打印到commend.txt文件中，然后用每50行切一个文件，用10个并发来执行这些文件\n\n```\nrm ./commend.txt*\necho_commend > ./commend.txt\nsplit -l 50 commend.txt commend.txt-\nparallel --gnu -j 10 \"bash\" ::: commend.txt-*\necho \"execute commend success\"\n\n```\n\n## 27.linux防止恶意ssh登录脚本\n\n参考: https://blog.51cto.com/732233048/1694987\n\n```\n#!/bin/bash\n\n#过滤Failed password for root,写入failIP.txt文件\ngrep -E 'Failed password for root from|Failed password for invalid user' /var/log/secure | awk '{print $(NF-3)}' | sort | uniq -c | sort -rn > /root/script/failIP.txt\n\n#失败次数大于100的，将其ip写入/etc/hosts.deny文件\nwhile read failStatus\ndo\n  failTimes=`echo $failStatus | awk '{print $1}'`\n  failIP=`echo $failStatus | awk '{print $2}'`\n  if [ $failTimes -gt 10 ];then\n    denyIP=`grep $failIP /etc/hosts.deny`\n    if [ -z \"$denyIP\" ];then\n      echo \"sshd:$failIP\" >> /etc/hosts.deny\n    fi\n  fi\ndone < /root/script/failIP.txt\n\n```\n\ncrontab -e\n\n```\n30 5 * * * bash /root/script/denyRootSsh.sh > /dev/null 2>&amp;1 &amp;\n\n```\n\n## 28.定制登录motd图案\n\n```\nhttp://www.kammerl.de/ascii/AsciiSignature.php\n\n```\n\n在/etc/profile中添加\n\n```\n#Login Info\nsh /home/lintong/motd/login_bash_ok.sh\n\n```\n\nlogin_bash_ok.sh内容\n\n```\n# * Variables\nuser=$USER\npath=$PWD\nhome=$HOME\n\n# * Check if we're somewhere in /home\nif [ ! -d ${home} ];then\n    return 0\nfi\n\n# * ASCII head\ncat /home/lintong/motd/login_logo.txt\n# * Print Output\necho \" ::::::::::::::::::::::::::::::::::-STATUS-::::::::::::::::::::::::::::::::::\"\nreset_terminal=$(tput sgr0)\ntotal_mem=$(top -n 1 -b | grep \"KiB\\ Mem\" | awk '{ print $4/1024 \" Mb\"}')\necho -e '\\E[32m'\" Total Memmory  :\" $reset_terminal ${total_mem}\ntotal_mem_free=$(top -n 1 -b | grep \"KiB\\ Mem\" | awk '{ print $6/1024 \" Mb\"}')\necho -e '\\E[32m'\" Total Memmory Free :\" $reset_terminal ${total_mem_free}\nbuff_cache_used=$(top -n 1 -b | grep \"KiB\\ Mem\" | awk '{ print $10/1024 \" Mb\"}')\necho -e '\\E[32m'\" Buff Cache Used :\" $reset_terminal ${buff_cache_used}\n```\n\n## 29.如何挂载硬盘\n\n```\nhttps://docs.aws.amazon.com/zh_cn/AWSEC2/latest/UserGuide/add-instance-store-volumes.html\n\n```\n\n重启自动挂卷\n\n```\nhttps://docs.aws.amazon.com/zh_cn/AWSEC2/latest/UserGuide/ebs-using-volumes.html#ebs-mount-after-reboot\n\n```\n\n## 30.查看linux内核版本\n\n```\nuname -a\n\n```\n\n查看可以安装的linux内核\n\n```\napt-cache search linux-image\n\n```\n\n安装内核\n\n```\nsudo apt-get install linux-image-4.9.0-0.bpo.12-amd64\nsudo reboot\n```\n\n## 31.修改主机名\n\n```\nvim /etc/hostname\n\n```\n\n## 32.测网速\n\n```\niperf3\n```\n\n## 33.ubuntu下交换键位\n\n```\nsetxkbmap -option altwin:swap_lalt_lwin\n\n```\n\n参考：[轻松交换键位](https://johnniang.me/archives/swap-keyboard-keys-easily)\n\n## 34.linux下网络配置\n\n<img src=\"/images/517519-20160403234223801-1003877124.png\" alt=\"\" width=\"472\" height=\"186\" />\n\n&nbsp;\n\n<img src=\"/images/517519-20160404100604031-34466247.png\" alt=\"\" width=\"474\" height=\"187\" />\n\n&nbsp;\n\n<img src=\"/images/517519-20160404100653140-415694705.png\" alt=\"\" width=\"472\" height=\"193\" />\n\n## 35.无法获得锁 /var/lib/dpkg/lock\n\n解决办法如下：<br />\n1. 终端输入 ps&nbsp; -aux 直接sudo kill\nPID<br />\n2. 解决：<br />其实这是因为有另外一个程序在运行，导致锁不可用。原因可能是上次运行更新或安装没有正常完成。解决办法是杀死此进程<br />\n\n```\nsudo rm /var/cache/apt/archives/lock\nsudo rm /var/lib/dpkg/lock \n\n```\n\n然后还要记得更新软件信息\n\n## 36.ubuntu下gedit闪退，遇到问题：ERROR:../../gi/pygi-argument.c:1583:_pygi_argument_to_object: code should not be reached 已放弃 (核心已转储)\n\n解决方法:编辑->首选项关闭->插件->取消\"多文件编辑\"\n\n## 37.ubuntu更换源\n\n<img src=\"/images/517519-20180428161012394-1798945937.png\" alt=\"\" />\n\n建议使用ustc.edu的源，其他例如清华的，阿里的连sublime都没有\n\n参考：[ubuntu更改镜像源（软件源）](https://blog.csdn.net/weixin_41762173/article/details/79480832)\n\n## 38.ubuntu右上角时间不显示\n\n重启unity，命令\n\n```\nsudo killall unity-panel-service\n\n```\n\n## 39.mac下制作ubuntu启动盘\n\n参考：[2019-06-27 Mac OS制作Ubuntu的U盘启动盘](https://www.jianshu.com/p/64ef0bd6709c)\n\n## 40.Ubuntu在命令行下将默认语言改为英语\n\n参考：[Ubuntu在命令行下将默认语言改为英语](https://www.jianshu.com/p/af8e2639bd68)\n\n## 41.linux用户设置免密root\n\n```\nsudo vim /etc/sudoers\n\n```\n\n在%sudo&nbsp;&nbsp; ALL=(ALL:ALL) ALL的下面添加如下命令，注意必须在该位置添加\n\n```\nyour_username ALL=(ALL) NOPASSWD:ALL\n\n```\n\n## 42.ubuntu14.04安装了im-switch后系统设置中不见了语言支持\n\n```\nsudo apt-get install language-selector-gnome\n\n```\n\n## 43.删除ibus之后导致系统设置进不了\n\n```\nsudo apt-get instal ubuntu-desktop\n\n```\n\n快捷键调出sogou拼音，默认为\"ctrl+,\"\n\n## 44.ubuntu16.04固定ip\n\n<img src=\"/images/517519-20240209150559295-1255454356.png\" width=\"400\" height=\"329\" loading=\"lazy\" />\n\n## 45./etc/profile、/etc/bashrc、~/.bash_profile、~/.bashrc的区别\n\n**/etc/profile：**此文件为系统的每个用户设置环境信息,当用户第一次登录时,该文件被执行并从/etc/profile.d目录的配置文件中搜集shell的设置. <br />\n\n**/etc/bashrc：**为每一个运行bash shell的用户执行此文件.当bash shell被打开时,该文件被读取<br />\n\n**~/.bash_profile**：每个用户都可使用该文件输入专用于自己使用的shell信息,当用户登录时,该文件仅仅执行一次!默认情况下,他设置一些环境变量,执行用户的.bashrc文件.\n\n**~/.bashrc：**该文件包含专用于你的bash shell的bash信息,当登录时以及每次打开新的shell时,该文件被读取**~/.bash_logout：**当每次退出系统(退出bash shell)时,执行该文件. <br />\n\n## 46.Ubuntu bash的几个初始化文件<br />\n\n\n（1）/etc/profile <br />\n\n\n全局（公有）配置，不管是哪个用户，登录时都会读取该文件。 \n\n（2）/ect/bashrc \n\nUbuntu没有此文件，与之对应的是/ect/bash.bashrc；它也是全局（公有）的 ；bash执行时，不管是何种方式，都会读取此文件。 \n\n（3）~/.profile <br />\n\n\n若bash是以login方式执行时，读取~/.bash_profile，若它不存在，则读取~/.bash_login，若前两者不存在，读取~/.profile。 <br />\n\n\n另外，图形模式登录时，此文件将被读取，即使存在~/.bash_profile和~/.bash_login。 &nbsp;\n\n\n（4）~/.bash_login <br />\n\n\n若bash是以login方式执行时，读取~/.bash_profile，若它不存在，则读取~/.bash_login，若前两者不存在，读取~/.profile。 &nbsp;\n\n\n（5）~/.bash_profile <br />\n\n\nUnbutu默认没有此文件，可新建。 <br />\n\n\n只有bash是以login形式执行时，才会读取此文件。通常该配置文件还会配置成去读取~/.bashrc。&nbsp;&nbsp;\n\n\n（6）~/.bashrc <br />\n当bash是以non-login形式执行时，读取此文件。若是以login形式执行，则不会读取此文件。 \n\n（7）~/.bash_logout <br />\n注销时，且是longin形式，此文件才会读取。也就是说，在文本模式注销时，此文件会被读取，图形模式注销时，此文件不会被读取。\n\n下面是在本机的几个例子： <br />\n1. 图形模式登录时，顺序读取：**/etc/profile**和**~/.profile** <br />\n2. 图形模式登录后，打开终端时，顺序读取：**/etc/bash.bashrc**和**~/.bashrc** <br />\n3. 文本模式登录时，顺序读取：**/etc/bash.bashrc**，**/etc/profile**和**~/.bash_profile** <br />\n4. 从其它用户su到该用户，则分两种情况： <br />\n&nbsp;&nbsp;&nbsp; （1）如果带-l参数（或-参数，--login参数），如：su -l username，则bash是lonin的，它将顺序读取以下配置文件：/etc/bash.bashrc，/etc/profile和~/.bash_profile。 <br />\n&nbsp;&nbsp;&nbsp; （2）如果没有带-l参数，则bash是non-login的，它将顺序读取：/etc/bash.bashrc和~/.bashrc <br />\n5. 注销时，或退出su登录的用户，如果是longin方式，那么bash会读取：~/.bash_logout <br />\n6. 执行自定义的shell文件时，若使用&ldquo;bash -l \na.sh&rdquo;的方式，则bash会读取行：/etc/profile和~/.bash_profile，若使用其它方式，如：bash a.sh， \n./a.sh，sh a.sh（这个不属于bash shell），则不会读取上面的任何文件。 <br />\n7. 上面的例子凡是读取到~/.bash_profile的，若该文件不存在，则读取~/.bash_login，若前两者不存在，读取~/.profile。 <br />\n\n\n## \n47.bashrc与profile的区别\n\n要搞清bashrc与profile的区别，首先要弄明白什么是交互式shell和非交互式shell，什么是login shell 和non-login shell。&nbsp; \n\n交互式模式就是shell等待你的输入，并且执行你提交的命令。这种模式被称作交互式是因为shell与用户进行交互。这种模式也是大多数用户非常熟悉\n的：登录、执行一些命令、签退。当你签退后，shell也终止了。 \nshell也可以运行在另外一种模式：非交互式模式。在这种模式下，shell不与你进行交互，而是读取存放在文件中的命令,并且执行它们。当它读到文件\n的结尾，shell也就终止了。&nbsp; <br />\nbashrc与profile都用于保存用户的环境信息，bashrc用于交互式non-loginshell，而profile用于交互式login shell。系统中存在许多bashrc和profile文件，下面逐一介绍： \n\n/etc/pro此文件为系统的每个用户设置环境信息,当第一个用户登录时,该文件被执行.&nbsp; \n\n并从/etc/profile.d目录的配置文件中搜集shell的设置.&nbsp; \n\n/etc/bashrc:为每一个运行bash shell的用户执行此文件.当bash shell被打开时,该文件被读取。有些linux版本中的/etc目录下已经没有了bashrc文件。&nbsp; \n\n~/. pro每个用户都可使用该文件输入专用于自己使用的shell信息,当用户登录时,该文件仅仅执行一次!默认情况下,它设置一些环境变量,然后执行用户的.bashrc文件. \n\n~/.bashrc:该文件包含专用于某个用户的bash shell的bash信息,当该用户登录时以及每次打开新的shell时,该文件被读取.&nbsp; \n\n另外,/etc/profile中设定的变量(全局)的可以作用于任何用户,而~/.bashrc等中设定的变量(局部)只能继承/etc/profile中的变量,他们是\"父子\"关系. \n\n某网友总结如下： \n\n/etc/profile，/etc/bashrc 是系统全局环境变量设定&nbsp; <br />\n~/.profile，~/.bashrc用户家目录下的私有环境变量设定 <br />\n当登入系统时候获得一个shell进程时，其读取环境设定档有三步 <br />\n1首先读入的是全局环境变量设定档/etc/profile，然后根据其内容读取额外的设定的文档，如 <br />\n/etc/profile.d和/etc/inputrc <br />\n2然后根据不同使用者帐号，去其家目录读取~/.bash_profile，如果这读取不了就读取~/.bash_login，这个也读取不了才会读取 <br />\n~/.profile，这三个文档设定基本上是一样的，读取有优先关系 <br />\n3然后在根据用户帐号读取~/.bashrc <br />\n至于~/.profile与~/.bashrc的不区别 <br />\n都具有个性化定制功能 <br />~/.profile可以设定本用户专有的路径，环境变量，等，它只能登入的时候执行一次 <br />~/.bashrc也是某用户专有设定文档，可以设定路径，命令别名，每次shell script的执行都会使用它一次 \n\n参考：http://www.hx95.com/Article/Tech/201207/58094.html\n\n&nbsp;\n","tags":["Linux"]},{"title":"电子设计推荐看的好书","url":"/电子设计推荐看的好书.html","content":"1、《电子设计从零开始》清华大学出版社 杨欣&mdash;&mdash;一本开始自己的电子设计之旅的好书，很有趣，实践联系理论。\n\n2、《模拟电子技术基础》康华光或者童诗白&mdash;&mdash;模电是拉开系统性能的关键。\n\n3、《数字电子技术基础》康华光或者童诗白&mdash;&mdash;一个好的系统是绝对离不开数电的。\n\n4、《测量电子电路设计：滤波器篇（从滤波器设计到锁相放大器的应用）》远坂俊昭&mdash;&mdash;一本关于滤波器设计的好书，结合Filter Solution 2009的仿真，可以帮助设计出性能很好的滤波器。\n\n5、《LC滤波器设计与制作》森荣二&mdash;&mdash;已经入手的一本好书，准备看看，介绍LC无源滤波器。\n\n6、《新概念51单片机C语言教程：入门、提高、开发》&mdash;&mdash;郭天祥写的51单片机入门教科书，结合他的视频，可以很快的入门，但是个人觉得郭天祥的程序写的不是很规范。\n\n7、《ADI放大器应用手册》（第一册）&mdash;&mdash;ADI公司出品的应用手册合集，结合ADI官网的一些研讨会教学视频，学习很有好处。个人觉得ADI公司在教学方面做得比TI要好，不过样片申请的速度是不能和TI相比的。\n\n8、《运算放大器应用手册：基础知识篇》电子工业出版社&mdash;&mdash;TI公司出品，看过一遍，有部分写的很好，具体是什么内容忘记了 -.-\n\n9、《运算放大器权威指南》（第3版）&mdash;&mdash;绝对好书，可惜一直没时间看。\n\n10、《MSP430系列16位超低功耗单片机原理与实践》&mdash;&mdash;MSP430入门书籍，尚可。\n\n11、《晶体管电路设计》（上）（下）铃木雅臣&mdash;&mdash;晶体管电路设计的绝对好书，日本人写的书很实用。\n\n12、《OP放大电路设计》冈村廸夫&mdash;&mdash;对使用运放和理解运算放大器的内部结构很有帮助，实用，日本人写的又一本好书。\n","tags":["杂谈"]},{"title":"2013/4/15  第一天加油","url":"/2013／4／15  第一天加油.html","content":"第一天加油\n","tags":["杂谈"]}]