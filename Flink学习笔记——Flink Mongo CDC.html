<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Flink学习笔记——Flink Mongo CDC | tonglin0325的个人主页</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="1.Flink CDC介绍Flink CDC提供了一系列connector，用于从其他数据源获取变更数据（change data capture） 官方文档 12https:&#x2F;&#x2F;ververica.github.io&#x2F;flink-cdc-connectors&#x2F;release-2.3&#x2F;content&#x2F;about.html  官方github 12https:&#x2F;&#x2F;github.com&#x2F;ververic">
<meta property="og:type" content="article">
<meta property="og:title" content="Flink学习笔记——Flink Mongo CDC">
<meta property="og:url" content="http://tonglin0325.github.io/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94Flink%20Mongo%20CDC.html">
<meta property="og:site_name" content="tonglin0325的个人主页">
<meta property="og:description" content="1.Flink CDC介绍Flink CDC提供了一系列connector，用于从其他数据源获取变更数据（change data capture） 官方文档 12https:&#x2F;&#x2F;ververica.github.io&#x2F;flink-cdc-connectors&#x2F;release-2.3&#x2F;content&#x2F;about.html  官方github 12https:&#x2F;&#x2F;github.com&#x2F;ververic">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://tonglin0325.github.io/images/517519-20230510104736582-758175905.png">
<meta property="og:image" content="http://tonglin0325.github.io/images/517519-20230510105152426-2035315116.png">
<meta property="og:image" content="http://tonglin0325.github.io/images/517519-20230612104048523-766630352.png">
<meta property="og:image" content="http://tonglin0325.github.io/images/517519-20230612104350331-818018650.png">
<meta property="og:image" content="http://tonglin0325.github.io/images/517519-20230612104651336-1315496035.png">
<meta property="og:image" content="http://tonglin0325.github.io/images/517519-20230612105149345-1488955550.png">
<meta property="og:image" content="http://tonglin0325.github.io/images/517519-20230612105234270-221963043.png">
<meta property="og:image" content="http://tonglin0325.github.io/images/517519-20230612110124477-344548227.png">
<meta property="og:image" content="http://tonglin0325.github.io/images/517519-20230529103820326-1293246401.png">
<meta property="og:image" content="http://tonglin0325.github.io/images/517519-20230529144635584-993756234.png">
<meta property="og:image" content="http://tonglin0325.github.io/images/517519-20230529111626345-2107398364.png">
<meta property="og:image" content="http://tonglin0325.github.io/images/517519-20230530113021882-1800563154.png">
<meta property="og:image" content="http://tonglin0325.github.io/images/517519-20230530110501820-1096710620.png">
<meta property="og:image" content="http://tonglin0325.github.io/images/517519-20230531153224219-2115333833.png">
<meta property="og:image" content="http://tonglin0325.github.io/images/517519-20230531154538085-2017072666.png">
<meta property="og:image" content="http://tonglin0325.github.io/images/517519-20230531154155825-238606117.png">
<meta property="og:image" content="http://tonglin0325.github.io/images/517519-20230531154446167-1421223842.png">
<meta property="og:image" content="http://tonglin0325.github.io/images/517519-20230614164231567-1109743112.png">
<meta property="article:published_time" content="2016-05-21T16:00:00.000Z">
<meta property="article:modified_time" content="2025-02-22T16:17:04.041Z">
<meta property="article:author" content="tonglin0325">
<meta property="article:tag" content="Flink">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://tonglin0325.github.io/images/517519-20230510104736582-758175905.png">
  
    <link rel="alternate" href="/atom.xml" title="tonglin0325的个人主页" type="application/atom+xml">
  
  
    <link rel="icon" href="https://tonglin0325.github.io/images/favicon.ico">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" integrity="sha384-XdYbMnZ/QjLh6iI4ogqCTaIjrFk87ip+ekIjefZch0Y+PvJ8CDYtEs1ipDmPorQ+" crossorigin="anonymous">

  
<link rel="stylesheet" href="/css/styles.css">

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-166238833-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->



  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5884015902351015"
     crossorigin="anonymous"></script>

<meta name="generator" content="Hexo 6.0.0"></head>

<body>
  <nav class="navbar navbar-inverse">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#main-menu-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="main-menu-navbar">
      <ul class="nav navbar-nav">
        
          <li><a class=""
                 href="/index.html">主页</a></li>
        
          <li><a class=""
                 href="/archives/">所有文章</a></li>
        
          <li><a class=""
                 target="_blank" rel="noopener" href="http://www.cnblogs.com/tonglin0325/">博客园</a></li>
        
          <li><a class=""
                 target="_blank" rel="noopener" href="https://github.com/tonglin0325">Github</a></li>
        
          <li><a class=""
                 target="_blank" rel="noopener" href="https://www.linkedin.com/in/tonglin0325/">LinkedIn</a></li>
        
        <li>
          <span class="local-search local-search-google local-search-plugin" style="">
  <input type="search" placeholder="站内搜索" id="local-search-input" class="local-search-input-cls" style="">
  <div id="local-search-result" class="local-search-result-cls"></div>
</span>

<script>
document.addEventListener('DOMContentLoaded', (event) => {
    const div = document.getElementById('local-search-result');
    const input = document.getElementById('local-search-input');

    // 点击 input 显示 div
    input.addEventListener('click', function() {
        div.style.display = 'block';
    });

    // 点击 div 之外的地方隐藏 div
    document.addEventListener('click', function(event) {
        if (!div.contains(event.target) && event.target !== div && event.target !== input) {
            div.style.display = 'none';
        }
    });
});
</script>
        </li>
      </ul>

      <!--
      <ul class="nav navbar-nav navbar-right">
        
          <li><a href="/atom.xml" title="RSS Feed"><i class="fa fa-rss"></i></a></li>
        
      </ul>
      -->
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>


<script src="https://cdn.staticfile.org/jquery/1.10.2/jquery.min.js"></script>
<script>
// 移动设备侦测
var isMobile = {
  Android: function () {
    return navigator.userAgent.match(/Android/i);
  },
  BlackBerry: function () {
    return navigator.userAgent.match(/BlackBerry/i);
  },
  iOS: function () {
    return navigator.userAgent.match(/iPhone|iPad|iPod/i);
  },
  Opera: function () {
    return navigator.userAgent.match(/Opera Mini/i);
  },
  Windows: function () {
    return navigator.userAgent.match(/IEMobile/i);
  },
  any: function () {
    return (isMobile.Android() || isMobile.BlackBerry() || isMobile.iOS() || isMobile.Opera() || isMobile.Windows());
  }
};

if ($('.local-search').resize() && !isMobile.any()) {
	$.getScript('/js/search.js', function() {
	  searchFunc("/search.json", 'local-search-input', 'local-search-result');
	});
}

// document.addEventListener('DOMContentLoaded', (event) => {
//
//     const input = document.getElementById('.local-search-input');
//     const div = document.getElementById('.local-search-result');
//
//     input.addEventListener('blur', function() {
//         div.style.display = 'none';
//     });
// });
</script>
  <div class="container">
    <div class="blog-header">
  <h1 class="blog-title">tonglin0325的个人主页</h1>
  
</div>

    <div class="row">
        <div class="col-sm-8 blog-main">
          <article id="post-Flink学习笔记——Flink Mongo CDC" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 class="article-title" itemprop="name">
      Flink学习笔记——Flink Mongo CDC
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94Flink%20Mongo%20CDC.html" class="article-date"><time datetime="2016-05-21T16:00:00.000Z" itemprop="datePublished">2016-05-22</time></a>
</div>

    <div class="article-author">tonglin0325</div>
    

  </div>

  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <!-- 目录 -->

<div id="toc">
  <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-flink-cdc-jie-shao"><span class="toc-text">1.Flink CDC介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-flink-mongo-cdc"><span class="toc-text">2.Flink Mongo CDC</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-snapshot-jie-duan-zhi-chi-checkpoint-duan-dian-xu-chuan"><span class="toc-text">1.snapshot阶段支持checkpoint（断点续传）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-snapshot-jie-duan-zhi-chi-bing-fa-tong-bu"><span class="toc-text">2.snapshot阶段支持并发同步</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-mongo-snapshot-de-split-ce-lue"><span class="toc-text">3.mongo snapshot的split策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-flink-mongo-cdc-yu-dao-de-bao-cuo-chu-li-fang-fa"><span class="toc-text">4.Flink Mongo CDC遇到的报错处理方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-caused-by-com-mongodb-mongoqueryexception-query-failed-with-error-code-280-and-error-message-cannot-resume-stream-the-resume-token-was-not-java-lang-runtimeexception-one-or-more-fetchers-have-encountered-exception"><span class="toc-text">1.Caused by: com.mongodb.MongoQueryException: Query failed with error code 280 and error message ‘cannot resume stream; the resume token was not java.lang.RuntimeException: One or more fetchers have encountered exception</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-java-lang-nosuchmethoderror-org-apache-commons-cli-option-builder-ljava-lang-string-lorg-apache-commons-cli-option-builder"><span class="toc-text">2.java.lang.NoSuchMethodError: org.apache.commons.cli.Option.builder(Ljava&#x2F;lang&#x2F;String;)Lorg&#x2F;apache&#x2F;commons&#x2F;cli&#x2F;Option$Builder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-caused-by-com-mongodb-mongocursornotfoundexception-query-failed-with-error-code-5-and-error-message-cursor-7652758736186712320-not-found-on-server-xxxx-27017-on-server-xxxx-27017"><span class="toc-text">3.Caused by: com.mongodb.MongoCursorNotFoundException: Query failed with error code -5 and error message ‘Cursor 7652758736186712320 not found on server xxxx:27017’ on server xxxx:27017</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-java-lang-arrayindexoutofboundsexception-1-nbsp-at-org-apache-hudi-index-bucket-bucketidentifier-lambda-gethashkeysusingindexfields-2-bucketidentifier-java-74"><span class="toc-text">4.java.lang.ArrayIndexOutOfBoundsException: 1 at org.apache.hudi.index.bucket.BucketIdentifier.lambda$ getHashKeysUsingIndexFields$2(BucketIdentifier.java:74)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-caused-by-java-lang-nosuchmethoderror-org-apache-hudi-org-apache-avro-specific-specificrecordbuilderbase-lorg-apache-hudi-org-apache-avro-schema-lorg-apache-hudi-org-apache-avro-specific-specificdata-v"><span class="toc-text">5.Caused by: java.lang.NoSuchMethodError: org.apache.hudi.org.apache.avro.specific.SpecificRecordBuilderBase.(Lorg&#x2F;apache&#x2F;hudi&#x2F;org&#x2F;apache&#x2F;avro&#x2F;Schema;Lorg&#x2F;apache&#x2F;hudi&#x2F;org&#x2F;apache&#x2F;avro&#x2F;specific&#x2F;SpecificData;)V</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-java-lang-outofmemoryerror-java-heap-space"><span class="toc-text">6.java.lang.OutOfMemoryError: Java heap space</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-caused-by-org-apache-flink-util-serializedthrowable-java-lang-outofmemoryerror-direct-buffer-memory"><span class="toc-text">7.Caused by: org.apache.flink.util.SerializedThrowable: java.lang.OutOfMemoryError: Direct buffer memory</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-caused-by-com-mongodb-mongoqueryexception-query-failed-with-error-code-286-and-error-message-error-on-remote-shard-172-31-xx-xx-27017-caused-by-resume-of-change-stream-was-not-possible-as-the-resume-point-may-no-longer-be-in-the-oplog-on-server-xxxx-27017"><span class="toc-text">8.Caused by: com.mongodb.MongoQueryException: Query failed with error code 286 and error message ‘Error on remote shard 172.31.xx.xx:27017 :: caused by :: Resume of change stream was not possible, as the resume point may no longer be in the oplog.’ on server xxxx:27017</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-unable-to-convert-to-rowtype-from-unexpected-value-bsonarray-values-of-type-array"><span class="toc-text">9.unable to convert to rowtype from unexpected value ‘bsonarray{values&#x3D;[]}’ of type array</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-java-util-concurrent-timeoutexception-heartbeat-of-taskmanager-with-id-container-1685522036674-0015-01-000014-ip-172-31-xxx-xx-us-compute-internal-8041-timed-out"><span class="toc-text">10.java.util.concurrent.TimeoutException: Heartbeat of TaskManager with id container_1685522036674_0015_01_000014(ip-172-31-xxx-xx.us.compute.internal:8041) timed out.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#11-caused-by-java-util-concurrent-timeoutexception-invocation-of-remoterpcinvocation-taskexecutorgateway-sendoperatoreventtotask-executionattemptid-operatorid-serializedvalue-at-recipient-akka-tcp-102-108-x69-x6e-107-64-x69-112-x2d-x31-x37-50-x2d-51-49-45-x78-120-120-x2d-120-x78-x2e-117-s-compute-internal-33139-user-rpc-taskmanager-0-timed-out"><span class="toc-text">11.Caused by: java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(TaskExecutorGateway.sendOperatorEventToTask(ExecutionAttemptID, OperatorID, SerializedValue))] at recipient [akka.tcp:&#x2F;&#x2F;flink@ip-172-31-xxx-xx.us-.compute.internal:33139&#x2F;user&#x2F;rpc&#x2F;taskmanager_0] timed out</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-org-apache-flink-runtime-io-network-netty-exception-remotetransportexception-connection-unexpectedly-closed-by-remote-task-manager-ip-172-31-xxx-xx-us-compute-internal-172-31-xxx-xx-39597-this-might-indicate-that-the-remote-task-manager-was-lost"><span class="toc-text">12.org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connection unexpectedly closed by remote task manager ‘ip-172-31-xxx-xx.us.compute.internal&#x2F;172.31.xxx.xx:39597’. This might indicate that the remote task manager was lost.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#13-caused-by-com-mongodb-mongocommandexception-command-failed-with-error-13-unauthorized-lsquo-not-authorized-on-admin-to-execute-command-aggregate-1-pipeline-changestream-allchangesforcluster-true-cursor-batchsize-1-db-ldquo-admin-rdquo-clustertime-clustertime-timestamp-1680783775-2-signature-hash-bindata-0-898fe82be1837f4bdbda1a12cc1d5f765527a25c-keyid-7175829065197158402-lsid-id-uuid-ldquo-9d99adfc-cc35-4619-b5c5-7ad71ec77df1-rdquo-rsquo-on-server-xxxx-27017"><span class="toc-text">13.Caused by: com.mongodb.MongoCommandException: Command failed with error 13 (Unauthorized): ‘not authorized on admin to execute command { aggregate: 1, pipeline: [ { $changeStream: { allChangesForCluster: true } } ], cursor: { batchSize: 1 }, $db: “admin”, $clusterTime: { clusterTime: Timestamp(1680783775, 2), signature: { hash: BinData(0, 898FE82BE1837F4BDBDA1A12CC1D5F765527A25C), keyId: 7175829065197158402 } }, lsid: { id: UUID(“9d99adfc-cc35-4619-b5c5-7ad71ec77df1”) } }’ on server xxxx:27017</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-caused-by-com-mongodb-mongocommandexception-command-failed-with-error-18-authenticationfailed-authentication-failed-on-server-xxx-27017"><span class="toc-text">14.Caused by: com.mongodb.MongoCommandException: Command failed with error 18 (AuthenticationFailed): ‘Authentication failed.’ on server xxx:27017.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#15-checkpoint-chao-shi"><span class="toc-text">15.checkpoint超时</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#16-org-apache-flink-streaming-runtime-tasks-exceptioninchainedoperatorexception-could-not-forward-element-to-next-operator"><span class="toc-text">16.org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator&#96;&#96;</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#17-java-lang-noclassdeffounderror-could-not-initialize-class-com-ververica-cdc-connectors-mongodb-internal-mongodbenvelope"><span class="toc-text">17.java.lang.NoClassDefFoundError: Could not initialize class com.ververica.cdc.connectors.mongodb.internal.MongoDBEnvelope</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-mongo-cdc-ye-jie-shi-yong-an-li"><span class="toc-text">5.Mongo CDC业界使用案例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-flink-cdc-xi-lie-flink-mongodb-cdc-zai-xtransfer-de-sheng-chan-shi-jian-nbsp-xtransfer"><span class="toc-text">1.Flink CDC 系列 - Flink MongoDB CDC 在 XTransfer 的生产实践 （XTransfer）</span></a></li></ol></li></ol></li></ol>
</div>

        <h2 id="1-flink-cdc-jie-shao"><a href="#1-Flink-CDC介绍" class="headerlink" title="1.Flink CDC介绍"></a>1.Flink CDC介绍<a href="#1-flink-cdc-jie-shao" class="header-anchor">#</a></h2><p>Flink CDC提供了一系列connector，用于从其他数据源获取变更数据（change data capture）</p>
<p>官方文档</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">https://ververica.github.io/flink-cdc-connectors/release-2.3/content/about.html</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>官方github</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/ververica/flink-cdc-connectors</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>各种数据源使用案例，参考：</p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/cn/blogs/china/use-emr-to-migrate-cdh-based-on-aws-s3-emr-flink-presto-and-hudi-real-time-data-lake/">基于 AWS S3、EMR Flink、Presto 和 Hudi 的实时数据湖仓 &ndash; 使用 EMR 迁移 CDH</a></p>
<p><a target="_blank" rel="noopener" href="http://junyao.tech/posts/748d18ad.html">Flink CDC关于source和sink全调研及实践</a></p>
<h2 id="2-flink-mongo-cdc"><a href="#2-Flink-Mongo-CDC" class="headerlink" title="2.Flink Mongo CDC"></a>2.Flink Mongo CDC<a href="#2-flink-mongo-cdc" class="header-anchor">#</a></h2><p>官方文档和原理，参考：<a target="_blank" rel="noopener" href="https://www.51cto.com/article/712109.html">Flink CDC MongoDB Connector 的实现原理和使用实践</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">https://ververica.github.io/flink-cdc-connectors/release-2.3/content/connectors/mongodb-cdc.html#</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>从<strong>Flink CDC 2.1.0</strong>开始，支持了<strong>Mongo CDC的connector</strong>，flink Mongo cdc的connector和flink MySQL cdc不同，是基于MongoDB的change stream，并不基于Debezium（Debezium Mongo Connector读取的是oplog）。</p>
<p>参考：<a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-cdc-docs-release-3.1/docs/connectors/flink-sources/mongodb-cdc/#change-streams">https://nightlies.apache.org/flink/flink-cdc-docs-release-3.1/docs/connectors/flink-sources/mongodb-cdc/#change-streams</a></p>
<p>截止2023年中，最新的版本是2.3.0，相比2.2.0，2.3.0版本主要新增了Incremental Snapshot的特性（在snapshot阶段支持断点续传和并发同步），且要求至少mongo 4.0版本，这样就可以解决大表在第一次snapshot同步的时候，一旦失败就得重头再开始同步的问题，以及之前只能单并发同步很慢的问题</p>
<p>具体新增的特性可以查看release：<a target="_blank" rel="noopener" href="https://github.com/ververica/flink-cdc-connectors/releases">https://github.com/ververica/flink-cdc-connectors/releases</a></p>
<span id="more"></span>
<p>&nbsp;</p>
<p>下面例子中的前flink mongo cdc的sink为hudi sink</p>
<h3 id="1-snapshot-jie-duan-zhi-chi-checkpoint-duan-dian-xu-chuan"><a href="#1-snapshot阶段支持checkpoint（断点续传）" class="headerlink" title="1.snapshot阶段支持checkpoint（断点续传）"></a>1.snapshot阶段支持checkpoint（断点续传）<a href="#1-snapshot-jie-duan-zhi-chi-checkpoint-duan-dian-xu-chuan" class="header-anchor">#</a></h3><p>这个具体值的是如果在snapshot阶段，某次checkpoint failed，或者停掉flink job，然后从最新的checkpoint重新flink job，都是能实现断点续传的，使用的话只需要添加如下配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scan.incremental.snapshot.enabled = true</span><br></pre></td></tr></table></figure>

<img src="/images/517519-20230510104736582-758175905.png" width="1200" height="500" loading="lazy">

<h3 id="2-snapshot-jie-duan-zhi-chi-bing-fa-tong-bu"><a href="#2-snapshot阶段支持并发同步" class="headerlink" title="2.snapshot阶段支持并发同步"></a>2.snapshot阶段支持并发同步<a href="#2-snapshot-jie-duan-zhi-chi-bing-fa-tong-bu" class="header-anchor">#</a></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/lib/flink/bin/flink run -t yarn-session -p 10 ...　　</span><br></pre></td></tr></table></figure>

<img src="/images/517519-20230510105152426-2035315116.png" width="1400" height="559" loading="lazy">

<p>bucket_write阶段的并发度由&nbsp;write.tasks 参数决定</p>
<p>compact_task阶段的并发度由&nbsp;compaction.tasks 参数决定</p>
<p>其他参数参考：<a target="_blank" rel="noopener" href="https://hudi.apache.org/cn/docs/0.9.0/flink-quick-start-guide#%E5%B9%B6%E8%A1%8C%E5%BA%A6-1">flink-quick-start-guide</a>&nbsp;和&nbsp;<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_44665283/article/details/129371508">数据湖架构Hudi（五）Hudi集成Flink案例详解</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Deployment took more than 60 seconds. Please check if the requested resources are available in the YARN cluster</span><br></pre></td></tr></table></figure>

<h3 id="3-mongo-snapshot-de-split-ce-lue"><a href="#3-mongo-snapshot的split策略" class="headerlink" title="3.mongo snapshot的split策略"></a>3.mongo snapshot的split策略<a href="#3-mongo-snapshot-de-split-ce-lue" class="header-anchor">#</a></h3><p>作者介绍其使用了3种split策略，分别是&nbsp;SampleBucketSplitStrategy，SplitVector split和&nbsp;MongoDBChunkSplitter，参考：<a target="_blank" rel="noopener" href="https://juejin.cn/post/7111523733734424612">Flink CDC MongoDB Connector 的实现原理和使用实践</a></p>
<p>为了在snapshot阶段支持并发，所以在snapshot阶段同步的时候，会使用 SnapshotSplitAssigner 对Mongo的collection进行切分</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/ververica/flink-cdc-connectors/blob/release-2.3.0/flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/assigner/SnapshotSplitAssigner.java</span><br></pre></td></tr></table></figure>

<img src="/images/517519-20230612104048523-766630352.png" width="500" height="489" loading="lazy">

<p>dialect实现的类为MongoDBDialect</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/ververica/flink-cdc-connectors/blob/release-2.3.0/flink-connector-mongodb-cdc/src/main/java/com/ververica/cdc/connectors/mongodb/source/dialect/MongoDBDialect.java</span><br></pre></td></tr></table></figure>

<img src="/images/517519-20230612104350331-818018650.png" width="500" height="92" loading="lazy">

<p>在&nbsp;MongoDBChunkSplitter 类中，对于sharded collection，会使用&nbsp;ShardedSplitStrategy 策略来对mongo进行切分，否则会使用&nbsp;SplitVectorSplitStrategy 策略进行切分</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/ververica/flink-cdc-connectors/blob/release-2.3.0/flink-connector-mongodb-cdc/src/main/java/com/ververica/cdc/connectors/mongodb/source/assigners/splitters/MongoDBChunkSplitter.java</span><br></pre></td></tr></table></figure>

<img src="/images/517519-20230612104651336-1315496035.png" width="600" height="416" loading="lazy">

<p>如果是sharded collection，当验证sharded collection失败或者没有config.collection或者config.chunks权限的时候，则会使用&nbsp;SampleBucketSplitStrategy 策略进行切分</p>
<img src="/images/517519-20230612105149345-1488955550.png" width="800" height="684" loading="lazy">

<p>如果不是sharded collection，当没有 splitVector 权限或者无法切分collection的话，则会使用&nbsp;SampleBucketSplitStrategy 策略进行切分</p>
<p><img src="/images/517519-20230612105234270-221963043.png" width="700" height="705" loading="lazy">&nbsp;</p>
<h3 id="4-flink-mongo-cdc-yu-dao-de-bao-cuo-chu-li-fang-fa"><a href="#4-Flink-Mongo-CDC遇到的报错处理方法" class="headerlink" title="4.Flink Mongo CDC遇到的报错处理方法"></a>4.Flink Mongo CDC遇到的报错处理方法<a href="#4-flink-mongo-cdc-yu-dao-de-bao-cuo-chu-li-fang-fa" class="header-anchor">#</a></h3><h4 id="1-caused-by-com-mongodb-mongoqueryexception-query-failed-with-error-code-280-and-error-message-cannot-resume-stream-the-resume-token-was-not-java-lang-runtimeexception-one-or-more-fetchers-have-encountered-exception"><a href="#1-Caused-by-com-mongodb-MongoQueryException-Query-failed-with-error-code-280-and-error-message-‘cannot-resume-stream-the-resume-token-was-not-java-lang-RuntimeException-One-or-more-fetchers-have-encountered-exception" class="headerlink" title="1.Caused by: com.mongodb.MongoQueryException: Query failed with error code 280 and error message ‘cannot resume stream; the resume token was not java.lang.RuntimeException: One or more fetchers have encountered exception"></a>1.Caused by: com.mongodb.MongoQueryException: Query failed with error code 280 and error message ‘cannot resume stream; the resume token was not java.lang.RuntimeException: One or more fetchers have encountered exception<a href="#1-caused-by-com-mongodb-mongoqueryexception-query-failed-with-error-code-280-and-error-message-cannot-resume-stream-the-resume-token-was-not-java-lang-runtimeexception-one-or-more-fetchers-have-encountered-exception" class="header-anchor">#</a></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)</span><br><span class="line">	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)</span><br><span class="line">	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)</span><br><span class="line">	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:385)</span><br><span class="line">	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)</span><br><span class="line">	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)</span><br><span class="line">	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)</span><br><span class="line">	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)</span><br><span class="line">	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)</span><br><span class="line">	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:750)</span><br><span class="line">Caused by: java.lang.RuntimeException: SplitFetcher thread 6 received unexpected exception while polling the records</span><br><span class="line">	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150)</span><br><span class="line">	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)</span><br><span class="line">	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</span><br><span class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	... 1 more</span><br><span class="line">Caused by: org.apache.flink.util.FlinkRuntimeException: Read split SnapshotSplit&#123;tableId=xxx.xxx, splitId=&#x27;xxxx.xx:661&#x27;, splitKeyType=[`_id` INT], splitStart=[&#123;&quot;_id&quot;: 1&#125;, &#123;&quot;_id&quot;: &quot;0MowwELT&quot;&#125;], splitEnd=[&#123;&quot;_id&quot;: 1&#125;, &#123;&quot;_id&quot;: &quot;0MoxTSHT&quot;&#125;], highWatermark=null&#125; error due to Query failed with error code 280 and error message &#x27;cannot resume stream; the resume token was not found. &#123;_data: &quot;82646B6EBA00000C5D2B022C0100296E5A1004B807779924DA402AB13486B3F67B6102463C5F6964003C306D483170686A75000004&quot;&#125;&#x27; on server xxxx:27017.</span><br><span class="line">	at com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceScanFetcher.checkReadException(IncrementalSourceScanFetcher.java:181)</span><br><span class="line">	at com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceScanFetcher.pollSplitRecords(IncrementalSourceScanFetcher.java:128)</span><br><span class="line">	at com.ververica.cdc.connectors.base.source.reader.IncrementalSourceSplitReader.fetch(IncrementalSourceSplitReader.java:73)</span><br><span class="line">	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)</span><br><span class="line">	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)</span><br><span class="line">	... 6 more</span><br><span class="line">Caused by: com.mongodb.MongoQueryException: Query failed with error code 280 and error message &#x27;cannot resume stream; the resume token was not found. &#123;_data: &quot;82646B6EBA00000C5D2B022C0100296E5A1004B807779924DA402AB13486B3F67B6102463C5F6964003C306D483170686A75000004&quot;&#125;&#x27; on server cxxxx:27017</span><br><span class="line">	at com.mongodb.internal.operation.QueryHelper.translateCommandException(QueryHelper.java:29)</span><br><span class="line">	at com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:282)</span><br><span class="line">	at com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:512)</span><br><span class="line">	at com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:270)</span><br><span class="line">	at com.mongodb.internal.operation.QueryBatchCursor.tryHasNext(QueryBatchCursor.java:223)</span><br><span class="line">	at com.mongodb.internal.operation.QueryBatchCursor.lambda$tryNext$0(QueryBatchCursor.java:206)</span><br><span class="line">	at com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:397)</span><br><span class="line">	at com.mongodb.internal.operation.QueryBatchCursor.tryNext(QueryBatchCursor.java:205)</span><br><span class="line">	at com.mongodb.internal.operation.ChangeStreamBatchCursor$3.apply(ChangeStreamBatchCursor.java:102)</span><br><span class="line">	at com.mongodb.internal.operation.ChangeStreamBatchCursor$3.apply(ChangeStreamBatchCursor.java:98)</span><br><span class="line">	at com.mongodb.internal.operation.ChangeStreamBatchCursor.resumeableOperation(ChangeStreamBatchCursor.java:195)</span><br><span class="line">	at com.mongodb.internal.operation.ChangeStreamBatchCursor.tryNext(ChangeStreamBatchCursor.java:98)</span><br><span class="line">	at com.mongodb.client.internal.MongoChangeStreamCursorImpl.tryNext(MongoChangeStreamCursorImpl.java:78)</span><br><span class="line">	at com.ververica.cdc.connectors.mongodb.source.reader.fetch.MongoDBStreamFetchTask.execute(MongoDBStreamFetchTask.java:116)</span><br><span class="line">	at com.ververica.cdc.connectors.mongodb.source.reader.fetch.MongoDBScanFetchTask.execute(MongoDBScanFetchTask.java:183)</span><br><span class="line">	at com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceScanFetcher.lambda$submitTask$0(IncrementalSourceScanFetcher.java:94)</span><br><span class="line">	... 5 more</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这是由于flink mongo cdc 2.3.0 及其以下版本的bug导致，可以考虑自行打包2.4 snapshot分支来进行fix，参考作者官方的fix：<a target="_blank" rel="noopener" href="https://github.com/ververica/flink-cdc-connectors/pull/1938">https://github.com/ververica/flink-cdc-connectors/pull/1938</a></p>
<p>resume token&nbsp;用来描述一个订阅点，本质上是 oplog 信息的一个封装，包含 clusterTime、uuid、documentKey等信息，当订阅 API 带上 resume token 时，MongoDB Server 会将 token 转换为对应的信息，并定位到 oplog 起点继续订阅操作。</p>
<p>参考：<a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/741514">MongoDB 4.2 内核解析 - Change Stream</a></p>
<h4 id="2-java-lang-nosuchmethoderror-org-apache-commons-cli-option-builder-ljava-lang-string-lorg-apache-commons-cli-option-builder"><a href="#2-java-lang-NoSuchMethodError-org-apache-commons-cli-Option-builder-Ljava-lang-String-Lorg-apache-commons-cli-Option-Builder" class="headerlink" title="2.java.lang.NoSuchMethodError: org.apache.commons.cli.Option.builder(Ljava/lang/String;)Lorg/apache/commons/cli/Option$Builder"></a>2.java.lang.NoSuchMethodError: org.apache.commons.cli.Option.builder(Ljava/lang/String;)Lorg/apache/commons/cli/Option$Builder<a href="#2-java-lang-nosuchmethoderror-org-apache-commons-cli-option-builder-ljava-lang-string-lorg-apache-commons-cli-option-builder" class="header-anchor">#</a></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoSuchMethodError: org.apache.commons.cli.Option.builder(Ljava/lang/String;)Lorg/apache/commons/cli/Option$Builder;</span><br><span class="line">	at org.apache.flink.yarn.cli.FlinkYarnSessionCli.&lt;init&gt;(FlinkYarnSessionCli.java:197)</span><br><span class="line">	at org.apache.flink.yarn.cli.FlinkYarnSessionCli.&lt;init&gt;(FlinkYarnSessionCli.java:173)</span><br><span class="line">	at org.apache.flink.yarn.cli.FlinkYarnSessionCli.main(FlinkYarnSessionCli.java:836)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这是由于commons-cli包版本过低导致的，从1.2.升级到1.5.0可以解决这个问题</p>
<h4 id="3-caused-by-com-mongodb-mongocursornotfoundexception-query-failed-with-error-code-5-and-error-message-cursor-7652758736186712320-not-found-on-server-xxxx-27017-on-server-xxxx-27017"><a href="#3-Caused-by-com-mongodb-MongoCursorNotFoundException-Query-failed-with-error-code-5-and-error-message-‘Cursor-7652758736186712320-not-found-on-server-xxxx-27017’-on-server-xxxx-27017" class="headerlink" title="3.Caused by: com.mongodb.MongoCursorNotFoundException: Query failed with error code -5 and error message ‘Cursor 7652758736186712320 not found on server xxxx:27017’ on server xxxx:27017"></a>3.Caused by: com.mongodb.MongoCursorNotFoundException: Query failed with error code -5 and error message ‘Cursor 7652758736186712320 not found on server xxxx:27017’ on server xxxx:27017<a href="#3-caused-by-com-mongodb-mongocursornotfoundexception-query-failed-with-error-code-5-and-error-message-cursor-7652758736186712320-not-found-on-server-xxxx-27017-on-server-xxxx-27017" class="header-anchor">#</a></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">java.lang.RuntimeException: One or more fetchers have encountered exception</span><br><span class="line">	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)</span><br><span class="line">	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)</span><br><span class="line">	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)</span><br><span class="line">	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:385)</span><br><span class="line">	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)</span><br><span class="line">	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)</span><br><span class="line">	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)</span><br><span class="line">	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)</span><br><span class="line">	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)</span><br><span class="line">	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:750)</span><br><span class="line">Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records</span><br><span class="line">	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150)</span><br><span class="line">	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)</span><br><span class="line">	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</span><br><span class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	... 1 more</span><br><span class="line">Caused by: org.apache.flink.util.FlinkRuntimeException: Read split SnapshotSplit&#123;tableId=xxxx.xxxx, splitId=&#x27;xxxx.xxxx:134146&#x27;, splitKeyType=[`_id` INT], splitStart=[&#123;&quot;_id&quot;: 1&#125;, &#123;&quot;_id&quot;: &quot;0mWXDXGK&quot;&#125;], splitEnd=[&#123;&quot;_id&quot;: 1&#125;, &#123;&quot;_id&quot;: &quot;0mWXkf7v&quot;&#125;], highWatermark=null&#125; error due to Query failed with error code -5 and error message &#x27;Cursor 7652758736186712320 not found on server xxxx:27017&#x27; on server xxxx:27017.</span><br><span class="line">	at com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceScanFetcher.checkReadException(IncrementalSourceScanFetcher.java:181)</span><br><span class="line">	at com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceScanFetcher.pollSplitRecords(IncrementalSourceScanFetcher.java:128)</span><br><span class="line">	at com.ververica.cdc.connectors.base.source.reader.IncrementalSourceSplitReader.fetch(IncrementalSourceSplitReader.java:73)</span><br><span class="line">	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)</span><br><span class="line">	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)</span><br><span class="line">	... 6 more</span><br><span class="line">Caused by: com.mongodb.MongoCursorNotFoundException: Query failed with error code -5 and error message &#x27;Cursor 7652758736186712320 not found on server xxxx:27017&#x27; on server xxxx:27017</span><br><span class="line">	at com.mongodb.internal.operation.QueryHelper.translateCommandException(QueryHelper.java:27)</span><br><span class="line">	at com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:282)</span><br><span class="line">	at com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:512)</span><br><span class="line">	at com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:270)</span><br><span class="line">	at com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:156)</span><br><span class="line">	at com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:397)</span><br><span class="line">	at com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:143)</span><br><span class="line">	at com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)</span><br><span class="line">	at com.ververica.cdc.connectors.mongodb.source.reader.fetch.MongoDBScanFetchTask.execute(MongoDBScanFetchTask.java:120)</span><br><span class="line">	at com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceScanFetcher.lambda$submitTask$0(IncrementalSourceScanFetcher.java:94)</span><br><span class="line">	... 5 more</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>mongo的cursor可以理解成为一个指针，使用这个指针可以访问mongo的文档，上面的报错的意思就是这个找不到这个指针</p>
<p>游标找不到的原因主要有2个：</p>
<ul>
<li>客户端游标超时，被服务端回收，再用游标向服务器请求数据时就会出现游标找不到的情况。</li>
<li>在 mongo 集群环境下，可能会出现游标找不到的情况。游标由 mongo 服务器生成，在集群环境下，当使用 find() 相关函数时返回一个游标，假设此时该游标由 A 服务器生成，迭代完数据继续请求数据时，访问到了 B 服务器，但是该游标不是 B 生成的，此时就会出现游标找不到的情况。正常情况下，在 mongo 集群时，会将 mongo 地址以 ip1:port1,ip2:port2,ip3:port3 形式传给 mongo 驱动，然后驱动能够自动完成负载均衡和保持会话转发到同一台服务器，此时不会出现游标找不到的情况。但当我们自己搭建了负载均衡层，且用load balancer地址来连接时，就会出现游标找不到的情况。</li>
</ul>
<p>游标找不到（仅考虑超时情况）的解决方案：</p>
<ul>
<li>在服务端增大 mongo 服务器的游标超时时间。参数是 cursorTimeoutMillis，其默认是 10 min。修改后需重启 mongo 服务器。</li>
<li>在客户端一次性获取到全部符合条件的数据。也即将batch_size设置为很大的数，但次数若真的有很多数据的话，则对系统内存要求较高，同时如果数据量过大或处理过程过慢依旧会出现游标超时的情况。所以batch_size的评估是一个技术活。</li><li>客户端设置游标永不超时:
<ul></ul></li>
<li>这种方式的缺点是如果程序意外停止或异常，该游标永远不会被释放，除非重启 mongo，否则会一直占用系统资源，属于危险操作。经过咨询DBA，一般很少对游标的数量进行监控，一般是由其引起的连锁反应如CPU/内存过高才能引起关注，一般的处理方式也就是重启mongo服务器，这样影响就比较大了。</li>
<li>经过查询，<strong>在mongo 3.6版本后</strong>，客户端就算把游标设置为永不超时。服务端仍然会在闲置30分钟后将其kill掉，所以大量查询若超过30分钟的话需要手动执行下refreshsession来防止超时。但在3.6以下版本则会一直存在。<a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=https://docs.mongodb.com/manual/reference/method/cursor.noCursorTimeout/%23mongodb-method-cursor.noCursorTimeout">mongo文档</a>。</li>
</ul>
<p>参考：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/c34539fb2ffa">springboot mongo查询游标(cursor)不存在错误&nbsp;</a></p>
<p>即在mongo-cdc connection的配置中将cursor batch size的参数batch.size调小，默认值为1024，可以尝试调成100</p>
<p>参考：<a target="_blank" rel="noopener" href="https://ververica.github.io/flink-cdc-connectors/master/content/connectors/mongodb-cdc.html#connector-options">https://ververica.github.io/flink-cdc-connectors/master/content/connectors/mongodb-cdc.html#connector-options</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41018861/article/details/113104156">mongodb游标超时报错：com.mongodb.MongoCursorNotFoundException: Query failed with error code -5的四种处理方式</a></p>
<h4 id="4-java-lang-arrayindexoutofboundsexception-1-nbsp-at-org-apache-hudi-index-bucket-bucketidentifier-lambda-gethashkeysusingindexfields-2-bucketidentifier-java-74"><a href="#4-java-lang-ArrayIndexOutOfBoundsException-1-nbsp-at-org-apache-hudi-index-bucket-BucketIdentifier-lambda-getHashKeysUsingIndexFields-2-BucketIdentifier-java-74" class="headerlink" title="4.java.lang.ArrayIndexOutOfBoundsException: 1&nbsp;at org.apache.hudi.index.bucket.BucketIdentifier.lambda$ getHashKeysUsingIndexFields$2(BucketIdentifier.java:74)"></a>4.java.lang.ArrayIndexOutOfBoundsException: 1&nbsp;at org.apache.hudi.index.bucket.BucketIdentifier.lambda$ getHashKeysUsingIndexFields$2(BucketIdentifier.java:74)<a href="#4-java-lang-arrayindexoutofboundsexception-1-nbsp-at-org-apache-hudi-index-bucket-bucketidentifier-lambda-gethashkeysusingindexfields-2-bucketidentifier-java-74" class="header-anchor">#</a></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">java.lang.ArrayIndexOutOfBoundsException: 1</span><br><span class="line">	at org.apache.hudi.index.bucket.BucketIdentifier.lambda$getHashKeysUsingIndexFields$2(BucketIdentifier.java:74)</span><br><span class="line">	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321)</span><br><span class="line">	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)</span><br><span class="line">	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)</span><br><span class="line">	at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)</span><br><span class="line">	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)</span><br><span class="line">	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)</span><br><span class="line">	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)</span><br><span class="line">	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)</span><br><span class="line">	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)</span><br><span class="line">	at org.apache.hudi.index.bucket.BucketIdentifier.getHashKeysUsingIndexFields(BucketIdentifier.java:74)</span><br><span class="line">	at org.apache.hudi.index.bucket.BucketIdentifier.getHashKeys(BucketIdentifier.java:63)</span><br><span class="line">	at org.apache.hudi.index.bucket.BucketIdentifier.getHashKeys(BucketIdentifier.java:58)</span><br><span class="line">	at org.apache.hudi.index.bucket.BucketIdentifier.getBucketId(BucketIdentifier.java:42)</span><br><span class="line">	at org.apache.hudi.sink.partitioner.BucketIndexPartitioner.partition(BucketIndexPartitioner.java:44)</span><br><span class="line">	at org.apache.hudi.sink.partitioner.BucketIndexPartitioner.partition(BucketIndexPartitioner.java:32)</span><br><span class="line">	at org.apache.flink.streaming.runtime.partitioner.CustomPartitionerWrapper.selectChannel(CustomPartitionerWrapper.java:57)</span><br><span class="line">	at org.apache.flink.streaming.runtime.partitioner.CustomPartitionerWrapper.selectChannel(CustomPartitionerWrapper.java:36)</span><br><span class="line">	at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:54)</span><br><span class="line">	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:104)</span><br><span class="line">	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:90)</span><br><span class="line">	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:44)</span><br><span class="line">	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)</span><br><span class="line">	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)</span><br><span class="line">	at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:38)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)</span><br><span class="line">	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)</span><br><span class="line">	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)</span><br><span class="line">	at org.apache.flink.table.runtime.util.StreamRecordCollector.collect(StreamRecordCollector.java:44)</span><br><span class="line">	at org.apache.flink.table.runtime.operators.sink.ConstraintEnforcer.processElement(ConstraintEnforcer.java:247)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)</span><br><span class="line">	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)</span><br><span class="line">	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)</span><br><span class="line">	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51)</span><br><span class="line">	at org.apache.flink.table.runtime.operators.deduplicate.DeduplicateFunctionHelper.processLastRowOnChangelog(DeduplicateFunctionHelper.java:112)</span><br><span class="line">	at org.apache.flink.table.runtime.operators.deduplicate.ProcTimeDeduplicateKeepLastRowFunction.processElement(ProcTimeDeduplicateKeepLastRowFunction.java:80)</span><br><span class="line">	at org.apache.flink.table.runtime.operators.deduplicate.ProcTimeDeduplicateKeepLastRowFunction.processElement(ProcTimeDeduplicateKeepLastRowFunction.java:32)</span><br><span class="line">	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)</span><br><span class="line">	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)</span><br><span class="line">	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)</span><br><span class="line">	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)</span><br><span class="line">	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)</span><br><span class="line">	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)</span><br><span class="line">	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)</span><br><span class="line">	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:750)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这是由于指定的hudi recorder key中有一些异常的数据导致的，比如这里指定的recorder key是_id，这个字段是个string类型的，但是会有少部分数据是json string，其中value包含的逗号会导致split后出现array越界的情况</p>
<h4 id="5-caused-by-java-lang-nosuchmethoderror-org-apache-hudi-org-apache-avro-specific-specificrecordbuilderbase-lorg-apache-hudi-org-apache-avro-schema-lorg-apache-hudi-org-apache-avro-specific-specificdata-v"><a href="#5-Caused-by-java-lang-NoSuchMethodError-org-apache-hudi-org-apache-avro-specific-SpecificRecordBuilderBase-Lorg-apache-hudi-org-apache-avro-Schema-Lorg-apache-hudi-org-apache-avro-specific-SpecificData-V" class="headerlink" title="5.Caused by: java.lang.NoSuchMethodError: org.apache.hudi.org.apache.avro.specific.SpecificRecordBuilderBase.(Lorg/apache/hudi/org/apache/avro/Schema;Lorg/apache/hudi/org/apache/avro/specific/SpecificData;)V"></a>5.Caused by: java.lang.NoSuchMethodError: org.apache.hudi.org.apache.avro.specific.SpecificRecordBuilderBase.<init>(Lorg/apache/hudi/org/apache/avro/Schema;Lorg/apache/hudi/org/apache/avro/specific/SpecificData;)V</init><a href="#5-caused-by-java-lang-nosuchmethoderror-org-apache-hudi-org-apache-avro-specific-specificrecordbuilderbase-lorg-apache-hudi-org-apache-avro-schema-lorg-apache-hudi-org-apache-avro-specific-specificdata-v" class="header-anchor">#</a></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">org.apache.flink.util.FlinkException: Global failure triggered by OperatorCoordinator for &#x27;bucket_write: hudi_test&#x27; (operator b1cd777bdf9179b3493b6420d82b014a).</span><br><span class="line">	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder$LazyInitializedCoordinatorContext.failJob(OperatorCoordinatorHolder.java:556)</span><br><span class="line">	at org.apache.hudi.sink.StreamWriteOperatorCoordinator.lambda$start$0(StreamWriteOperatorCoordinator.java:187)</span><br><span class="line">	at org.apache.hudi.sink.utils.NonThrownExecutor.handleException(NonThrownExecutor.java:146)</span><br><span class="line">	at org.apache.hudi.sink.utils.NonThrownExecutor.lambda$wrapAction$0(NonThrownExecutor.java:133)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:750)</span><br><span class="line">Caused by: org.apache.hudi.exception.HoodieException: Executor executes action [commits the instant 20230525063249024] error</span><br><span class="line">	... 6 more</span><br><span class="line">Caused by: java.lang.NoSuchMethodError: org.apache.hudi.org.apache.avro.specific.SpecificRecordBuilderBase.&lt;init&gt;(Lorg/apache/hudi/org/apache/avro/Schema;Lorg/apache/hudi/org/apache/avro/specific/SpecificData;)V</span><br><span class="line">	at org.apache.hudi.avro.model.HoodieCompactionPlan$Builder.&lt;init&gt;(HoodieCompactionPlan.java:226)</span><br><span class="line">	at org.apache.hudi.avro.model.HoodieCompactionPlan$Builder.&lt;init&gt;(HoodieCompactionPlan.java:217)</span><br><span class="line">	at org.apache.hudi.avro.model.HoodieCompactionPlan.newBuilder(HoodieCompactionPlan.java:184)</span><br><span class="line">	at org.apache.hudi.table.action.compact.strategy.CompactionStrategy.generateCompactionPlan(CompactionStrategy.java:76)</span><br><span class="line">	at org.apache.hudi.table.action.compact.HoodieCompactor.generateCompactionPlan(HoodieCompactor.java:317)</span><br><span class="line">	at org.apache.hudi.table.action.compact.ScheduleCompactionActionExecutor.scheduleCompaction(ScheduleCompactionActionExecutor.java:123)</span><br><span class="line">	at org.apache.hudi.table.action.compact.ScheduleCompactionActionExecutor.execute(ScheduleCompactionActionExecutor.java:93)</span><br><span class="line">	at org.apache.hudi.table.HoodieFlinkMergeOnReadTable.scheduleCompaction(HoodieFlinkMergeOnReadTable.java:112)</span><br><span class="line">	at org.apache.hudi.client.BaseHoodieWriteClient.scheduleTableServiceInternal(BaseHoodieWriteClient.java:1349)</span><br><span class="line">	at org.apache.hudi.client.BaseHoodieWriteClient.scheduleTableService(BaseHoodieWriteClient.java:1326)</span><br><span class="line">	at org.apache.hudi.client.BaseHoodieWriteClient.scheduleCompactionAtInstant(BaseHoodieWriteClient.java:1005)</span><br><span class="line">	at org.apache.hudi.client.BaseHoodieWriteClient.scheduleCompaction(BaseHoodieWriteClient.java:996)</span><br><span class="line">	at org.apache.hudi.util.CompactionUtil.scheduleCompaction(CompactionUtil.java:65)</span><br><span class="line">	at org.apache.hudi.sink.StreamWriteOperatorCoordinator.lambda$notifyCheckpointComplete$2(StreamWriteOperatorCoordinator.java:246)</span><br><span class="line">	at org.apache.hudi.sink.utils.NonThrownExecutor.lambda$wrapAction$0(NonThrownExecutor.java:130)</span><br><span class="line">	... 3 more</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这个问题网上有人讨论过，原因是flink 1.15.x版本使用的avro版本是1.8.2，而hudi-flink and hudi-flink-bundle使用的avro版本是1.10.0，上面报错中的找不到的方法需要在avro 1.10.0及以上版本中出现</p>
<p>参考：<a target="_blank" rel="noopener" href="https://github.com/apache/hudi/issues/7259">https://github.com/apache/hudi/issues/7259</a></p>
<p>这个报错将会导致flink任务失败，而且hive表无法同步</p>
<h4 id="6-java-lang-outofmemoryerror-java-heap-space"><a href="#6-java-lang-OutOfMemoryError-Java-heap-space" class="headerlink" title="6.java.lang.OutOfMemoryError: Java heap space"></a>6.java.lang.OutOfMemoryError: Java heap space<a href="#6-java-lang-outofmemoryerror-java-heap-space" class="header-anchor">#</a></h4><p>这是问题发生的原因是flink的<strong>heap内存不足</strong>，可能的原因是：</p>
<p><strong>1.没有使用rocksdb状态后端</strong></p>
<p>默认的backend是MemoryStateBackend。默认情况下，flink的状态会保存在taskmanager的内存中，而checkpoint会保存在jobManager的内存中。</p>
<p>参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/sinat_38079265/article/details/121736045">Flink状态后端配置（设置State Backend）</a></p>
<p>不过在使用flink streaming job中，使用rocksdb backend可能会使用一点managed memory（属于堆外内存）</p>
<p>参考：<a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/mem_setup_tm/#managed-memory">https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/mem_setup_tm/#managed-memory</a></p>
<p>官方作者也推荐使用rocksdb作为状态后端</p>
<img src="/images/517519-20230612110124477-344548227.png" alt loading="lazy">

<p>参考：<a target="_blank" rel="noopener" href="https://juejin.cn/post/7111523733734424612">Flink CDC MongoDB Connector 的实现原理和使用实践</a></p>
<p><strong>2.使用了默认的FLINK STATE index type</strong></p>
<p>由于FLINK STATE index type是in-memory的，都有可能导致flink任务的堆内存爆掉，解决方法是使用BUCKET index type</p>
<p>特别是当表很大的时候，第一次snapshot同步会消耗比较多的资源以及时间，这时候如果使用的是hudi sink的话建议将hudi的index.type设置成BUCKET，因为flink默认的index type是FLINK_STATE，FLINK_STATE默认是in-memory的，所以会消耗非常多的heap内存，无论是在运行过程中，还是在从checkpoint重新flink job的时候，对task manager的内存压力会很大，所以建议使用BUCKET index type替换默认的FLINK_STATE index type</p>
<p>参考：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/magic-x/articles/16114261.html">HUDI-0.11.0 BUCKET index on Flink 新特性试用</a></p>
<p>在使用BUCKET index type的时候，需要确定&nbsp;hoodie.bucket.index.num.buckets 参数，即bucket的数量，这个官方建议是3GB左右</p>
<p>参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/476967738">Hudi Bucket Index 在字节跳动的设计与实践</a></p>
<h4 id="7-caused-by-org-apache-flink-util-serializedthrowable-java-lang-outofmemoryerror-direct-buffer-memory"><a href="#7-Caused-by-org-apache-flink-util-SerializedThrowable-java-lang-OutOfMemoryError-Direct-buffer-memory" class="headerlink" title="7.Caused by: org.apache.flink.util.SerializedThrowable: java.lang.OutOfMemoryError: Direct buffer memory"></a>7.Caused by: org.apache.flink.util.SerializedThrowable: java.lang.OutOfMemoryError: Direct buffer memory<a href="#7-caused-by-org-apache-flink-util-serializedthrowable-java-lang-outofmemoryerror-direct-buffer-memory" class="header-anchor">#</a></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line">org.apache.flink.util.SerializedThrowable: Asynchronous task checkpoint failed.</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.handleExecutionException(AsyncCheckpointRunnable.java:320) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:155) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_372]</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_372]</span><br><span class="line">	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_372]</span><br><span class="line">Caused by: org.apache.flink.util.SerializedThrowable: Could not materialize checkpoint 114 for operator Source: mongo_cdc_test[1] -&gt; Calc[2] (1/10)#0.</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.handleExecutionException(AsyncCheckpointRunnable.java:298) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	... 4 more</span><br><span class="line">Caused by: org.apache.flink.util.SerializedThrowable: java.lang.OutOfMemoryError: Direct buffer memory</span><br><span class="line">	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_372]</span><br><span class="line">	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[?:1.8.0_372]</span><br><span class="line">	at org.apache.flink.util.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:645) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.&lt;init&gt;(OperatorSnapshotFinalizer.java:60) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.finalizeNonFinishedSnapshots(AsyncCheckpointRunnable.java:191) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:124) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	... 3 more</span><br><span class="line">Caused by: org.apache.flink.util.SerializedThrowable: Direct buffer memory</span><br><span class="line">	at java.nio.Bits.reserveMemory(Bits.java:695) ~[?:1.8.0_372]</span><br><span class="line">	at java.nio.DirectByteBuffer.&lt;init&gt;(DirectByteBuffer.java:123) ~[?:1.8.0_372]</span><br><span class="line">	at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311) ~[?:1.8.0_372]</span><br><span class="line">	at sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:247) ~[?:1.8.0_372]</span><br><span class="line">	at sun.nio.ch.IOUtil.write(IOUtil.java:58) ~[?:1.8.0_372]</span><br><span class="line">	at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211) ~[?:1.8.0_372]</span><br><span class="line">	at java.nio.channels.Channels.writeFullyImpl(Channels.java:78) ~[?:1.8.0_372]</span><br><span class="line">	at java.nio.channels.Channels.writeFully(Channels.java:101) ~[?:1.8.0_372]</span><br><span class="line">	at java.nio.channels.Channels.access$000(Channels.java:61) ~[?:1.8.0_372]</span><br><span class="line">	at java.nio.channels.Channels$1.write(Channels.java:174) ~[?:1.8.0_372]</span><br><span class="line">	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122) ~[?:1.8.0_372]</span><br><span class="line">	at java.security.DigestOutputStream.write(DigestOutputStream.java:145) ~[?:1.8.0_372]</span><br><span class="line">	at com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.write(MultipartUploadOutputStream.java:172) ~[emrfs-hadoop-assembly-2.54.0.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:63) ~[hadoop-common-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at java.io.DataOutputStream.write(DataOutputStream.java:107) ~[?:1.8.0_372]</span><br><span class="line">	at org.apache.flink.runtime.fs.hdfs.HadoopDataOutputStream.write(HadoopDataOutputStream.java:47) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.core.fs.FSDataOutputStreamWrapper.write(FSDataOutputStreamWrapper.java:65) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.write(FsCheckpointStreamFactory.java:296) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at java.io.DataOutputStream.write(DataOutputStream.java:107) ~[?:1.8.0_372]</span><br><span class="line">	at java.io.FilterOutputStream.write(FilterOutputStream.java:97) ~[?:1.8.0_372]</span><br><span class="line">	at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.serialize(BytePrimitiveArraySerializer.java:75) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.serialize(BytePrimitiveArraySerializer.java:31) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.state.PartitionableListState.write(PartitionableListState.java:117) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.state.DefaultOperatorStateBackendSnapshotStrategy.lambda$asyncSnapshot$1(DefaultOperatorStateBackendSnapshotStrategy.java:165) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.state.SnapshotStrategyRunner$1.callInternal(SnapshotStrategyRunner.java:91) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.state.SnapshotStrategyRunner$1.callInternal(SnapshotStrategyRunner.java:88) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.state.AsyncSnapshotCallable.call(AsyncSnapshotCallable.java:78) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_372]</span><br><span class="line">	at org.apache.flink.util.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:642) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.&lt;init&gt;(OperatorSnapshotFinalizer.java:60) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.finalizeNonFinishedSnapshots(AsyncCheckpointRunnable.java:191) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:124) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	... 3 more</span><br><span class="line">2023-05-28 17:48:00,588 WARN  org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger or complete checkpoint 114 for job 9635c4bd284c99b94a979bd50b5fd3bb. (0 consecutive failed attempts so far)</span><br><span class="line">org.apache.flink.runtime.checkpoint.CheckpointException: Asynchronous task checkpoint failed.</span><br><span class="line">	at org.apache.flink.runtime.messages.checkpoint.SerializedCheckpointException.unwrap(SerializedCheckpointException.java:51) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage(CheckpointCoordinator.java:1013) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$declineCheckpoint$2(ExecutionGraphHandler.java:103) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_372]</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_372]</span><br><span class="line">	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_372]</span><br><span class="line">Caused by: org.apache.flink.util.SerializedThrowable: Asynchronous task checkpoint failed.</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.handleExecutionException(AsyncCheckpointRunnable.java:320) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:155) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	... 3 more</span><br><span class="line">Caused by: org.apache.flink.util.SerializedThrowable: Could not materialize checkpoint 114 for operator Source: mongo_cdc_test[1] -&gt; Calc[2] (1/10)#0.</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.handleExecutionException(AsyncCheckpointRunnable.java:298) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:155) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	... 3 more</span><br><span class="line">Caused by: org.apache.flink.util.SerializedThrowable: java.lang.OutOfMemoryError: Direct buffer memory</span><br><span class="line">	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_372]</span><br><span class="line">	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[?:1.8.0_372]</span><br><span class="line">	at org.apache.flink.util.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:645) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.&lt;init&gt;(OperatorSnapshotFinalizer.java:60) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.finalizeNonFinishedSnapshots(AsyncCheckpointRunnable.java:191) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:124) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	... 3 more</span><br><span class="line">Caused by: org.apache.flink.util.SerializedThrowable: Direct buffer memory</span><br><span class="line">	at java.nio.Bits.reserveMemory(Bits.java:695) ~[?:1.8.0_372]</span><br><span class="line">	at java.nio.DirectByteBuffer.&lt;init&gt;(DirectByteBuffer.java:123) ~[?:1.8.0_372]</span><br><span class="line">	at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311) ~[?:1.8.0_372]</span><br><span class="line">	at sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:247) ~[?:1.8.0_372]</span><br><span class="line">	at sun.nio.ch.IOUtil.write(IOUtil.java:58) ~[?:1.8.0_372]</span><br><span class="line">	at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211) ~[?:1.8.0_372]</span><br><span class="line">	at java.nio.channels.Channels.writeFullyImpl(Channels.java:78) ~[?:1.8.0_372]</span><br><span class="line">	at java.nio.channels.Channels.writeFully(Channels.java:101) ~[?:1.8.0_372]</span><br><span class="line">	at java.nio.channels.Channels.access$000(Channels.java:61) ~[?:1.8.0_372]</span><br><span class="line">	at java.nio.channels.Channels$1.write(Channels.java:174) ~[?:1.8.0_372]</span><br><span class="line">	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122) ~[?:1.8.0_372]</span><br><span class="line">	at java.security.DigestOutputStream.write(DigestOutputStream.java:145) ~[?:1.8.0_372]</span><br><span class="line">	at com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.write(MultipartUploadOutputStream.java:172) ~[emrfs-hadoop-assembly-2.54.0.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:63) ~[hadoop-common-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at java.io.DataOutputStream.write(DataOutputStream.java:107) ~[?:1.8.0_372]</span><br><span class="line">	at org.apache.flink.runtime.fs.hdfs.HadoopDataOutputStream.write(HadoopDataOutputStream.java:47) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.core.fs.FSDataOutputStreamWrapper.write(FSDataOutputStreamWrapper.java:65) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.write(FsCheckpointStreamFactory.java:296) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at java.io.DataOutputStream.write(DataOutputStream.java:107) ~[?:1.8.0_372]</span><br><span class="line">	at java.io.FilterOutputStream.write(FilterOutputStream.java:97) ~[?:1.8.0_372]</span><br><span class="line">	at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.serialize(BytePrimitiveArraySerializer.java:75) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.serialize(BytePrimitiveArraySerializer.java:31) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.state.PartitionableListState.write(PartitionableListState.java:117) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.state.DefaultOperatorStateBackendSnapshotStrategy.lambda$asyncSnapshot$1(DefaultOperatorStateBackendSnapshotStrategy.java:165) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.state.SnapshotStrategyRunner$1.callInternal(SnapshotStrategyRunner.java:91) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.state.SnapshotStrategyRunner$1.callInternal(SnapshotStrategyRunner.java:88) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.state.AsyncSnapshotCallable.call(AsyncSnapshotCallable.java:78) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_372]</span><br><span class="line">	at org.apache.flink.util.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:642) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.&lt;init&gt;(OperatorSnapshotFinalizer.java:60) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.finalizeNonFinishedSnapshots(AsyncCheckpointRunnable.java:191) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:124) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	... 3 more</span><br><span class="line">2023-05-28 17:48:00,591 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Trying to recover from a global failure.</span><br><span class="line">org.apache.flink.util.FlinkRuntimeException: Exceeded checkpoint tolerable failure threshold.</span><br><span class="line">	at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.checkFailureAgainstCounter(CheckpointFailureManager.java:206) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleTaskLevelCheckpointException(CheckpointFailureManager.java:191) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleCheckpointException(CheckpointFailureManager.java:124) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2082) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage(CheckpointCoordinator.java:1039) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$declineCheckpoint$2(ExecutionGraphHandler.java:103) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_372]</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_372]</span><br><span class="line">	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_372]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这是flink job manager或者task manager的<strong>堆外内存不足</strong>，其中的可能性是：</p>
<p><strong>1.flink job manager的堆外内存过小</strong>，默认是128 MB，参考</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#jobmanager-memory-off-heap-size</span><br></pre></td></tr></table></figure>

<img src="/images/517519-20230529103820326-1293246401.png" width="800" height="328" loading="lazy">

<p>可以使用 -D&nbsp;jobmanager.memory.off-heap.size=1024m 手动指定大小，比如</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/usr/lib/flink/bin/yarn-session.sh -s 1 -jm 51200 -tm 51200 -qu data -D jobmanager.memory.off-heap.size=1024m --detached</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>参考&nbsp;</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/mem_setup_jobmanager/</span><br></pre></td></tr></table></figure>

<p><strong>2.flink task manager的堆外内存过小</strong>，在使用task executor的时候其默认是0，即不使用堆外内存，参考</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#taskmanager-memory-task-off-heap-size</span><br></pre></td></tr></table></figure>

<img src="/images/517519-20230529144635584-993756234.png" alt loading="lazy">

<p>可以使用 -D&nbsp;taskmanager.memory.task.off-heap.size=4g手动指定大小，比如</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/lib/flink/bin/yarn-session.sh -s 1 -jm 51200 -tm 51200 -qu data -D taskmanager.memory.task.off-heap.size=4G --detached</span><br></pre></td></tr></table></figure>

<p>参考</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/mem_setup_tm/#configure-off-heap-memory-direct-or-native</span><br></pre></td></tr></table></figure>

<img src="/images/517519-20230529111626345-2107398364.png" alt loading="lazy">

<p>如果是 compaction 阶段发生内存溢出，还会导致 compaction 一直卡住 INFLIGHT 状态，这时需要查看日志排查是否是内存爆掉了</p>
<img src="/images/517519-20230530113021882-1800563154.png" alt loading="lazy">

<p>如果 compacion 阶段发生内存溢出导致<strong>compaction失败</strong>，会使得flink任务重启，hudi会对失败的compaction阶段生成的部分parquet文件进行清理，对应的就在文件目录下可以发现时间戳在失败的compaction instant之后的parquet文件会被清理掉</p>
<p>一个<strong>compaction未完成</strong>的时候，即使合并出部分parquet文件，使用hive来查询的时候，这部分新的parquet是不会被查询到的</p>
<p>此外hudi写parquet文件的时候，是需要消耗一定的堆外内存的，参考：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/leesf456/p/13055010.html">调优 | Apache Hudi应用调优指南</a></p>
<h4 id="8-caused-by-com-mongodb-mongoqueryexception-query-failed-with-error-code-286-and-error-message-error-on-remote-shard-172-31-xx-xx-27017-caused-by-resume-of-change-stream-was-not-possible-as-the-resume-point-may-no-longer-be-in-the-oplog-on-server-xxxx-27017"><a href="#8-Caused-by-com-mongodb-MongoQueryException-Query-failed-with-error-code-286-and-error-message-‘Error-on-remote-shard-172-31-xx-xx-27017-caused-by-Resume-of-change-stream-was-not-possible-as-the-resume-point-may-no-longer-be-in-the-oplog-’-on-server-xxxx-27017" class="headerlink" title="8.Caused by: com.mongodb.MongoQueryException: Query failed with error code 286 and error message ‘Error on remote shard 172.31.xx.xx:27017 :: caused by :: Resume of change stream was not possible, as the resume point may no longer be in the oplog.’ on server xxxx:27017"></a>8.Caused by: com.mongodb.MongoQueryException: Query failed with error code 286 and error message ‘Error on remote shard 172.31.xx.xx:27017 :: caused by :: Resume of change stream was not possible, as the resume point may no longer be in the oplog.’ on server xxxx:27017<a href="#8-caused-by-com-mongodb-mongoqueryexception-query-failed-with-error-code-286-and-error-message-error-on-remote-shard-172-31-xx-xx-27017-caused-by-resume-of-change-stream-was-not-possible-as-the-resume-point-may-no-longer-be-in-the-oplog-on-server-xxxx-27017" class="header-anchor">#</a></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">java.lang.RuntimeException: One or more fetchers have encountered exception</span><br><span class="line">	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)</span><br><span class="line">	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)</span><br><span class="line">	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)</span><br><span class="line">	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:385)</span><br><span class="line">	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)</span><br><span class="line">	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)</span><br><span class="line">	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)</span><br><span class="line">	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)</span><br><span class="line">	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)</span><br><span class="line">	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:750)</span><br><span class="line">Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records</span><br><span class="line">	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150)</span><br><span class="line">	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)</span><br><span class="line">	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</span><br><span class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	... 1 more</span><br><span class="line">Caused by: org.apache.flink.util.FlinkRuntimeException: Read split StreamSplit&#123;splitId=&#x27;stream-split&#x27;, offset=&#123;resumeToken=&#123;&quot;_data&quot;: &quot;826472E86F0000000A2B022C0100296E5A1004B807779924DA402AB13486B3F67B6102463C5F6964003C306D5A776877724A000004&quot;&#125;, timestamp=7238103114576822282&#125;, endOffset=&#123;resumeToken=null, timestamp=9223372034707292159&#125;&#125; error due to Query failed with error code 286 and error message &#x27;Error on remote shard 172.31.xx.xx:27017 :: caused by :: Resume of change stream was not possible, as the resume point may no longer be in the oplog.&#x27; on server xxxx:27017.</span><br><span class="line">	at com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceStreamFetcher.checkReadException(IncrementalSourceStreamFetcher.java:124)</span><br><span class="line">	at com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceStreamFetcher.pollSplitRecords(IncrementalSourceStreamFetcher.java:106)</span><br><span class="line">	at com.ververica.cdc.connectors.base.source.reader.IncrementalSourceSplitReader.fetch(IncrementalSourceSplitReader.java:73)</span><br><span class="line">	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)</span><br><span class="line">	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)</span><br><span class="line">	... 6 more</span><br><span class="line">Caused by: com.mongodb.MongoQueryException: Query failed with error code 286 and error message &#x27;Error on remote shard 172.31.xx.xx:27017 :: caused by :: Resume of change stream was not possible, as the resume point may no longer be in the oplog.&#x27; on server xxxx:27017</span><br><span class="line">	at com.mongodb.internal.operation.QueryHelper.translateCommandException(QueryHelper.java:29)</span><br><span class="line">	at com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:282)</span><br><span class="line">	at com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:512)</span><br><span class="line">	at com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:270)</span><br><span class="line">	at com.mongodb.internal.operation.QueryBatchCursor.tryHasNext(QueryBatchCursor.java:223)</span><br><span class="line">	at com.mongodb.internal.operation.QueryBatchCursor.lambda$tryNext$0(QueryBatchCursor.java:206)</span><br><span class="line">	at com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:397)</span><br><span class="line">	at com.mongodb.internal.operation.QueryBatchCursor.tryNext(QueryBatchCursor.java:205)</span><br><span class="line">	at com.mongodb.internal.operation.ChangeStreamBatchCursor$3.apply(ChangeStreamBatchCursor.java:102)</span><br><span class="line">	at com.mongodb.internal.operation.ChangeStreamBatchCursor$3.apply(ChangeStreamBatchCursor.java:98)</span><br><span class="line">	at com.mongodb.internal.operation.ChangeStreamBatchCursor.resumeableOperation(ChangeStreamBatchCursor.java:195)</span><br><span class="line">	at com.mongodb.internal.operation.ChangeStreamBatchCursor.tryNext(ChangeStreamBatchCursor.java:98)</span><br><span class="line">	at com.mongodb.client.internal.MongoChangeStreamCursorImpl.tryNext(MongoChangeStreamCursorImpl.java:78)</span><br><span class="line">	at com.ververica.cdc.connectors.mongodb.source.reader.fetch.MongoDBStreamFetchTask.execute(MongoDBStreamFetchTask.java:116)</span><br><span class="line">	at com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceStreamFetcher.lambda$submitTask$0(IncrementalSourceStreamFetcher.java:86)</span><br><span class="line">	... 5 more</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>同时使用 timeline show incomplete 命令查看未完成的timeline，会发现有 deltacommit 一直卡在 INFLIGHT 状态，说明拉取增量数据出现了问题</p>
<img src="/images/517519-20230530110501820-1096710620.png" alt loading="lazy">

<p>这个原因可能是oplog过期被清理掉了，这时候只能重头开始进行snapshot，之后可以调大oplog的保存大小或者时间&nbsp;</p>
<h4 id="9-unable-to-convert-to-rowtype-from-unexpected-value-bsonarray-values-of-type-array"><a href="#9-unable-to-convert-to-rowtype-from-unexpected-value-‘bsonarray-values-’-of-type-array" class="headerlink" title="9.unable to convert to rowtype from unexpected value ‘bsonarray{values=[]}’ of type array"></a>9.unable to convert to rowtype from unexpected value ‘bsonarray{values=[]}’ of type array<a href="#9-unable-to-convert-to-rowtype-from-unexpected-value-bsonarray-values-of-type-array" class="header-anchor">#</a></h4><p>这是由于mongo是无强数据类型的，所以在实际Flink Mongo CDC的使用中，有时就会出现数据类型对不上的时候，比如mongo中一个字段，应该是一个array，但是实际上却出现了json类型，这时候就会出现以上报错</p>
<p>解决方法是对 flink-connector-mongodb-cdc 项目中的 MongoDBConnectorDeserializationSchema 代码进行修改，对数据类型对不上时候的报错进行捕获，直接填充字符串NULL，这和MapReduce dump Mongodb时候的对于字段类型无法对上的处理方法是一致的</p>
<h4 id="10-java-util-concurrent-timeoutexception-heartbeat-of-taskmanager-with-id-container-1685522036674-0015-01-000014-ip-172-31-xxx-xx-us-compute-internal-8041-timed-out"><a href="#10-java-util-concurrent-TimeoutException-Heartbeat-of-TaskManager-with-id-container-1685522036674-0015-01-000014-ip-172-31-xxx-xx-us-compute-internal-8041-timed-out" class="headerlink" title="10.java.util.concurrent.TimeoutException: Heartbeat of TaskManager with id container_1685522036674_0015_01_000014(ip-172-31-xxx-xx.us.compute.internal:8041) timed out."></a>10.java.util.concurrent.TimeoutException: Heartbeat of TaskManager with id container_1685522036674_0015_01_000014(ip-172-31-xxx-xx.us.compute.internal:8041) timed out.<a href="#10-java-util-concurrent-timeoutexception-heartbeat-of-taskmanager-with-id-container-1685522036674-0015-01-000014-ip-172-31-xxx-xx-us-compute-internal-8041-timed-out" class="header-anchor">#</a></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">java.util.concurrent.TimeoutException: Heartbeat of TaskManager with id container_1685522036674_0015_01_000014(ip-172-31-xxx-xx.us.compute.internal:8041) timed out.</span><br><span class="line">	at org.apache.flink.runtime.jobmaster.JobMaster$TaskManagerHeartbeatListener.notifyHeartbeatTimeout(JobMaster.java:1371)</span><br><span class="line">	at org.apache.flink.runtime.heartbeat.HeartbeatMonitorImpl.run(HeartbeatMonitorImpl.java:155)</span><br><span class="line">	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</span><br><span class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line">	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:443)</span><br><span class="line">	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)</span><br><span class="line">	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:443)</span><br><span class="line">	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:213)</span><br><span class="line">	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)</span><br><span class="line">	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)</span><br><span class="line">	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)</span><br><span class="line">	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)</span><br><span class="line">	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)</span><br><span class="line">	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)</span><br><span class="line">	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)</span><br><span class="line">	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)</span><br><span class="line">	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)</span><br><span class="line">	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)</span><br><span class="line">	at akka.actor.Actor.aroundReceive(Actor.scala:537)</span><br><span class="line">	at akka.actor.Actor.aroundReceive$(Actor.scala:535)</span><br><span class="line">	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)</span><br><span class="line">	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)</span><br><span class="line">	at akka.actor.ActorCell.invoke(ActorCell.scala:548)</span><br><span class="line">	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)</span><br><span class="line">	at akka.dispatch.Mailbox.run(Mailbox.scala:231)</span><br><span class="line">	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)</span><br><span class="line">	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)</span><br><span class="line">	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)</span><br><span class="line">	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)</span><br><span class="line">	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这个报错问题是由于task manager所在机器内存被占满等机器层面的问题，排查方法同下面第10条</p>
<h4 id="11-caused-by-java-util-concurrent-timeoutexception-invocation-of-remoterpcinvocation-taskexecutorgateway-sendoperatoreventtotask-executionattemptid-operatorid-serializedvalue-at-recipient-akka-tcp-102-108-x69-x6e-107-64-x69-112-x2d-x31-x37-50-x2d-51-49-45-x78-120-120-x2d-120-x78-x2e-117-s-compute-internal-33139-user-rpc-taskmanager-0-timed-out"><a href="#11-Caused-by-java-util-concurrent-TimeoutException-Invocation-of-RemoteRpcInvocation-TaskExecutorGateway-sendOperatorEventToTask-ExecutionAttemptID-OperatorID-SerializedValue-at-recipient-akka-tcp-102-108-x69-x6e-107-64-x69-112-x2d-x31-x37-50-x2d-51-49-45-x78-120-120-x2d-120-x78-x2e-117-s-compute-internal-33139-user-rpc-taskmanager-0-timed-out" class="headerlink" title="11.Caused by: java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(TaskExecutorGateway.sendOperatorEventToTask(ExecutionAttemptID, OperatorID, SerializedValue))] at recipient [akka.tcp://&#102;&#108;&#x69;&#x6e;&#107;&#64;&#x69;&#112;&#x2d;&#x31;&#x37;&#50;&#x2d;&#51;&#49;&#45;&#x78;&#120;&#120;&#x2d;&#120;&#x78;&#x2e;&#117;s-.compute.internal:33139/user/rpc/taskmanager_0] timed out"></a>11.Caused by: java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(TaskExecutorGateway.sendOperatorEventToTask(ExecutionAttemptID, OperatorID, SerializedValue))] at recipient [akka.tcp://<a href="mailto:&#102;&#108;&#x69;&#x6e;&#107;&#64;&#x69;&#112;&#x2d;&#x31;&#x37;&#50;&#x2d;&#51;&#49;&#45;&#x78;&#120;&#120;&#x2d;&#120;&#x78;&#x2e;&#117;">&#102;&#108;&#x69;&#x6e;&#107;&#64;&#x69;&#112;&#x2d;&#x31;&#x37;&#50;&#x2d;&#51;&#49;&#45;&#x78;&#120;&#120;&#x2d;&#120;&#x78;&#x2e;&#117;</a>s-.compute.internal:33139/user/rpc/taskmanager_0] timed out<a href="#11-caused-by-java-util-concurrent-timeoutexception-invocation-of-remoterpcinvocation-taskexecutorgateway-sendoperatoreventtotask-executionattemptid-operatorid-serializedvalue-at-recipient-akka-tcp-102-108-x69-x6e-107-64-x69-112-x2d-x31-x37-50-x2d-51-49-45-x78-120-120-x2d-120-x78-x2e-117-s-compute-internal-33139-user-rpc-taskmanager-0-timed-out" class="header-anchor">#</a></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">org.apache.flink.util.FlinkException: An OperatorEvent from an OperatorCoordinator to a task was lost. Triggering task failover to ensure consistency. Event: &#x27;SourceEventWrapper[com.ververica.cdc.connectors.base.source.meta.events.FinishedSnapshotSplitsRequestEvent@19b057ed]&#x27;, targetTask: Source: mongo_cdc_test[1] -&gt; Calc[2] (6/60) - execution #1</span><br><span class="line">	at org.apache.flink.runtime.operators.coordination.SubtaskGatewayImpl.lambda$null$0(SubtaskGatewayImpl.java:90)</span><br><span class="line">	at org.apache.flink.runtime.util.Runnables.lambda$withUncaughtExceptionHandler$0(Runnables.java:49)</span><br><span class="line">	at org.apache.flink.runtime.util.Runnables.assertNoException(Runnables.java:33)</span><br><span class="line">	at org.apache.flink.runtime.operators.coordination.SubtaskGatewayImpl.lambda$sendEvent$1(SubtaskGatewayImpl.java:88)</span><br><span class="line">	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)</span><br><span class="line">	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)</span><br><span class="line">	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)</span><br><span class="line">	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:443)</span><br><span class="line">	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)</span><br><span class="line">	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:443)</span><br><span class="line">	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:213)</span><br><span class="line">	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)</span><br><span class="line">	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)</span><br><span class="line">	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)</span><br><span class="line">	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)</span><br><span class="line">	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)</span><br><span class="line">	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)</span><br><span class="line">	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)</span><br><span class="line">	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)</span><br><span class="line">	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)</span><br><span class="line">	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)</span><br><span class="line">	at akka.actor.Actor.aroundReceive(Actor.scala:537)</span><br><span class="line">	at akka.actor.Actor.aroundReceive$(Actor.scala:535)</span><br><span class="line">	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)</span><br><span class="line">	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)</span><br><span class="line">	at akka.actor.ActorCell.invoke(ActorCell.scala:548)</span><br><span class="line">	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)</span><br><span class="line">	at akka.dispatch.Mailbox.run(Mailbox.scala:231)</span><br><span class="line">	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)</span><br><span class="line">	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)</span><br><span class="line">	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)</span><br><span class="line">	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)</span><br><span class="line">	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)</span><br><span class="line">Caused by: java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(TaskExecutorGateway.sendOperatorEventToTask(ExecutionAttemptID, OperatorID, SerializedValue))] at recipient [akka.tcp://flink@ip-172-31-xxx-xx.us-.compute.internal:33139/user/rpc/taskmanager_0] timed out. This is usually caused by: 1) Akka failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase akka.ask.timeout.</span><br><span class="line">	at org.apache.flink.runtime.jobmaster.RpcTaskManagerGateway.sendOperatorEventToTask(RpcTaskManagerGateway.java:127)</span><br><span class="line">	at org.apache.flink.runtime.executiongraph.Execution.sendOperatorEvent(Execution.java:874)</span><br><span class="line">	at org.apache.flink.runtime.operators.coordination.ExecutionSubtaskAccess.lambda$createEventSendAction$1(ExecutionSubtaskAccess.java:67)</span><br><span class="line">	at org.apache.flink.runtime.operators.coordination.OperatorEventValve.callSendAction(OperatorEventValve.java:180)</span><br><span class="line">	at org.apache.flink.runtime.operators.coordination.OperatorEventValve.sendEvent(OperatorEventValve.java:94)</span><br><span class="line">	at org.apache.flink.runtime.operators.coordination.SubtaskGatewayImpl.lambda$sendEvent$2(SubtaskGatewayImpl.java:98)</span><br><span class="line">	... 26 more</span><br><span class="line">Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@ip-172-31-xxx-xx.us-.compute.internal:33139/user/rpc/taskmanager_0#-116058576]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn&#x27;t send a reply.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这需要去job manager报错日志里面的ip对应的task manager上查看具体日志，排查下来发现是访问s3时候有问题导致的</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line">2023-05-30 10:18:59,460 ERROR org.apache.hudi.sink.compact.CompactFunction                 [] - Executor executes action [Execute compaction for instant 20230530095353629 from task 1] error</span><br><span class="line">org.apache.hudi.exception.HoodieIOException: Could not check if s3a://xxxx/hudi_test35 is a valid table</span><br><span class="line">	at org.apache.hudi.exception.TableNotFoundException.checkTableValidity(TableNotFoundException.java:59) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	at org.apache.hudi.common.table.HoodieTableMetaClient.&lt;init&gt;(HoodieTableMetaClient.java:128) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	at org.apache.hudi.common.table.HoodieTableMetaClient.newMetaClient(HoodieTableMetaClient.java:642) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	at org.apache.hudi.common.table.HoodieTableMetaClient.access$000(HoodieTableMetaClient.java:80) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	at org.apache.hudi.common.table.HoodieTableMetaClient$Builder.build(HoodieTableMetaClient.java:711) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	at org.apache.hudi.table.HoodieFlinkTable.create(HoodieFlinkTable.java:59) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	at org.apache.hudi.client.HoodieFlinkWriteClient.getHoodieTable(HoodieFlinkWriteClient.java:607) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	at org.apache.hudi.sink.compact.CompactFunction.reloadWriteConfig(CompactFunction.java:125) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	at org.apache.hudi.sink.compact.CompactFunction.lambda$processElement$0(CompactFunction.java:95) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	at org.apache.hudi.sink.utils.NonThrownExecutor.lambda$wrapAction$0(NonThrownExecutor.java:130) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_372]</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_372]</span><br><span class="line">	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_372]</span><br><span class="line">Caused by: org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a://xxxx/hudi_test35/.hoodie: com.amazonaws.SdkClientException: Unable to unmarshall response (Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration &#x27;classloader.check-leaked-classloader&#x27;.). Response Code: 200, Response Text: OK: Unable to unmarshall response (Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration &#x27;classloader.check-leaked-classloader&#x27;.). Response Code: 200, Response Text: OK</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:214) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3861) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getFileStatus$24(S3AFileSystem.java:3556) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499) ~[hadoop-common-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444) ~[hadoop-common-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3554) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hudi.common.fs.HoodieWrapperFileSystem.lambda$getFileStatus$17(HoodieWrapperFileSystem.java:402) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	at org.apache.hudi.common.fs.HoodieWrapperFileSystem.executeFuncWithTimeMetrics(HoodieWrapperFileSystem.java:106) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	at org.apache.hudi.common.fs.HoodieWrapperFileSystem.getFileStatus(HoodieWrapperFileSystem.java:396) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	at org.apache.hudi.exception.TableNotFoundException.checkTableValidity(TableNotFoundException.java:51) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	... 12 more</span><br><span class="line">Caused by: com.amazonaws.SdkClientException: Unable to unmarshall response (Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration &#x27;classloader.check-leaked-classloader&#x27;.). Response Code: 200, Response Text: OK</span><br><span class="line">	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleResponse(AmazonHttpClient.java:1818) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleSuccessResponse(AmazonHttpClient.java:1477) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1384) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5397) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:971) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$11(S3AFileSystem.java:2595) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499) ~[hadoop-common-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:2586) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3832) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getFileStatus$24(S3AFileSystem.java:3556) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499) ~[hadoop-common-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444) ~[hadoop-common-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3554) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hudi.common.fs.HoodieWrapperFileSystem.lambda$getFileStatus$17(HoodieWrapperFileSystem.java:402) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	at org.apache.hudi.common.fs.HoodieWrapperFileSystem.executeFuncWithTimeMetrics(HoodieWrapperFileSystem.java:106) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	at org.apache.hudi.common.fs.HoodieWrapperFileSystem.getFileStatus(HoodieWrapperFileSystem.java:396) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	at org.apache.hudi.exception.TableNotFoundException.checkTableValidity(TableNotFoundException.java:51) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	... 12 more</span><br><span class="line">Caused by: java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration &#x27;classloader.check-leaked-classloader&#x27;.</span><br><span class="line">	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:164) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.loadClass(FlinkUserCodeClassLoaders.java:172) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.xml.sax.helpers.NewInstance.newInstance(NewInstance.java:82) ~[?:1.8.0_372]</span><br><span class="line">	at org.xml.sax.helpers.XMLReaderFactory.loadClass(XMLReaderFactory.java:228) ~[?:1.8.0_372]</span><br><span class="line">	at org.xml.sax.helpers.XMLReaderFactory.createXMLReader(XMLReaderFactory.java:191) ~[?:1.8.0_372]</span><br><span class="line">	at com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.&lt;init&gt;(XmlResponsesSaxParser.java:135) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsV2Unmarshaller.unmarshall(Unmarshallers.java:135) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsV2Unmarshaller.unmarshall(Unmarshallers.java:125) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:62) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:31) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.http.response.AwsResponseHandlerAdapter.handle(AwsResponseHandlerAdapter.java:69) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleResponse(AmazonHttpClient.java:1794) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleSuccessResponse(AmazonHttpClient.java:1477) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1384) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5397) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:971) ~[aws-java-sdk-bundle-1.12.331.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$11(S3AFileSystem.java:2595) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499) ~[hadoop-common-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:2586) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3832) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getFileStatus$24(S3AFileSystem.java:3556) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499) ~[hadoop-common-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444) ~[hadoop-common-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3554) ~[hadoop-aws-3.3.3-amzn-1.jar:?]</span><br><span class="line">	at org.apache.hudi.common.fs.HoodieWrapperFileSystem.lambda$getFileStatus$17(HoodieWrapperFileSystem.java:402) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	at org.apache.hudi.common.fs.HoodieWrapperFileSystem.executeFuncWithTimeMetrics(HoodieWrapperFileSystem.java:106) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	at org.apache.hudi.common.fs.HoodieWrapperFileSystem.getFileStatus(HoodieWrapperFileSystem.java:396) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	at org.apache.hudi.exception.TableNotFoundException.checkTableValidity(TableNotFoundException.java:51) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	... 12 more</span><br><span class="line">2023-05-30 10:18:59,461 ERROR org.apache.flink.runtime.util.ClusterUncaughtExceptionHandler [] - WARNING: Thread &#x27;pool-17-thread-2&#x27; produced an uncaught exception. If you want to fail on uncaught exceptions, then configure cluster.uncaught-exception-handling accordingly</span><br><span class="line">org.apache.flink.runtime.execution.CancelTaskException: Buffer pool has already been destroyed.</span><br><span class="line">	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.checkDestroyed(LocalBufferPool.java:404) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegment(LocalBufferPool.java:373) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestBufferBuilder(LocalBufferPool.java:316) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewBufferBuilderFromPool(BufferWritingResultPartition.java:394) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewUnicastBufferBuilder(BufferWritingResultPartition.java:377) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.appendUnicastDataForNewRecord(BufferWritingResultPartition.java:281) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.emitRecord(BufferWritingResultPartition.java:157) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:106) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:54) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:104) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:90) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:44) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.hudi.sink.compact.CompactFunction.lambda$processElement$1(CompactFunction.java:96) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	at org.apache.hudi.sink.utils.NonThrownExecutor.handleException(NonThrownExecutor.java:146) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	at org.apache.hudi.sink.utils.NonThrownExecutor.lambda$wrapAction$0(NonThrownExecutor.java:133) ~[hudi-flink1.15-bundle-0.12.1.jar:0.12.1]</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_372]</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_372]</span><br><span class="line">	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_372]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="12-org-apache-flink-runtime-io-network-netty-exception-remotetransportexception-connection-unexpectedly-closed-by-remote-task-manager-ip-172-31-xxx-xx-us-compute-internal-172-31-xxx-xx-39597-this-might-indicate-that-the-remote-task-manager-was-lost"><a href="#12-org-apache-flink-runtime-io-network-netty-exception-RemoteTransportException-Connection-unexpectedly-closed-by-remote-task-manager-‘ip-172-31-xxx-xx-us-compute-internal-172-31-xxx-xx-39597’-This-might-indicate-that-the-remote-task-manager-was-lost" class="headerlink" title="12.org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connection unexpectedly closed by remote task manager ‘ip-172-31-xxx-xx.us.compute.internal/172.31.xxx.xx:39597’. This might indicate that the remote task manager was lost."></a>12.org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connection unexpectedly closed by remote task manager ‘ip-172-31-xxx-xx.us.compute.internal/172.31.xxx.xx:39597’. This might indicate that the remote task manager was lost.<a href="#12-org-apache-flink-runtime-io-network-netty-exception-remotetransportexception-connection-unexpectedly-closed-by-remote-task-manager-ip-172-31-xxx-xx-us-compute-internal-172-31-xxx-xx-39597-this-might-indicate-that-the-remote-task-manager-was-lost" class="header-anchor">#</a></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connection unexpectedly closed by remote task manager &#x27;ip-172-31-xxx-xx.us.compute.internal/172.31.xxx.xx:39597&#x27;. This might indicate that the remote task manager was lost.</span><br><span class="line">	at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.channelInactive(CreditBasedPartitionRequestClientHandler.java:127)</span><br><span class="line">	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)</span><br><span class="line">	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)</span><br><span class="line">	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)</span><br><span class="line">	at org.apache.flink.shaded.netty4.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)</span><br><span class="line">	at org.apache.flink.runtime.io.network.netty.NettyMessageClientDecoderDelegate.channelInactive(NettyMessageClientDecoderDelegate.java:94)</span><br><span class="line">	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)</span><br><span class="line">	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)</span><br><span class="line">	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)</span><br><span class="line">	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)</span><br><span class="line">	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)</span><br><span class="line">	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)</span><br><span class="line">	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)</span><br><span class="line">	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:831)</span><br><span class="line">	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)</span><br><span class="line">	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)</span><br><span class="line">	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384)</span><br><span class="line">	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)</span><br><span class="line">	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:750)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这种报错通常发生在机器出现内存不足等问题的时候</p>
<p>去对应ip的task manager上查看日志，日志最后显示这个task manager当时正在加载checkpoint</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">2023-05-31 03:19:00,920 INFO  com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem           [] - Opening &#x27;s3://xxx/hudi_test_checkpoint/131ca6b0b81354dfb130360ccf9e9d30/shared/a5edc84c-840c-4ccd-8038-7da256d08cbd&#x27; for reading</span><br><span class="line">2023-05-31 03:19:03,369 INFO  com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem           [] - Opening &#x27;s3://xxx/hudi_test_checkpoint/131ca6b0b81354dfb130360ccf9e9d30/chk-125/9d7e2217-10e5-436d-b678-6ca27ea1c6a0&#x27; for reading</span><br><span class="line">2023-05-31 03:19:03,424 INFO  com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem           [] - Opening &#x27;s3://xxx/hudi_test_checkpoint/131ca6b0b81354dfb130360ccf9e9d30/shared/ad969d73-a803-4e82-b4fa-38a57469d48a&#x27; for reading</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>对比报错发生的时间点和当时机器的内存监控指标</p>
<img src="/images/517519-20230531153224219-2115333833.png" width="1500" height="862" loading="lazy">

<p>发现报错的时候对应ip机器的内存都几乎满了</p>
<img src="/images/517519-20230531154538085-2017072666.png" width="1000" height="259" loading="lazy">

<img src="/images/517519-20230531154155825-238606117.png" width="1000" height="249" loading="lazy">

<img src="/images/517519-20230531154446167-1421223842.png" width="1000" height="214" loading="lazy">

<h4 id="13-caused-by-com-mongodb-mongocommandexception-command-failed-with-error-13-unauthorized-lsquo-not-authorized-on-admin-to-execute-command-aggregate-1-pipeline-changestream-allchangesforcluster-true-cursor-batchsize-1-db-ldquo-admin-rdquo-clustertime-clustertime-timestamp-1680783775-2-signature-hash-bindata-0-898fe82be1837f4bdbda1a12cc1d5f765527a25c-keyid-7175829065197158402-lsid-id-uuid-ldquo-9d99adfc-cc35-4619-b5c5-7ad71ec77df1-rdquo-rsquo-on-server-xxxx-27017"><a href="#13-Caused-by-com-mongodb-MongoCommandException-Command-failed-with-error-13-Unauthorized-lsquo-not-authorized-on-admin-to-execute-command-aggregate-1-pipeline-changeStream-allChangesForCluster-true-cursor-batchSize-1-db-ldquo-admin-rdquo-clusterTime-clusterTime-Timestamp-1680783775-2-signature-hash-BinData-0-898FE82BE1837F4BDBDA1A12CC1D5F765527A25C-keyId-7175829065197158402-lsid-id-UUID-ldquo-9d99adfc-cc35-4619-b5c5-7ad71ec77df1-rdquo-rsquo-on-server-xxxx-27017" class="headerlink" title="13.Caused by: com.mongodb.MongoCommandException: Command failed with error 13 (Unauthorized): &lsquo;not authorized on admin to execute command { aggregate: 1, pipeline: [ { $changeStream: { allChangesForCluster: true } } ], cursor: { batchSize: 1 }, $db: &ldquo;admin&rdquo;, $clusterTime: { clusterTime: Timestamp(1680783775, 2), signature: { hash: BinData(0, 898FE82BE1837F4BDBDA1A12CC1D5F765527A25C), keyId: 7175829065197158402 } }, lsid: { id: UUID(&ldquo;9d99adfc-cc35-4619-b5c5-7ad71ec77df1&rdquo;) } }&rsquo; on server xxxx:27017"></a>13.Caused by: com.mongodb.MongoCommandException: Command failed with error 13 (Unauthorized): &lsquo;not authorized on admin to execute command { aggregate: 1, pipeline: [ { $changeStream: { allChangesForCluster: true } } ], cursor: { batchSize: 1 }, $db: &ldquo;admin&rdquo;, $clusterTime: { clusterTime: Timestamp(1680783775, 2), signature: { hash: BinData(0, 898FE82BE1837F4BDBDA1A12CC1D5F765527A25C), keyId: 7175829065197158402 } }, lsid: { id: UUID(&ldquo;9d99adfc-cc35-4619-b5c5-7ad71ec77df1&rdquo;) } }&rsquo; on server xxxx:27017<a href="#13-caused-by-com-mongodb-mongocommandexception-command-failed-with-error-13-unauthorized-lsquo-not-authorized-on-admin-to-execute-command-aggregate-1-pipeline-changestream-allchangesforcluster-true-cursor-batchsize-1-db-ldquo-admin-rdquo-clustertime-clustertime-timestamp-1680783775-2-signature-hash-bindata-0-898fe82be1837f4bdbda1a12cc1d5f765527a25c-keyid-7175829065197158402-lsid-id-uuid-ldquo-9d99adfc-cc35-4619-b5c5-7ad71ec77df1-rdquo-rsquo-on-server-xxxx-27017" class="header-anchor">#</a></h4><p>这是由于使用的mongo账号没有admin库的读取权限导致的，需要添加一下权限，参考：<a target="_blank" rel="noopener" href="https://ververica.github.io/flink-cdc-connectors/release-2.3/content/connectors/mongodb-cdc.html?highlight=mongo#setup-mongodb">https://ververica.github.io/flink-cdc-connectors/release-2.3/content/connectors/mongodb-cdc.html?highlight=mongo#setup-mongodb</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Caused by: org.apache.flink.util.FlinkRuntimeException: Read split SnapshotSplit&#123;tableId=xxx.xxx, splitId=&amp;lsquo;xxx.xxx:224&amp;rsquo;, splitKeyType=[`_id` INT], splitStart=[&#123;&amp;ldquo;_id&amp;rdquo;: 1.0&#125;, &#123;&amp;ldquo;_id&amp;rdquo;: &amp;ldquo;xxxx&amp;rdquo;&#125;], splitEnd=[&#123;&amp;ldquo;_id&amp;rdquo;: 1.0&#125;, &#123;&amp;ldquo;_id&amp;rdquo;: &amp;ldquo;xxxx&quot;&#125;], highWatermark=null&#125; error due to Command failed with error 13 (Unauthorized): &amp;lsquo;not authorized on admin to execute command &#123; aggregate: 1, pipeline: [ &#123; $changeStream: &#123; allChangesForCluster: true &#125; &#125; ], cursor: &#123; batchSize: 1 &#125;, $db: &amp;ldquo;admin&amp;rdquo;, $clusterTime: &#123; clusterTime: Timestamp(1680783775, 2), signature: &#123; hash: BinData(0, 898FE82BE1837F4BDBDA1A12CC1D5F765527A25C), keyId: 7175829065197158402 &#125; &#125;, lsid: &#123; id: UUID(&amp;ldquo;9d99adfc-cc35-4619-b5c5-7ad71ec77df1&quot;) &#125; &#125;&amp;rsquo; on server xxxx:27017. The full response is &#123;&amp;ldquo;ok&amp;rdquo;: 0.0, &amp;ldquo;errmsg&amp;rdquo;: &amp;ldquo;not authorized on admin to execute command &#123; aggregate: 1, pipeline: [ &#123; $changeStream: &#123; allChangesForCluster: true &#125; &#125; ], cursor: &#123; batchSize: 1 &#125;, $db: \&amp;ldquo;admin\&amp;ldquo;, $clusterTime: &#123; clusterTime: Timestamp(1680783775, 2), signature: &#123; &#125;&#125;&#125;.</span><br><span class="line">    at com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceScanFetcher.checkReadException(IncrementalSourceScanFetcher.java:181)</span><br><span class="line">    at com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceScanFetcher.pollSplitRecords(IncrementalSourceScanFetcher.java:128)</span><br><span class="line">    at com.ververica.cdc.connectors.base.source.reader.IncrementalSourceSplitReader.fetch(IncrementalSourceSplitReader.java:73)</span><br><span class="line">    at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)</span><br><span class="line">    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)</span><br><span class="line">    ... 6 more</span><br><span class="line">Caused by: com.mongodb.MongoCommandException: Command failed with error 13 (Unauthorized): &amp;lsquo;not authorized on admin to execute command &#123; aggregate: 1, pipeline: [ &#123; $changeStream: &#123; allChangesForCluster: true &#125; &#125; ], cursor: &#123; batchSize: 1 &#125;, $db: &amp;ldquo;admin&amp;rdquo;, $clusterTime: &#123; clusterTime: Timestamp(1680783775, 2), signature: &#123;  &#125; &#125;, lsid: &#123;  &#125;&amp;rsquo; on server xxxx:27017. The full response is &#123;&amp;ldquo;ok&amp;rdquo;: 0.0, &amp;ldquo;errmsg&amp;rdquo;: &amp;ldquo;not authorized on admin to execute command &#123; aggregate: 1, pipeline: [ &#123; $changeStream: &#123; allChangesForCluster: true &#125; &#125; ], cursor: &#123; batchSize: 1 &#125;, $db: \&amp;ldquo;admin\&amp;ldquo;, $clusterTime: &#123; clusterTime: Timestamp(1680783775, 2), signature: &#123;  &#125;, lsid: &#123;  &#125;&amp;ldquo;, &amp;ldquo;code&amp;rdquo;: 13, &amp;ldquo;codeName&amp;rdquo;: &amp;ldquo;Unauthorized&amp;rdquo;, &amp;ldquo;operationTime&amp;rdquo;: &#123;&amp;ldquo;$timestamp&amp;rdquo;: &#123;&amp;ldquo;t&amp;rdquo;: 1680783775, &amp;ldquo;i&amp;rdquo;: 2&#125;&#125;, &amp;ldquo;$clusterTime&amp;rdquo;: &#123;&amp;ldquo;clusterTime&amp;rdquo;: &#123;&amp;ldquo;$timestamp&amp;rdquo;: &#123;&amp;ldquo;t&amp;rdquo;: 1680783775, &amp;ldquo;i&amp;rdquo;: 2&#125;&#125;, &amp;ldquo;signature&amp;rdquo;: &#123;&amp;ldquo;hash&amp;rdquo;: &#123;&amp;ldquo;$binary&amp;rdquo;: &#123;&amp;ldquo;base64&quot;: &amp;ldquo;&amp;ldquo;, &amp;ldquo;subType&amp;rdquo;: &amp;ldquo;00&quot;&#125;&#125;, &amp;ldquo;keyId&amp;rdquo;:xxx&#125;&#125;&#125;</span><br></pre></td></tr></table></figure>

<h4 id="14-caused-by-com-mongodb-mongocommandexception-command-failed-with-error-18-authenticationfailed-authentication-failed-on-server-xxx-27017"><a href="#14-Caused-by-com-mongodb-MongoCommandException-Command-failed-with-error-18-AuthenticationFailed-‘Authentication-failed-’-on-server-xxx-27017" class="headerlink" title="14.Caused by: com.mongodb.MongoCommandException: Command failed with error 18 (AuthenticationFailed): ‘Authentication failed.’ on server xxx:27017."></a>14.Caused by: com.mongodb.MongoCommandException: Command failed with error 18 (AuthenticationFailed): ‘Authentication failed.’ on server xxx:27017.<a href="#14-caused-by-com-mongodb-mongocommandexception-command-failed-with-error-18-authenticationfailed-authentication-failed-on-server-xxx-27017" class="header-anchor">#</a></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">org.apache.flink.util.FlinkException: Global failure triggered by OperatorCoordinator for &#x27;Source: mongo_cdc_test[1] -&gt; Calc[2]&#x27; (operator cbc357ccb763df2852fee8c4fc7d55f2).</span><br><span class="line">	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder$LazyInitializedCoordinatorContext.failJob(OperatorCoordinatorHolder.java:556)</span><br><span class="line">	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$QuiesceableContext.failJob(RecreateOnResetOperatorCoordinator.java:231)</span><br><span class="line">	at org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext.failJob(SourceCoordinatorContext.java:316)</span><br><span class="line">	at org.apache.flink.runtime.source.coordinator.SourceCoordinator.start(SourceCoordinator.java:201)</span><br><span class="line">	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$DeferrableCoordinator.resetAndStart(RecreateOnResetOperatorCoordinator.java:394)</span><br><span class="line">	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.lambda$resetToCheckpoint$6(RecreateOnResetOperatorCoordinator.java:144)</span><br><span class="line">	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)</span><br><span class="line">	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)</span><br><span class="line">	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)</span><br><span class="line">	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)</span><br><span class="line">	at org.apache.flink.runtime.operators.coordination.ComponentClosingUtils.lambda$closeAsyncWithTimeout$0(ComponentClosingUtils.java:77)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:750)</span><br><span class="line">Caused by: org.apache.flink.util.FlinkRuntimeException: Failed to discover captured tables for enumerator</span><br><span class="line">	at com.ververica.cdc.connectors.base.source.IncrementalSource.createEnumerator(IncrementalSource.java:151)</span><br><span class="line">	at org.apache.flink.runtime.source.coordinator.SourceCoordinator.start(SourceCoordinator.java:197)</span><br><span class="line">	... 8 more</span><br><span class="line">Caused by: com.mongodb.MongoSecurityException: Exception authenticating MongoCredential&#123;mechanism=SCRAM-SHA-1, userName=&#x27;xxxx&#x27;, source=&#x27;admin&#x27;, password=&lt;hidden&gt;, mechanismProperties=&lt;hidden&gt;&#125;</span><br><span class="line">	at com.mongodb.internal.connection.SaslAuthenticator.wrapException(SaslAuthenticator.java:273)</span><br><span class="line">	at com.mongodb.internal.connection.SaslAuthenticator.getNextSaslResponse(SaslAuthenticator.java:137)</span><br><span class="line">	at com.mongodb.internal.connection.SaslAuthenticator.access$100(SaslAuthenticator.java:48)</span><br><span class="line">	at com.mongodb.internal.connection.SaslAuthenticator$1.run(SaslAuthenticator.java:63)</span><br><span class="line">	at com.mongodb.internal.connection.SaslAuthenticator$1.run(SaslAuthenticator.java:57)</span><br><span class="line">	at com.mongodb.internal.connection.SaslAuthenticator.doAsSubject(SaslAuthenticator.java:280)</span><br><span class="line">	at com.mongodb.internal.connection.SaslAuthenticator.authenticate(SaslAuthenticator.java:57)</span><br><span class="line">	at com.mongodb.internal.connection.DefaultAuthenticator.authenticate(DefaultAuthenticator.java:55)</span><br><span class="line">	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.authenticate(InternalStreamConnectionInitializer.java:205)</span><br><span class="line">	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.finishHandshake(InternalStreamConnectionInitializer.java:79)</span><br><span class="line">	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:170)</span><br><span class="line">	at com.mongodb.internal.connection.UsageTrackingInternalConnection.open(UsageTrackingInternalConnection.java:53)</span><br><span class="line">	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.open(DefaultConnectionPool.java:496)</span><br><span class="line">	at com.mongodb.internal.connection.DefaultConnectionPool$OpenConcurrencyLimiter.openWithConcurrencyLimit(DefaultConnectionPool.java:865)</span><br><span class="line">	at com.mongodb.internal.connection.DefaultConnectionPool$OpenConcurrencyLimiter.openOrGetAvailable(DefaultConnectionPool.java:806)</span><br><span class="line">	at com.mongodb.internal.connection.DefaultConnectionPool.get(DefaultConnectionPool.java:155)</span><br><span class="line">	at com.mongodb.internal.connection.DefaultConnectionPool.get(DefaultConnectionPool.java:145)</span><br><span class="line">	at com.mongodb.internal.connection.DefaultServer.getConnection(DefaultServer.java:92)</span><br><span class="line">	at com.mongodb.internal.binding.ClusterBinding$ClusterBindingConnectionSource.getConnection(ClusterBinding.java:141)</span><br><span class="line">	at com.mongodb.client.internal.ClientSessionBinding$SessionBindingConnectionSource.getConnection(ClientSessionBinding.java:163)</span><br><span class="line">	at com.mongodb.internal.operation.CommandOperationHelper.lambda$executeCommand$4(CommandOperationHelper.java:190)</span><br><span class="line">	at com.mongodb.internal.operation.OperationHelper.withReadConnectionSource(OperationHelper.java:583)</span><br><span class="line">	at com.mongodb.internal.operation.CommandOperationHelper.executeCommand(CommandOperationHelper.java:189)</span><br><span class="line">	at com.mongodb.internal.operation.ListDatabasesOperation.execute(ListDatabasesOperation.java:201)</span><br><span class="line">	at com.mongodb.internal.operation.ListDatabasesOperation.execute(ListDatabasesOperation.java:54)</span><br><span class="line">	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:184)</span><br><span class="line">	at com.mongodb.client.internal.MongoIterableImpl.execute(MongoIterableImpl.java:135)</span><br><span class="line">	at com.mongodb.client.internal.MongoIterableImpl.iterator(MongoIterableImpl.java:92)</span><br><span class="line">	at com.mongodb.client.internal.MongoIterableImpl.forEach(MongoIterableImpl.java:121)</span><br><span class="line">	at com.mongodb.client.internal.MappingIterable.forEach(MappingIterable.java:59)</span><br><span class="line">	at com.ververica.cdc.connectors.mongodb.source.utils.CollectionDiscoveryUtils.databaseNames(CollectionDiscoveryUtils.java:56)</span><br><span class="line">	at com.ververica.cdc.connectors.mongodb.source.dialect.MongoDBDialect.lambda$discoverAndCacheDataCollections$0(MongoDBDialect.java:99)</span><br><span class="line">	at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)</span><br><span class="line">	at com.ververica.cdc.connectors.mongodb.source.dialect.MongoDBDialect.discoverAndCacheDataCollections(MongoDBDialect.java:94)</span><br><span class="line">	at com.ververica.cdc.connectors.mongodb.source.dialect.MongoDBDialect.discoverDataCollections(MongoDBDialect.java:75)</span><br><span class="line">	at com.ververica.cdc.connectors.mongodb.source.dialect.MongoDBDialect.discoverDataCollections(MongoDBDialect.java:60)</span><br><span class="line">	at com.ververica.cdc.connectors.base.source.IncrementalSource.createEnumerator(IncrementalSource.java:139)</span><br><span class="line">	... 9 more</span><br><span class="line">Caused by: com.mongodb.MongoCommandException: Command failed with error 18 (AuthenticationFailed): &#x27;Authentication failed.&#x27; on server xxx:27017. The full response is &#123;&quot;operationTime&quot;: &#123;&quot;$timestamp&quot;: &#123;&quot;t&quot;: 1686237336, &quot;i&quot;: 24&#125;&#125;, &quot;ok&quot;: 0.0, &quot;errmsg&quot;: &quot;Authentication failed.&quot;, &quot;code&quot;: 18, &quot;codeName&quot;: &quot;AuthenticationFailed&quot;, &quot;$clusterTime&quot;: &#123;&quot;clusterTime&quot;: &#123;&quot;$timestamp&quot;: &#123;&quot;t&quot;: 1686237336, &quot;i&quot;: 24&#125;&#125;, &quot;signature&quot;: &#123;&quot;hash&quot;: &#123;&quot;$binary&quot;: &#123;&quot;base64&quot;: &quot;xjwzreIFobScDm4vZ2UH319Iklo=&quot;, &quot;subType&quot;: &quot;00&quot;&#125;&#125;, &quot;keyId&quot;: 7192072244963573763&#125;&#125;&#125;</span><br><span class="line">	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:195)</span><br><span class="line">	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:400)</span><br><span class="line">	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:324)</span><br><span class="line">	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)</span><br><span class="line">	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)</span><br><span class="line">	at com.mongodb.internal.connection.SaslAuthenticator.sendSaslStart(SaslAuthenticator.java:228)</span><br><span class="line">	at com.mongodb.internal.connection.SaslAuthenticator.getNextSaslResponse(SaslAuthenticator.java:135)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这也是由于使用的mongo账号没有权限导致的，除了admin库的读取权限之外，还需要如下权限，参考：<a target="_blank" rel="noopener" href="https://ververica.github.io/flink-cdc-connectors/release-2.3/content/connectors/mongodb-cdc.html?highlight=mongo#setup-mongodb">https://ververica.github.io/flink-cdc-connectors/release-2.3/content/connectors/mongodb-cdc.html?highlight=mongo#setup-mongodb</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">use admin;</span><br><span class="line">db.createRole(</span><br><span class="line">    &#123;</span><br><span class="line">        role: &quot;flinkrole&quot;,</span><br><span class="line">        privileges: [&#123;</span><br><span class="line">            // Grant privileges on all non-system collections in all databases</span><br><span class="line">            resource: &#123; db: &quot;&quot;, collection: &quot;&quot; &#125;,</span><br><span class="line">            actions: [</span><br><span class="line">                &quot;splitVector&quot;,</span><br><span class="line">                &quot;listDatabases&quot;,</span><br><span class="line">                &quot;listCollections&quot;,</span><br><span class="line">                &quot;collStats&quot;,</span><br><span class="line">                &quot;find&quot;,</span><br><span class="line">                &quot;changeStream&quot; ]</span><br><span class="line">        &#125;],</span><br><span class="line">        roles: [</span><br><span class="line">            // Read config.collections and config.chunks</span><br><span class="line">            // for sharded cluster snapshot splitting.</span><br><span class="line">            &#123; role: &#x27;read&#x27;, db: &#x27;config&#x27; &#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">db.createUser(</span><br><span class="line">  &#123;</span><br><span class="line">      user: &#x27;flinkuser&#x27;,</span><br><span class="line">      pwd: &#x27;flinkpw&#x27;,</span><br><span class="line">      roles: [</span><br><span class="line">         &#123; role: &#x27;flinkrole&#x27;, db: &#x27;admin&#x27; &#125;</span><br><span class="line">      ]</span><br><span class="line">  &#125;</span><br><span class="line">);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="15-checkpoint-chao-shi"><a href="#15-checkpoint超时" class="headerlink" title="15.checkpoint超时"></a>15.checkpoint超时<a href="#15-checkpoint-chao-shi" class="header-anchor">#</a></h4><p>如果发现checkpoint间歇性失败，可以看一下是否是checkpoint超过默认10分钟的限制，对于大表，可能checkpoint会达到30分钟，可以通过以下方式调整checkpoint超时时间</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// checkpoint1小时超时</span><br><span class="line">env.getCheckpointConfig().setCheckpointTimeout(3600000);</span><br></pre></td></tr></table></figure>

<img src="/images/517519-20230614164231567-1109743112.png" alt loading="lazy">

<h4 id="16-org-apache-flink-streaming-runtime-tasks-exceptioninchainedoperatorexception-could-not-forward-element-to-next-operator"><a href="#16-org-apache-flink-streaming-runtime-tasks-ExceptionInChainedOperatorException-Could-not-forward-element-to-next-operator" class="headerlink" title="16.org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator``"></a>16.org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator``<a href="#16-org-apache-flink-streaming-runtime-tasks-exceptioninchainedoperatorexception-could-not-forward-element-to-next-operator" class="header-anchor">#</a></h4><p>这是由于flink的tuple类型不支持包含null，比如[“test1”,null,”test2”]，从而导致遍历数组的时候出现NullPointException</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">2023-07-04 14:53:47</span><br><span class="line">java.lang.NullPointerException</span><br><span class="line">	at org.apache.flink.table.runtime.typeutils.StringDataSerializer.copy(StringDataSerializer.java:56)</span><br><span class="line">	at org.apache.flink.table.runtime.typeutils.StringDataSerializer.copy(StringDataSerializer.java:34)</span><br><span class="line">	at org.apache.flink.table.runtime.typeutils.ArrayDataSerializer.copyGenericArray(ArrayDataSerializer.java:128)</span><br><span class="line">	at org.apache.flink.table.runtime.typeutils.ArrayDataSerializer.copy(ArrayDataSerializer.java:86)</span><br><span class="line">	at org.apache.flink.table.runtime.typeutils.ArrayDataSerializer.copy(ArrayDataSerializer.java:47)</span><br><span class="line">	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copyRowData(RowDataSerializer.java:170)</span><br><span class="line">	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:131)</span><br><span class="line">	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:48)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:80)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask$AsyncDataOutputToOutput.emitRecord(SourceOperatorStreamTask.java:196)</span><br><span class="line">	at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:110)</span><br><span class="line">	at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:101)</span><br><span class="line">	at com.ververica.cdc.connectors.base.source.reader.IncrementalSourceRecordEmitter$OutputCollector.collect(IncrementalSourceRecordEmitter.java:166)</span><br><span class="line">	at com.ververica.cdc.connectors.mongodb.table.MongoDBConnectorDeserializationSchema.emit(MongoDBConnectorDeserializationSchema.java:216)</span><br><span class="line">	at com.ververica.cdc.connectors.mongodb.table.MongoDBConnectorDeserializationSchema.deserialize(MongoDBConnectorDeserializationSchema.java:143)</span><br><span class="line">	at com.ververica.cdc.connectors.base.source.reader.IncrementalSourceRecordEmitter.emitElement(IncrementalSourceRecordEmitter.java:141)</span><br><span class="line">	at com.ververica.cdc.connectors.mongodb.source.reader.MongoDBRecordEmitter.processElement(MongoDBRecordEmitter.java:79)</span><br><span class="line">	at com.ververica.cdc.connectors.base.source.reader.IncrementalSourceRecordEmitter.emitRecord(IncrementalSourceRecordEmitter.java:86)</span><br><span class="line">	at com.ververica.cdc.connectors.base.source.reader.IncrementalSourceRecordEmitter.emitRecord(IncrementalSourceRecordEmitter.java:55)</span><br><span class="line">	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:143)</span><br><span class="line">	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:351)</span><br><span class="line">	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)</span><br><span class="line">	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)</span><br><span class="line">	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)</span><br><span class="line">	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)</span><br><span class="line">	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)</span><br><span class="line">	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)</span><br><span class="line">	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:750)　　</span><br></pre></td></tr></table></figure>

<p>参考：<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/45567505/apache-flink-nullpointerexception-caused-by-tupleserializer">https://stackoverflow.com/questions/45567505/apache-flink-nullpointerexception-caused-by-tupleserializer</a></p>
<h4 id="17-java-lang-noclassdeffounderror-could-not-initialize-class-com-ververica-cdc-connectors-mongodb-internal-mongodbenvelope"><a href="#17-java-lang-NoClassDefFoundError-Could-not-initialize-class-com-ververica-cdc-connectors-mongodb-internal-MongoDBEnvelope" class="headerlink" title="17.java.lang.NoClassDefFoundError: Could not initialize class com.ververica.cdc.connectors.mongodb.internal.MongoDBEnvelope"></a>17.java.lang.NoClassDefFoundError: Could not initialize class com.ververica.cdc.connectors.mongodb.internal.MongoDBEnvelope<a href="#17-java-lang-noclassdeffounderror-could-not-initialize-class-com-ververica-cdc-connectors-mongodb-internal-mongodbenvelope" class="header-anchor">#</a></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">2023-07-13 08:08:55,200 ERROR org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Failed to create Source Enumerator for source Source: mongo_cdc_test[1]</span><br><span class="line">java.lang.NoClassDefFoundError: Could not initialize class com.ververica.cdc.connectors.mongodb.internal.MongoDBEnvelope</span><br><span class="line">	at com.ververica.cdc.connectors.mongodb.source.utils.MongoUtils.buildConnectionString(MongoUtils.java:373) ~[flink-sql-connector-mongodb-cdc-2.5-SNAPSHOT.jar:2.5-SNAPSHOT]</span><br><span class="line">	at com.ververica.cdc.connectors.mongodb.source.config.MongoDBSourceConfig.&lt;init&gt;(MongoDBSourceConfig.java:79) ~[flink-sql-connector-mongodb-cdc-2.5-SNAPSHOT.jar:2.5-SNAPSHOT]</span><br><span class="line">	at com.ververica.cdc.connectors.mongodb.source.config.MongoDBSourceConfigFactory.create(MongoDBSourceConfigFactory.java:253) ~[flink-sql-connector-mongodb-cdc-2.5-SNAPSHOT.jar:2.5-SNAPSHOT]</span><br><span class="line">	at com.ververica.cdc.connectors.mongodb.source.config.MongoDBSourceConfigFactory.create(MongoDBSourceConfigFactory.java:41) ~[flink-sql-connector-mongodb-cdc-2.5-SNAPSHOT.jar:2.5-SNAPSHOT]</span><br><span class="line">	at com.ververica.cdc.connectors.base.source.IncrementalSource.createEnumerator(IncrementalSource.java:134) ~[flink-sql-connector-mongodb-cdc-2.5-SNAPSHOT.jar:2.5-SNAPSHOT]</span><br><span class="line">	at org.apache.flink.runtime.source.coordinator.SourceCoordinator.start(SourceCoordinator.java:197) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$DeferrableCoordinator.resetAndStart(RecreateOnResetOperatorCoordinator.java:394) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.lambda$resetToCheckpoint$6(RecreateOnResetOperatorCoordinator.java:144) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_372]</span><br><span class="line">	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_372]</span><br><span class="line">	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_372]</span><br><span class="line">	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) ~[?:1.8.0_372]</span><br><span class="line">	at org.apache.flink.runtime.operators.coordination.ComponentClosingUtils.lambda$closeAsyncWithTimeout$0(ComponentClosingUtils.java:77) ~[flink-dist-1.15.2.jar:1.15.2]</span><br><span class="line">	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_372]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>该报错是由于没有按照flink cdc文档来使用jar导致的</p>
<p>在项目的pom.xml中使用的依赖应该如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.ververica&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-connector-mongodb-cdc&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.5-SNAPSHOT&lt;/version&gt;</span><br><span class="line">    &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在${FLINK_HOME}/lib目录下部署的jar应该如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">flink-sql-connector-mongodb-cdc-2.5-SNAPSHOT.jar</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>参考文档：<a target="_blank" rel="noopener" href="https://github.com/apache/flink-cdc/blob/master/docs/content/docs/connectors/flink-sources/mongodb-cdc.md">https://github.com/apache/flink-cdc/blob/master/docs/content/docs/connectors/flink-sources/mongodb-cdc.md</a></p>
<h3 id="5-mongo-cdc-ye-jie-shi-yong-an-li"><a href="#5-Mongo-CDC业界使用案例" class="headerlink" title="5.Mongo CDC业界使用案例"></a>5.Mongo CDC业界使用案例<a href="#5-mongo-cdc-ye-jie-shi-yong-an-li" class="header-anchor">#</a></h3><h4 id="1-flink-cdc-xi-lie-flink-mongodb-cdc-zai-xtransfer-de-sheng-chan-shi-jian-nbsp-xtransfer"><a href="#1-Flink-CDC-系列-Flink-MongoDB-CDC-在-XTransfer-的生产实践-nbsp-（XTransfer）" class="headerlink" title="1.Flink CDC 系列 - Flink MongoDB CDC 在 XTransfer 的生产实践&nbsp;（XTransfer）"></a>1.<a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/845666">Flink CDC 系列 - Flink MongoDB CDC 在 XTransfer 的生产实践</a>&nbsp;（XTransfer）<a href="#1-flink-cdc-xi-lie-flink-mongodb-cdc-zai-xtransfer-de-sheng-chan-shi-jian-nbsp-xtransfer" class="header-anchor">#</a></h4><p>&nbsp;</p>

        <div class="article-entry">
          <div id="footer-info" class="inner">
          本文只发表于博客园和<a href="https://tonglin0325.github.io" target="_blank">tonglin0325的博客</a>，作者：tonglin0325，转载请注明原文链接：<a href="https://tonglin0325.github.io/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94Flink%20Mongo%20CDC.html">Flink学习笔记——Flink Mongo CDC</a>
          </div>
        </div>
      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://tonglin0325.github.io/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94Flink%20Mongo%20CDC.html" data-id="clzz1punh0001jr0lav3mdpn4" class="article-share-link">
        <i class="fa fa-share"></i> 分享
      </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Flink/" rel="tag">Flink</a></li></ul>


    </footer>
  </div>
  
    
<ul id="article-nav" class="nav nav-pills nav-justified">
  
  <li role="presentation">
    <a href="/Hive%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%87%BD%E6%95%B0.html" id="article-nav-older" class="article-nav-link-wrap">
      <i class="fa fa-chevron-left pull-left"></i>
      <span class="article-nav-link-title">Hive学习笔记——函数</span>
    </a>
  </li>
  
  
  <li role="presentation">
    <a href="/Spring%20MVC%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%94%A8%E6%88%B7%E5%A2%9E%E5%88%A0%E8%AF%A5%E6%9F%A5%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E9%AA%8C%E8%AF%81.html" id="article-nav-newer" class="article-nav-link-wrap">
      <span class="article-nav-link-title">Spring MVC学习笔记——用户增删该查和服务器端验证</span>
      <i class="fa fa-chevron-right pull-right"></i>
    </a>
  </li>
  
</ul>


  
</article>




        </div>
        <div class="col-sm-3 col-sm-offset-1 blog-sidebar">
          
  <div class="sidebar-module sidebar-module-inset">
  <h4>tonglin0325</h4>
  <div style="text-align:center;"><img src="/images/WechatIMG1.jpeg" width="150" height="150" style="vertical-align:middle;"/></div>
</div>


  


  
  <div class="sidebar-module">
    <h4>标签</h4>
    <ul class="sidebar-module-list" itemprop="keywords"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/AWS/" rel="tag">AWS</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Airbnb/" rel="tag">Airbnb</a><span class="sidebar-module-list-count">10</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Android/" rel="tag">Android</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/CDH/" rel="tag">CDH</a><span class="sidebar-module-list-count">11</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Doris/" rel="tag">Doris</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/ELK/" rel="tag">ELK</a><span class="sidebar-module-list-count">14</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Flink/" rel="tag">Flink</a><span class="sidebar-module-list-count">29</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Git/" rel="tag">Git</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Grafana/" rel="tag">Grafana</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/HAProxy/" rel="tag">HAProxy</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/HBase/" rel="tag">HBase</a><span class="sidebar-module-list-count">7</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a><span class="sidebar-module-list-count">7</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Hive/" rel="tag">Hive</a><span class="sidebar-module-list-count">20</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Hudi/" rel="tag">Hudi</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/InfluxDB/" rel="tag">InfluxDB</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/JDBC/" rel="tag">JDBC</a><span class="sidebar-module-list-count">9</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/JMeter/" rel="tag">JMeter</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/JVM/" rel="tag">JVM</a><span class="sidebar-module-list-count">5</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Java/" rel="tag">Java</a><span class="sidebar-module-list-count">21</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/JavaWeb/" rel="tag">JavaWeb</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/LaTex/" rel="tag">LaTex</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Linkedin/" rel="tag">Linkedin</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Linux/" rel="tag">Linux</a><span class="sidebar-module-list-count">34</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/ML/" rel="tag">ML</a><span class="sidebar-module-list-count">23</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/ML-Infra/" rel="tag">ML Infra</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/MPP/" rel="tag">MPP</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Maven/" rel="tag">Maven</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/MySQL/" rel="tag">MySQL</a><span class="sidebar-module-list-count">16</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Nexus/" rel="tag">Nexus</a><span class="sidebar-module-list-count">5</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/OpenTSDB/" rel="tag">OpenTSDB</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Paper/" rel="tag">Paper</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Play/" rel="tag">Play</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Prometheus/" rel="tag">Prometheus</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Python/" rel="tag">Python</a><span class="sidebar-module-list-count">21</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/React/" rel="tag">React</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Redis/" rel="tag">Redis</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/RocksDB/" rel="tag">RocksDB</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Scala/" rel="tag">Scala</a><span class="sidebar-module-list-count">10</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Solr/" rel="tag">Solr</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Spark/" rel="tag">Spark</a><span class="sidebar-module-list-count">26</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/SpringBoot/" rel="tag">SpringBoot</a><span class="sidebar-module-list-count">19</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/SpringMVC/" rel="tag">SpringMVC</a><span class="sidebar-module-list-count">14</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Thrift/" rel="tag">Thrift</a><span class="sidebar-module-list-count">6</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/YARN/" rel="tag">YARN</a><span class="sidebar-module-list-count">5</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/antlr/" rel="tag">antlr</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/arthas/" rel="tag">arthas</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/avro/" rel="tag">avro</a><span class="sidebar-module-list-count">10</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/cassandra/" rel="tag">cassandra</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/clickhouse/" rel="tag">clickhouse</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/confluent/" rel="tag">confluent</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/docker/" rel="tag">docker</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/filebeat/" rel="tag">filebeat</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/flume/" rel="tag">flume</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/golang/" rel="tag">golang</a><span class="sidebar-module-list-count">17</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/google/" rel="tag">google</a><span class="sidebar-module-list-count">8</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/gradle/" rel="tag">gradle</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/hexo/" rel="tag">hexo</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/impala/" rel="tag">impala</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/jenkins/" rel="tag">jenkins</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/k8s/" rel="tag">k8s</a><span class="sidebar-module-list-count">11</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/kafka/" rel="tag">kafka</a><span class="sidebar-module-list-count">12</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/kerberos/" rel="tag">kerberos</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/kudu/" rel="tag">kudu</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/ldap/" rel="tag">ldap</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/mac/" rel="tag">mac</a><span class="sidebar-module-list-count">5</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/mongo/" rel="tag">mongo</a><span class="sidebar-module-list-count">5</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/mybatis/" rel="tag">mybatis</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/nginx/" rel="tag">nginx</a><span class="sidebar-module-list-count">6</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/nlp/" rel="tag">nlp</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/open-falcon/" rel="tag">open-falcon</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/openwrt/" rel="tag">openwrt</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/parquet/" rel="tag">parquet</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/presto/" rel="tag">presto</a><span class="sidebar-module-list-count">6</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/ranger/" rel="tag">ranger</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/scyllaDB/" rel="tag">scyllaDB</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/twitter/" rel="tag">twitter</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/zookeeper/" rel="tag">zookeeper</a><span class="sidebar-module-list-count">12</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E5%88%B7%E9%A2%98/" rel="tag">刷题</a><span class="sidebar-module-list-count">22</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E5%89%8D%E7%AB%AF/" rel="tag">前端</a><span class="sidebar-module-list-count">6</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E5%9B%BE%E5%AD%98%E5%82%A8%E5%8F%8A%E8%AE%A1%E7%AE%97/" rel="tag">图存储及计算</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/" rel="tag">多线程</a><span class="sidebar-module-list-count">9</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E5%B9%BF%E5%91%8A%E7%B3%BB%E7%BB%9F/" rel="tag">广告系统</a><span class="sidebar-module-list-count">8</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/" rel="tag">开发工具</a><span class="sidebar-module-list-count">16</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" rel="tag">数据结构</a><span class="sidebar-module-list-count">18</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E6%9D%82%E8%B0%88/" rel="tag">杂谈</a><span class="sidebar-module-list-count">9</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a><span class="sidebar-module-list-count">25</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/" rel="tag">系统设计</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/" rel="tag">计算机基础</a><span class="sidebar-module-list-count">9</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" rel="tag">设计模式</a><span class="sidebar-module-list-count">5</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>标签云</h4>
    <p class="tagcloud">
      <a href="/tags/AWS/" style="font-size: 11.25px;">AWS</a> <a href="/tags/Airbnb/" style="font-size: 13.75px;">Airbnb</a> <a href="/tags/Android/" style="font-size: 11.25px;">Android</a> <a href="/tags/CDH/" style="font-size: 14.17px;">CDH</a> <a href="/tags/Doris/" style="font-size: 10px;">Doris</a> <a href="/tags/ELK/" style="font-size: 15px;">ELK</a> <a href="/tags/Flink/" style="font-size: 19.58px;">Flink</a> <a href="/tags/Git/" style="font-size: 10.42px;">Git</a> <a href="/tags/Grafana/" style="font-size: 10px;">Grafana</a> <a href="/tags/HAProxy/" style="font-size: 10.83px;">HAProxy</a> <a href="/tags/HBase/" style="font-size: 12.5px;">HBase</a> <a href="/tags/Hadoop/" style="font-size: 12.5px;">Hadoop</a> <a href="/tags/Hive/" style="font-size: 17.08px;">Hive</a> <a href="/tags/Hudi/" style="font-size: 11.25px;">Hudi</a> <a href="/tags/InfluxDB/" style="font-size: 10px;">InfluxDB</a> <a href="/tags/JDBC/" style="font-size: 13.33px;">JDBC</a> <a href="/tags/JMeter/" style="font-size: 10px;">JMeter</a> <a href="/tags/JVM/" style="font-size: 11.67px;">JVM</a> <a href="/tags/Java/" style="font-size: 17.5px;">Java</a> <a href="/tags/JavaWeb/" style="font-size: 10.83px;">JavaWeb</a> <a href="/tags/LaTex/" style="font-size: 10px;">LaTex</a> <a href="/tags/Linkedin/" style="font-size: 10px;">Linkedin</a> <a href="/tags/Linux/" style="font-size: 20px;">Linux</a> <a href="/tags/ML/" style="font-size: 18.33px;">ML</a> <a href="/tags/ML-Infra/" style="font-size: 11.25px;">ML Infra</a> <a href="/tags/MPP/" style="font-size: 10px;">MPP</a> <a href="/tags/Maven/" style="font-size: 10.42px;">Maven</a> <a href="/tags/MySQL/" style="font-size: 15.42px;">MySQL</a> <a href="/tags/Nexus/" style="font-size: 11.67px;">Nexus</a> <a href="/tags/OpenTSDB/" style="font-size: 10.42px;">OpenTSDB</a> <a href="/tags/Paper/" style="font-size: 10px;">Paper</a> <a href="/tags/Play/" style="font-size: 10px;">Play</a> <a href="/tags/Prometheus/" style="font-size: 10.42px;">Prometheus</a> <a href="/tags/Python/" style="font-size: 17.5px;">Python</a> <a href="/tags/React/" style="font-size: 10.42px;">React</a> <a href="/tags/Redis/" style="font-size: 10.83px;">Redis</a> <a href="/tags/RocksDB/" style="font-size: 10px;">RocksDB</a> <a href="/tags/Scala/" style="font-size: 13.75px;">Scala</a> <a href="/tags/Solr/" style="font-size: 10.83px;">Solr</a> <a href="/tags/Spark/" style="font-size: 19.17px;">Spark</a> <a href="/tags/SpringBoot/" style="font-size: 16.67px;">SpringBoot</a> <a href="/tags/SpringMVC/" style="font-size: 15px;">SpringMVC</a> <a href="/tags/Thrift/" style="font-size: 12.08px;">Thrift</a> <a href="/tags/YARN/" style="font-size: 11.67px;">YARN</a> <a href="/tags/antlr/" style="font-size: 10.42px;">antlr</a> <a href="/tags/arthas/" style="font-size: 10px;">arthas</a> <a href="/tags/avro/" style="font-size: 13.75px;">avro</a> <a href="/tags/cassandra/" style="font-size: 10px;">cassandra</a> <a href="/tags/clickhouse/" style="font-size: 10.42px;">clickhouse</a> <a href="/tags/confluent/" style="font-size: 10.42px;">confluent</a> <a href="/tags/docker/" style="font-size: 10.83px;">docker</a> <a href="/tags/filebeat/" style="font-size: 11.25px;">filebeat</a> <a href="/tags/flume/" style="font-size: 10.42px;">flume</a> <a href="/tags/golang/" style="font-size: 15.83px;">golang</a> <a href="/tags/google/" style="font-size: 12.92px;">google</a> <a href="/tags/gradle/" style="font-size: 10px;">gradle</a> <a href="/tags/hexo/" style="font-size: 10px;">hexo</a> <a href="/tags/impala/" style="font-size: 11.25px;">impala</a> <a href="/tags/jenkins/" style="font-size: 10px;">jenkins</a> <a href="/tags/k8s/" style="font-size: 14.17px;">k8s</a> <a href="/tags/kafka/" style="font-size: 14.58px;">kafka</a> <a href="/tags/kerberos/" style="font-size: 10px;">kerberos</a> <a href="/tags/kudu/" style="font-size: 10.42px;">kudu</a> <a href="/tags/ldap/" style="font-size: 10.42px;">ldap</a> <a href="/tags/mac/" style="font-size: 11.67px;">mac</a> <a href="/tags/mongo/" style="font-size: 11.67px;">mongo</a> <a href="/tags/mybatis/" style="font-size: 10.83px;">mybatis</a> <a href="/tags/nginx/" style="font-size: 12.08px;">nginx</a> <a href="/tags/nlp/" style="font-size: 10px;">nlp</a> <a href="/tags/open-falcon/" style="font-size: 10.83px;">open-falcon</a> <a href="/tags/openwrt/" style="font-size: 10px;">openwrt</a> <a href="/tags/parquet/" style="font-size: 10.83px;">parquet</a> <a href="/tags/presto/" style="font-size: 12.08px;">presto</a> <a href="/tags/ranger/" style="font-size: 10px;">ranger</a> <a href="/tags/scyllaDB/" style="font-size: 10.83px;">scyllaDB</a> <a href="/tags/twitter/" style="font-size: 10.42px;">twitter</a> <a href="/tags/zookeeper/" style="font-size: 14.58px;">zookeeper</a> <a href="/tags/%E5%88%B7%E9%A2%98/" style="font-size: 17.92px;">刷题</a> <a href="/tags/%E5%89%8D%E7%AB%AF/" style="font-size: 12.08px;">前端</a> <a href="/tags/%E5%9B%BE%E5%AD%98%E5%82%A8%E5%8F%8A%E8%AE%A1%E7%AE%97/" style="font-size: 10.42px;">图存储及计算</a> <a href="/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/" style="font-size: 13.33px;">多线程</a> <a href="/tags/%E5%B9%BF%E5%91%8A%E7%B3%BB%E7%BB%9F/" style="font-size: 12.92px;">广告系统</a> <a href="/tags/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/" style="font-size: 15.42px;">开发工具</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 16.25px;">数据结构</a> <a href="/tags/%E6%9D%82%E8%B0%88/" style="font-size: 13.33px;">杂谈</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 18.75px;">算法</a> <a href="/tags/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/" style="font-size: 10.42px;">系统设计</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/" style="font-size: 13.33px;">计算机基础</a> <a href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" style="font-size: 11.67px;">设计模式</a>
    </p>
  </div>



  
  <div class="sidebar-module">
    <h4>文章归档</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2025/02/">二月 2025</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2024/10/">十月 2024</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2024/08/">八月 2024</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2024/05/">五月 2024</a><span class="sidebar-module-list-count">6</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2023/08/">八月 2023</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2022/07/">七月 2022</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2022/01/">一月 2022</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2021/12/">十二月 2021</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2021/11/">十一月 2021</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2021/09/">九月 2021</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2021/07/">七月 2021</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2021/05/">五月 2021</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2021/03/">三月 2021</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2021/01/">一月 2021</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/12/">十二月 2020</a><span class="sidebar-module-list-count">8</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/11/">十一月 2020</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/10/">十月 2020</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/09/">九月 2020</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/08/">八月 2020</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/07/">七月 2020</a><span class="sidebar-module-list-count">12</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/06/">六月 2020</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/05/">五月 2020</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/04/">四月 2020</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/03/">三月 2020</a><span class="sidebar-module-list-count">10</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/02/">二月 2020</a><span class="sidebar-module-list-count">6</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/01/">一月 2020</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/10/">十月 2019</a><span class="sidebar-module-list-count">12</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/09/">九月 2019</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/08/">八月 2019</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/07/">七月 2019</a><span class="sidebar-module-list-count">8</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/04/">四月 2019</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/03/">三月 2019</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/01/">一月 2019</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/12/">十二月 2018</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/11/">十一月 2018</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/09/">九月 2018</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/06/">六月 2018</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/05/">五月 2018</a><span class="sidebar-module-list-count">10</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/04/">四月 2018</a><span class="sidebar-module-list-count">10</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/02/">二月 2018</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/01/">一月 2018</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/11/">十一月 2017</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/09/">九月 2017</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/07/">七月 2017</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/06/">六月 2017</a><span class="sidebar-module-list-count">5</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/05/">五月 2017</a><span class="sidebar-module-list-count">13</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/04/">四月 2017</a><span class="sidebar-module-list-count">23</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/03/">三月 2017</a><span class="sidebar-module-list-count">6</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/01/">一月 2017</a><span class="sidebar-module-list-count">23</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/12/">十二月 2016</a><span class="sidebar-module-list-count">15</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/11/">十一月 2016</a><span class="sidebar-module-list-count">21</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/10/">十月 2016</a><span class="sidebar-module-list-count">12</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/09/">九月 2016</a><span class="sidebar-module-list-count">16</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/08/">八月 2016</a><span class="sidebar-module-list-count">10</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/07/">七月 2016</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/06/">六月 2016</a><span class="sidebar-module-list-count">12</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/05/">五月 2016</a><span class="sidebar-module-list-count">32</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/04/">四月 2016</a><span class="sidebar-module-list-count">27</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/03/">三月 2016</a><span class="sidebar-module-list-count">134</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/02/">二月 2016</a><span class="sidebar-module-list-count">22</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2015/09/">九月 2015</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2015/08/">八月 2015</a><span class="sidebar-module-list-count">13</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2015/07/">七月 2015</a><span class="sidebar-module-list-count">32</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2015/06/">六月 2015</a><span class="sidebar-module-list-count">27</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2015/04/">四月 2015</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2013/04/">四月 2013</a><span class="sidebar-module-list-count">2</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>最近文章</h4>
    <ul class="sidebar-module-list">
      
        <li>
          <a href="/golang%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94gorm.html">golang学习笔记——gorm</a>
        </li>
      
        <li>
          <a href="/go%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%A1%86%E6%9E%B6DTM.html">go学习笔记——分布式事务框架DTM</a>
        </li>
      
        <li>
          <a href="/golang%E9%A1%B9%E7%9B%AE%E5%BC%95%E7%94%A8GitHub%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93module.html">golang项目引用GitHub私有仓库module</a>
        </li>
      
        <li>
          <a href="/k8s%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94Istio%E5%92%8CKong.html">k8s学习笔记——Istio和Kong</a>
        </li>
      
        <li>
          <a href="/k8s%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94ingress.html">k8s学习笔记——ingress</a>
        </li>
      
    </ul>
  </div>




        </div>
    </div>
  </div>
  <footer class="blog-footer">
  <div class="container">
    <div id="footer-info" class="inner">
      &copy; 2025 tonglin0325<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

  

<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha384-8gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9HZSXL1LDJ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-9HZSXL1LDJ');
</script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>



<script src="/js/jquery.qrcode.min.js"></script>



<script src="/js/jquery-1.9.0.min.js"></script>


</body>
</html>
